[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tidy Finance Blog",
    "section": "",
    "text": "Experimental and external contributions based on Tidy Finance with R. Contribute your ideas!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nISS Shareholder Proposals\n\n21 min\n\n\nData\n\nR\n\n\n\nCode for preparing ISS Voting Analytics data for further analysis on shareholder proposals\n\n\n\nAlexander Pasler, Moritz Rodenkirchen\n\n\nJun 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyfinance 0.4.0: new data downloads\n\n3 min\n\n\nData\n\nR\n\n\n\ntidyfinance 0.4.0 is now on CRAN. Discover the new data download options it includes.\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nAug 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplicating Gu, Kelly & Xiu (2020)\n\n22 min\n\n\nR\n\nReplications\n\nMachine learning\n\n\n\nA partial replication of the paper Empirical Asset Pricing via Machine Learning using R.\n\n\n\nStefan Voigt\n\n\nJun 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nFast Portfolio Sorts\n\n8 min\n\n\nR\n\nPortfolio Sorts\n\ndata.table\n\n\n\nA benchmark of R approaches for efficient portfolio sorts\n\n\n\nChristoph Scheuch\n\n\nJun 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nCIR Model Calibration using Python\n\n10 min\n\n\nInterest rates\n\nPython\n\n\n\nRoutine to calibrate the Cox-Ingersoll-Ross model\n\n\n\nYuri Antonelli\n\n\nApr 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nCRSP 2.0 Update\n\n6 min\n\n\nData\n\nR\n\nPython\n\n\n\nThe highlights of the recent switch to CRSP 2.0 data\n\n\n\nPatrick Weiss, Christoph Scheuch, Stefan Voigt, Christoph Frey\n\n\nMar 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyfinance 0.1.0\n\n5 min\n\n\nData\n\nR\n\n\n\ntidyfinance 0.1.0 is now on CRAN. Discover what this release includes.\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nMar 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Market Microstructure\n\n76 min\n\n\nMarket microstructure\n\nR\n\ndata.table\n\n\n\nA beginner’s guide to market quality measurement in high-frequency data using R.\n\n\n\nBjörn Hagströmer, Niklas Landsberg\n\n\nJan 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing DuckDB with WRDS Data\n\n10 min\n\n\nData\n\nR\n\n\n\nDemonstrate the power of DuckDB and dbplyr with WRDS data.\n\n\n\nIan Gow\n\n\nDec 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Fama-French Three vs Five Factors\n\n7 min\n\n\nData\n\nReplications\n\nR\n\n\n\nAn explanation for the difference in the size factors of Fama and French 3 and 5 factor data\n\n\n\nChristoph Scheuch\n\n\nOct 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDummy Data for Tidy Finance Readers without Access to WRDS\n\n12 min\n\n\nData\n\nR\n\n\n\nR code to generate dummy data that can be used to run the code chunks in Tidy Finance with R\n\n\n\nChristoph Scheuch\n\n\nSep 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvert Raw TRACE Data to a Local SQLite Database\n\n37 min\n\n\nData\n\nR\n\n\n\nAn R code that converts TRACE files from FINRA into a SQLite for facilitated analysis and filtering\n\n\n\nKevin Riehl, Lukas Müller\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Collaborative Filtering: Building A Stock Recommender\n\n13 min\n\n\nRecommender System\n\nR\n\n\n\nA simple implementation for prototyping multiple collaborative filtering algorithms\n\n\n\nChristoph Scheuch\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Standard Errors in Portfolio Sorts\n\n39 min\n\n\nReplications\n\nR\n\n\n\nAn all-in-one implementation of non-standard errors in portfolio sorts\n\n\n\nPatrick Weiss\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstruction of a Historical S&P 500 Total Return Index\n\n8 min\n\n\nData\n\nR\n\n\n\nAn approximation of total returns using Robert Shiller’s stock market data\n\n\n\nChristoph Scheuch\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Tidy Finance?\n\n5 min\n\n\nOp-Ed\n\n\n\nAn op-ed about the motives behind Tidy Finance with R\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at Workshops for Ukraine\n\n2 min\n\n\nWorkshops\n\n\n\nYou can learn Tidy Finance and support Ukraine at the same time\n\n\n\nPatrick Weiss\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at the useR!2022 Conference\n\n1 min\n\n\nConferences\n\n\n\nTidy Finance presentation at the gathering supported by the R Foundation\n\n\n\nPatrick Weiss\n\n\nJun 23, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html",
    "href": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html",
    "title": "tidyfinance 0.4.0: new data downloads",
    "section": "",
    "text": "We are happy to annouce the release of version 0.4.0 of the tidyfinance R package on CRAN. The package contains a set of helper functions for empirical research in financial economics, addressing a variety of topics covered in Tidy Finance with R (TFWR). We designed the package to provide easy shortcuts for the applications that we discuss in the book. If you want to inspect the details of the package or propose new features, feel free to visit the package repository on Github.\nAs the new release brings many new features, we split them up into two blog posts. In this blog post, we discuss the new data downloads that tidyfinance now supports."
  },
  {
    "objectID": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html#install-the-package",
    "href": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html#install-the-package",
    "title": "tidyfinance 0.4.0: new data downloads",
    "section": "Install the package",
    "text": "Install the package\nYou can install the released version of tidyfinance from CRAN via:\n\ninstall.packages(\"tidyfinance\")\n\nYou can install the development version of tidyfinance from GitHub using the pak package:\n\npak::pak(\"tidy-finance/r-tidyfinance\")\n\nYou then load the package via:\n\nlibrary(tidyfinance)"
  },
  {
    "objectID": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html#download-stock-data",
    "href": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html#download-stock-data",
    "title": "tidyfinance 0.4.0: new data downloads",
    "section": "Download stock data",
    "text": "Download stock data\nYou can download stock prices from Yahoo Finance using the type stock_prices and provide symbols:\n\ndownload_data(\"stock_prices\", symbols = c(\"AAPL\", \"MSFT\"))\n\nNo `start_date` or `end_date` provided.\nUsing the range 2022-08-30 to 2023-08-30 to avoid downloading large\namounts of data.\n\n\n# A tibble: 502 × 8\n  symbol date         volume  open   low  high close adjusted_close\n  &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1 AAPL   2022-08-30 77906200  162.  158.  163.  159.           157.\n2 AAPL   2022-08-31 87991100  160.  157.  161.  157.           155.\n3 AAPL   2022-09-01 74229900  157.  155.  158.  158.           156.\n4 AAPL   2022-09-02 76957800  160.  155.  160.  156.           154.\n5 AAPL   2022-09-06 73714800  156.  154.  157.  155.           153.\n# ℹ 497 more rows\n\n\nAs you can see, we included defaults for start_date and end_date that you can of course overwrite. We introduced default dates across all datasets to allow for fast initial data analysis for your convenience.\n\ndownload_data(\"stock_prices\", symbols = c(\"AAPL\", \"MSFT\"),\n              start_date = \"2020-01-01\", end_date = \"2020-12-31\")\n\n# A tibble: 504 × 8\n  symbol date          volume  open   low  high close adjusted_close\n  &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1 AAPL   2020-01-02 135480400  74.1  73.8  75.2  75.1           72.9\n2 AAPL   2020-01-03 146322800  74.3  74.1  75.1  74.4           72.2\n3 AAPL   2020-01-06 118387200  73.4  73.2  75.0  74.9           72.7\n4 AAPL   2020-01-07 108872000  75.0  74.4  75.2  74.6           72.4\n5 AAPL   2020-01-08 132079200  74.3  74.3  76.1  75.8           73.6\n# ℹ 499 more rows\n\n\nTo inspect details about the download approach, you can call the documentation via ?download_data_stock_prices.\nThe above function takes stock symbols as inputs (i.e., stock-specific identifiers). To get symbols you have now the following option: you can download index constituents from selected iShares ETFs that physically replicate the corresponding index:\n\ndownload_data(\"constituents\", index = \"DAX\")\n\n# A tibble: 40 × 5\n  symbol name                  location    exchange      currency\n  &lt;chr&gt;  &lt;chr&gt;                 &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   \n1 SAP.DE SAP                   Deutschland Xetra         EUR     \n2 SIE.DE SIEMENS N AG          Deutschland Xetra         EUR     \n3 ALV.DE ALLIANZ               Deutschland Xetra         EUR     \n4 DTE.DE DEUTSCHE TELEKOM N AG Deutschland Xetra         EUR     \n5 AIR.BE AIRBUS                Frankreich  Boerse Berlin EUR     \n# ℹ 35 more rows\n\n\nThe documentation ?download_data_constituents provides details. The list of supported indexes for download_data_constituents() can be inspected via:\n\nlist_supported_indexes()\n\n# A tibble: 13 × 3\n  index                        url                               skip\n  &lt;chr&gt;                        &lt;chr&gt;                            &lt;dbl&gt;\n1 DAX                          https://www.ishares.com/de/priv…     2\n2 EURO STOXX 50                https://www.ishares.com/de/priv…     2\n3 Dow Jones Industrial Average https://www.ishares.com/de/priv…     2\n4 Russell 1000                 https://www.ishares.com/ch/prof…     9\n5 Russell 2000                 https://www.ishares.com/ch/prof…     9\n# ℹ 8 more rows\n\n\nYou can easily use the index or constituents data to download stock prices:\n\nconstituents_dax &lt;- download_data(\"constituents\", index = \"DAX\")\ndownload_data(\"stock_prices\", symbols = constituents_dax$symbol)"
  },
  {
    "objectID": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html#download-macro-data",
    "href": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html#download-macro-data",
    "title": "tidyfinance 0.4.0: new data downloads",
    "section": "Download macro data",
    "text": "Download macro data\nWe also added support for the Federal Reserve Economic Databa (FRED), where you can easily download multiple series (see ?download_data_fred for details):\n\ndownload_data(\"fred\", series = c(\"GDP\", \"CPIAUCNS\"))\n\nNo `start_date` or `end_date` provided. Returning the full data set.\n\n\n# A tibble: 1,649 × 3\n  date       value series\n  &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt; \n1 1947-01-01  243. GDP   \n2 1947-04-01  246. GDP   \n3 1947-07-01  250. GDP   \n4 1947-10-01  260. GDP   \n5 1948-01-01  266. GDP   \n# ℹ 1,644 more rows\n\n\nOur approach is a simple wrapper around the FRED download data site. If you want to systematically download FRED data via API, please consider using fredr (Boysel and Vaughan 2021) package.\nThe macro predictors by Ivo Welch and Amig Goyal, which we prepare for immediate use in our book chapter on Accessing and Managing Financial Data, now also come in different flavors: monthly, quarterly, and annual. For instance, the annual data can be fetched via:\n\ndownload_data(\"macro_predictors_annual\")\n\nNo `start_date` or `end_date` provided. Returning the full data set.\n\n\n# A tibble: 97 × 15\n  date       rp_div    dp    dy    ep      de    svar    bm   ntis\n  &lt;date&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 1926-01-01  0.232 -2.97 -2.89 -2.39 -0.586  0.0165  0.441 0.0509\n2 1927-01-01  0.275 -3.13 -2.86 -2.77 -0.366  0.00942 0.375 0.0765\n3 1928-01-01 -0.163 -3.36 -3.03 -2.87 -0.485  0.0198  0.260 0.0631\n4 1929-01-01 -0.342 -3.10 -3.22 -2.59 -0.507  0.125   0.338 0.164 \n5 1930-01-01 -0.612 -2.75 -3.09 -2.76  0.0103 0.0666  0.555 0.114 \n# ℹ 92 more rows\n# ℹ 6 more variables: tbl &lt;dbl&gt;, lty &lt;dbl&gt;, ltr &lt;dbl&gt;, tms &lt;dbl&gt;,\n#   dfy &lt;dbl&gt;, infl &lt;dbl&gt;\n\n\nSee ?download_data_macro_predictors for details.\nFinally, we also added support to download a collection of replicated time series of factor portfolios from Open Source Asset Pricing (OSAP) compiled by Andrew Y. Chen and Tom Zimmermann:\n\ndownload_data(\"osap\")\n\nNo `start_date` or `end_date` provided. Returning the full data set.\n\n\n# A tibble: 1,164 × 213\n  date          am   aop abnormal_accruals accruals accruals_bm\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 1926-01-30    NA    NA                NA       NA          NA\n2 1926-02-27    NA    NA                NA       NA          NA\n3 1926-03-31    NA    NA                NA       NA          NA\n4 1926-04-30    NA    NA                NA       NA          NA\n5 1926-05-28    NA    NA                NA       NA          NA\n# ℹ 1,159 more rows\n# ℹ 207 more variables: activism1 &lt;dbl&gt;, activism2 &lt;dbl&gt;,\n#   ad_exp &lt;dbl&gt;, age_ipo &lt;dbl&gt;, analyst_revision &lt;dbl&gt;,\n#   analyst_value &lt;dbl&gt;, announcement_return &lt;dbl&gt;,\n#   asset_growth &lt;dbl&gt;, bm &lt;dbl&gt;, bmdec &lt;dbl&gt;, bpebm &lt;dbl&gt;,\n#   beta &lt;dbl&gt;, beta_fp &lt;dbl&gt;, beta_liquidity_ps &lt;dbl&gt;,\n#   beta_tail_risk &lt;dbl&gt;, bid_ask_spread &lt;dbl&gt;, …\n\n\nYou can get more information by calling ?download_data_osap."
  },
  {
    "objectID": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html#concluding-remarks",
    "href": "blog/r-tidyfinance-0-4-0-new-data-downloads/index.html#concluding-remarks",
    "title": "tidyfinance 0.4.0: new data downloads",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nTo get a list of supported data types, you can call:\n\nlist_supported_types()\n\n# A tibble: 317 × 3\n  type                  dataset_name                   domain  \n  &lt;chr&gt;                 &lt;chr&gt;                          &lt;chr&gt;   \n1 factors_q5_daily      q5_factors_daily_2023.csv      Global Q\n2 factors_q5_weekly     q5_factors_weekly_2023.csv     Global Q\n3 factors_q5_weekly_w2w q5_factors_weekly_w2w_2023.csv Global Q\n4 factors_q5_monthly    q5_factors_monthly_2023.csv    Global Q\n5 factors_q5_quarterly  q5_factors_quarterly_2023.csv  Global Q\n# ℹ 312 more rows\n\n\nWe are curious to learn in which direction we should extend the package, so please consider opening an issue in the package repository. For instance, we could support more data sources, add more parameters to the download_* family of functions, or we could put more emphasis on the generality of portfolio assignment or other modeling functions. Moreover, if you discover a bug, we are very grateful if you raise them in the repository."
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html",
    "href": "blog/fama-french-three-vs-five-factors/index.html",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "",
    "text": "In our book chapters Replicating Fama and French Factors (R Version) and Replicating Fama and French Factors (Python Version), we show how to construct factor portfolios that are fairly close to the popular data from Prof. Kenneth French finance data library. In this blog post, I want to elaborate a bit more on the subtle difference between the size data in the Fama-French three (FF3)1 and five (FF5)2 factor data."
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html#analyzing-monthly-factor-data",
    "href": "blog/fama-french-three-vs-five-factors/index.html#analyzing-monthly-factor-data",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "Analyzing monthly factor data",
    "text": "Analyzing monthly factor data\nI first start by downloading the monthly Fama-French factors using the frenchdata package. The currently available data ranges from July 1926 to August 2023. 1926-07-01 to 2023-08-01.\n\nlibrary(tidyverse)\nlibrary(frenchdata)\nlibrary(fixest)\n\nstart_date &lt;- \"1926-07-01\"\nend_date &lt;- \"2023-08-01\"\n\nfactors_ff3_monthly_raw &lt;- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff3_monthly &lt;- factors_ff3_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`)\n\nfactors_ff5_monthly_raw &lt;- download_french_data(\"Fama/French 5 Factors (2x3)\")\nfactors_ff5_monthly &lt;- factors_ff5_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    across(c(RF, `Mkt-RF`, SMB, HML, RMW, CMA), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) \n\nfactors_ff_monthly &lt;- factors_ff3_monthly |&gt; \n  rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff3\")) |&gt; \n  inner_join(\n    factors_ff5_monthly |&gt; \n      select(month, mkt_excess, rf, smb, hml) |&gt; \n      rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff5\")), by = \"month\") |&gt; \n  filter(month &gt;= start_date & month &lt;= end_date)\n\nindustries_ff_monthly_raw &lt;- download_french_data(\"10 Industry Portfolios\")\nindustries_ff_monthly &lt;- industries_ff_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(month = floor_date(ymd(str_c(date, \"01\")), \"month\")) |&gt;\n  mutate(across(where(is.numeric), ~ . / 100)) |&gt;\n  select(month, everything(), -date) |&gt;\n  rename_with(str_to_lower)\n\nLet us first inspect the summary statistics of each factor.\n\nfactors_ff_monthly |&gt; \n  pivot_longer(cols = - month) |&gt; \n  select(name, value) |&gt;\n  drop_na() |&gt;\n  group_by(name) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  ) |&gt; \n  print(n = Inf)\n\n# A tibble: 8 × 9\n  name         mean      sd    min     q05    q50    q95    max     n\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 hml_ff3   0.00284 0.0300  -0.139 -0.0413 0.0022 0.0541 0.128    721\n2 hml_ff5   0.00284 0.0300  -0.139 -0.0413 0.0022 0.0541 0.128    721\n3 mkt_exce… 0.00568 0.0449  -0.232 -0.0726 0.0092 0.0713 0.161    721\n4 mkt_exce… 0.00568 0.0449  -0.232 -0.0726 0.0092 0.0713 0.161    721\n5 rf_ff3    0.00362 0.00266  0      0      0.0038 0.0081 0.0135   721\n6 rf_ff5    0.00362 0.00266  0      0      0.0038 0.0081 0.0135   721\n7 smb_ff3   0.00186 0.0304  -0.172 -0.0421 0.0011 0.0501 0.214    721\n8 smb_ff5   0.00219 0.0302  -0.153 -0.0431 0.001  0.0481 0.183    721\n\n\nThe above table shows that risk free rates rf_*, market excess returns mkt_excess_*, and value factors hml_* show de facto identical value across all statistics for FF3 and FF5. However, the size factors smb_* seem to be different between the data sets. Another way to show the difference is running regressions, as we do in our replication chapters:\n\nmodel_smb &lt;- lm(smb_ff3 ~ smb_ff5, data = factors_ff_monthly)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb_ff3 ~ smb_ff5, data = factors_ff_monthly)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.03543 -0.00192  0.00032  0.00205  0.03373 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.000303   0.000222   -1.36     0.17    \nsmb_ff5      0.987055   0.007338  134.51   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00595 on 719 degrees of freedom\nMultiple R-squared:  0.962, Adjusted R-squared:  0.962 \nF-statistic: 1.81e+04 on 1 and 719 DF,  p-value: &lt;2e-16\n\n\nRegressing the FF3 size factor on its FF5 counterpart yields a coefficient of 0.99 and an R-squared around 96%, so definitely no perfect co-movement.\nIs this difference just an artifact in the data, limited to a certain time period? Figure Figure 1 shows that there are differences throughout the whole sample.\n\nfactors_ff_monthly |&gt; \n  mutate(difference = smb_ff3 - smb_ff5) |&gt; \n  ggplot(aes(x = month, y = difference, fill = difference &gt; 0)) +\n  geom_col() +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  theme(legend.position = \"none\") + \n  labs(\n    x = NULL, y = \"smb_ff3 - smb_ff5\",\n    title = \"Difference between monthly size factors from FF3 and FF5 data\"\n  )\n\n\n\n\n\n\n\nFigure 1: End-of-month difference between monthly size factors from Fama-French three (FF3) and five (FF5) factor data.\n\n\n\n\n\nWhere does this difference come from? In my opinion, there is one likelyexplanation for the differences: the size portfolios portfolio_size and value portfolios portfolio_bm are constructed as independent sorts in FF3, while portfolio_bm, portfolio_op, and portfolio_inv are the result of dependent sorts in FF5 (depending on portfolio_size). In FF5, portfolio_size is then calculated on averages based on portfolio_bm, portfolio_op, and portfolio_inv portfolios. As all portfolios are the result of value-weighted return aggregation, it is hence very likely that these aggregations lead to different values.\nThese subtle differences might even impact your statistical tests. As an example, let us consider one of the industry portfolios from industry_ff_monthly. We use the ‘other’ portfolio, which contains sectors such as mines, construction, entertainment, finance, etc. We run a gression of the corresponding industry portfolios against the market, size, and value factors of FF3 and FF5, respectively.\n\nindustry_returns &lt;- industries_ff_monthly |&gt; \n  select(month, ret_other = other) |&gt; \n  inner_join(factors_ff_monthly, by = \"month\")\n\nmodel_ff3 &lt;- feols(\n  ret_other ~ mkt_excess_ff3 + smb_ff3 + hml_ff3, \n  industry_returns\n)\n\nmodel_ff5 &lt;- feols(\n  ret_other ~ mkt_excess_ff5 + smb_ff5 + hml_ff5, \n  industry_returns\n)\n\netable(model_ff3, model_ff5, coefstat = \"tstat\")\n\n                        model_ff3         model_ff5\nDependent Var.:         ret_other         ret_other\n                                                   \nConstant         0.0017** (2.694)  0.0017** (2.689)\nmkt_excess_ff3   1.139*** (76.67)                  \nsmb_ff3           0.0500* (2.299)                  \nhml_ff3         0.3984*** (18.48)                  \nmkt_excess_ff5                     1.135*** (76.35)\nsmb_ff5                            0.0690** (3.196)\nhml_ff5                           0.3897*** (18.23)\n_______________ _________________ _________________\nVCOV type                     IID               IID\nObservations                  721               721\nR2                        0.89991           0.90059\nAdj. R2                   0.89949           0.90017\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results show that the size factor is only significant at the 5% level for the FF3 data, but it is significant at the 1% level for the FF5 version!"
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html#a-quick-look-at-daily-factors",
    "href": "blog/fama-french-three-vs-five-factors/index.html#a-quick-look-at-daily-factors",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "A quick look at daily factors",
    "text": "A quick look at daily factors\nLet us take a quick look at the daily factors to check whether the difference in size premia exists there as well. If my explanation for the difference is correct, then there should be differences. We can download the daily factor data in a similar fashion as the monthly data.\n\nfactors_ff3_daily_raw &lt;- download_french_data(\"Fama/French 3 Factors [Daily]\")\nfactors_ff3_daily &lt;- factors_ff3_daily_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = ymd(date),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`)\n\nfactors_ff5_daily_raw &lt;- download_french_data(\"Fama/French 5 Factors (2x3) [Daily]\")\nfactors_ff5_daily &lt;- factors_ff5_daily_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = ymd(date),\n    across(c(RF, `Mkt-RF`, SMB, HML, RMW, CMA), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) \n\nfactors_ff_daily &lt;- factors_ff3_daily |&gt; \n  rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff3\")) |&gt; \n  inner_join(\n    factors_ff5_daily |&gt; \n      select(date, mkt_excess, rf, smb, hml) |&gt; \n      rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff5\")), by = \"date\")  |&gt; \n  filter(date &gt;= start_date & date &lt;= end_date)\n\nmodel_smb &lt;- lm(smb_ff3 ~ smb_ff5, data = factors_ff_daily)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb_ff3 ~ smb_ff5, data = factors_ff_daily)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.019389 -0.000393  0.000026  0.000419  0.013334 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.50e-05   1.02e-05   -1.48     0.14    \nsmb_ff5      9.72e-01   1.88e-03  517.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00125 on 15121 degrees of freedom\nMultiple R-squared:  0.946, Adjusted R-squared:  0.946 \nF-statistic: 2.67e+05 on 1 and 15121 DF,  p-value: &lt;2e-16\n\n\nRegressing the FF3 size factor on its FF5 counterpart yields a coefficient of 0.97 and an R-squared around 95%, so again no perfect co-movement. Unreported results of the distributions and differences over time confirm the regression results and are in line with differences among monthly factor data."
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html#conclusion",
    "href": "blog/fama-french-three-vs-five-factors/index.html#conclusion",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "Conclusion",
    "text": "Conclusion\nAre there any implications for empirical applications? In my opinion, you should be careful when you want to test your portfolios against FF3 and FF5 factors. It is strictly speaking not correct to just use a subsample of factors from FF5 if you want to test against the FF3 factors. I rather recommend downloading both FF3 and FF5 and run tests with each data set separately."
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html#footnotes",
    "href": "blog/fama-french-three-vs-five-factors/index.html#footnotes",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFama, E. F.; French, K. R. (1993). “Common risk factors in the returns on stocks and bonds”. Journal of Financial Economics. 33: 3–56. https://doi.org/10.1016/0304-405X(93)90023-5↩︎\nFama, E. F.; French, K. R. (2015). “A Five-Factor Asset Pricing Model”. Journal of Financial Economics. 116: 1–22. CiteSeerX 10.1.1.645.3745. https://doi.org/10.1016/j.jfineco.2014.10.010↩︎"
  },
  {
    "objectID": "blog/gu-kelly-xiu-replication/index.html",
    "href": "blog/gu-kelly-xiu-replication/index.html",
    "title": "Replicating Gu, Kelly & Xiu (2020)",
    "section": "",
    "text": "This blog post is an attempt to replicate the core elements of the paper Empirical Asset Pricing via Machine Learning by Shihao Gu, Brian Kelly, and Dacheng Xiu1. In its essence, the paper presents a horse-race for off-the-shelf machine learning (ML) models to predict return premia. For the replication I often refer to the Online Appendix of the paper, which is available here.\nThe code relies on the following R packages. Note that the packages ranger, glmnet, and brulee are only needed if you want to implement the random forests, elastic net, or neural networks, respectively.\nlibrary(tidyverse)\nlibrary(archive)\nlibrary(arrow)\nlibrary(RSQLite)\nlibrary(tidymodels)\nlibrary(butcher)\nlibrary(future)\nlibrary(tidyfinance) # for portfolio sorts\n# install.packages(\"ranger\")\n# install.packages(\"glmnet\") \n# install.packages(\"brulee\")"
  },
  {
    "objectID": "blog/gu-kelly-xiu-replication/index.html#data-preparation",
    "href": "blog/gu-kelly-xiu-replication/index.html#data-preparation",
    "title": "Replicating Gu, Kelly & Xiu (2020)",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload stock-characteristics\n\nwe build a large collection of stock-level predictive characteristics based on the cross-section of stock returns literature. These include 94 characteristics (61 of which are updated annually, 13 are updated quarterly, and 20 are updated monthly). In addition, we include 74 industry dummies corresponding to the first two digits of Standard Industrial Classification (SIC) codes.\n\nThe authors provide the dataset with stock-level characteristics online. While the original sample from the published version ended in December 2016, the author furnished the stock-level characteristics dataset until 2021. I download this (large) dataset directly from within R from the Dacheng Xiu’s homepage. The data is stored as a .csv file within a zipped folder. For such purposes, the archive package is useful. I set options(timeout = 1200), which allows the R session to download the dataset for 20 minutes. The default in R is 60 seconds which was too short on my machine.\n\noptions(timeout = 1200)\ncharacteristics &lt;- read_csv(archive_read(\"https://dachxiu.chicagobooth.edu/download/datashare.zip\", \n                                         file = \"datashare.csv\"))\n\ncharacteristics &lt;- characteristics |&gt;\n  rename(month = DATE) |&gt;\n  mutate(\n    month = ymd(month),\n    month = floor_date(month, \"month\")\n  ) |&gt;\n  rename_with(~ paste0(\"characteristic_\", .), \n              -c(permno, month, sic2))\n\ncharacteristics &lt;- characteristics |&gt;\n  drop_na(sic2) |&gt;\n  mutate(sic2 = as_factor(sic2))\n\nThe characteristics appear in raw format. As many machine-learning applications are sensitive to the scaling of the data, one typically employs some form of standardization.\n\nWe cross-sectionally rank all stock characteristics period-by-period and map these ranks into the [-1, 1] interval (footnote 29).\n\nThe idea is that at each date, the cross-section of each predictor should be scaled such that the maximum value is 1 and the minimum value is -1. The cross-sectional ranking is time-consuming. The function below explicitly handles NA values, so they do not tamper with the ranking.\n\nrank_transform &lt;- function(x) {\n  rank_x &lt;- rank(x)\n  rank_x[is.na(x)] &lt;- NA\n  min_rank &lt;- 1\n  max_rank &lt;- length(na.omit(x))\n  \n  if (max_rank == 0) { # only NAs\n    return(rep(NA, length(x)))    \n  } else {\n    return(2 * ((rank_x - min_rank) / (max_rank - min_rank) - 0.5)) \n  }\n}\n\ncharacteristics &lt;- characteristics |&gt;\n  group_by(month) |&gt;\n  mutate(across(contains(\"characteristic\"), rank_transform)) |&gt;\n  ungroup()\n\n\n\n\n\n\n\nWarning\n\n\n\nIn this blog post I only run code which fits a random forest. Random forests are known for being low-maintenance when it comes to feature engineering and indeed, the transformation and scaling of the predictors does not affect the fitted Random forest. However, as I also show how to fit methods which are sensitive to the scaling of the predictors, I include this crucial part of data preparation.\n\n\n\nAnother issue is missing characteristics, which we replace with the cross-sectional median at each month for each stock, respectively (footnote 30).\n\nThe paper actually claims that each missing value is replaced by the cross-sectional median at each month. However, one of the paper’s coauthors also claims otherwise:\n\nNA values are set to zero (Q&A file furnished by Shihoa Gu)\n\nI interpret the divergence as follows: First, I replace missing values with the cross-sectional median. However, for some months, few characteristics (e.g. absacc) do not have any value at all, leaving the cross-sectional median undefined. Thus, in a second step, I replace remaining missing characteristics with zero.\n\ncharacteristics &lt;- characteristics |&gt;\n  group_by(month) |&gt; \n  mutate(across(contains(\"characteristic\"), \n                \\(x) replace_na(x, median(x, na.rm = TRUE)))) |&gt; \n  ungroup()\n\ncharacteristics &lt;- characteristics |&gt;\n  mutate(across(contains(\"characteristic\"), \n                \\(x) replace_na(x, 0)))\n\n\nWe obtain monthly total individual equity returns from CRSP for all firms listed in the NYSE, AMEX, and NASDAQ. Our sample begins in March 1957 (the start date of the S&P 500) (p. 2248)\n\nTo retrieve CRSP excess returns, we merge the stock-level characteristics with the prepared monthly CRSP data as described in WRDS, CRSP, and Compustat. Note that in the book we start the CRSP sample in the year 1960 (partially due to data availability). The difference of three years may explain some deviations in the predictive performance. You can easily adjust the variable start_date &lt;- ymd(\"1957-03-01\") when running the code to reproduce the file tidy_finance_r.sqlite.\nTo create portfolio sorts based on the predictions, mktcap_lag remains in the sample (albeit it should be mentioned that the main results in Section 2.4.2 of the paper are derived computing equal-weighted portfolios).\n\nWe also construct eight macroeconomic predictors following the variable definitions detailed in Welch and Goyal (2008), including dividend-price ratio (dp), earnings-price ratio (ep), book-to-market ratio (bm), net equity expansion (ntis), Treasury-bill rate (tbl), term spread (tms), default spread (dfy), and stock variance (svar). (p. 2248)\n\nWe rely on the preparation of the macroeconomic predictors from the corresponding chapter in Accessing and Managing Financial Data. The following code snippets merges the stock-level characteristics with the CRSP sample and the macroeconomic predictors. Note that no further adjustment of the timing of excess returns and stock-level characteristics is necessary:\n\nIn this dataset, we’ve already adjusted the lags. (e.g. When DATE=19570329 in our dataset, you can use the monthly RET at 195703 as the response variable.) (readme.txt from the available dataset on Dacheng Xiu’s homepage).\n\nTo remain consistent, we only lag the macropredictors by one month.\n\ntidy_finance &lt;- dbConnect(SQLite(), \n                          \"data/tidy_finance_r.sqlite\",\n                          extended_types = TRUE)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(month, permno, mktcap_lag, ret_excess) |&gt;\n  collect()\n\nmacro_predictors &lt;- tbl(tidy_finance, \"macro_predictors\") |&gt;\n  select(month, dp, ep, bm, ntis, tbl, tms, dfy, svar) |&gt;\n  collect() |&gt;\n  rename_with(~ paste0(\"macro_\", .), -month)\n\nmacro_predictors &lt;- macro_predictors |&gt;\n  select(month) |&gt;\n  left_join(macro_predictors |&gt; mutate(month = month %m+% months(1)), \n            join_by(month))\n\ncharacteristics &lt;- characteristics |&gt;\n  inner_join(crsp_monthly, join_by(month, permno)) |&gt;\n  inner_join(macro_predictors, join_by(month)) |&gt;\n  arrange(month, permno) |&gt;\n  mutate(macro_intercept = 1) |&gt;\n  select(permno, month, ret_excess, mktcap_lag, \n         sic2, contains(\"macro\"), contains(\"characteristic\"))\n\n\n\nCreate dataset with all interaction terms\nBefore applying machine-learning models, the authors include interaction terms between macroeconomic predictors and stock characteristics in the sample.\n\nWe include 74 industry dummies corresponding to the first two digits of Standard Industrial Classification (SIC) codes. […] [The stock-level covariates also include] interactions between stock-level characteristics and macroeconomic state variables. The total number of covariates is 94×(8+1)+74=920. (p. 2249)\n\nThe following recipe creates dummies for each sic2 code and adds interaction terms between macro variables and stock characteristics. Note that data for the recipe is only required to identify the columns in the sample. No computations are performed at the first step. For that reason, I call head() to keep the computational burden a bit lower without affecting the outcome.\n\nrec &lt;- recipe(ret_excess ~ ., data = characteristics |&gt; head())|&gt;\n  update_role(permno, month, mktcap_lag, \n              new_role = \"id\") |&gt;\n  step_interact(terms = ~ contains(\"characteristic\"):contains(\"macro\"), \n                keep_original_cols = FALSE) |&gt;\n  step_dummy(sic2, \n             one_hot = TRUE)\n\nrec &lt;- prep(rec)\n\nNext, I create the full dataset containing 3,046,332 monthly observations and 920 covariates.\n\ncharacteristics_prepared &lt;- bake(rec, new_data = characteristics)\n\nI store the prepared file for your convenience. Furthermore, the authors never use the entire dataset for training, instead, only a fraction needs to be available in memory at any given point in time.\n\ncharacteristics_prepared |&gt;\n  group_by(year = year(month)) |&gt; \n  write_dataset(path = \"data/characteristics_prepared\")"
  },
  {
    "objectID": "blog/gu-kelly-xiu-replication/index.html#sample-splitting-and-tuning",
    "href": "blog/gu-kelly-xiu-replication/index.html#sample-splitting-and-tuning",
    "title": "Replicating Gu, Kelly & Xiu (2020)",
    "section": "Sample splitting and tuning",
    "text": "Sample splitting and tuning\n\nWe divide the 60 years of data into 18 years of training sample (1957–1974), 12 years of validation sample (1975–1986), and the remaining 30 years (1987–2016) for out-of-sample testing. Because machine learning algorithms are computationally intensive, we avoid recursively refitting models each month. Instead, we refit once every year as most of our signals are updated once per year. Each time we refit, we increase the training sample by 1 year. We maintain the same size of the validation sample, but roll it forward to include the most recent 12 months\n\nI visualize my understanding of this data-budget below.\n\nvalidation_length &lt;- 12\n\nestimation_periods &lt;- tibble(oos_year = 1987:2021,\n                             validation_end = oos_year - 1,\n                             validation_start = oos_year - validation_length,\n                             training_start = 1957,\n                             training_end = validation_start - 1) |&gt;\n  select(contains(\"training\"), validation_start, validation_end, oos_year)\n\nvisualization_data = pmap_dfr(estimation_periods, \n                              \\(training_start, \n                                training_end, \n                                validation_start, \n                                validation_end, \n                                oos_year){\n  tibble(year = 1957:2021,\n         classification = case_when(year &gt;= training_start & year &lt;= training_end ~ \"Training\",\n                                    year &gt;= validation_start & year &lt;= validation_end ~ \"Validation\",\n                                    year == oos_year ~ \"OOS\"), \n         oos_year)\n  }) |&gt; \n  drop_na()\n\nggplot(visualization_data, \n       aes(x = year, y = oos_year, color = classification)) +\n  geom_point(size = 1) +\n  labs(title = \"Data classification timeline\",\n       x = NULL, y = NULL, color = NULL) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\nWarning: The `scale_name` argument of `discrete_scale()` is deprecated as of\nggplot2 3.5.0.\n\n\n\n\n\n\n\n\n\nThe simple scheme makes model tuning computationally less complex. As there is no need to create several validation sets and thus to reestimate the models several times, the overall time to compute predictions is reduced substantially. One may discuss the substantial sampling variation of the resulting MSPE from the validation sample (typically cross-validation is used to obtain more than one estimate of the prediction error). However, the authors clearly state:\n\nWe do not use cross-validation to maintain the temporal ordering of the data. (footnote 32)\n\n\nModel specification\nThe paper describes a collection of machine learning methods that was used in the analysis. I refer to Section 1 for a detailed description of the individual models. This blog post focuses on the implementation issues that arise when applying off-the-shelf ML methods on such a large dataset.\n\ncharacteristics_prepared &lt;- open_dataset(sources = \"data/characteristics_prepared\")\n\n\nTable A.5 (online appendix) describes the set of hyperparameters and their potential values used for tuning each machine learning model.\n\nIn the following I rely on the well-documented tidymodels workflow (e.g. Factor Selection with Machine Learning) to estimate the random forest model.\nFor the random forest, Table A5 in the Online Appendix states that the number of grown trees is fixed to 300. The number of features in each split is set to \\(\\in \\{3, 5, 10, 20, 30, 50, ...\\}\\). It is not clear to me what \\(...\\) means in that context, for that reason I fix the number of choices to six. Finally, in the paper depth is chosen as a hyperparameter, ranging from one (pruned tree) to six (more complex tree). ranger does not support depth as a hyperparameter but rather allows to dynamically stop growing further branches once a minimum number of observations in a group is reached. The effect is similar (if min_n is small, the tree can grow deep, otherwise we expect pruned trees), but cannot be translated directly into depth. I set the option num.threads to allow parallelization for the construction of the trees according to the documentation of the ranger engine in tidymodels.\n\n# install.packages(\"ranger\")\n\nrf_model &lt;- rand_forest(\n  mtry = tune(),\n  trees = 300,\n  min_n = tune()\n) |&gt;\n  set_engine(\"ranger\", \n             num.threads = parallel::detectCores(logical = FALSE) - 1) |&gt;\n  set_mode(\"regression\")\n\nrf_grid &lt;- expand_grid(\n  mtry = c(3, 5, 10, 20, 30, 50),\n  min_n = c(5000, 10000) \n)\n\n\nrec_final &lt;- recipe(ret_excess ~ 0 + ., \n                    data = characteristics_prepared |&gt; head() |&gt; collect()) |&gt;\n  update_role(permno, month, mktcap_lag, year, new_role = \"id\")\n\nml_workflow &lt;- workflow() |&gt;\n  add_model(rf_model) |&gt;\n  add_recipe(rec_final)\n\n\n\nModel fitting\nThe following performs the model estimation to create predictions for all the years until 2021. For a complete replication of the procedure laid out in the paper, you’ll have to run a loop over all out-of-sample years (j).\n\nj &lt;- 1 # loop until nrow(estimation_periods) for full replication\n\nsplit_dates &lt;- estimation_periods |&gt; \n  filter(row_number() == j)\n\ntrain &lt;- characteristics_prepared |&gt; \n  filter(year(month) &lt; split_dates$training_end) |&gt;\n  collect() \n\nvalidation &lt;- characteristics_prepared |&gt;  \n  filter(year(month) &gt;= split_dates$validation_start & \n           year(month) &lt;= split_dates$validation_end) |&gt;\n  collect()\n\ndata_split &lt;- make_splits(\n  x = train,\n  assessment = validation )\n\nfolds &lt;- manual_rset(list(data_split), \n                     ids = \"split\")\n\nI tune the model and - after selecting the optimal tuning parameter value based on the prediction error in the validation set - fit the penalized regression using the available data (training and validation test set). The process can be faster by distribution the parameter tuning across multiple workers. tidymodels takes care of this as long as the user specifies a plan. I set the option parallel_over = \"resamples\" so that the formula is processed only once and then reused across all models that need to be fit. parallelization across the hyperparameter grid in tidymodels is done by initiating a plan() according to the documentation.\n\nplan(multisession, workers = nbrOfWorkers())\n\nml_fit &lt;- ml_workflow |&gt;\n  tune_grid(\n    resample = folds,\n    grid = rf_grid,\n    metrics = metric_set(rmse),\n    control = control_grid(parallel_over = \"resamples\"))\n\nIf more computation time is available, it is likely advisable to expand the number of evaluation by providing a wider grid of possible parameter constellations. Next, we finalize the process by selecting the hyperparameter constellation which delivered the smallest mean squared prediction error and fit the model once more using the entire (training and validation) datasets.\n\nml_fit_best &lt;- ml_fit |&gt;\n  select_best(metric = \"rmse\")\n\nml_workflow_final &lt;- ml_workflow |&gt; \n  finalize_workflow(ml_fit_best) |&gt; \n  fit(bind_rows(train, validation))\n\nTrained workflows contain a lot of information which may be redundant for the task at hand: generating predictions. The butcher package provides tooling to “axe” parts of the fitted output that are no longer needed, without sacrificing prediction functionality from the original model object. To provide an intermediary anchor (also to ensure reprocudibility), I follow this example to store the trained model. Julia Silge provided some reflections on best practices here which I tried to incorporate.\n\nfitted_workflow &lt;- butcher(ml_workflow_final)\nwrite_rds(fitted_workflow, file = paste0(\"data/fitted_workflow_\",split_dates$oos_year,\".rds\"))"
  },
  {
    "objectID": "blog/gu-kelly-xiu-replication/index.html#other-ml-models",
    "href": "blog/gu-kelly-xiu-replication/index.html#other-ml-models",
    "title": "Replicating Gu, Kelly & Xiu (2020)",
    "section": "Other ML models",
    "text": "Other ML models\nWhile this blog post only shows how to implement the prediction procedure based on the random forest, it is no problem to generate predictions for the remaining main methods (elastic net, neural networks, …) presented in the original paper. Below, I provide code for models that come as close as possible to the specifications laid out in the Online Appendix of the paper.\n\nFor OLS, ENet, GLM, and GBRT, we present their robust versions using Huber loss, which perform better than the version without. (p. 2250)\n\nFor the sake of simplicity, I only show how to fit linear models using \\(l2\\) loss functions. In R, the package hqreg implements algorithms for fitting regularization paths for Lasso or elastic net penalized regression models with Huber loss. However, hqreg is not a running engine for pairsnip (yet).\nFor instance, to fit the elastic net it seems necessary to evaluate the mean-squared prediction error in the validation sample for only two values of the penalty term. Instead of tuning the elastic net mixture coefficient, I follow the paper and show how to fix its value to \\(0.5\\).\n\n# install.packages(\"glmnet\")\n\nelastic_net &lt;- linear_reg( \n  penalty = tune(),\n  mixture = 0.5\n) |&gt;\n  set_engine(\"glmnet\")\n\npenalty_grid = tibble(penalty = c(1e-4, 1e-1))\n\nFor the neural network, the paper specifies the manifold architecture choices\n\nOur shallowest neural network has a single hidden layer of 32 neurons, which we denoted NN1. Next, NN2 has two hidden layers with 32 and 16 neurons, respectively; NN3 has three hidden layers with 32, 16, and 8 neurons, respectively; NN4 has four hidden layers with 32, 16, 8, and 4 neurons, respectively; and NN5 has five hidden layers with 32, 16, 8, 4, and 2 neurons, respectively. (p. 2244)\n\n\nWe use the same activation function at all nodes, and choose a popular functional form in recent literature known as the rectified linear unit (ReLU) (p. 2244)\n\nIn the following I show the exemplary implementation of the neural network with two hidden layers using the package brulee. Most information is taken from Table A5 in the online appendix. The table mentions a parameter patience = 5 which I understand as a command on how many iterations with no improvement before stopping the optimization process.\n\n# install.packages(\"brulee\")\n\ndeep_nnet_model &lt;- mlp(\n  epochs = 100,\n  hidden_units = c(32, 16),\n  activation = \"relu\",\n  learn_rate = tune(),\n  penalty = tune(),\n  batch_size = 10000,\n  stop_iter = 5\n) |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"brulee\")\n\nnn_grid &lt;- expand_grid(\n  learn_rate = c(0.001, 0.01),\n  penalty = c(1e-5, 1e-3)\n)"
  },
  {
    "objectID": "blog/gu-kelly-xiu-replication/index.html#machine-learning-portfolios",
    "href": "blog/gu-kelly-xiu-replication/index.html#machine-learning-portfolios",
    "title": "Replicating Gu, Kelly & Xiu (2020)",
    "section": "Machine-learning portfolios",
    "text": "Machine-learning portfolios\nAfter fitting the model(s), there are plenty ways to evaluate the predictive performance. In the remainder of the blog post I focus on the economically most meaningful (imho) application presented in the paper:\n\nAt the end of each month, we calculate 1-month-ahead out-of-sample stock return predictions for each method. We then sort stocks into deciles based on each model’s forecasts. We reconstitute portfolios each month using value weights. Finally, we construct a zero-net-investment portfolio that buys the highest expected return stocks (decile 10) and sells the lowest (decile 1). (p. 2264)\n\nThe following code chunks performs the predictions for every out-of-sample year (starting from 1987): First, I predict stock excess returns based on the fitted model, then we sort assets into portfolios based on the prediction. In line with the original paper, I chose an equal-weighted scheme which is arguably more sensitive to trading cost (see the discussion on p. 2265 of the paper). Note that in the original paper, the model parameters are updated each year.\n\nfitted_workflow &lt;- read_rds(\"data/fitted_workflow_1987.rds\")\n\ncreate_predictions &lt;- function(oos_year){\n\n  out_of_sample &lt;- characteristics_prepared |&gt; \n    filter(year(month) == oos_year) |&gt;\n    collect()\n\n  #fitted_workflow &lt;- read_rds(paste0(\"data/fitted_workflow_\", oos_year,\".rds\"))\n  \n  out_of_sample_predictions &lt;- fitted_workflow |&gt; \n    predict(out_of_sample)\n\n out_of_sample &lt;- out_of_sample |&gt;\n    select(permno, month, mktcap_lag, ret_excess) |&gt;\n    bind_cols(out_of_sample_predictions)\n\nreturn(out_of_sample)\n}\n\n\noos_predictions &lt;- estimation_periods |&gt;\n  pull(oos_year) |&gt;\n  map_dfr(\\(x) create_predictions(x))\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that I am calling the fitted model from the first out-of-sample year (1987) in each iteration. In other words, the model parameters do not get updated every year as in the original paper. If you estimated the model for each value of \\(j\\) as described above, you can replace the line above to replicate the original procedure.\n\n\nTo assign portfolios using the predictions, I use the assign_portfolio() from the tidyfinance package.\n\nml_portfolios &lt;- oos_predictions |&gt;\n  group_by(month) |&gt;\n  mutate(\n    portfolio = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \".pred\",\n      breakpoint_options(n_portfolios = 10)\n    ),\n    portfolio = as.factor(portfolio)\n  ) |&gt;\n  group_by(portfolio, month) |&gt;\n  summarize(\n    ret_predicted = mean(.pred),\n    ret_excess = mean(ret_excess),\n    .groups = \"drop\"\n  )\n\nFinally, we evaluate the performance of the ML-portfolios. The table reports the predicted return, the average realized return, the standard deviation, and the out-of-sample Sharpe ratio for each of the decile portfolios as well as the long-short zero-cost portfolio.\n\nhml_portfolio &lt;- ml_portfolios |&gt; \n  reframe(\n    ret_excess = ret_excess[portfolio == 10] - ret_excess[portfolio == 1],\n    ret_predicted = ret_predicted[portfolio == 10] - ret_predicted[portfolio == 1],\n    month = month[portfolio == 10],\n    portfolio = factor(\"H-L\", levels = as.character(c(1:10, \"H-L\"))),\n  ) \n\nml_portfolios |&gt;\n  bind_rows(hml_portfolio) |&gt;\n  group_by(portfolio) |&gt;\n  summarize(predicted_mean = mean(ret_predicted), \n            realized_mean = mean(ret_excess),\n            realized_sd = sd(ret_excess),\n            sharpe_ratio = realized_mean / realized_sd) |&gt;\n  print(n = Inf)\n\n# A tibble: 11 × 5\n   portfolio predicted_mean realized_mean realized_sd sharpe_ratio\n   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 1              -0.0516        -0.162        0.105        -1.54 \n 2 2              -0.0287        -0.126        0.0808       -1.55 \n 3 3              -0.0181        -0.0977       0.0814       -1.20 \n 4 4              -0.0111        -0.0639       0.0672       -0.950\n 5 5              -0.00558       -0.0350       0.0534       -0.655\n 6 6              -0.000535      -0.00536      0.0462       -0.116\n 7 7               0.00447        0.0222       0.0504        0.441\n 8 8               0.0101         0.0502       0.0688        0.730\n 9 9               0.0165         0.0849       0.0977        0.870\n10 10              0.0290         0.198        0.218         0.909\n11 H-L             0.0805         0.360        0.168         2.14 \n\n\nObviously, the paper is doing much more. In particular, the debate on “which covariates matter” is interesting and not necessarily trivial to implement. However, covering even more goes beyond the scope of a simple blog post. I hope that the code base above helps to overcome any issues with implementing the more intriguing parts of the paper.\n\n\n\n\n\n\nSome comments on parallelization\n\n\n\n\n\nArguably, the dataset used for this application is huge. So is the computational effort to train the ML methods. Even though I have substantial computing power available, I decided to keep the computational burden limited in the interest of simplicity and time. Should you not have a supercomputer available, the following tipps may help:\n\nWhen expanding the characteristics to include all interaction terms by applying the recipe (bake()), you do not have to use the entire dataset in memory as I do above. It seems plausible to store the characteristics (e.g., in parquet form) and apply bake() to each year individually before storing the data year-by-year. The all-in procedure from above requires around 140GB in memory which can be reduced to \\(\\approx 140/60\\) when applying the recipe on an annual basis.\nIn the blog post, I only fit the random forest for one single year (j = 1, the first of the 35 out-of-sample periods). It is straightforward to loop over j, starting from 1 until 35 and to store the predictions individually for each year.\n\nTuning the model for one out-of-sample period like above is still going to be computational heavy, and I am afraid there is no way around that. For the final out-of-sample period 2021, the training set contains more than two million rows with 920 columns as the set of predictors. The only way I can imagine to reduce the computational burden and still retain some (hopefully) useful predictions is to pre-select a well-balanced set of firms from the CRSP sample which differ with respect to some firm characteristics, e.g. by sampling from the well-known size and book-to-market double sort decile groups."
  },
  {
    "objectID": "blog/gu-kelly-xiu-replication/index.html#footnotes",
    "href": "blog/gu-kelly-xiu-replication/index.html#footnotes",
    "title": "Replicating Gu, Kelly & Xiu (2020)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Review of Financial Studies, Volume 33, Issue 5, May 2020, Pages 2223–2273, https://doi.org/10.1093/rfs/hhaa009↩︎"
  },
  {
    "objectID": "blog/convert-raw-trace-data/index.html",
    "href": "blog/convert-raw-trace-data/index.html",
    "title": "Convert Raw TRACE Data to a Local SQLite Database",
    "section": "",
    "text": "Corporate bond research is gaining momentum, but data availability can be a challenge for most researchers. While FINRA makes its TRACE Enhanced Data on U.S. corporate bond trading available through vendors such as Wharton Research Data Services (WRDS), not every researcher has access to these services. Alternatively, FINRA offers its Enhanced Historical Data and Academic Data (the latter exclusively) as one-off purchases. However, organizing this data can be complex and time consuming due to the nested and zipped structure of raw TXT files, making it cumbersome for researchers to explore the exciting world of fixed income securities.\nOur R code enables you organizing the TRACE academic data and TRACE enhanced data by eliminating the complexities of multiple, nested files, simplifying the data conversion process. Drawing from the existing SAS-based solution of Dick-Nielsen (2014) 1 and (2019) 2, you can now easily convert TRACE data into a single, organized SQLite database, allowing for seamless and efficient downstream analysis.\nHere are the key benefits our solution offers:\nWith our R code, you can simplify your corporate bond research and overcome current limitations. Don’t let complex data organizing hold you back! Prepare yourself to be immersed in the extraordinary realm of TRACE, the key to unlock a deeper understanding of over-the-counter transactions and to unveil the mysterious pathways of fixed-income securities. With TRACE as your guide, you will embark on a journey of discovery, unraveling market trends and unearthing invaluable insights that will forever enrich your understanding of this mesmerizing world. Embrace the power of TRACE and unveil the secrets that lie within.\nIn the following, we will discuss first, the structure of the bond trading data “TRACE” provided by FINRA, second the proposed normalized database schema for the generated SQLite database, and finally third, give you a brief hands-on guide to downloading data and using our R code to generate the SQLite."
  },
  {
    "objectID": "blog/convert-raw-trace-data/index.html#final-remarks",
    "href": "blog/convert-raw-trace-data/index.html#final-remarks",
    "title": "Convert Raw TRACE Data to a Local SQLite Database",
    "section": "Final Remarks",
    "text": "Final Remarks\nWe hope that this blog post is helpful to the reader working with TRACE academic data and TRACE enhanced data and in facilitating analyses in this emerging, exciting, and promising field of research.\nAt the end, we would like to thank the editors of Tidy Finance for their helpful suggestions and support with writing this blog post, and our supervisor Prof. Dr. Dirk Schiereck from the Chair of Corporate Finance at TU Darmstadt (Germany) who made the data available for this project."
  },
  {
    "objectID": "blog/convert-raw-trace-data/index.html#footnotes",
    "href": "blog/convert-raw-trace-data/index.html#footnotes",
    "title": "Convert Raw TRACE Data to a Local SQLite Database",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDick-Nielsen, J. (2014). How to clean enhanced TRACE data. Available at [SSRN 2337908] (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2337908).↩︎\nDick-Nielsen, J., & Poulsen, T. K. (2019). How to clean academic trace data. Available at [SSRN 3456082] (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3456082).↩︎"
  },
  {
    "objectID": "blog/iss-shareholder-proposals/index.html",
    "href": "blog/iss-shareholder-proposals/index.html",
    "title": "ISS Shareholder Proposals",
    "section": "",
    "text": "In this blog post, we show how to obtain and analyze shareholder proposal voting data for publicly listed firms in the US. For context, the SEC allows shareholders to submit proposals a few months before the annual general meeting (AGM), and these proposals end up on the ballot and are voted on if (1) a firm’s management does not submit a ‘no-action request’ accepted by the SEC or (2) management and shareholders do not reach an agreement prior to the AGM. Voting outcomes are non-binding, but management usually experiences pressure from organizations such as the Council of Institutional Investors (CII) if they do not adequately act on a proposal’s voting outcome (Bach and Metzger 2017). We refer to data provided by Institutional Shareholder Services (ISS), which gained popularity as one of the major US proxy advisors. A proxy advisor provides recommendations to institutional investors on how to vote on proposals at AGMs, and in this context, ISS has used its advantageous position to build an extensive database that collects information on management and shareholder proposals.\nOur code relies on the following R packages.\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(RSQLite)\nlibrary(readxl)\nlibrary(janitor)"
  },
  {
    "objectID": "blog/iss-shareholder-proposals/index.html#data-preparation",
    "href": "blog/iss-shareholder-proposals/index.html#data-preparation",
    "title": "ISS Shareholder Proposals",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start by establishing a connection to WRDS using the tidyfinance package (you can find more information here) to query the data directly through their servers. We also load stocks’ identifying information to match ISS identifiers (8-digit CUSIPs) with CRSP PERMNOs. Finally, we read in an Excel file containing granular ESG-type categorization of proposal types, which is separately provided by ISS.\n\nwrds &lt;- get_wrds_connection()\n\n\nFetching Shareholder Proposals\nISS provides two main datasets that contain shareholder proposals related to publicly listed US firms:\n\nISS Voting Analytics - Shareholder Proposals\nISS Voting Analytics - Company Vote Results US\n\nThe Shareholder Proposals table contains all shareholder proposals that were submitted to the SEC, regardless of whether they eventually came to a vote at the AGM (sample coverage: 2006 - 2024). The Company Vote Results table, on the other hand, has all proposals that made it to the ballot at AGM and hence also includes management proposals1 (sample coverage: 2002 - 2024).2 We use the Company Vote Results data as we are primarily interested in proposals that shareholders actually voted on, and exploit the slightly longer sample period. We extract company names and identifiers, meeting dates, vote shares, and short descriptions of proposal contents.\n\nva_vote_results &lt;- tbl(wrds, I(\"iss_va_vote_us.vavoteresults\")) |&gt; \n  select(cusip, companyid, name, ticker,\n         meetingdate, recorddate, meetingid,\n         sponsor, itemonagendaid, issagendaitemid,\n         agendageneraldesc, itemdesc,\n         outstandingshare, voterequirement, base,\n         votedabstain, votedagainst, votedfor, votedwithheld) |&gt;\n  collect()\n\nIn order to filter for proposals submitted by shareholders, we refer to the variable sponsor and keep the sponsor type denoted as \"Shareholder\".\n\nva_vote_results &lt;- va_vote_results |&gt; \n  filter(sponsor == \"Shareholder\")\n\n\n\nAdd Variables on Vote Outcomes\nAs we are dealing with shareholder proposal voting data, our main interest is, of course, the voting outcome, which we determine by computing vote shares. Firms define voting rules in their corporate charters, and the variable base informs us about the voting rule that applies to a specific observation in our sample. One primarily distinguishes two cases: either abstentions are counted as votes against the proposal (base == \"F+A+AB\"), or they are simply ignored (base == \"F+A\"). We need to be careful with the values in base since the voting rules are spelled inconsistently. Two additional, though unpopular, voting categories need to be accounted for: \"Outstanding\" and \"Capital Represe\". These require dividing the votes in favor of the proposals by the total number of shares outstanding.\n\nva_vote_results &lt;- va_vote_results |&gt; \n  mutate(vote_share = case_when(base %in% c(\"F+A\", \"F A\") ~ votedfor / (votedfor + votedagainst),\n                          base %in% c(\"F+A+AB\", \"F A AB\") ~ votedfor / (votedfor + votedagainst + votedabstain),\n                          base %in% c(\"Outstanding\", \"Capital Represe\") ~ votedfor / outstandingshare,\n                          .default = NA))\n\nAs pointed out by Cunat, Giné, and Guadalupe (2020), early studies mainly used a so-called simple majority rule, which translates into dividing votes in favor of the proposal by votes against the proposal. The reason is that earlier versions of the ISS database did not contain specific information on the voting rules defined in corporate charters but only provided votes in favor versus votes against a proposal. To be able to replicate prior results, we add a variable that computes the simple voting rule, which will collapse to the sophisticated voting rule if firms use votedfor / votedagainst (i.e., base == \"F+A\"). Next, we determine whether a proposal passed by comparing the vote share with the vote requirement.3 If the vote share exceeds the vote requirement, we assign a value equal to one and zero otherwise.\n\nva_vote_results &lt;- va_vote_results |&gt; \n  mutate(vote_share_simple = votedfor / (votedfor + votedagainst))\n  \nva_vote_results &lt;- va_vote_results |&gt; \n  mutate(pass = if_else(vote_share &gt;= voterequirement, 1, 0),\n         pass_simple = if_else(vote_share_simple &gt;= voterequirement, 1, 0))\n\nWe also compute the absolute distance of the vote share to the majority threshold.\n\nva_vote_results &lt;- va_vote_results |&gt; \n  mutate(distance_threshold = abs(voterequirement - vote_share),\n         distance_threshold_simple = abs(voterequirement - vote_share_simple))\n\nWe detected a few obvious data errors, which we account for by applying some additional cleaning steps. We require that proposals must have received at least one vote, no matter whether for or against, and that base is unequal to Votes Represent because we are not sure what this voting rule refers to.\n\nva_vote_results &lt;- va_vote_results |&gt; \n  filter(votedfor + votedagainst &gt; 0) |&gt;\n  filter(base != \"Votes Represent\")\n\nWe also make sure to exclude proposals for which we cannot properly compute vote shares.\n\nva_vote_results &lt;- va_vote_results |&gt; \n  filter(!is.na(vote_share) | !is.na(vote_share_simple),\n         !is.na(voterequirement))"
  },
  {
    "objectID": "blog/iss-shareholder-proposals/index.html#proposal-classification",
    "href": "blog/iss-shareholder-proposals/index.html#proposal-classification",
    "title": "ISS Shareholder Proposals",
    "section": "Proposal Classification",
    "text": "Proposal Classification\nFor most analyses, we want to know the reason for the proposal’s submission, or more precisely, what is actually voted on at AGMs. For that matter, ISS divides proposals into two resolution types, governance (GOV) and socially responsible investing (SRI), but also provides a more detailed categorization within GOV and SRI (see variable issagendaitemid). We refer to this more granular categorization and additionally rely on He, Kahraman, and Lowry (2023), who check and classify proposals manually to ensure we correctly capture proposals that are related to the issues we would like to analyze. ISS separately provides broad ESG-type classifications that can be linked to issagendaitemid. The respective Excel file containing the ESG classifications can be downloaded from WRDS (in the subsequent code, this file is ISSAgendaCodes_2023_All_Codes.xlsx).\n\niss_agenda_ids &lt;- read_excel(\"ISSAgendaCodes_2023_All_Codes.xlsx\") |&gt; \n  clean_names() |&gt; \n  mutate(agenda_esg_type = gsub(\"[^a-zA-Z]\", \"\", proposal_class)) |&gt; \n  select(agenda_code, agenda_esg_type)\n  \nva_vote_results &lt;- va_vote_results |&gt; \n  left_join(iss_agenda_ids,\n            join_by(issagendaitemid == agenda_code))\n\nOne additional comment on proposal classification is in order. As He, Kahraman, and Lowry (2023) investigate ISS resolution types, they find that not all proposals labeled as SRI (GOV) actually deal with issues related to SRI (GOV). Hence, one should be cautious when using ISS resolution types and issagendaitemid without additional checks. In their online appendix, He, Kahraman, and Lowry (2023) provide a list of issagendaitemid that actually reflects proposal types related to ecological and social issues, which allows us to gain more confidence in the classification. In the past, researchers have primarily relied on the coarse resolution type classification by ISS Flammer (2015), which introduces a potential source for replication failures when using more sophisticated classification procedures instead.\nAs the resolution type variable is only available in the Shareholder Proposals table, we now also extract unique pairs of resolution type and issagendaitemid from this dataset. We match these to our Vote Results table. Since it is possible that a given issagendaitemid refers to both GOV and SRI, we remove issagendaitemids that correspond to both, GOV and SRI. We document that this is only the case for id S0810 in the dataset at hand.\n\nva_shareholder &lt;- tbl(wrds, I(\"iss_va_shareholder.va_proposals\")) |&gt; \n  distinct(issagendaitemid, resolution_type) |&gt; \n  collect() |&gt; \n  drop_na()\n  \nva_shareholder |&gt; \n  group_by(issagendaitemid) |&gt;\n  summarize(n = n_distinct(resolution_type),\n            .groups = \"drop\") |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 1 × 2\n  issagendaitemid     n\n  &lt;chr&gt;           &lt;int&gt;\n1 S0810               2\n\nresolution_types &lt;- va_shareholder |&gt;\n  filter(issagendaitemid != \"S0810\")\n  \nva_vote_results &lt;- va_vote_results |&gt;\n  left_join(resolution_types,\n            join_by(issagendaitemid))\n\nWe have substantially manipulated the raw data, and to keep things tractable, we now make sure to keep only relevant variables for further analysis.\n\nva_vote_results &lt;- va_vote_results |&gt;\n  select(cusip, name, ticker,\n         date = meetingdate, recorddate,\n         meetingid, itemonagendaid,\n         agenda_esg_type, issagendaitemid, agendageneraldesc, itemdesc, \n         voterequirement, vote_share, vote_share_simple,\n         distance_threshold, distance_threshold_simple, pass, pass_simple)\n\n\nConfounding Proposal Identification\nAssume that we aim to explore the effect of passing close-vote ecological and social proposals, i.e., those that have passed or failed close to the majority threshold, on expected and realized returns. In this case, Cuñat, Gine, and Guadalupe (2012) and Flammer (2015) correctly point out that there are possibly multiple close-vote proposals per AGM. It is straightforward to account for these proposals if they are also related to ecological or social concerns from an econometric point of view. However, if confounding proposals are of type governance, it becomes infeasible to disentangle the effects of close-vote governance versus close-vote ecological and social proposals on expected and realized returns. We decide to construct count variables that inform us about the number of confounding close-vote proposals (at different distances to the threshold: 5%, 10%, and 20%, respectively) that are not ecologically or socially related. Later, this procedure allows us to run robustness checks by excluding AGMs that had close-vote governance proposals in addition to close-vote ecological or social proposals.\n\nva_vote_results &lt;- va_vote_results |&gt;\n  group_by(cusip, date) |&gt;\n  mutate(confounding_close_votes_five = \n           sum(distance_threshold &lt;= 0.05 & !(agenda_esg_type %in% c(\"E\", \"S\", \"ES\"))),\n         confounding_close_votes_ten =\n           sum(distance_threshold &lt;= 0.1 & !(agenda_esg_type %in% c(\"E\", \"S\", \"ES\"))),\n         confounding_close_votes_twenty =\n           sum(distance_threshold &lt;= 0.2 & !(agenda_esg_type %in% c(\"E\", \"S\", \"ES\")))) |&gt; \n  ungroup()"
  },
  {
    "objectID": "blog/iss-shareholder-proposals/index.html#matching-with-other-data-sources",
    "href": "blog/iss-shareholder-proposals/index.html#matching-with-other-data-sources",
    "title": "ISS Shareholder Proposals",
    "section": "Matching with Other Data Sources",
    "text": "Matching with Other Data Sources\nTo conduct meaningful analyses, we must link shareholder proposal voting outcomes from ISS with other data sources such as CRSP or Compustat. In what follows, we provide code and describe how to match ISS identifiers with CRSP identifiers.\n\nva_vote_results &lt;- va_vote_results |&gt; \n  mutate(cusip6 = substr(cusip, 1, 6),\n         cusip8 = substr(cusip, 1, 8)) |&gt; \n  rowid_to_column(\"id\")\n\nBefore the actual matching can be performed, we need to prepare a linking table based on the Security Information History table and the Company Names table from CRSP.\n\nstksecurityinfohist_db &lt;- tbl(wrds, I(\"crsp.stksecurityinfohist\"))\n\nstksecurityinfohist &lt;- stksecurityinfohist_db |&gt; \n  mutate(cusip6 = substr(cusip, 1, 6),\n         cusip8 = cusip) |&gt; \n  select(secinfostartdt, secinfoenddt,\n         permno, cusip6, cusip8, cusip9, ticker) |&gt; \n  filter(!is.na(cusip8) | !is.na(ticker)) |&gt; \n  collect()\n\nstocknames_db &lt;- tbl(wrds, I(\"crsp.stocknames\"))\n\nstocknames &lt;- stocknames_db |&gt; \n  select(namedt, nameenddt,\n         permno, comnam) |&gt; \n  collect()\n\nlinking_table &lt;- stksecurityinfohist |&gt; \n  full_join(stocknames,\n            join_by(permno == permno,\n                    overlaps(secinfostartdt, secinfoenddt,\n                             namedt, nameenddt))) |&gt;\n  mutate(start_date = pmax(secinfostartdt, namedt, na.rm = TRUE),\n         end_date = pmin(secinfoenddt, nameenddt, na.rm = TRUE)) |&gt; \n  select(permno, contains(\"cusip\"), ticker, comnam, start_date, end_date) |&gt; \n  distinct()\n\nIn the first step, we match ISS CUSIPs with 8- and 6-digit CUSIPs from CRSP. In case there is no appropriate match using CUSIPs, we continue the matching procedure based on standardized company names and tickers. Finally, we check for potential backward-filling in the ISS data by using a forward-looking name/CUSIP/ticker combination. Overall, we can match most shareholder proposals from ISS with CRSP identifying information following our procedure.4\n\nmatched_va_vote_results &lt;- bind_rows(\n  va_vote_results,\n  va_vote_results |&gt;\n    inner_join(linking_table |&gt;\n                 select(start_date, end_date, cusip8, permno) |&gt; \n                 drop_na() |&gt; \n                 distinct(),\n               join_by(between(date, start_date, end_date),\n                       cusip8 == cusip8)) |&gt;\n    select(-start_date, -end_date) |&gt; \n    mutate(match = 1),\n  va_vote_results |&gt;\n    inner_join(linking_table |&gt;\n                 select(start_date, end_date, cusip6, permno) |&gt;\n                 drop_na() |&gt;\n                 distinct(),\n               join_by(between(date, start_date, end_date),\n                       cusip6 == cusip6)) |&gt;\n    select(-start_date, -end_date) |&gt;\n    mutate(match = 2),\n  va_vote_results |&gt; # match on name\n    mutate(check_name = substr(tolower(gsub(\"[^a-zA-Z]\", \"\", name)), 1, 10)) |&gt; \n    inner_join(linking_table |&gt;\n                 mutate(comnam = substr(tolower(gsub(\"[^a-zA-Z]\", \"\", comnam)), 1, 10)) |&gt; \n                 select(start_date, end_date, comnam, permno) |&gt; \n                 drop_na() |&gt;\n                 distinct(),\n               join_by(between(date, start_date, end_date),\n                       check_name == comnam)) |&gt;\n    select(-start_date, -end_date, -check_name) |&gt; \n    mutate(match = 3),\n  va_vote_results |&gt;\n    inner_join(linking_table |&gt;\n                 select(start_date, end_date, ticker, permno) |&gt;\n                 drop_na() |&gt;\n                 distinct(),\n               join_by(between(date, start_date, end_date),\n                       ticker == ticker)) |&gt;\n    select(-start_date, -end_date) |&gt; \n    mutate(match = 4),\n  va_vote_results |&gt;\n    mutate(check_name = substr(tolower(gsub(\"[^a-zA-Z]\", \"\", name)), 1, 10)) |&gt;\n    inner_join(linking_table |&gt;\n                 mutate(comnam = substr(tolower(gsub(\"[^a-zA-Z]\", \"\", comnam)), 1, 10)) |&gt;\n                 select(comnam, cusip8, ticker, permno) |&gt; \n                 drop_na() |&gt; \n                 distinct(),\n               join_by(check_name == comnam,\n                       cusip8 == cusip8,\n                       ticker == ticker)) |&gt;\n    select(-check_name) |&gt; \n    mutate(match = 5))\n    \nmatched_va_vote_results &lt;- matched_va_vote_results |&gt; \n  arrange(match) |&gt; \n  group_by(id) |&gt; \n  slice_head(n = 1) |&gt; \n  ungroup() |&gt; \n  select(-id, -cusip6, -cusip8, -match)"
  },
  {
    "objectID": "blog/iss-shareholder-proposals/index.html#coverage",
    "href": "blog/iss-shareholder-proposals/index.html#coverage",
    "title": "ISS Shareholder Proposals",
    "section": "Coverage",
    "text": "Coverage\nFollowing our proposed procedure, out of 13,952 proposals after our cleaning steps, we end up with a sample of 13,603 shareholder proposals in total (that can be matched to CRSP) spanning a period from 2002 to 2024.\nIn the following plot, we show the number of proposals over time grouped by ESG type.\n\n\n\n\n\n\n\n\nFigure 1: Number of shareholder proposals over time grouped by ESG types.\n\n\n\n\n\nFollowing the ESG-type classification of ISS, the share of proposals focused on ecological and social issues hardly changes over most parts of the sample period but experiences a spike in recent years (2022 - 2024). This might come as a surprise, considering how environmental and sustainability considerations have already shaped the investment industry throughout the last decade. On the other hand, taking into account that governance-type proposals cover “standard topics” such as shareholder rights or issues related to the board of directors, it should be no surprise that this group makes up the largest fraction of proposals throughout.\nIn the next plot, we take a closer look at the distribution of the vote share across different proposal classifications.\n\n\n\n\n\n\n\n\nFigure 2: Vote Share across ESG types.\n\n\n\n\n\nMost proposals related to ecological and social issues fail, and they usually do so by large margins as the majority threshold is commonly set to 50%. However, for governance proposals, the distribution looks quite different. Their vote shares are way more balanced, and there is no clear tendency for proposal failure. The observed pattern suggests that investors are more skeptical of ecological and social proposals than governance proposals.\n\nExtensions on the Data\nThe ISS Voting Analytics data is most widely used in empirical finance research studies when it comes to shareholder proposals and proxy voting. Nevertheless, we would like to mention that there are other sources for voting data on shareholder proposals as well. One other prominent source is FactSet, which provides voting data for US firms in the dataset “Proxy Proposals & N-PX”. This database was previously known as SharkRepellent and was acquired by FactSet in 2005.5 Currently, we do not have access to this data, but judging by studies that use ISS and FactSet (see, e.g., Flammer 2015) jointly, there seems to be a substantial amount of proposals that are either covered by ISS or FactSet. This implies that one can significantly increase overall sample sizes by drawing from both sources.\nIf one wishes to analyze European shareholder proposal voting data, we refer to the Company Vote Results Global database by ISS. Here, data is available for non-US companies from 2013 onward."
  },
  {
    "objectID": "blog/iss-shareholder-proposals/index.html#footnotes",
    "href": "blog/iss-shareholder-proposals/index.html#footnotes",
    "title": "ISS Shareholder Proposals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA typical example of a management proposal is the election of new board members.↩︎\nUpon request, ISS also provides legacy data going back until 1997.↩︎\nTypically, the vote requirement equals 50%. Note, however, that there are also exceptions, e.g., when shareholders elect new board members. This is often a pro forma vote with a minuscule threshold.↩︎\nNote that we recommend checking unmatched proposals manually and assigning identifying information if available, as the total number of unmatched proposals is manageable.↩︎\nInformation on the data can be found here.↩︎"
  },
  {
    "objectID": "blog/workshops-for-ukraine/index.html",
    "href": "blog/workshops-for-ukraine/index.html",
    "title": "Tidy Finance at Workshops for Ukraine",
    "section": "",
    "text": "Dariia Mykhailyshyna, an Economics PhD student at the University of Bologna, set up a collection of workshops that can be accessed in exchange for a donation in support of Ukraine. We contributed two workshops based on our book Tidy Finance With R. The seminars are recorded and available on demand, and the collection is continuously expanded with interesting topics. Check out the extensive workshop program to register for upcoming events and get recordings and materials of the previous workshops.\nYou can find the workshop descriptions of our contributions below. We believe in making a humanitarian contribution to this cause and would appreciate it if you consider this tremendous effort."
  },
  {
    "objectID": "blog/workshops-for-ukraine/index.html#financial-data-in-r",
    "href": "blog/workshops-for-ukraine/index.html#financial-data-in-r",
    "title": "Tidy Finance at Workshops for Ukraine",
    "section": "Financial Data in R",
    "text": "Financial Data in R\nThis workshop explores financial data available for research and practical applications in financial economics. It relies on material available on www.tidy-finance.org and covers: (1) How to access freely available data from Yahoo Finance and other vendors. (2) Where to find the data most commonly used in academic research. This main part covers data from CRSP, Compustat, and TRACE. (3) How to store and access data for your research project efficiently. (4) What other data providers are available and how to access their services within R."
  },
  {
    "objectID": "blog/workshops-for-ukraine/index.html#empirical-asset-pricing-in-r",
    "href": "blog/workshops-for-ukraine/index.html#empirical-asset-pricing-in-r",
    "title": "Tidy Finance at Workshops for Ukraine",
    "section": "Empirical Asset Pricing in R",
    "text": "Empirical Asset Pricing in R\nThis workshop explores empirical asset pricing and combines explanations of theoretical concepts with practical implementations. The course relies on material available on www.tidy-finance.org and proceeds in three steps: (1) We dive into the most used data sources and show how to work with data from WRDS, forming the basis for the analysis. We also briefly introduce some other possible sources of financial data. (2) We show how to implement the capital asset pricing model in rolling-window regressions. (3) We introduce the widely used method of portfolio sorts in empirical asset pricing. During the workshop, we will combine some theoretical insights with hands-on implementations in R."
  },
  {
    "objectID": "blog/cir-calibration/index.html",
    "href": "blog/cir-calibration/index.html",
    "title": "CIR Model Calibration using Python",
    "section": "",
    "text": "The Cox–Ingersoll–Ross (CIR)1 model stands as a cornerstone within the vast expanse of Financial Mathematics literature. Originally conceived to refine the well-known Vasicek2 model in Interest Rate Modeling, the CIR model addressed a notable limitation of its predecessor—specifically, the propensity of Gaussian models like Vasicek’s to generate negative interest rates, a feature often deemed undesirable despite the theoretical possibility of negative rates in reality.\nIn this concise exposition, I will delineate the process of calibrating the Cox–Ingersoll–Ross model using Python. From a theoretical point of view, I will define linear models to calibrate the CIR model and test their feasibility via Monte-Carlo simulations."
  },
  {
    "objectID": "blog/cir-calibration/index.html#footnotes",
    "href": "blog/cir-calibration/index.html#footnotes",
    "title": "CIR Model Calibration using Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCox, J. C., Ingersoll Jr, J. E., & Ross, S. A. (1985). A Theory of the Term Structure of Interest Rates. Econometrica, 53(2), 385-408. Link.↩︎\nVasicek, O. (1977). An equilibrium characterization of the term structure. Journal of financial economics, 5(2), 177-188. Link.↩︎\nOrlando, G., Mininni, R. M., and Bufalo, M. (2020). Forecasting interest rates through vasicek and cir models: A partitioning approach. Journal of Forecasting, 39(4):569–579. Link.↩︎"
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html",
    "href": "blog/tidy-finance-dummy-data/index.html",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "",
    "text": "Since we published our book Tidy Finance with R, we have received feedback from readers who don’t have access to WRDS that they cannot run the code we provide. To alleviate their constraints, we decided to create a dummy database that contains the WRDS tables and corresponding columns such that all code chunks in our book can be executed with this dummy database. The resulting database can be found through this link (around 50 MB). Just download the database and put it into your data folder (I already renamed it to tidy_finance.sqlite). Note that we do not create dummy data for macro tables because they can be freely downloaded from the original sources - check out Accessing and Managing Financial Data.\nWe deliberately use the dummy label because the data is not meaningful in the sense that it allows readers to actually replicate the results of the book. For legal reasons, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books.\nTo generate the dummy database, we use the following packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nLet us initialize a tidy_finance.sqlite database or connect to your existing one. Be careful, if you already downloaded the data from WRDS, then the code in this blog post will overwrite your data!\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\nSince we draw random numbers for most of the columns, we also define a seed to ensure that the generated numbers are replicable. We also initialize vectors of dates of different frequencies over 10 years that we then use to create yearly, monthly, and daily data, respectively.\nset.seed(1234)\n\nstart_date &lt;- as.Date(\"2003-01-01\")\nend_date &lt;- as.Date(\"2022-12-31\")\n\ntime_series_years &lt;- seq(year(start_date), year(end_date), 1)\ntime_series_months &lt;- seq(start_date, end_date, \"1 month\")\ntime_series_days &lt;- seq(start_date, end_date, \"1 day\")"
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html#create-stock-dummy-data",
    "href": "blog/tidy-finance-dummy-data/index.html#create-stock-dummy-data",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "Create stock dummy data",
    "text": "Create stock dummy data\nLet us start with the core data used throughout the book: stock and firm characteristics. We first generate a table with a cross-section of stock identifiers with unique permno and gvkey values, as well as associated exchcd, exchange, industry, and siccd values. The generated data is based on the characteristics of stocks in the crsp_monthly table of the original database, ensuring that the generated stocks roughly reflect the distribution of industries and exchanges in the original data, but the identifiers and corresponding exchanges or industries do not reflect actual firms. Similarly, the permno-gvkey combinations are purely nonsensical and should not be used together with actual CRSP or Compustat data.\n\nnumber_of_stocks &lt;- 100\n\nindustries &lt;- tibble(\n  industry = c(\"Agriculture\", \"Construction\", \"Finance\", \n               \"Manufacturing\", \"Mining\", \"Public\", \"Retail\", \n               \"Services\", \"Transportation\", \"Utilities\", \n               \"Wholesale\"),\n  n = c(81, 287, 4682, 8584, 1287, 1974, 1571, 4277, 1249, \n        457, 904),\n  prob = c(0.00319, 0.0113, 0.185, 0.339, 0.0508, 0.0779, \n           0.0620, 0.169, 0.0493, 0.0180, 0.0357)\n)\n\nexchanges &lt;- exchanges &lt;- tibble(\n  exchange = c(\"AMEX\", \"NASDAQ\", \"NYSE\"),\n  n = c(2893, 17236, 5553),\n  prob = c(0.113, 0.671, 0.216)\n)\n\nstock_identifiers &lt;- 1:number_of_stocks |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        permno = x,\n        gvkey = as.character(x + 10000),\n        exchange = sample(exchanges$exchange, 1, \n                          prob = exchanges$prob),\n        industry = sample(industries$industry, 1, \n                          prob = industries$prob)\n      ) |&gt; \n        mutate(\n          exchcd = case_when(\n            exchange == \"NYSE\" ~ sample(c(1, 31), n()),\n            exchange == \"AMEX\" ~ sample(c(2, 32), n()),\n            exchange == \"NASDAQ\" ~ sample(c(3, 33), n())\n          ),\n          siccd = case_when(\n            industry == \"Agriculture\" ~ sample(1:999, n()),\n            industry == \"Mining\" ~ sample(1000:1499, n()),\n            industry == \"Construction\" ~ sample(1500:1799, n()),\n            industry == \"Manufacturing\" ~ sample(1800:3999, n()),\n            industry == \"Transportation\" ~ sample(4000:4899, n()),\n            industry == \"Utilities\" ~ sample(4900:4999, n()),\n            industry == \"Wholesale\" ~ sample(5000:5199, n()),\n            industry == \"Retail\" ~ sample(5200:5999, n()),\n            industry == \"Finance\" ~ sample(6000:6799, n()),\n            industry == \"Services\" ~ sample(7000:8999, n()),\n            industry == \"Public\" ~ sample(9000:9999, n())\n          )\n        )\n    }\n  )\n\nNext, we construct three panels of stock data with varying frequencies: yearly, monthly, and daily. We begin by creating the stock_panel_yearly panel. To achieve this, we combine the stock_identifiers table with a new table containing the variable year from time_series_years. The expand_grid() function ensures that we get all possible combinations of the two tables. After combining, we select only the gvkey and year columns for our final yearly panel.\nNext, we construct the stock_panel_monthly panel. Similar to the yearly panel, we use the expand_grid() function to combine stock_identifiers with a new table that has the month variable from time_series_months. After merging, we select the columns permno, gvkey, month, siccd, industry, exchcd, and exchange to form our monthly panel.\nLastly, we create the stock_panel_daily panel. We combine stock_identifiers with a table containing the date variable from time_series_days. After merging, we retain only the permno and date columns for our daily panel.\n\nstock_panel_yearly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(year = time_series_years)\n) |&gt; \n  select(gvkey, year)\n\nstock_panel_monthly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(month = time_series_months)\n) |&gt; \n  select(permno, gvkey, month, siccd, industry, exchcd, exchange)\n\nstock_panel_daily &lt;- expand_grid(\n  stock_identifiers, \n  tibble(date = time_series_days)\n)|&gt; \n  select(permno, date)\n\n\nDummy beta table\nWe then proceed to create dummy beta values for our stock_panel_monthly table. We generate monthly beta values beta_monthly using the rnorm() function with a mean and standard deviation of 1. For daily beta values beta_daily, we take the dummy monthly beta and add a small random noise to it. This noise is generated again using the rnorm() function, but this time we divide the random values by 100 to ensure they are small deviations from the monthly beta.\n\nbeta_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    beta_monthly = rnorm(n(), mean = 1, sd = 1),\n    beta_daily = beta_monthly + rnorm(n()) / 100\n  )\n\ndbWriteTable(\n  tidy_finance,\n  \"beta\", \n  beta_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy compustat table\nTo create dummy firm characteristics, we take all columns from the compustat table and create random numbers between 0 and 1. For simplicity, we set the datadate for each firm-year observation to the last day of the year, although it is empirically not the case. We then use the !!! operator to unlist and execute a list of commands. This trick actually helps us to avoid typing the same function for each column individually.\n\nrelevant_columns &lt;- c(\n  \"seq\", \"ceq\", \"at\", \"lt\", \"txditc\", \"txdb\", \"itcb\", \n  \"pstkrv\", \"pstkl\", \"pstk\", \"capx\", \"oancf\", \"sale\", \n  \"cogs\", \"xint\", \"xsga\", \"be\", \"op\", \"at_lag\", \"inv\"\n)\n\ncommands &lt;- unlist(\n  map(\n    relevant_columns, \n    ~rlang::exprs(!!..1 := runif(n()))\n  )\n)\n\ncompustat_dummy &lt;- stock_panel_yearly |&gt; \n  mutate(\n    datadate = ymd(str_c(year, \"12\", \"31\")),\n    !!!commands\n  )\n\ndbWriteTable(\n  tidy_finance, \n  \"compustat\", \n  compustat_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_monthly table\nThe crsp_monthly table only lacks a few more columns compared to stock_panel_monthly: the returns ret drawn from a normal distribution, the excess returns ret_excess with small deviations from the returns, the shares outstanding shrout and the last price per month altprc both drawn from uniform distributions, and the market capitalization mktcap as the product of shrout and altprc.\n\ncrsp_monthly_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    date = ceiling_date(month, \"month\") - 1,\n    ret = pmax(rnorm(n()), -1),\n    ret_excess = pmax(ret - runif(n(), 0, 0.0025), -1),\n    shrout = runif(n(), 1, 50) * 1000,\n    altprc = runif(n(), 0, 1000),\n    mktcap = shrout * altprc\n  ) |&gt; \n  group_by(permno) |&gt; \n  arrange(month) |&gt; \n  mutate(mktcap_lag = lag(mktcap)) |&gt; \n  ungroup()\n\ndbWriteTable(\n  tidy_finance, \n  \"crsp_monthly\",\n  crsp_monthly_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_daily table\nThe crsp_daily table only contains a month column and the daily excess returns ret_excess as additional columns to stock_panel_daily.\n\ncrsp_daily_dummy &lt;- stock_panel_daily |&gt; \n  mutate(\n    month = floor_date(date, \"month\"),\n    ret_excess = pmax(rnorm(n()), -1)\n  )\n\ndbWriteTable(\n  tidy_finance,\n  \"crsp_daily\",\n  crsp_daily_dummy, \n  overwrite = TRUE\n)"
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html#create-bond-dummy-data",
    "href": "blog/tidy-finance-dummy-data/index.html#create-bond-dummy-data",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "Create bond dummy data",
    "text": "Create bond dummy data\nLastly, we move to the bond data that we use in our books.\n\nDummy fisd data\nTo create dummy data with the structure of Mergent FISD, we calculate the empirical probabilities of actual bonds for several variables: maturity, offering_amt, interest_frequency, coupon, and sic_code. We use these probabilities to sample a small cross-section of bonds with completely made up complete_cusip, issue_id, and issuer_id.\n\nnumber_of_bonds &lt;- 100\n\nfisd_dummy &lt;- 1:number_of_bonds |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        complete_cusip = str_to_upper(\n          str_c(\n            sample(c(letters, 0:9), 12, replace = TRUE), \n            collapse = \"\"\n          )\n        ),\n      )\n    }\n  ) |&gt; \n  mutate(\n    maturity = sample(time_series_days, n(), replace = TRUE),\n    offering_amt = sample(seq(1:100) * 100000, n(), replace = TRUE),\n    offering_date = maturity - sample(seq(1:25) * 365, n(),replace = TRUE),\n    dated_date = offering_date - sample(-10:10, n(), replace = TRUE),\n    interest_frequency = sample(c(0, 1, 2, 4, 12), n(), replace = TRUE),\n    coupon = sample(seq(0, 2, by = 0.1), n(), replace = TRUE),\n    last_interest_date = pmax(maturity, offering_date, dated_date),\n    issue_id = row_number(),\n    issuer_id = sample(1:250, n(), replace = TRUE),\n    sic_code = as.character(sample(seq(1:9)*1000, n(), replace = TRUE))\n  )\n  \ndbWriteTable(\n  tidy_finance, \n  \"fisd\", \n  fisd_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy trace_enhanced data\nFinally, we create a dummy bond transaction data for the fictional CUSIPs of the dummy fisd data. We take the date range that we also analyze in the book and ensure that we have at least five transactions per day to fulfill a filtering step in the book.\n\nstart_date &lt;- as.Date(\"2014-01-01\")\nend_date &lt;- as.Date(\"2016-11-30\")\n\nbonds_panel &lt;- expand_grid(\n  fisd_dummy |&gt; \n    select(cusip_id = complete_cusip),\n  tibble(\n    trd_exctn_dt = seq(start_date, end_date, \"1 day\")\n  )\n)\n\ntrace_enhanced_dummy &lt;- bind_rows(\n  bonds_panel, bonds_panel, \n  bonds_panel, bonds_panel, \n  bonds_panel) |&gt; \n  mutate(\n    trd_exctn_tm = str_c(\n      sample(0:24, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE)\n    ),\n    rptd_pr = runif(n(), 10, 200),\n    entrd_vol_qt = sample(1:20, n(), replace = TRUE) * 1000,\n    yld_pt = runif(n(), -10, 10),\n    rpt_side_cd = sample(c(\"B\", \"S\"), n(), replace = TRUE),\n    cntra_mp_id = sample(c(\"C\", \"D\"), n(), replace = TRUE)\n  ) \n  \ndbWriteTable(\n  tidy_finance, \n  \"trace_enhanced\", \n  trace_enhanced_dummy, \n  overwrite = TRUE\n)\n\nAs stated in the introduction, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books. You can find the database with the dummy data here."
  },
  {
    "objectID": "disclaimer.html",
    "href": "disclaimer.html",
    "title": "Tidy Finance",
    "section": "",
    "text": "The providers of this website (www.tidy-finance.org) and its sites are Christoph Frey, Christoph Scheuch, Stefan Voigt, and Patrick Weiss. Direct inquiries related to its contents to contact@tidy-finance.org.\nThe use of the website is free of charge. Various open-source licenses govern parts of the distributed content (such as packages or programming languages) created by third parties, and any further use has to agree with these rules. Any links to other websites or documents are not under the influence of this website’s providers, and no responsibility can be assumed for them. We do not claim any ownership rights in any content created by third parties.\nThe content on this website is for illustrative and educational use only. While the content is well-researched, no guarantee can be given for the accuracy, completeness, or timeliness of the information. All liability for the content on this website is excluded. No statements shall be interpreted as an offer to purchase/contract or advisory service. In particular, this website does not offer any investment advice. No contractual relationship can be created from any statements on this website.\nFor data privacy concerns or inquiries, please contact us via contact@tidy-finance.org. The website does use cookies if you agree to them, which are managed via the preferences at the bottom of the website.\nThese policies may be updated at any time."
  },
  {
    "objectID": "r/the-tidyfinance-r-package.html",
    "href": "r/the-tidyfinance-r-package.html",
    "title": "The tidyfinance R package",
    "section": "",
    "text": "tidyfinance is an R package that contains a set of helper functions for empirical research in financial economics, addressing a variety of topics covered in this book. We designed the package to provide easy shortcuts for the applications that we discuss in the book. If you want to inspect the details of the package or propose new features, feel free to visit the package repository on Github.",
    "crumbs": [
      "R",
      "Prerequisits",
      "The `tidyfinance` R package"
    ]
  },
  {
    "objectID": "r/the-tidyfinance-r-package.html#installation",
    "href": "r/the-tidyfinance-r-package.html#installation",
    "title": "The tidyfinance R package",
    "section": "Installation",
    "text": "Installation\nYou can install the released version of tidyfinance from CRAN via:\ninstall.packages(\"tidyfinance\")\nYou can install the development version of tidyfinance from GitHub (which might not be fully tested) via:\n# install.packages(\"pak\")\npak::pak(\"tidy-finance/r-tidyfinance\")",
    "crumbs": [
      "R",
      "Prerequisits",
      "The `tidyfinance` R package"
    ]
  },
  {
    "objectID": "r/the-tidyfinance-r-package.html#usage",
    "href": "r/the-tidyfinance-r-package.html#usage",
    "title": "The tidyfinance R package",
    "section": "Usage",
    "text": "Usage\nThroughout the book, we refer to the corresponding features of the tidyfinance package. If you want to get an overview of the existing functionality, we suggest the blog posts that discuss specific releases and new features:\n\nVersion 0.1.0\nVersion 0.4.0",
    "crumbs": [
      "R",
      "Prerequisits",
      "The `tidyfinance` R package"
    ]
  },
  {
    "objectID": "r/the-tidyfinance-r-package.html#feature-requests",
    "href": "r/the-tidyfinance-r-package.html#feature-requests",
    "title": "The tidyfinance R package",
    "section": "Feature requests",
    "text": "Feature requests\nWe are curious to learn in which direction we should extend the package, so please consider opening an issue in the package repository. For instance, we could support more data sources, add more parameters to the family of functions for data downloads, or we could put more emphasis on the generality of portfolio assignment or other modeling functions. Moreover, if you discover a bug, we are very grateful if you report the issue in our repository.",
    "crumbs": [
      "R",
      "Prerequisits",
      "The `tidyfinance` R package"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html",
    "href": "r/setting-up-your-environment.html",
    "title": "Setting Up Your Environment",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nWe aim to lower the bar for starting empirical research in financial economics. We want that using R is easy for you. However, given that Tidy Finance is a platform that supports multiple programming languages, we also consider the possibility that you are not familiar with R at all. Hence, we provide you with a simple guide to get started with R and RStudio. If you were not using R before, you will be able to use it after reading this chapter.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#the-r-language",
    "href": "r/setting-up-your-environment.html#the-r-language",
    "title": "Setting Up Your Environment",
    "section": "The R language",
    "text": "The R language\nSome good news first: The software you need is free and easy to download. We will start with downloading and installing R and follow up with doing the same for RStudio.\nR is provided via The Comprehensive R Archive Network (or short CRAN). CRAN does not only provide the main software but also nearly all extensions that you need. We will cover these extensions or packages later, as we usually visit the CRAN website only to download the base version. Now, go ahead and visit CRAN. On the landing page, you can choose your operating systems (i.e., Linux, macOS, and Windows). Click the respective link that fits your system:\n\nR comes as a part of many Linux distributions. If it does not, CRAN provides installation guides for individual Linux distributions.\nFor macOS, the choice currently depends on some hardware specifications, but the right version for your system is clearly indicated.\nFor Windows, you want to use the base version provided.\n\nAfter downloading and installing the software to your system, you are nearly ready to go. In fact, you could just use R now. Unfortunately for many users, R is not a program but a programming language and comes with an interpreter that you would use like a command line. While using R like this might make you feel like a hacker (not that we do not endorse any criminal activity), it is in your best interest to combine R with RStudio.\nR is constantly being updated, with new versions being released multiple times a year. This means that you might want to return to CRAN in the future to fetch yourself an update. You know it is time for an update if packages remind you that you are using an outdated version of R.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#rstudio",
    "href": "r/setting-up-your-environment.html#rstudio",
    "title": "Setting Up Your Environment",
    "section": "RStudio",
    "text": "RStudio\nAssuming you are looking for a more comfortable way of using R, you will get RStudio next. You can download it for free from Posit (i.e., the company that created RStudio, which was previously called RStudio itself). When you follow the instructions, you will see that Posit asks you to install R. However, you should have done that already and can move straight to downloading and installing RStudio.\nRStudio is a program similar to other programs you most likely use, like a browser, text editor, or anything else. It comes with many advantages, including a project manager, Github integration, and much more. Unfortunately, Tidy Finance is not the right scope to elaborate more on these possibilities or introduce the basics of programming, but we point you to some excellent resources below. For the purposes of this book, you have completed your excursions to websites that provide you with the necessary software installers.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#r-packages-and-environments",
    "href": "r/setting-up-your-environment.html#r-packages-and-environments",
    "title": "Setting Up Your Environment",
    "section": "R Packages and Environments",
    "text": "R Packages and Environments\nFollowing your read of the preface to this book, you might now wonder why we did not download the tidyverse yet. Therefore, you must understand one more concept, namely packages in R. You can think of them as extensions that you use for specific purposes, whereas R itself is the core pillar upon which everything rests. Comfortably, you can install packages within R with the following code.\n\ninstall.packages(\"tidyverse\")\n\nSimply specify the package you want where we placed tidyverse. You typically only need to install packages once - except for updates or project-specific R environments. Once installed, you can then load a package with a call to library(tidyverse) to use it.\nTo keep track of the packages’ versions and make our results replicatable, we rely on the package renv. It creates a project-specific installation of R packages and you can find the full list of packages used here in the colophon below. The recorded package versions can also be shared with collaborators to ensure consistency. Our use of renv also makes it easier for you to install the exact package versions we were using (if you want that) by initializing renv with our renv.lock-file from Github. \nOne more piece of advice is the use of RStudio projects. They are a powerful tool to save you some time and make working with R more fun. Without going into more detail here, we refer you to Wickham, Çetinkaya-Rundel, and Grolemund (2023)’s chapter on Workflow: scripts and projects.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#your-first-steps-with-r",
    "href": "r/setting-up-your-environment.html#your-first-steps-with-r",
    "title": "Setting Up Your Environment",
    "section": "Your First Steps with R",
    "text": "Your First Steps with R\nWhile we believe that downloading and installing R and RStudio is sufficiently easy, you might find help from Grolemund (2014) on R and RStudio, packages, as well as updating the software.\nThis book’s scope cannot be to give you an introduction to R itself. It is not our comparative advantage. However, we can point you to a possible path that you could follow to familiarize yourself with R. Therefore, we make the following suggestion:\n\nIf you are new to R itself, a very gentle and good introduction to the workings of R can be found in Grolemund (2014). He provides a wonderful example in the form of the weighted dice project. Once you are done setting up R on your machine, try to follow the instructions in this project.\nThe main book on the tidyverse, Wickham, Çetinkaya-Rundel, and Grolemund (2023), is available online and for free: R for Data Science explains the majority of the tools we use in our book. Working through this text is an eye-opening experience and really useful.\n\nAdditional resources we can encourage you to use are the following:\n\nIf you are an instructor searching to effectively teach R and data science methods, we recommend taking a look at the excellent data science toolbox by Mine Cetinkaya-Rundel.\nRStudio provides a range of excellent cheat sheets with extensive information on how to use the tidyverse packages.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#creating-environment-variables",
    "href": "r/setting-up-your-environment.html#creating-environment-variables",
    "title": "Setting Up Your Environment",
    "section": "Creating Environment Variables",
    "text": "Creating Environment Variables\nIf you plan to share your own code with collaborators or the public, you may encounter the situation that your projects require sensitive information, such as login credentials, that you don’t want to publish. Environment variables are widely used in software development projects because they provide a flexible and secure way to configure applications and store secrets. In later chapters, we use such environment variables to store private login data for a remote database.\nYou can use .Renviron-files to store environment variables. Upon startup, R and RStudio look for .Renviron files in your home and project directory. .Renviron-files can be either at the user or project level. If there is a project-level .Renviron, the user-level file will not be sourced. A simple way to create your own .Renviron-file is the function usethis::edit_r_environ().\n\nusethis::edit_r_environ(scope = \"project\")\n\nThis command will open your .Renviron-file and you can add variables. For the purpose of this book, we create and save the following variables (where user and password are our private login credentials)\nWRDS_USER=user\nWRDS_PASSWORD=password\nAfter you have restarted your RStudio session, you can access these environment variables via Sys.getenv() for future sessions using the specific project or user.\n\nSys.getenv(\"WRDS_USER\")\nSys.getenv(\"WRDS_PASSWORD\")\n\nNote that you can also store other login credentials, API keys, or file paths in the same environment file.\nIf you use version control, then you should make sure that the .Renviron-file is included in your .gitignore with the following code line.\n\nusethis::edit_git_ignore(scope = \"project\")",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#colophon",
    "href": "r/setting-up-your-environment.html#colophon",
    "title": "Setting Up Your Environment",
    "section": "Colophon",
    "text": "Colophon\nThis book was written in RStudio using bookdown (Xie 2016). The website was rendered using quarto (Allaire et al. 2022) and it is hosted via GitHub Pages. The complete source is available from GitHub. We generated all plots in this book using ggplot2 and its classic dark-on-light theme (theme_bw()).\nThis version of the book was built with R (R Core Team 2022) version 4.4.1 (2024-06-14, Race for Your Life) and the following packages: \n\n\n\n\n\nPackage\nVersion\n\n\n\n\nRPostgres\n1.4.5\n\n\nRSQLite\n2.3.1\n\n\nbroom\n1.0.5\n\n\nbrulee\n0.3.0\n\n\ndbplyr\n2.5.0\n\n\ndplyr\n1.1.4\n\n\nfixest\n0.11.1\n\n\nforcats\n1.0.0\n\n\nfrenchdata\n0.2.0\n\n\nfurrr\n0.3.1\n\n\nggplot2\n3.4.3\n\n\nglmnet\n4.1-8\n\n\nhardhat\n1.3.0\n\n\nhexSticker\n0.4.9\n\n\nhttr2\n1.0.3\n\n\njsonlite\n1.8.8\n\n\nkableExtra\n1.3.4\n\n\nlmtest\n0.9-40\n\n\nlubridate\n1.9.3\n\n\nnloptr\n2.1.1\n\n\npurrr\n1.0.2\n\n\nranger\n0.15.1\n\n\nreadr\n2.1.4\n\n\nrenv\n1.0.3\n\n\nrlang\n1.1.3\n\n\nrmarkdown\n2.21\n\n\nsandwich\n3.0-2\n\n\nscales\n1.2.1\n\n\nslider\n0.3.1\n\n\nstringr\n1.5.0\n\n\ntibble\n3.2.1\n\n\ntidyfinance\n0.4.1\n\n\ntidymodels\n1.1.0\n\n\ntidyr\n1.3.1\n\n\ntidyverse\n2.0.0\n\n\ntimetk\n2.8.3\n\n\ntorch\n0.11.0\n\n\nwesanderson\n0.3.6",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/beta-estimation.html",
    "href": "r/beta-estimation.html",
    "title": "Beta Estimation",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we introduce an important concept in financial economics: the exposure of an individual stock to changes in the market portfolio. According to the Capital Asset Pricing Model (CAPM) of Sharpe (1964), Lintner (1965), and Mossin (1966), cross-sectional variation in expected asset returns should be a function of the covariance between the excess return of the asset and the excess return on the market portfolio. The regression coefficient of excess market returns on excess stock returns is usually called the market beta. We show an estimation procedure for the market betas. We do not go into details about the foundations of market beta but simply refer to any treatment of the CAPM for further information. Instead, we provide details about all the functions that we use to compute the results. In particular, we leverage useful computational concepts: rolling-window estimation and parallelization.\nWe use the following R packages throughout this chapter:\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(slider)\nlibrary(furrr)\nCompared to previous chapters, we introduce slider (Vaughan 2021) for sliding window functions, and furrr (Vaughan and Dancho 2022) to apply mapping functions in parallel.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#estimating-beta-using-monthly-returns",
    "href": "r/beta-estimation.html#estimating-beta-using-monthly-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta Using Monthly Returns",
    "text": "Estimating Beta Using Monthly Returns\nThe estimation procedure is based on a rolling-window estimation, where we may use either monthly or daily returns and different window lengths. First, let us start with loading the monthly CRSP data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, date, industry, ret_excess) |&gt;\n  collect()\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(date, mkt_excess) |&gt;\n  collect()\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(factors_ff3_monthly, join_by(date))\n\nTo estimate the CAPM regression coefficients\n\\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{i, t}\n\\]\nwe regress stock excess returns ret_excess on excess returns of the market portfolio mkt_excess. R provides a simple solution to estimate (linear) models with the function lm(). lm() requires a formula as input that is specified in a compact symbolic form. An expression of the form y ~ model is interpreted as a specification that the response y is modeled by a linear predictor specified symbolically by model. Such a model consists of a series of terms separated by + operators. In addition to standard linear models, lm() provides a lot of flexibility. You should check out the documentation for more information. To start, we restrict the data only to the time series of observations in CRSP that correspond to Apple’s stock (i.e., to permno 14593 for Apple) and compute \\(\\hat\\alpha_i\\) as well as \\(\\hat\\beta_i\\).\n\nmodel_fit &lt;- lm(\n  \"ret_excess ~ mkt_excess\",\n  data = crsp_monthly |&gt;\n    filter(permno == \"14593\")\n)\ncoefficients &lt;- summary(model_fit)$coefficients\ncoefficients\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.00992    0.00488    2.03 4.27e-02\nmkt_excess   1.37566    0.10761   12.78 8.90e-33\n\n\nlm() returns an object of class lm which contains all information we usually care about with linear models. summary() returns information about the estimated parameters. The output above indicates that Apple moves excessively with the market as the estimated \\(\\hat\\beta_i\\) is above one (\\(\\hat\\beta_i \\approx 1.4\\)).",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#rolling-window-estimation",
    "href": "r/beta-estimation.html#rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Rolling-Window Estimation",
    "text": "Rolling-Window Estimation\nAfter we estimated the regression coefficients on an example, we scale the estimation of \\(\\beta_i\\) to a whole different level and perform rolling-window estimations for the entire CRSP sample. The following function implements the CAPM regression for a data frame (or a part thereof) containing at least min_obs observations to avoid huge fluctuations if the time series is too short. The function conveniently returns the regression results as a data frame, which ensures that our approach is scalable. If the min_obs-condition is violated, that is, the time series is too short, the function returns an empty data frame for consistency.\n\nestimate_capm &lt;- function(data, min_obs = 1) {\n  if (nrow(data) &lt; min_obs) {\n    return(tibble())\n  } else {\n    fit &lt;- lm(\"ret_excess ~ mkt_excess\", data = data)\n    coefficients &lt;- summary(fit)$coefficients\n\n    capm &lt;- tibble(\n      coefficient = rownames(coefficients),\n      estimate = coefficients[, \"Estimate\"],\n      t_statistic = coefficients[, \"t value\"]\n    ) |&gt;\n      mutate(\n        coefficient = if_else(coefficient == \"(Intercept)\", \"alpha\", coefficient)\n      )\n  }\n\n  capm\n}\n\nNext, we define a function that performs the rolling estimation. The slide_period function is able to handle months in its window input in a straightforward manner. The following function takes input data and slides across the date vector, considering only a total of look_back months. The function essentially performs three steps: (i) arrange all rows, (ii) compute betas by sliding across months, and (iii) return a tibble with months and corresponding parameter estimates. As we demonstrate further below, we can also apply the same function to daily returns data.\n\nroll_capm_estimation &lt;- function(data, look_back = 60, min_obs = 48) {\n  data &lt;- data |&gt;\n    arrange(date)\n\n  slide_period_dfr(\n    .x = data,\n    .i = data$date,\n    .period = \"month\",\n    .f = function(x) {\n      estimate_capm(x, min_obs = min_obs) |&gt;\n        mutate(date = max(x$date))\n    },\n    .before = look_back - 1,\n    .complete = FALSE\n  )\n}\n\nBefore we attack the whole CRSP sample, let us focus on a couple of examples for well-known firms.\n\nexamples &lt;- tibble(\n  permno = c(14593, 10107, 93436, 17778),\n  company = c(\"Apple\", \"Microsoft\", \"Tesla\", \"Berkshire Hathaway\")\n)\n\nThe main idea is to apply the function to each stock individually and then combine the results into a single data frame. First, we nest the data by permno. Nested data means we now have a list of permno with corresponding grouped time series data. We get one row of output for each unique combination of non-nested variables which is only permno in this case.\n\ncapm_examples_nested &lt;- crsp_monthly |&gt;\n  filter(permno %in% examples$permno) |&gt;\n  nest(data = c(date, ret_excess, mkt_excess, industry))\ncapm_examples_nested\n\n# A tibble: 4 × 2\n  permno data              \n   &lt;dbl&gt; &lt;list&gt;            \n1  10107 &lt;tibble [465 × 4]&gt;\n2  14593 &lt;tibble [528 × 4]&gt;\n3  17778 &lt;tibble [578 × 4]&gt;\n4  93436 &lt;tibble [174 × 4]&gt;\n\n\nNext, we want to apply the roll_capm_estimation() function to each stock. This situation is an ideal use case for map(), which takes a list or vector as input and returns an object of the same length as the input. In our case, map() returns a single data frame with a time series of beta estimates for each stock. Therefore, we use unnest() to transform the list of outputs to a tidy data frame.\n\ncapm_examples &lt;- capm_examples_nested |&gt;\n  mutate(capm = map(data, roll_capm_estimation)) |&gt;\n  unnest(capm) |&gt;\n  select(permno, date, coefficient, estimate, t_statistic)\ncapm_examples\n\n# A tibble: 3,114 × 5\n  permno date       coefficient estimate t_statistic\n   &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1  10107 1990-03-01 alpha         0.0417        2.31\n2  10107 1990-03-01 mkt_excess    1.40          4.17\n3  10107 1990-04-01 alpha         0.0427        2.41\n4  10107 1990-04-01 mkt_excess    1.39          4.20\n5  10107 1990-05-01 alpha         0.0443        2.53\n# ℹ 3,109 more rows\n\n\nFigure 1 displays the resulting beta estimates, focusing exclusively on the coefficient fo \"mkt_excess\".\n\nbeta_examples &lt;- capm_examples |&gt;\n  left_join(examples, join_by(permno)) |&gt;\n  filter(coefficient == \"mkt_excess\") \n  \nbeta_examples |&gt;\n  ggplot(aes(x = date,y = estimate, color = company, linetype = company)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"Monthly beta estimates for example stocks using 5 years of data\"\n  )\n\n\n\n\n\n\n\nFigure 1: The CAPM betas are estimated with monthly data and a rolling window of length 5 years based on adjusted excess returns from CRSP. We use market excess returns from Kenneth French data library.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#parallelized-rolling-window-estimation",
    "href": "r/beta-estimation.html#parallelized-rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Parallelized Rolling-Window Estimation",
    "text": "Parallelized Rolling-Window Estimation\nEven though we could now just apply the function using the approach from above on the whole CRSP sample, we advise against doing it as it is computationally quite expensive. Remember that we have to perform rolling-window estimations across all stocks and time periods. However, this estimation problem is an ideal scenario to employ the power of parallelization. Parallelization means that we split the tasks which perform rolling-window estimations across different workers (or cores on your local machine).\nIf you have a Windows or Mac machine, it makes most sense to define multisession, which means that separate R processes are running in the background on the same machine to perform the individual jobs. If you check out the documentation of plan(), you can also see other ways to resolve the parallelization in different environments. Note that we use availableCores() to determine the number of cores available for parallelization, but keep one core free for other tasks. Some machines might freeze if all cores are busy with R jobs. \n\nn_cores = availableCores() - 1\nplan(multisession, workers = n_cores)\n\nUsing eight cores, the estimation for our sample of around 25k stocks takes around 20 minutes. Of course, you can speed up things considerably by having more cores available to share the workload or by having more powerful cores. Notice the difference in the code below? All you need to do is to replace map() with future_map(), which uses the furrr package in the background to handle the parallelization.\n\ncapm_monthly &lt;- crsp_monthly |&gt;\n  nest(data = c(date, ret_excess, mkt_excess, industry)) |&gt;\n  mutate(capm = future_map(data, roll_capm_estimation)) |&gt;\n  unnest(capm) |&gt;\n  select(permno, date, coefficient, estimate, t_statistic)\ncapm_monthly\n\n# A tibble: 4,665,570 × 5\n  permno date       coefficient estimate t_statistic\n   &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1  10001 1990-01-01 alpha         0.0111       1.38 \n2  10001 1990-01-01 mkt_excess    0.0983       0.677\n3  10001 1990-02-01 alpha         0.0106       1.34 \n4  10001 1990-02-01 mkt_excess    0.0976       0.678\n5  10001 1990-03-01 alpha         0.0105       1.35 \n# ℹ 4,665,565 more rows",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#estimating-beta-using-daily-returns",
    "href": "r/beta-estimation.html#estimating-beta-using-daily-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta Using Daily Returns",
    "text": "Estimating Beta Using Daily Returns\nBefore we provide some descriptive statistics of our beta estimates, we implement the estimation for the daily CRSP sample as well. Depending on the application, you might either use longer horizon beta estimates based on monthly data or shorter horizon estimates based on daily returns. As loading the full daily CRSP data requires relatively large amounts of memory, we split the beta estimation into smaller chunks. The logic follows the approach that we use to download the daily CRSP data (see WRDS, CRSP, and Compustat).\nFirst, we load the daily Fama-French market excess returns.\n\nfactors_ff3_daily &lt;- tbl(tidy_finance, \"factors_ff3_daily\") |&gt;\n  select(date, mkt_excess) |&gt;\n  collect()\n\nWe then create a connection to the daily CRSP data in our database, but we don’t load the whole table into our memory. We only extract all distinct permno because we loop the beta estimation over batches of stocks.\n\ncrsp_daily_db &lt;- tbl(tidy_finance, \"crsp_daily\")\n\nWe use the stocks from the monthly CRSP dataset as our reference point and process them in batches of 500. To estimate the CAPM over a consistent lookback window while accommodating different return frequencies, we adjust the minimum required number of observations accordingly. Specifically, we require at least 1,000 daily returns over a five‑year period for a valid estimation. This threshold is consistent with the monthly requirement of 48 observations out of 60 months, given that there are roughly 252 trading days in a year.\n\npermnos &lt;- crsp_daily_db |&gt;\n  distinct(permno) |&gt;\n  pull(permno)\n\nbatch_size &lt;- 500\nbatches &lt;- ceiling(length(permnos) / batch_size)\nmin_obs &lt;- 1000\n\nWe then proceed to perform the same steps as with the monthly CRSP data, just in batches: Load in daily returns, nest the data by stock, and parallelize the beta estimation across stocks.\n\ncapm_daily &lt;- list()\n\nfor (j in 1:batches) {\n  permno_batch &lt;- permnos[\n    ((j - 1) * batch_size + 1):min(j * batch_size, length(permnos))\n  ]\n\n  crsp_daily_sub &lt;- crsp_daily_db |&gt;\n    filter(permno %in% permno_batch) |&gt;\n    select(permno, date, ret_excess) |&gt;\n    collect()\n\n  crsp_daily_sub_nested &lt;- crsp_daily_sub |&gt;\n    inner_join(factors_ff3_daily, join_by(date)) |&gt;\n    mutate(date = floor_date(date, \"month\")) |&gt;\n    nest(data = c(date, ret_excess, mkt_excess))\n\n  capm_daily[[j]] &lt;- crsp_daily_sub_nested |&gt;\n    mutate(capm = future_map(\n      data, \\(x) roll_capm_estimation(x, min_obs = min_obs))\n    ) |&gt;\n    unnest(capm) |&gt;\n    select(permno, date, coefficient, estimate, t_statistic)\n\n  message(\n    \"Batch \", j, \" out of \", batches, \" done (\", percent(j / batches), \")\\n\"\n  )\n}\ncapm_daily &lt;- bind_rows(capm_daily)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#comparing-beta-estimates",
    "href": "r/beta-estimation.html#comparing-beta-estimates",
    "title": "Beta Estimation",
    "section": "Comparing Beta Estimates",
    "text": "Comparing Beta Estimates\nWhat is a typical value for stock betas? First, let us extract the relevant estimates from our CAPM results based on monthly returns.\n\nbeta_monthly &lt;- capm_monthly |&gt; \n  filter(coefficient == \"mkt_excess\") |&gt; \n  select(permno, date, beta = estimate) |&gt; \n  mutate(return_type = \"monthly\")\n\nTo get some feeling, we illustrate the dispersion of the estimated \\(\\hat\\beta_i\\) across different industries and across time below. Figure 2 shows that typical business models across industries imply different exposure to the general market economy. However, there are barely any firms that exhibit a negative exposure to the market factor.\n\ncrsp_monthly |&gt;\n  left_join(beta_monthly, join_by(permno, date)) |&gt;\n  drop_na(beta) |&gt;\n  group_by(industry, permno) |&gt;\n  summarize(beta = mean(beta), .groups = \"drop\") |&gt;\n  ggplot(aes(x = reorder(industry, beta, FUN = median), y = beta)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Firm-specific beta distributions by industry\"\n  )\n\n\n\n\n\n\n\nFigure 2: The box plots show the average firm-specific beta estimates by industry.\n\n\n\n\n\nNext, we illustrate the time-variation in the cross-section of estimated betas. Figure 3 shows the monthly deciles of estimated betas (based on monthly data) and indicates an interesting pattern: First, betas seem to vary over time in the sense that during some periods, there is a clear trend across all deciles. Second, the sample exhibits periods where the dispersion across stocks increases in the sense that the lower decile decreases and the upper decile increases, which indicates that for some stocks the correlation with the market increases while for others it decreases. Note also here: stocks with negative betas are a rare exception.\n\nbeta_monthly |&gt;\n  group_by(date) |&gt;\n  reframe(\n    x = quantile(beta, seq(0.1, 0.9, 0.1)),\n    quantile = 100 * seq(0.1, 0.9, 0.1)\n  ) |&gt;\n  ggplot(aes(\n    x = date,\n    y = x,\n    color = as_factor(quantile),\n    linetype = as_factor(quantile)\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"Monthly deciles of estimated betas\",\n  )\n\n\n\n\n\n\n\nFigure 3: Each line corresponds to the monthly cross-sectional quantile of the estimated CAPM beta.\n\n\n\n\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table.\n\nbeta_daily &lt;- capm_daily |&gt; \n  filter(coefficient == \"mkt_excess\") |&gt; \n  select(permno, date, beta = estimate) |&gt; \n  mutate(return_type = \"daily\")\n\nbeta &lt;- bind_rows(beta_monthly, beta_daily)\n\nThen, we use the table to plot a comparison of beta estimates for our example stocks in Figure 4.\n\nbeta |&gt;\n  inner_join(examples, join_by(permno)) |&gt;\n  ggplot(aes(\n    x = date,\n    y = beta,\n    color = return_type,\n    linetype = return_type\n  )) +\n  geom_line() +\n  facet_wrap(~company, ncol = 1) +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"Comparison of beta estimates using monthly and daily data\"\n  )\n\n\n\n\n\n\n\nFigure 4: CAPM betas are computed using 5 years of monthly or daily return data. The two lines show the monthly estimates based on a rolling window for few exemplary stocks.\n\n\n\n\n\nThe estimates in Figure 4 look as expected. As you can see, it really depends on the data frequency how your beta estimates turn out because the estimates based on daily data are much smoother due to the higher number of observations in each regression.\nFinally, we write the estimates to our database such that we can use them in later chapters.\n\ndbWriteTable(\n  tidy_finance,\n  \"beta\",\n  value = beta,\n  overwrite = TRUE\n)\n\nWhenever you perform some kind of estimation, it also makes sense to do rough plausibility tests. A possible check is to plot the share of stocks with beta estimates over time. This descriptive helps us discover potential errors in our data preparation or estimation procedure. For instance, suppose there was a gap in our output where we do not have any betas. In this case, we would have to go back and check all previous steps to find out what went wrong.\n\nbeta_coverage &lt;- crossing(\n    crsp_monthly,\n    tibble(return_type = c(\"monthly\", \"daily\"))\n  ) |&gt; \n  left_join(beta, join_by(permno, date, return_type)) |&gt; \n  group_by(date, return_type) |&gt;\n  summarize(share = sum(!is.na(beta)) / n(), .groups = \"drop\")\n\nbeta_coverage |&gt;\n  ggplot(\n    aes(x = date, y = share, color = return_type, linetype = return_type)\n  ) +\n  geom_line() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"End-of-month share of securities with beta estimates\"\n  ) +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\nFigure 5: The two lines show the share of securities with beta estimates using 5 years of monthly or daily return data.\n\n\n\n\n\nFigure 5 shows no issues, as the two coverage lines track each other closely, so we can proceed to the next check.\nWe also encourage everyone to always look at the distributional summary statistics of variables. You can easily spot outliers or weird distributions when looking at such tables.\n\nbeta |&gt; \n  group_by(return_type) |&gt;\n  summarize(\n    mean = mean(beta),\n    sd = sd(beta),\n    min = min(beta),\n    q05 = quantile(beta, 0.05),\n    q50 = quantile(beta, 0.50),\n    q95 = quantile(beta, 0.95),\n    max = max(beta),\n    n = n()\n  )\n\n# A tibble: 2 × 9\n  return_type  mean    sd    min    q05   q50   q95   max       n\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 daily       0.794 0.495  -3.66 0.0856 0.754  1.66  4.97 2354591\n2 monthly     1.11  0.714 -13.1  0.132  1.04   2.33 11.7  2332785\n\n\nThe summary statistics indicate that estimates based on daily returns are, on average, lower and less variable than those derived from monthly returns.\nFinally, since we have two different estimators for the same theoretical object, we expect the estimators should be at least positively correlated (although not perfectly as the estimators are based on different frequencies).\n\nbeta |&gt;\n  pivot_wider(names_from = return_type, values_from = beta) |&gt;\n  select(monthly, daily) |&gt; \n  cor(use = \"complete.obs\")\n\n        monthly daily\nmonthly   1.000 0.618\ndaily     0.618 1.000\n\n\nIndeed, we find a positive correlation between our beta estimates. In the subsequent chapters, we mainly use the estimates based on monthly data, as most readers should be able to replicate them due to potential memory limitations that might arise with the daily data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#key-takeaways",
    "href": "r/beta-estimation.html#key-takeaways",
    "title": "Beta Estimation",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nCAPM betas can be estimated using rolling-window estimation via the slider package and processed in parallel via furrr.\nBoth monthly and daily return data can be used to estimate betas with different frequencies and window lengths, depending on the application.\nSummary statistics, visualization, and plausibility checks help to validate beta estimates across time and industries.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#exercises",
    "href": "r/beta-estimation.html#exercises",
    "title": "Beta Estimation",
    "section": "Exercises",
    "text": "Exercises\n\nCompute beta estimates based on monthly data using one, three, and five years of data and impose a minimum number of observations of 10, 28, and 48 months with return data, respectively. How strongly correlated are the estimated betas?\nCompute beta estimates based on monthly data using five years of data and impose different numbers of minimum observations. How does the share of permno-date observations with successful beta estimates vary across the different requirements? Do you find a high correlation across the estimated betas?\nInstead of using future_map(), perform the beta estimation in a loop (using either monthly or daily data) for a subset of 100 permnos of your choice. Verify that you get the same results as with the parallelized code from above.\nFilter out the stocks with negative betas. Do these stocks frequently exhibit negative betas, or do they resemble estimation errors?\nCompute beta estimates for multi-factor models such as the Fama-French three-factor model by extending the estimate_capm() function with a model parameter. In particular, your regression should support the model \\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\sum\\limits_{j=1}^k\\beta_{i,k}(r_{j, t}-r_{f,t})+\\varepsilon_{i, t}\n\\tag{1}\\] where \\(r_{i, t}\\) are the \\(k\\) factor returns. Thus, you estimate four parameters (\\(\\alpha_i\\) and the slope coefficients). Provide some summary statistics of the cross-section of firms and their exposure to the different factors.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/proofs.html",
    "href": "r/proofs.html",
    "title": "Proofs",
    "section": "",
    "text": "The minimum variance portfolio weights are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega= 1,\\] where \\(\\iota\\) is an \\((N \\times 1)\\) vector of ones. The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1).\\] We can solve the first-order conditions of the Lagrangian equation: \\[\n\\begin{aligned}\n& \\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega} = 0 \\Leftrightarrow 2\\Sigma \\omega = \\lambda\\iota \\Rightarrow \\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota \\\\ \\end{aligned}\n\\] Next, the constraint that weights have to sum up to one delivers: \\(1 = \\iota'\\omega = \\frac{\\lambda}{2}\\iota'\\Sigma^{-1}\\iota \\Rightarrow \\lambda = \\frac{2}{\\iota'\\Sigma^{-1}\\iota}.\\) Finally, plug-in the derived value of \\(\\lambda\\) to get \\[\n\\begin{aligned}\n\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}.\n\\end{aligned}\n\\]\n\n\n\nConsider an investor who aims to achieve minimum variance given a desired expected return \\(\\bar{\\mu}\\), that is: \\[\\omega_\\text{eff}\\left(\\bar{\\mu}\\right) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1) - \\tilde{\\lambda}(\\omega'\\mu - \\bar{\\mu}). \\] We can solve the first-order conditions to get \\[\n\\begin{aligned}\n2\\Sigma \\omega &= \\lambda\\iota + \\tilde\\lambda \\mu\\\\\n\\Rightarrow\\omega &= \\frac{\\lambda}{2}\\Sigma^{-1}\\iota + \\frac{\\tilde\\lambda}{2}\\Sigma^{-1}\\mu.\n\\end{aligned}\n\\]\nNext, the two constraints (\\(w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}\\)) imply \\[\n\\begin{aligned}\n1 &= \\iota'\\omega = \\frac{\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\iota}_{C} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\mu}_D\\\\\n\\Rightarrow \\lambda&= \\frac{2 - \\tilde\\lambda D}{C}\\\\\n\\bar\\mu &= \\mu'\\omega = \\frac{\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\iota}_{D} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\mu}_E = \\frac{1}{2}\\left(\\frac{2 - \\tilde\\lambda D}{C}\\right)D+\\frac{\\tilde\\lambda}{2}E  \\\\&=\\frac{D}{C}+\\frac{\\tilde\\lambda}{2}\\left(E - \\frac{D^2}{C}\\right)\\\\\n\\Rightarrow \\tilde\\lambda &= 2\\frac{\\bar\\mu - D/C}{E-D^2/C}.\n\\end{aligned}\n\\] As a result, the efficient portfolio weight takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right).\\] Thus, the efficient portfolio allocates wealth in the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and a levered (self-financing) portfolio to increase the expected return.\nNote that the portfolio weights sum up to one as \\[\\iota'\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) = D - D = 0\\text{ so }\\iota'\\omega_\\text{eff} = \\iota'\\omega_\\text{mvp} = 1.\\] Finally, the expected return of the efficient portfolio is \\[\\mu'\\omega_\\text{eff} = \\frac{D}{C} + \\bar\\mu - \\frac{D}{C} = \\bar\\mu.\\]\n\n\n\nWe argue that an investor with a quadratic utility function with certainty equivalent \\[\\max_\\omega CE(\\omega) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen with the aim to minimize volatility given a pre-specified level or expected returns \\[\\min_\\omega \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1.\\] Note the difference: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines their optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return they want to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk-averse investors (higher \\(\\gamma\\)) will chose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nTo proof this equivalence, consider first the optimal portfolio weights for a certainty equivalent maximizing investor. The first-order condition reads \\[\n\\begin{aligned}\n\\mu - \\lambda \\iota &= \\gamma \\Sigma \\omega \\\\\n\\Leftrightarrow \\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\lambda\\iota\\right)\n\\end{aligned}\n\\] Next, we make use of the constraint \\(\\iota'\\omega = 1\\). \\[\n\\begin{aligned}\n\\iota'\\omega &= 1 = \\frac{1}{\\gamma}\\left(\\iota'\\Sigma^{-1}\\mu - \\lambda\\iota'\\Sigma^{-1}\\iota\\right)\\\\\n\\Rightarrow \\lambda &= \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right).\n\\end{aligned}\n\\] Plugging in the value of \\(\\lambda\\) reveals the desired portfolio for an investor with risk aversion \\(\\gamma\\). \\[\n\\begin{aligned}\n\\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right)\\right) \\\\\n\\Rightarrow \\omega &= \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\iota'\\Sigma^{-1}\\right)\\mu\\\\\n&= \\omega_\\text{mvp} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1}\\mu - \\frac{\\iota'\\Sigma^{-1}\\mu}{\\iota'\\Sigma^{-1}\\iota}\\Sigma^{-1}\\iota\\right).\n\\end{aligned}\n\\] The resulting weights correspond to the efficient portfolio with desired return \\(\\bar r\\) such that (in the notation of book) \\[\\frac{1}{\\gamma} = \\frac{\\tilde\\lambda}{2} = \\frac{\\bar\\mu - D/C}{E - D^2/C}\\] which implies that the desired return is just \\[\\bar\\mu = \\frac{D}{C} + \\frac{1}{\\gamma}\\left({E - D^2/C}\\right)\\] which is \\(\\frac{D}{C} = \\mu'\\omega_\\text{mvp}\\) for \\(\\gamma\\rightarrow \\infty\\) as expected. For instance, letting \\(\\gamma \\rightarrow \\infty\\) implies \\(\\bar\\mu = \\frac{D}{C} = \\omega_\\text{mvp}'\\mu\\).",
    "crumbs": [
      "R",
      "Appendix",
      "Proofs"
    ]
  },
  {
    "objectID": "r/proofs.html#optimal-portfolio-choice",
    "href": "r/proofs.html#optimal-portfolio-choice",
    "title": "Proofs",
    "section": "",
    "text": "The minimum variance portfolio weights are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega= 1,\\] where \\(\\iota\\) is an \\((N \\times 1)\\) vector of ones. The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1).\\] We can solve the first-order conditions of the Lagrangian equation: \\[\n\\begin{aligned}\n& \\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega} = 0 \\Leftrightarrow 2\\Sigma \\omega = \\lambda\\iota \\Rightarrow \\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota \\\\ \\end{aligned}\n\\] Next, the constraint that weights have to sum up to one delivers: \\(1 = \\iota'\\omega = \\frac{\\lambda}{2}\\iota'\\Sigma^{-1}\\iota \\Rightarrow \\lambda = \\frac{2}{\\iota'\\Sigma^{-1}\\iota}.\\) Finally, plug-in the derived value of \\(\\lambda\\) to get \\[\n\\begin{aligned}\n\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}.\n\\end{aligned}\n\\]\n\n\n\nConsider an investor who aims to achieve minimum variance given a desired expected return \\(\\bar{\\mu}\\), that is: \\[\\omega_\\text{eff}\\left(\\bar{\\mu}\\right) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1) - \\tilde{\\lambda}(\\omega'\\mu - \\bar{\\mu}). \\] We can solve the first-order conditions to get \\[\n\\begin{aligned}\n2\\Sigma \\omega &= \\lambda\\iota + \\tilde\\lambda \\mu\\\\\n\\Rightarrow\\omega &= \\frac{\\lambda}{2}\\Sigma^{-1}\\iota + \\frac{\\tilde\\lambda}{2}\\Sigma^{-1}\\mu.\n\\end{aligned}\n\\]\nNext, the two constraints (\\(w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}\\)) imply \\[\n\\begin{aligned}\n1 &= \\iota'\\omega = \\frac{\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\iota}_{C} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\mu}_D\\\\\n\\Rightarrow \\lambda&= \\frac{2 - \\tilde\\lambda D}{C}\\\\\n\\bar\\mu &= \\mu'\\omega = \\frac{\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\iota}_{D} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\mu}_E = \\frac{1}{2}\\left(\\frac{2 - \\tilde\\lambda D}{C}\\right)D+\\frac{\\tilde\\lambda}{2}E  \\\\&=\\frac{D}{C}+\\frac{\\tilde\\lambda}{2}\\left(E - \\frac{D^2}{C}\\right)\\\\\n\\Rightarrow \\tilde\\lambda &= 2\\frac{\\bar\\mu - D/C}{E-D^2/C}.\n\\end{aligned}\n\\] As a result, the efficient portfolio weight takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right).\\] Thus, the efficient portfolio allocates wealth in the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and a levered (self-financing) portfolio to increase the expected return.\nNote that the portfolio weights sum up to one as \\[\\iota'\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) = D - D = 0\\text{ so }\\iota'\\omega_\\text{eff} = \\iota'\\omega_\\text{mvp} = 1.\\] Finally, the expected return of the efficient portfolio is \\[\\mu'\\omega_\\text{eff} = \\frac{D}{C} + \\bar\\mu - \\frac{D}{C} = \\bar\\mu.\\]\n\n\n\nWe argue that an investor with a quadratic utility function with certainty equivalent \\[\\max_\\omega CE(\\omega) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen with the aim to minimize volatility given a pre-specified level or expected returns \\[\\min_\\omega \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1.\\] Note the difference: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines their optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return they want to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk-averse investors (higher \\(\\gamma\\)) will chose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nTo proof this equivalence, consider first the optimal portfolio weights for a certainty equivalent maximizing investor. The first-order condition reads \\[\n\\begin{aligned}\n\\mu - \\lambda \\iota &= \\gamma \\Sigma \\omega \\\\\n\\Leftrightarrow \\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\lambda\\iota\\right)\n\\end{aligned}\n\\] Next, we make use of the constraint \\(\\iota'\\omega = 1\\). \\[\n\\begin{aligned}\n\\iota'\\omega &= 1 = \\frac{1}{\\gamma}\\left(\\iota'\\Sigma^{-1}\\mu - \\lambda\\iota'\\Sigma^{-1}\\iota\\right)\\\\\n\\Rightarrow \\lambda &= \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right).\n\\end{aligned}\n\\] Plugging in the value of \\(\\lambda\\) reveals the desired portfolio for an investor with risk aversion \\(\\gamma\\). \\[\n\\begin{aligned}\n\\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right)\\right) \\\\\n\\Rightarrow \\omega &= \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\iota'\\Sigma^{-1}\\right)\\mu\\\\\n&= \\omega_\\text{mvp} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1}\\mu - \\frac{\\iota'\\Sigma^{-1}\\mu}{\\iota'\\Sigma^{-1}\\iota}\\Sigma^{-1}\\iota\\right).\n\\end{aligned}\n\\] The resulting weights correspond to the efficient portfolio with desired return \\(\\bar r\\) such that (in the notation of book) \\[\\frac{1}{\\gamma} = \\frac{\\tilde\\lambda}{2} = \\frac{\\bar\\mu - D/C}{E - D^2/C}\\] which implies that the desired return is just \\[\\bar\\mu = \\frac{D}{C} + \\frac{1}{\\gamma}\\left({E - D^2/C}\\right)\\] which is \\(\\frac{D}{C} = \\mu'\\omega_\\text{mvp}\\) for \\(\\gamma\\rightarrow \\infty\\) as expected. For instance, letting \\(\\gamma \\rightarrow \\infty\\) implies \\(\\bar\\mu = \\frac{D}{C} = \\omega_\\text{mvp}'\\mu\\).",
    "crumbs": [
      "R",
      "Appendix",
      "Proofs"
    ]
  },
  {
    "objectID": "r/modern-portfolio-theory.html",
    "href": "r/modern-portfolio-theory.html",
    "title": "Modern Portfolio Theory",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn the previous chapter, we show how to download and analyze stock market data with figures and summary statistics. Now, we move to a typical question in finance: How should an investor allocate their wealth across assets that differ in expected returns, variance, and correlations to optimize their portfolio’s performance? Modern Portfolio Theory (MPT), introduced by Markowitz (1952), revolutionized the way how we think about such investment decisions by formalizing the trade-off between risk and expected return. Markowitz’s framework laid the foundation for much of modern finance, also earning him the Sveriges Riksbank Prize in Economic Sciences in 1990.\nMPT relies on the fact that portfolio risk depends on individual asset volatilities as well as on the correlations between asset returns. This insight highlights the power of diversification: Combining assets with low or negative correlations with a given portfolio reduces the overall portfolio risk. This principle is often illustrated with the analogy of a fruit basket: If all you have are apples and they spoil, you lose everything. With a variety of fruits, some fruits may spoil, but others will stay fresh.\nAt the heart of MPT is mean-variance analysis, which evaluates portfolios based on two dimensions: expected return and risk, defined as the variance of the portfolio returns. By balancing these two components, investors can construct portfolios that either maximize their expected return for a given level of risk or minimize their taken risk for a desired level of return. In this chapter, we first derive the optimal portfolio decisions and implement the mean-variance approach in R.\nWe use the following packages throughout this chapter:\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(scales)\nlibrary(ggrepel)\nWe introduce the ggrepel package for adding text labels to the figures in this chapter (Slowikowski 2024).",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "r/modern-portfolio-theory.html#the-asset-universe",
    "href": "r/modern-portfolio-theory.html#the-asset-universe",
    "title": "Modern Portfolio Theory",
    "section": "The Asset Universe",
    "text": "The Asset Universe\nSuppose that \\(N\\) different risky assets are available to the investor. Each asset \\(i\\) delivers expected returns \\(\\mu_i\\), representing the anticipated profit from holding the asset for one period. The investor can allocate their wealth across these assets by choosing the portfolio weights \\(\\omega_i\\) for each asset \\(i\\). We impose that the portfolio weights sum up to one to ensure that the investor is fully invested. There is no outside option, such as keeping your money under a mattress. The overarching question of this chapter is: How should the investor allocate their wealth across these assets to optimize their portfolio’s performance?\nAccording to Markowitz (1952), portfolio selection involves two stages: First, forming expectations about future security performance based on observations and experience. Second, using these expectations to choose a portfolio. In practice, these two steps cannot be separated. You need historical data or other considerations to generate estimates of the distribution of future returns. Only then can one proceed proceed to optimal decision-making conditional on your estimation.\nTo keep things conceptually simple, we focus on the latter part for now and assume that the actual distribution of the asset returns is known. In later chapters, we discuss the issues that arise once we take estimation uncertainty into account. To provide some meaningful illustrations, we rely on historical data to compute reasonable proxies for the expected returns and the variance-covariance of the assets returns, but we will work under the assumption that these are the true parameters of the return distribution.\nThus, leveraging the approach introduced in Working with Stock Returns, we download the constituents of the Dow Jones Industrial Average as an example portfolio as well as their daily adjusted close prices.\n\nsymbols &lt;- download_data(\n  type = \"constituents\",\n  index = \"Dow Jones Industrial Average\"\n)\n\nprices_daily &lt;- download_data(\n  type = \"stock_prices\", \n  symbol = symbols$symbol,\n  start_date = \"2000-01-01\",\n  end_date = \"2024-12-31\"\n)\n\nTo have a stable stock universe and to keep the analysis simple, we ensure that all stocks were traded over the whole sample period:\n\nprices_daily &lt;- prices_daily |&gt;\n  group_by(symbol) |&gt;\n  mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  filter(n == max(n)) |&gt;\n  select(-n)\n\nWe compute the sample average returns as \\(\\frac{1}{T} \\sum_{t=1}^{T} r_{i,t},\\) where \\(r_{i,t}\\) is the return of asset \\(i\\) in period \\(t\\), and \\(T\\) is the total number of periods. As noted above, we treat the vector of sample averages as the true expected returns of the assets. For simplicity and easier interpretation, we focus on monthly returns going forward.\n\nreturns_monthly &lt;- prices_daily |&gt;\n  mutate(date = floor_date(date, \"month\")) |&gt;\n  group_by(symbol, date) |&gt;\n  summarize(price = last(adjusted_close), .groups = \"drop_last\") |&gt;\n  mutate(ret = price / lag(price) - 1) |&gt;\n  drop_na(ret) |&gt;\n  select(-price)\n\nIndividual asset risk in MPT is typically quantified using variance (i.e., \\(\\sigma^2_i\\)) or volatilities (i.e., \\(\\sigma_i\\)).1 We suppose that the true volatilities of the assets are also given by the sample standard deviation.\nWe compute the sample standard deviation for each asset by using the sd() function.\n\nassets &lt;- returns_monthly |&gt; \n  group_by(symbol) |&gt; \n  summarize(\n    mu = mean(ret),\n    sigma = sd(ret)\n  )\n\nWe can illustrate the resulting distribution of the asset returns in Figure 1, showing the volatility on the horizontal axis and the expected return on the vertical axis.\n\nassets |&gt; \n  ggplot(aes(x = sigma, y = mu, label = symbol)) +\n  geom_point() +\n  geom_text_repel() +\n  scale_x_continuous(labels = percent) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Volatility\", y = \"Expected return\",\n    title = \"Expected returns and volatilities of Dow Jones index constituents\"\n  )\n\n\n\n\n\n\n\nFigure 1: Expected returns and volatilities based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nAs highlighted above, a key innovation of MPT is to consider interactions between assets. The variance-covariance matrix collects this information. Again, we proxy the true variance-covariance matrix \\(\\Sigma\\) of the returns by the sample covariance.\nThe interpretation of the covariance is straightforward: While a positive covariance between assets indicates that these assets tend to move in the same direction, a negative covariance indicates that the assets move in opposite directions.\nWe can use the cov() function that takes a matrix of returns as inputs. We thus need to transform the returns from a data frame into a \\((T \\times N)\\) matrix with one column for each of the \\(N\\) symbols and one row for each of the \\(T\\) trading days. We achieve this by using pivot_wider() with the new column names from the symbol-column and setting the values to ret.\n\nreturns_wide &lt;- returns_monthly |&gt; \n  pivot_wider(names_from = symbol, values_from = ret) \n\nsigma &lt;- returns_wide |&gt; \n  select(-date) |&gt; \n  cov()\n\nFigure Figure 2 illustrates the resulting variance-covariance matrix.\n\nsigma |&gt; \n  as_tibble(rownames = \"symbol_a\") |&gt; \n  pivot_longer(-symbol_a, names_to = \"symbol_b\") |&gt; \n  ggplot(aes(x = symbol_a, y = fct_rev(symbol_b), fill = value)) +\n  geom_tile() +\n  labs(\n    x = NULL, y = NULL, fill = \"(Co-)Variance\",\n    title = \"Sample Variance-covariance matrix of Dow Jones index constituents\"\n  ) + \n  scale_fill_continuous(labels = percent)  +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  guides(fill = guide_colorbar(barwidth = 15, barheight = 0.5))\n\n\n\n\n\n\n\nFigure 2: Variances and covariances based on monthly returns adjusted for dividend payments and stock splits.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "r/modern-portfolio-theory.html#the-minimum-variance-framework",
    "href": "r/modern-portfolio-theory.html#the-minimum-variance-framework",
    "title": "Modern Portfolio Theory",
    "section": "The Minimum-Variance Framework",
    "text": "The Minimum-Variance Framework\nSuppose now the investor allocates their wealth in a portfolio given by the weight vector \\(\\omega\\). The resulting portfolio returns \\(\\omega^\\prime r\\) have an expected return \\(\\mu_\\omega = \\omega^{\\prime}\\mu = \\sum_{i=1}^N \\omega_i \\mu_i\\). The variance of the portfolio returns is \\(\\sigma^2_\\omega = \\omega^{\\prime}\\Sigma\\omega = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\omega_i \\omega_j \\sigma_{ij}\\) where \\(\\omega_i\\) and \\(\\omega_j\\) are the weights of assets \\(i\\) and \\(j\\) in the portfolio, respectively, and \\(\\sigma_{ij}\\) is the covariance between returns of assets \\(i\\) and \\(j\\).\nWe first consider an investor who wants to invest in a portfolio that delivers the lowest possible variance as a reference point. Thus, the optimization problem of the minimum-variance investor is given by\n\\[\\min_{\\omega_1, ... \\omega_n} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\omega_i \\omega_j \\sigma_{ij} = \\min_\\omega \\omega^{\\prime}\\Sigma\\omega.\\]\nWhile staying fully invested across all available assets \\(N\\), \\(\\sum_{i=1}^{N} \\omega_i = 1\\). The analytical solution for the minimum-variance portfolio is\n\\[\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota^{\\prime}\\Sigma^{-1}\\iota},\\]\nwhere \\(\\iota\\) is a vector of 1’s and \\(\\Sigma^{-1}\\) is the inverse of the variance-covariance matrix \\(\\Sigma\\). See Proofs in the Appendix for details on the derivation. In the following code chunk, we calculate the weights of the minimum-variance portfolio:\n\niota &lt;- rep(1, dim(sigma)[1])\nsigma_inv &lt;- solve(sigma)\nomega_mvp &lt;- as.vector(sigma_inv %*% iota) / \n  as.numeric(t(iota) %*% sigma_inv %*% iota)\n\nFigure Figure 3 shows the resulting portfolio weights.\n\nassets &lt;- bind_cols(assets, omega_mvp = omega_mvp)\n\nassets |&gt;\n  ggplot(aes(x = omega_mvp, y = fct_reorder(symbol, omega_mvp), \n             fill = omega_mvp &gt; 0)) +\n  geom_col() +\n  scale_x_continuous(labels = percent) + \n  labs(x = NULL, y = NULL, \n       title = \"Minimum-variance portfolio weights\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 3: Weights are based on historical moments of monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nBefore we move on to other portfolios, we collect the return and volatility of the minimum-variance portfolio in a data frame:\n\nmu &lt;- assets$mu\n\nsummary_mvp &lt;- tibble(\n  mu = as.numeric(t(omega_mvp) %*% mu),\n  sigma = as.numeric(sqrt(t(omega_mvp) %*% sigma %*% omega_mvp)),\n  type = \"Minimum-Variance Portfolio\"\n)\nsummary_mvp\n\n# A tibble: 1 × 3\n       mu  sigma type                      \n    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     \n1 0.00875 0.0324 Minimum-Variance Portfolio",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "r/modern-portfolio-theory.html#efficient-portfolios",
    "href": "r/modern-portfolio-theory.html#efficient-portfolios",
    "title": "Modern Portfolio Theory",
    "section": "Efficient Portfolios",
    "text": "Efficient Portfolios\nIn many instances, earning the lowest possible variance may not be the desired outcome. Instead, we generalize the concept of efficient portfolios, where, in addition to minimizing portfolio variance, the investor aims to earn a minimum expected return \\(\\omega^{\\prime}\\mu \\geq \\bar{\\mu}.\\) In other words, when \\(\\bar\\mu\\geq \\omega_\\text{mvp}^{\\prime}\\mu\\), the investor is willing to accept a higher portfolio variance in return for earning a higher expected return.\nSuppose, for instance, the investor wants to earn at least the historical average return of the asset that delivered the highest average returns in the past:\n\nmu_bar &lt;- max(assets$mu)\n\nFormally, the optimization problem is given by\n\\[\\min_\\omega \\omega^{\\prime}\\Sigma\\omega \\text{ s.t. } \\omega^{\\prime}\\iota = 1 \\text{ and } \\omega^{\\prime}\\mu\\geq\\bar\\mu.\\]\nThe analytic solution for the efficient portfolio can be derived as:\n\\[\\omega_{efp} = \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right),\\]\nwhere \\(\\lambda^* = 2\\frac{\\bar\\mu - D/C}{E-D^2/C}\\), \\(C = \\iota'\\Sigma^{-1}\\iota\\), \\(D=\\iota'\\Sigma^{-1}\\mu\\), and \\(E=\\mu'\\Sigma^{-1}\\mu\\). For details, we again refer to the Proofs in the Appendix.\nThe code below implements the analytic solution to this optimization problem and collects the resulting portfolio return and risk in a data frame.\n\nC &lt;- as.numeric(t(iota) %*% sigma_inv %*% iota)\nD &lt;- as.numeric(t(iota) %*% sigma_inv %*% mu)\nE &lt;- as.numeric(t(mu) %*% sigma_inv %*% mu)\nlambda_tilde &lt;- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nomega_efp &lt;- as.vector(omega_mvp + lambda_tilde / 2 * (sigma_inv %*% mu - D * omega_mvp))\n\nsummary_efp &lt;- tibble(\n  mu = as.numeric(t(omega_efp) %*% mu),\n  sigma = as.numeric(sqrt(t(omega_efp) %*% sigma %*% omega_efp)),\n  type = \"Efficient Portfolio\"\n)\n\nFigure Figure 4 shows the average return and volatility of the minimum-variance and the efficient portfolio relative to the index constituents. As expected, the efficient portfolio has a higher expected return at the cost of higher volatility compared to the minimum-variance portfolio.\n\nsummaries &lt;- bind_rows(\n  assets, summary_mvp, summary_efp\n) \n\nsummaries |&gt; \n  ggplot(aes(x = sigma, y = mu)) +\n  geom_point(\n    data = summaries |&gt; filter(is.na(type))\n  ) +\n  geom_point(\n    data = summaries |&gt; filter(!is.na(type)), color = \"#F21A00\", size = 3\n  ) +\n  geom_label_repel(aes(label = type)) +\n  scale_x_continuous(labels = percent) +\n  scale_y_continuous(labels = percent) + \n  labs(\n    x = \"Volatility\", y = \"Expected return\",\n    title = \"Efficient & minimum-variance portfolios\"\n  ) \n\n\n\n\n\n\n\nFigure 4: The big dots indicate the location of the minimum-variance and the efficient portfolio that delivers the expected return of the stock with the higehst return, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe figure illustrates the substantial diversification benefits: Instead of allocating all wealth into one asset that delivered a high average return in the past (at a substantial volatility), the efficient portfolio promises exactly the same expected returns but at a much lower volatility.\nIt should be noted that the level of desired returns \\(\\bar\\mu\\) reflects the risk-aversion of the investor. Less risk-averse investors may require a higher level of desired returns. In contrast, more risk-averse investors may only choose \\(\\bar\\mu\\) closer to the expected return of the minimum-variance portfolio. Very often, the mean-variance framework is instead derived as the optimal decision framework of an investor with a mean-variance utility function with a coefficient of relative risk aversion \\(\\gamma\\). In the Proofs in the Appendix, we show that there is a one-to-one mapping from \\(\\gamma\\) to the desired level of expected returns \\(\\bar\\mu\\), which implies that the resulting efficient portfolios are equivalent and do not depend on the way the optimization problem is formulated.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "r/modern-portfolio-theory.html#the-efficient-frontier",
    "href": "r/modern-portfolio-theory.html#the-efficient-frontier",
    "title": "Modern Portfolio Theory",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n The set of portfolios that satisfies the condition that no other portfolio exists with a higher expected return for a given level of volatility is called the efficient frontier, see, e.g., Merton (1972). . To derive the portfolios that span the efficient frontier, the mutual fund separation theorem proves very helpful. In short, the theorem states that as soon as we have two efficient portfolios (such as the minimum-variance portfolio \\(\\omega_\\text{mvp}\\) and the efficient portfolio for a higher required level of expected returns \\(\\omega_\\bar\\mu\\), we can characterize the entire efficient frontier by combining these two portfolios. That is, any linear combination of the two portfolio weights will again represent an efficient portfolio. In other words, the efficient frontier can be characterized by the following equation:\n\\[\\omega_{a\\mu_1 + (1-a)\\mu_2} = a \\cdot \\omega_{\\mu_1} + (1-a) \\cdot\\omega_{\\mu_2},\\]\nwhere \\(a\\) is a scalar between 0 and 1, \\(\\omega_{\\mu_i}\\) is an efficient portfolio that delivers the expected return \\(\\mu_i\\). It is straightforward to prove the theorem. Consider the analytical solution for the efficient portfolio, which delivers expected returns \\(\\mu_i\\), implying:\n\\[a \\cdot \\omega_{\\mu_1} + (1-a) \\cdot\\omega_{\\mu_2} = \\left(\\frac{a\\mu_1 + (1-a)\\mu_2- D/C }{E-D^2/C}\\right)\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right),\\]\nwhich corresponds to the efficient portfolio earning \\(a\\mu_1 + (1-a)\\mu_2\\) in expectation.\nThe code below implements the construction of this efficient frontier, which characterizes the highest expected return achievable at each level of risk.\n\nefficient_frontier &lt;- tibble(\n  a = seq(from = -1, to = 2, by = 0.01),\n) |&gt; \n  mutate(\n    omega = map(a, \\(x) x * omega_efp + (1 - x) * omega_mvp),\n    mu = map_dbl(omega, \\(x) t(x) %*% mu),\n    sigma = map_dbl(omega, \\(x) sqrt(t(x) %*% sigma %*% x)),\n  ) \n\nFinally, it is simple to visualize the efficient frontier alongside the two efficient portfolios in a figure using ggplot (see Figure 5). We also add the individual stocks in the same plot.\n\nsummaries &lt;- bind_rows(\n    summaries, efficient_frontier\n  )\n\nsummaries |&gt; \n  ggplot(aes(x = sigma, y = mu)) +\n  geom_point(\n    data = summaries |&gt; filter(is.na(type))\n  ) +\n  geom_point(\n    data = summaries |&gt; filter(!is.na(type)), color = \"#F21A00\", size = 3\n  ) +\n  geom_label_repel(aes(label = type)) +\n  scale_x_continuous(labels = percent) +\n  scale_y_continuous(labels = percent) + \n  labs(x = \"Volatility\", y = \"Expected return\",\n       title = \"Efficient frontier from historical moments of Dow Jones index constituents\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 5: The big dots indicate the location of the minimum-variance and the efficient portfolio that delivers three times the expected return of the minimum-variance portfolio, respectively. The small dots indicate the location of the individual constituents.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "r/modern-portfolio-theory.html#key-takeaways",
    "href": "r/modern-portfolio-theory.html#key-takeaways",
    "title": "Modern Portfolio Theory",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nModern Portfolio Theory provides a an intuitive framework to allocate capital by balancing expected return against risk.\nMean-variance optimization allows investors to construct portfolios that either minimize risk or maximize return based on historical return and volatility data.\nPortfolio risk is not only determined by individual asset volatility but also by the correlation between assets, which highlights the value of diversification.\nThe minimum-variance portfolio identifies the asset allocation that yields the lowest possible volatility while remaining fully invested.\nEfficient portfolios are those that deliver the highest expected return for a given level of risk.\nThe efficient frontier visually represents the set of optimal portfolios, helping investors understand the trade-off between risk and return.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "r/modern-portfolio-theory.html#exercises",
    "href": "r/modern-portfolio-theory.html#exercises",
    "title": "Modern Portfolio Theory",
    "section": "Exercises",
    "text": "Exercises\n\nWe restricted our sample to all assets trading every day since 2000-01-01. Discuss why this restriction might introduce survivorship bias and how it could affect inferences about future expected portfolio performance.\nThe efficient frontier characterizes portfolios with the highest expected return for different levels of risk. Identify the portfolio with the highest expected return per unit of standard deviation (risk). Which famous performance measure corresponds to the ratio of average returns to standard deviation?\nAnalyze how varying the correlation coefficients between asset returns influences the shape of the efficient frontier. Use hypothetical data for a small number of assets to visualize and interpret these changes.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "r/modern-portfolio-theory.html#footnotes",
    "href": "r/modern-portfolio-theory.html#footnotes",
    "title": "Modern Portfolio Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlternative approaches include value-at-risk, expected shortfall, or higher-order moments such as skewness and kurtosis.↩︎",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "r/capital-asset-pricing-model.html",
    "href": "r/capital-asset-pricing-model.html",
    "title": "The Capital Asset Pricing Model",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nThe Capital Asset Pricing Model (CAPM) is one of the most influential theories in finance and builds on the foundation laid by Modern Portfolio Theory (MPT). It was simultaneously developed by Sharpe (1964), Lintner (1965), and Mossin (1966). While MPT shows how to construct efficient portfolios, the CAPM extends this framework to explain how assets should be priced in equilibrium when all investors follow MPT principles. The CAPM is the simplest model that aims to explain equilibrium asset prices and hence the cornerstone for a myriad of extensions. In this chapter, we derive the CAPM, illustrate the theoretical underpinnings and show how to estimate the coefficients of the CAPM. For this final purpose, we download stock market data, estimate betas using regression analysis, and evaluate asset performance.\nWe use the following packages throughout this chapter:\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(scales)\nlibrary(ggrepel)",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "r/capital-asset-pricing-model.html#asset-returns-volatilities",
    "href": "r/capital-asset-pricing-model.html#asset-returns-volatilities",
    "title": "The Capital Asset Pricing Model",
    "section": "Asset Returns & Volatilities",
    "text": "Asset Returns & Volatilities\nBuilding on our analysis from the previous chapter on Modern Portfolio Theory, we again examine the Dow Jones Industrial Average constituents as an exemplary asset universe. We download and prepare our monthly return data:\n\nsymbols &lt;- download_data(\n  type = \"constituents\",\n  index = \"Dow Jones Industrial Average\"\n)\n\nprices_daily &lt;- download_data(\n    type = \"stock_prices\", symbol = symbols$symbol,\n    start_date = \"2000-01-01\", end_date = \"2024-12-31\"\n) |&gt; \n  select(symbol, date, adjusted_close)\n\nprices_daily &lt;- prices_daily |&gt;\n  group_by(symbol) |&gt;\n  mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  filter(n == max(n)) |&gt;\n  select(-n)\n\nreturns_monthly &lt;- prices_daily |&gt;\n  mutate(date = floor_date(date, \"month\")) |&gt;\n  group_by(symbol, date) |&gt;\n  summarize(price = last(adjusted_close), .groups = \"drop_last\") |&gt;\n  mutate(ret = price / lag(price) - 1) |&gt;\n  drop_na(ret) |&gt;\n  select(-price)\n\nThe relationship between risk and return is central to the CAPM. Intuitively, one may expect that assets with higher volatility should also deliver higher expected returns. Instead, the CAPM’s key insight states that not all risks are rewarded in equilibrium. Only so-called systematic risk of an asset will determine the assets expected return. To understand this, we need to distinguish between systematic and idiosyncratic risk.\nCompany-specific events might affect individual stock prices, e.g., CEO resignations, product launches, and earnings reports. These idiosyncratic events don’t necessarily impact the overall market and this asset-specific risk can be eliminated through diversification. Therefore, we call this risk idiosyncratic. Systematic risk, on the other hand, affects all assets in the market at the same time and investors really dislike it because it cannot be diversified away.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "r/capital-asset-pricing-model.html#portfolio-return-variance",
    "href": "r/capital-asset-pricing-model.html#portfolio-return-variance",
    "title": "The Capital Asset Pricing Model",
    "section": "Portfolio Return & Variance",
    "text": "Portfolio Return & Variance\nWhile we covered efficient portfolio choice in detail in the previous chapter, CAPM relies on the introduction of a crucial new element: a risk-free asset. This additional asset fundamentally changes the investment opportunity set and leads to powerful conclusions about efficient portfolio choice.\nTo recap, we considered a portfolio weight vector \\(\\omega\\in\\mathbb{R}^N\\) which denotes investments into the available \\(N\\) risky assets. So far we assumed \\(\\sum_i^N \\omega_i=1\\), indicating that all available wealth is invested across the asset universe without any outside option. Now, we relax this assumption and instead assume that all the remaining wealth, \\(1-\\iota'\\omega\\), is invested in a risk-free asset which pays a constant interest \\(r_f\\). The expected portfolio return for the portfolio of risky assets \\(\\omega\\) is then\n\\[\\mu_\\omega = \\omega^{\\prime}\\mu + (1-\\iota^{\\prime}\\omega)r_f = r_f + \\omega^{\\prime}\\underbrace{(\\mu-r_f)}_{\\tilde\\mu}.\\]\nwhere \\(\\mu\\) is the the vector of expected return of assets and \\(r_f\\) is the risk-free rate. In what follows, we refer to \\(\\tilde\\mu\\) as the vector of expected excess returns.\nBy assumption, the risk-free asset has zero volatility. Therefore, the volatility of the portfolio is given by\n\\[\\sigma_\\omega = \\sqrt{\\omega^{\\prime}\\Sigma\\omega}\\]\nwhere \\(\\Sigma\\) is the variance-covariance matrix of the returns. Restating the optimal decision problem of an investor who wants to earn a desired level of expected portfolio (excess) returns (\\(\\bar\\mu-r_f\\)) with the lowest possible variance leads to the optimization problem\n\\[\\min_\\omega Z(\\omega) = \\min_\\omega \\omega^{\\prime}\\Sigma\\omega - \\lambda \\left(\\omega^{\\prime}\\tilde\\mu-\\bar\\mu\\right).\\]\nThe first-order conditions for this optimization problem yield:\n\\[\\frac{\\partial Z}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda \\tilde\\mu = 0 \\\\\\Leftrightarrow \\omega^* = \\frac{\\lambda}{2}\\Sigma^{-1}\\tilde\\mu\\]\nThe constraint \\(\\omega^{\\prime}\\tilde\\mu_\\omega\\geq \\bar\\mu\\) delivers\n\\[\\bar\\mu = \\tilde\\mu^{\\prime}\\omega^* = \\frac{\\lambda}{2}\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu \\\\\n\\Rightarrow \\lambda = \\frac{2\\bar\\mu}{\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu}.\\] Thus, the optimal portfolio weights are given by \\[\\omega^* = \\frac{\\bar\\mu}{\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu}\\Sigma^{-1}\\tilde\\mu.\\]\nNote that \\(\\omega^*\\) does not necessarily sum up to one as, mechanically, the remainder \\(1-\\iota^{\\prime}\\omega^*\\) is invested in the risk-free asset. However, scaling \\(\\omega^*\\) delivers the portfolio of risky assets \\(\\omega_\\text{tan}\\) such that\n\\[\\omega_\\text{tan} := \\frac{\\omega^*}{\\iota'\\omega^*}= \\frac{\\Sigma^{-1}(\\mu-r_f)}{\\iota^{\\prime}\\Sigma^{-1}(\\mu-r_f)}.\\]\nImportantly, \\(\\omega_\\text{tan}\\) is independent from \\(\\bar\\mu\\)! Thus, irrespective of the desired level of expected returns (or the investors’ risk aversion), everybody chooses the same portfolio of risky assets. The only variation arises from the amount of wealth invested into the risk-free asset. Some investors may even choose to lever their risky position by borrowing at the risk-free rate and investing more than their actual wealth into the portfolio \\(\\omega_\\text{tan}\\).\nThe portfolio\n\\[\\omega_{tan}=\\frac{\\Sigma^{-1}\\tilde\\mu}{\\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu}\\]\nis central to the CAPM and is typically called the efficient tangency portfolio.\nFor illustrative purposes, we compute the efficient tangency portfolio for our hypothetical asset universe. As a realistic proxy for the risk-free rate, we use the average13-week T-bill rate (traded with the symbol ^IRX). Since the prices are quoted in annualized percentage yields, we have to divide them by 100 and convert them to monthly rates.\n\nrisk_free_monthly &lt;- download_data(\n  type = \"stock_prices\", symbol = \"^IRX\", \n  start_date = \"2019-10-01\", end_date = \"2024-09-30\"\n) |&gt; \n  mutate(\n    risk_free = (1 + adjusted_close / 100)^(1 / 12) - 1\n  ) |&gt; \n  select(date, risk_free) |&gt; \n  drop_na()\n\nrf &lt;- mean(risk_free_monthly$risk_free)\n\nNext, we define the core parameters governing the distribution of asset returns, \\(\\Sigma\\) and \\(\\tilde\\mu\\).\n\nassets &lt;- returns_monthly |&gt; \n  group_by(symbol) |&gt; \n  summarise(\n    mu = mean(ret),\n    sigma = sd(ret)\n  )\n\nsigma &lt;- returns_monthly |&gt; \n  pivot_wider(names_from = symbol, values_from = ret)  |&gt; \n  select(-date) |&gt; \n  cov()\n\nmu &lt;- returns_monthly |&gt;\n  group_by(symbol) |&gt;\n  summarise(mu = mean(ret)) |&gt;\n  pull(mu)\n\nWe are ready to illustrate the resulting efficient frontier. Every investor chooses to allocate a fraction of her wealth in the efficient tangency portfolio \\(\\omega_\\text{tan}\\) and the remainder in the risk-free asset. The optimal allocation depends on the investor’s risk aversion. As the risk-free asset, by definition, has a zero volatility, the efficient frontier is a straight line connecting the risk-free asset with the tangency portfolio. The slope of the line connecting the risk-free asset and the tangency portfolio is\n\\[\\frac{\\omega^{\\prime}_\\text{tan}\\mu-r_f}{\\omega_\\text{tan}^{\\prime}\\Sigma\\omega_\\text{tan}^{\\prime}}.\\]\nTypically, the excess return of an asset scaled by its volatility, \\(\\frac{\\tilde\\mu_i}{\\sigma_i}\\), is called the Sharpe ratio of the asset. Thus, the slope of the efficient frontier corresponds to the Sharpe ratio of the tangency portfolio returns. By construction, the tangency portfolio is the maximum Sharpe ratio portfolio.1\n\nw_tan &lt;- solve(sigma) %*% (mu - rf)\nw_tan &lt;- w_tan / sum(w_tan)\n\nmu_w &lt;- as.numeric(t(w_tan) %*% mu)\nsigma_w &lt;- as.numeric(sqrt(t(w_tan) %*% sigma %*% w_tan))\n\nefficient_portfolios &lt;- tribble(\n  ~symbol,    ~mu,         ~sigma,\n  \"omega[tan]\", mu_w, sigma_w,\n  \"r[f]\", rf,   0\n)\n\nsharpe_ratio &lt;- (mu_w - rf) / sigma_w\n\nFigure 1 shows the resulting efficient frontiert with the efficient portfolio and a risk-free asset.\n\nassets |&gt; \n  ggplot(aes(x = sigma, y = mu)) +\n  geom_point() + \n  geom_point(data = efficient_portfolios, color = \"blue\") +\n  geom_label_repel(data = efficient_portfolios,\n                   aes(label = symbol), parse = TRUE) +\n  scale_x_continuous(labels = percent) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Volatility\", y = \"Expected return\",\n    title = \"The efficient frontier with a risk-free asset and Dow index constituents\"\n  ) +\n  geom_abline(aes(intercept = rf, slope = sharpe_ratio), linetype = \"dotted\")\n\n\n\n\n\n\n\nFigure 1: Expected returns and volatilities based on monthly returns adjusted for dividend payments and stock splits.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "r/capital-asset-pricing-model.html#the-capital-market-line",
    "href": "r/capital-asset-pricing-model.html#the-capital-market-line",
    "title": "The Capital Asset Pricing Model",
    "section": "The Capital Market Line",
    "text": "The Capital Market Line\nTaking another look at the efficient tangency portfolio \\(\\omega_\\text{tan}\\) reveals that expected asset excess returns \\(\\tilde\\mu\\) cannot be arbitrarily large or small. From the first order condition of the optimization problem above we get, via simple rearranging,\n\\[\\frac{\\partial Z}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda \\tilde\\mu =0 \\\\\\Leftrightarrow \\tilde\\mu = \\frac{2}{\\lambda}\\Sigma\\omega^* = \\frac{2}{\\lambda}\\underbrace{\\iota'\\omega^*}_{=\\frac{\\lambda}{2}\\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu}\\Sigma\\omega_\\text{tan} \\\\ = \\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu\\Sigma\\omega_\\text{tan}\\]\nNow, note the following three simple derivations:\n\nThe expected excess return of the efficient tangency portfolio, \\(\\tilde\\mu_\\text{tan}\\) is given by \\(E(\\omega_\\text{tan}^{\\prime} r - r_f) = \\omega_\\text{tan} \\tilde\\mu = \\frac{\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu}{\\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu}\\).\nThe variance of the returns of the efficient tangency portfolio \\(\\sigma_\\text{tan}^2\\) is given by \\(\\text{Var}(\\omega_\\text{tan}^{\\prime} r) = \\omega_\\text{tan}^{\\prime} \\Sigma \\omega_\\text{tan}^{\\prime} = \\frac{\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu}{(\\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu)^2}\\). It follows that \\(\\frac{\\tilde\\mu_\\text{tan}}{\\sigma_\\text{tan}^2} = \\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu\\)\nThe \\(i\\)-th element of the vector \\(\\Sigma\\omega_\\text{tan}\\) is \\(\\sum\\limits_{j=1}^N \\sigma_{ij}\\omega_{j,\\text{tan}}= \\text{Cov}\\left(r_i, \\sum\\limits_{j=1}^N r_i'\\omega_{j,\\text{tan}}\\right) = \\text{Cov}\\left(r_i, r_\\text{tan}\\right)\\), which is the covariance of assets \\(i\\) returns with the returns of the efficient tangency portfolio.\n\nPutting everything together yields for the expected excess return of asset \\(i\\):\n\\[\\tilde{\\mu}_i = \\frac{E(\\omega_\\text{tan}^{\\prime}r - r_f)}{\\sigma_{\\text{tan}}^2} \\text{Cov}\\left(r_i,r_\\text{tan}\\right) = \\beta_i \\tilde{\\mu}_\\text{tan}.\\]\nThe equation above is the famous CAPM equation and central to asset pricing. It states that an assets expected excess return is proportional to the assets return covariance with the efficient portfolio excess returns. The price of risk is the excess return of the efficient tangency portfolio. An asset with 0 beta has the same expected return as the risk-free rate. An asset with a beta of 1 has the same expected return as the efficient tangency portfolio. An asset with a negative beta has expected returns lower than the risk-free asset - the very definition of an insurance. Therefore, the CAPM equation explains why some assets may have the same volatility but different returns: Because their systematic risk (\\(\\beta_i\\)) is different.\nWe derived the CAPM equation as a consequence of efficient wealth allocation. Suppose an asset delivers high expected returns. The investor will increase her holdings of the assets in order to benefit from the high promised returns. As a consequence, the covariance of the asset with the efficient tangency portfolio will increase (mechanically, as the asset gradually becomes a larger part of the efficient tangency portfolio). At some point, the weight of the asset is so high that gain of \\(\\tilde\\mu_i\\) of marginally increasing the holding does not offset the implied increase in systematic risk. The investor will stop increasing her holdings and the asset’s expected return will be proportional to its systematic risk.\nWe can illustrate the relationship between systematic risk and expected returns in the so-called security market line.\n\nbetas &lt;- (sigma %*% w_tan) / as.numeric(t(w_tan) %*% sigma %*% w_tan)\nassets &lt;- assets |&gt; \n  mutate(beta = betas)\n\nprice_of_risk &lt;- as.numeric(t(w_tan) %*% mu - rf)\n\nassets |&gt; \n  ggplot(aes(x = beta, y = mu)) +\n  geom_point() +\n  geom_abline(intercept = rf, slope = price_of_risk) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Beta\", y = \"Expected return\",\n    title = \"Security market line\"\n  ) +\n  annotate(\"text\", x = 0, y = rf, label = \"Risk-free\")\n\n\n\n\n\n\n\n\nThe security market line shows the relationship between systematic risk (\\(\\beta_i\\)) and the expected return of an asset. The slope of the line is the price of risk, which is the expected return of the efficient tangency portfolio. The risk-free asset is represented by the intercept with the vertical axis. The CAPM equation states that an asset’s expected return is proportional to its beta, with the efficient tangency portfolio’s expected return as the price of risk.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "r/capital-asset-pricing-model.html#asset-pricing-and-the-market-portfolio",
    "href": "r/capital-asset-pricing-model.html#asset-pricing-and-the-market-portfolio",
    "title": "The Capital Asset Pricing Model",
    "section": "Asset Pricing and the Market Portfolio",
    "text": "Asset Pricing and the Market Portfolio\nSo far, we focused on the optimal decision problem of an individual investor. The capital asset pricing model goes an important step further and considers the equilibrium in which all investors in the market choose to hold efficient portfolios.\nBased on the previous derivations, it should be clear that\n\nAll rational investors prefer efficient portfolios to individual assets or any other portfolios.\nThe tangency portfolio \\(\\omega_\\text{tan}\\) serves as the optimal risky portfolio for all investors.\n\nThis leads to a powerful conclusion: In equilibrium, all investors hold some combination of the risk-free asset and the tangency portfolio, regardless of their risk preferences. Their only choice is how much to allocate to each of the two funds. Aggregating all capital allocated to risky assets yields the total market capitalization of all risky assets. Given that everybody holds the same portfolio of risky assets, the market capitalization of each asset is proportional to its weight in the efficient tangency portfolio. The market portfolio is the sum of all risky assets weighted by their market capitalization. For practical purposes the insight that the market portfolio is the efficient tangency portfolio, has substantial value: Instead of having to derive the vector of expected asset returns and the variance-covariance matrix, we can use the market portfolio as a proxy for the efficient tangency portfolio. Market capitalization is readily observable and the market portfolio is easy to replicate.2\nEmpirically, the CAPM-equation boils down to a linear regression:\n\\[r_{t,i}-r_f =  \\beta_i (r_{m,t} - r_f) + \\varepsilon_{i,t}.\\]\nThus, a naive, but straightforward way of ‘estimating’ the CAPM is to run a linear regression of asset excess returns on market excess returns. The slope of the regression line is then the asset’s beta.\nThe potential intercept is the asset’s alpha, which measures the asset’s risk-adjusted performance. If the CAPM holds, the alpha should be statistically indistinguishable from zero for all assets. Alpha provides a risk-adjusted performance measure. A positive alpha indicates that the asset outperformed its risk-adjusted benchmark, while a negative alpha suggests underperformance.\nIn practice, we hence estimate both alpha and beta using regression analysis. The empirical model is:\n\\[r_{i,t}  - r_{f,t}  = \\alpha_i + \\beta_i \\cdot (r_{m,t} - r_{f,t} ) + \\varepsilon_{i,t}, \\] where \\(r_{i,t}\\) and \\(r_{m,t}\\) are the realized returns of the asset and market portfolio on day \\(t\\), respectively. The error term \\(\\varepsilon_{i,t}\\) captures the asset’s idiosyncratic risk.\nLet’s turn to estimating CAPM parameters using real market data. Instead of using our previously constructed tangency portfolio, we employ the Fama-French market excess returns, which provide a widely accepted proxy for the market portfolio. These returns are already adjusted to represent excess returns over the risk-free rate, simplifying our analysis.\n\nfactors &lt;- download_data(\n  type = \"factors_ff_5_2x3_monthly\", \n  start_date = \"2000-01-01\", end_date = \"2024-09-30\"\n) |&gt; \n  select(date, mkt_excess, risk_free)\n\nFor our regression analysis, we first prepare the data by calculating excess returns for each stock. We join our monthly returns with the Fama-French factors and subtract the risk-free rate to obtain excess returns. The estimate_capm() function then implements the regression equation we previously discussed. We leverage nested dataframes to efficiently run these regressions for all assets simultaneously. The map() function applies our regression to each nested dataset and extracts the coefficients, giving us a clean data frame of assets and their corresponding betas.\n\nreturns_excess_monthly &lt;- returns_monthly |&gt; \n  left_join(factors, join_by(date)) |&gt; \n  mutate(ret_excess = ret - risk_free) |&gt; \n  select(symbol, date, ret_excess, mkt_excess)\n\nestimate_capm &lt;- function(data) {\n  fit &lt;- lm(\"ret_excess ~ mkt_excess\", data = data)\n  tibble(\n    coefficient = c(\"alpha\", \"beta\"),\n    estimate = coefficients(fit),\n    t_statistic = summary(fit)$coefficients[, \"t value\"]\n  )\n}\n  \ncapm_results &lt;- returns_excess_monthly |&gt; \n  nest(data = -symbol) |&gt; \n  mutate(capm = map(data, estimate_capm)) |&gt; \n  unnest(capm) |&gt; \n  select(symbol, coefficient, estimate, t_statistic)\n\nThe results are particularly interesting when we visualize the alphas across our sample of Dow Jones constituents. Figure 2 reveals the cross-sectional distribution of risk-adjusted performance, with positive values indicating outperformance and negative values indicating underperformance relative to what CAPM would predict. Statistical significance is indicated through color coding, showing which alphas are statistically different from zero at the 95% confidence level.\n\ncapm_results |&gt; \n  filter(coefficient == \"alpha\") |&gt; \n  mutate(is_significant = abs(t_statistic) &gt;= 1.96) |&gt; \n  ggplot(aes(x = estimate, y = fct_reorder(symbol, estimate), \n             fill = is_significant)) +\n  geom_col() +\n  scale_x_continuous(labels = percent) + \n  labs(\n    x = \"Estimated asset alphas\", y = NULL, fill = \"Significant at 95%?\",\n    title = \"Estimated CAPM alphas for Dow index constituents\"\n  )\n\n\n\n\n\n\n\nFigure 2: Estimates are based on returns adjusted for dividend payments and stock splits and using the Fama-French market excess returns as a measure for the market.\n\n\n\n\n\nMost notably, our analysis shows that only very few assets exhibit a statistically significant positive alpha during our sample period. This finding aligns with the exceptional performance of technology stocks, particularly those involved in AI and chip manufacturing, but suggests that most Dow components’ returns can be explained by their market exposure alone.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "r/capital-asset-pricing-model.html#shortcomings-extensions",
    "href": "r/capital-asset-pricing-model.html#shortcomings-extensions",
    "title": "The Capital Asset Pricing Model",
    "section": "Shortcomings & Extensions",
    "text": "Shortcomings & Extensions\nWhile the CAPM’s elegance and simplicity have made it a cornerstone of modern finance, the model faces several important challenges in practice. A fundamental challenge lies in the identification of the market portfolio. The CAPM theory requires a truly universal market portfolio that includes all investable assets – not just stocks, but also real estate, private businesses, human capital, and even intangible assets. In practice, we must rely on proxies like the S&P 500, DAX, or TOPIX. The choice of market proxy can significantly impact our estimates and may need to be tailored to specific contexts. For instance, a U.S.-focused investor might use the S&P 500, while a Japanese investor might prefer the TOPIX.\nAnother crucial limitation concerns the stability of beta over time. The model assumes that an asset’s systematic risk remains constant, but this rarely holds in practice. Companies undergo significant changes that can affect their market sensitivity: they may alter their capital structure, enter new markets, face new competitors, or transform their business models. Consider how tech companies’ betas might change as they mature from growth startups to established enterprises, or how a retailer’s beta might shift as it expands its online presence.\nPerhaps most importantly, empirical evidence suggests that systematic risk alone cannot fully explain asset returns. Numerous studies have documented patterns in stock returns that CAPM cannot explain. Small-cap stocks tend to outperform large-cap stocks, and value stocks (those with high book-to-market ratios) tend to outperform growth stocks, even after adjusting for market risk. These “anomalies” suggest that investors may care about multiple dimensions of risk beyond market sensitivity.\nThese limitations have spawned a rich literature of alternative and extended models. The Fama-French three-factor model (@ Fama and French 1992) represents a seminal extension, adding two factors to capture the size and value effects:\n\nThe SMB (Small Minus Big) factor captures the tendency of small stocks to outperform large stocks, as we discuss in our chapter Size Sorts and P-Hacking.\nThe HML (High Minus Low) factor captures the tendency of value stocks to outperform growth stocks, as we show in Value and Bivariate Sorts.\n\nBuilding on this framework, the Fama-French five-factor model (Fama and French 2015) adds two more dimensions, which we later discuss in Replicating Fama-French Factors:\n\nThe RMW (Robust Minus Weak) factor captures the outperformance of companies with strong operating profitability\nThe CMA (Conservative Minus Aggressive) factor reflects the tendency of companies with conservative investment policies to outperform those with aggressive investment policies\n\nThe field continues to evolve with various theoretical and empirical innovations. The Consumption CAPM links asset prices to macroeconomic risks through aggregate consumption. The Conditional CAPM (Jagannathan and Wang 1996) allows risk premiums and betas to vary with the business cycle. The Carhart four-factor model (Carhart 1997) adds momentum to the three-factor framework, while the Q-factor model and investment CAPM (Hou, Xue, and Zhang 2014) provide alternative theoretical foundations rooted in corporate finance.\nDespite its limitations, the CAPM remains valuable as a conceptual framework and practical tool. Its core insight – that only systematic risk should be priced in equilibrium – continues to influence how we think about risk and return. Understanding both its strengths and weaknesses helps us apply it more effectively and appreciate the contributions of newer models that build upon its foundation.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "r/capital-asset-pricing-model.html#key-takeaways",
    "href": "r/capital-asset-pricing-model.html#key-takeaways",
    "title": "The Capital Asset Pricing Model",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe CAPM explains asset prices in equilibrium based on systematic risk and investor preferences under ideal market conditions.\nIn equilibrium, all investors invest in a mix of the market portfolio and a risk-free asset, with individual risk tolerance determining the allocation between the two.\nExpected returns are linearly related to systematic risk, meaning assets with higher beta values should offer higher expected returns to compensate for undiversifiable risk.\nBeta is a measure of an asset’s sensitivity to overall market movements and is estimated using regression analysis of excess asset returns on excess market returns.\nDespite its simplifying assumptions, the CAPM remains foundational in finance and highlights the critical distinction between systematic and idiosyncratic risk.\nEmpirical limitations of the CAPM, such as instability of beta and unexplained return anomalies, have led to the development of multifactor models like the Fama-French and Carhart models.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "r/capital-asset-pricing-model.html#exercises",
    "href": "r/capital-asset-pricing-model.html#exercises",
    "title": "The Capital Asset Pricing Model",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily returns for a German stock of your choice and the S&P 500 index for the past five years. Calculate the stock’s beta and interpret its meaning. How does your estimate change if you use daily instead of monthly returns?\nCompare the betas of stocks estimated using different market proxies (e.g., S&P 500, Russell 3000, MSCI World). How do the differences in market definition affect your conclusions about systematic risk?\nSelect a mutual fund and estimate its alpha and beta relative to its benchmark index. Is the fund’s performance statistically significant after accounting for market risk? How do your conclusions change if you use a different benchmark?\nCompare betas of multinational companies using both local and global market indices. How do the estimates differ? What might explain these differences?",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "r/capital-asset-pricing-model.html#footnotes",
    "href": "r/capital-asset-pricing-model.html#footnotes",
    "title": "The Capital Asset Pricing Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe proof in the Appendix Proofs that the efficient tangency portfolio \\(\\omega_\\text{tan}\\) can also be derived as a solution to the optimization problem \\(\\max_{\\omega} \\frac{\\omega^{\\prime}\\tilde\\mu}{\\sqrt{\\omega^{\\prime}\\Sigma\\omega}} \\text{ s.t. } \\omega^{\\prime}\\iota=1.\\)↩︎\nObviously, the market portfolio is the efficient tangency portfolio only if the strict assumptions of the CAPM hold: The model describes equilibrium in a single-period economy, markets are frictionless, with no transaction costs or taxes, all investors can borrow and lend at the risk-free rate, investors share the same expectations about returns and risks, investors are rational, seeking to maximize returns for a given level of risk.↩︎",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html",
    "href": "r/parametric-portfolio-policies.html",
    "title": "Parametric Portfolio Policies",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we apply different portfolio performance measures to evaluate and compare portfolio allocation strategies. For this purpose, we introduce a direct way to estimate optimal portfolio weights for large-scale cross-sectional applications. More precisely, the approach of Brandt, Santa-Clara, and Valkanov (2009) proposes to parametrize the optimal portfolio weights as a function of stock characteristics instead of estimating the stock’s expected return, variance, and covariances with other stocks in a prior step. We choose weights as a function of the characteristics, which maximize the expected utility of the investor. This approach is feasible for large portfolio dimensions (such as the entire CRSP universe) and has been proposed by Brandt, Santa-Clara, and Valkanov (2009). See the review paper by Brandt (2010) for an excellent treatment of related portfolio choice methods.\nThe current chapter relies on the following set of R packages:\nlibrary(tidyverse)\nlibrary(RSQLite)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#data-preparation",
    "href": "r/parametric-portfolio-policies.html#data-preparation",
    "title": "Parametric Portfolio Policies",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the monthly CRSP file, which forms our investment universe. We load the data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(), \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, date, ret_excess, mktcap, mktcap_lag) |&gt;\n  collect()\n\nTo evaluate the performance of portfolios, we further use monthly market returns as a benchmark to compute CAPM alphas.\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(date, mkt_excess) |&gt;\n  collect()\n\nNext, we retrieve some stock characteristics that have been shown to have an effect on the expected returns or expected variances (or even higher moments) of the return distribution. In particular, we record the lagged one-year return momentum (momentum_lag), defined as the compounded return between months \\(t-13\\) and \\(t-2\\) for each firm, which we calculate using market capitalization for simplicity. In finance, momentum is the empirically observed tendency for rising asset prices to rise further, and falling prices to keep falling (Jegadeesh and Titman 1993). We refer to the exercise for a more elaborate measure of momentum. The second characteristic is the firm’s market equity (size_lag), defined as the log of the price per share times the number of shares outstanding (Banz 1981). To construct the correct lagged values, we use the approach introduced in WRDS, CRSP, and Compustat.\n\ncrsp_monthly_lags &lt;- crsp_monthly |&gt;\n  transmute(permno,\n    date_13 = date %m+% months(13),\n    mktcap\n  )\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  inner_join(crsp_monthly_lags,\n    join_by(permno, date == date_13),\n    suffix = c(\"\", \"_13\")\n  )\n\ndata_portfolios &lt;- crsp_monthly |&gt;\n  mutate(\n    momentum_lag = mktcap_lag / mktcap_13,\n    size_lag = log(mktcap_lag)\n  ) |&gt;\n  drop_na(contains(\"lag\"))",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "href": "r/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "title": "Parametric Portfolio Policies",
    "section": "Parametric Portfolio Policies",
    "text": "Parametric Portfolio Policies\nThe basic idea of parametric portfolio weights is as follows. Suppose that at each date \\(t\\) we have \\(N_t\\) stocks in the investment universe, where each stock \\(i\\) has a return of \\(r_{i, t+1}\\) and is associated with a vector of firm characteristics \\(x_{i, t}\\) such as time-series momentum or the market capitalization. The investor’s problem is to choose portfolio weights \\(w_{i,t}\\) to maximize the expected utility of the portfolio return: \\[\\begin{aligned}\n\\max_{\\omega} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{i=1}^{N_t}\\omega_{i,t}r_{i,t+1}\\right)\\right]\n\\end{aligned}\\] where \\(u(\\cdot)\\) denotes the utility function.\nWhere do the stock characteristics show up? We parameterize the optimal portfolio weights as a function of the stock characteristic \\(x_{i,t}\\) with the following linear specification for the portfolio weights: \\[\\omega_{i,t} = \\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t},\\] where \\(\\bar{\\omega}_{i,t}\\) is a stock’s weight in a benchmark portfolio (we use the value-weighted or naive portfolio in the application below), \\(\\theta\\) is a vector of coefficients which we are going to estimate, and \\(\\hat{x}_{i,t}\\) are the characteristics of stock \\(i\\), cross-sectionally standardized to have zero mean and unit standard deviation.\nIntuitively, the portfolio strategy is a form of active portfolio management relative to a performance benchmark. Deviations from the benchmark portfolio are derived from the individual stock characteristics. Note that by construction the weights sum up to one as \\(\\sum_{i=1}^{N_t}\\hat{x}_{i,t} = 0\\) due to the standardization. Moreover, the coefficients are constant across assets and over time. The implicit assumption is that the characteristics fully capture all aspects of the joint distribution of returns that are relevant for forming optimal portfolios.\nWe first implement cross-sectional standardization for the entire CRSP universe. We also keep track of (lagged) relative market capitalization relative_mktcap, which will represent the value-weighted benchmark portfolio, while n denotes the number of traded assets \\(N_t\\), which we use to construct the naive portfolio benchmark.\n\ndata_portfolios &lt;- data_portfolios |&gt;\n  group_by(date) |&gt;\n  mutate(\n    n = n(),\n    relative_mktcap = mktcap_lag / sum(mktcap_lag),\n    across(contains(\"lag\"), ~ (. - mean(.)) / sd(.)),\n  ) |&gt;\n  ungroup() |&gt;\n  select(-mktcap_lag)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#computing-portfolio-weights",
    "href": "r/parametric-portfolio-policies.html#computing-portfolio-weights",
    "title": "Parametric Portfolio Policies",
    "section": "Computing Portfolio Weights",
    "text": "Computing Portfolio Weights\nNext, we move on to identify optimal choices of \\(\\theta\\). We rewrite the optimization problem together with the weight parametrization and can then estimate \\(\\theta\\) to maximize the objective function based on our sample \\[\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{i=1}^{N_t}\\left(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\right)r_{i,t+1}\\right).\n\\end{aligned}\\] The allocation strategy is straightforward because the number of parameters to estimate is small. Instead of a tedious specification of the \\(N_t\\) dimensional vector of expected returns and the \\(N_t(N_t+1)/2\\) free elements of the covariance matrix, all we need to focus on in our application is the vector \\(\\theta\\). \\(\\theta\\) contains only two elements in our application: the relative deviation from the benchmark due to size and momentum.\nTo get a feeling for the performance of such an allocation strategy, we start with an arbitrary initial vector \\(\\theta_0\\). The next step is to choose \\(\\theta\\) optimally to maximize the objective function. We automatically detect the number of parameters by counting the number of columns with lagged values.\n\nn_parameters &lt;- sum(str_detect(\n  colnames(data_portfolios), \"lag\"\n))\n\ntheta &lt;- rep(1.5, n_parameters)\n\nnames(theta) &lt;- colnames(data_portfolios)[str_detect(\n  colnames(data_portfolios), \"lag\"\n)]\n\nThe function compute_portfolio_weights() below computes the portfolio weights \\(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\) according to our parametrization for a given value \\(\\theta_0\\). Everything happens within a single pipeline. Hence, we provide a short walk-through.\nWe first compute characteristic_tilt, the tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{i, t}\\) which resemble the deviation from the benchmark portfolio. Next, we compute the benchmark portfolio weight_benchmark, which can be any reasonable set of portfolio weights. In our case, we choose either the value or equal-weighted allocation. weight_tilt completes the picture and contains the final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt which deviate from the benchmark portfolio depending on the stock characteristics.\nThe final few lines go a bit further and implement a simple version of a no-short sale constraint. While it is generally not straightforward to ensure portfolio weight constraints via parameterization, we simply normalize the portfolio weights such that they are enforced to be positive. Finally, we make sure that the normalized weights sum up to one again: \\[\\omega_{i,t}^+ = \\frac{\\max(0, \\omega_{i,t})}{\\sum_{j=1}^{N_t}\\max(0, \\omega_{i,t})}.\\]\nThe following function computes the optimal portfolio weights in the way just described.\n\ncompute_portfolio_weights &lt;- function(theta,\n                                      data,\n                                      value_weighting = TRUE,\n                                      allow_short_selling = TRUE) {\n  data |&gt;\n    group_by(date) |&gt;\n    bind_cols(\n      characteristic_tilt = data |&gt;\n        transmute(across(contains(\"lag\"), ~ . / n)) |&gt;\n        as.matrix() %*% theta |&gt; as.numeric()\n    ) |&gt;\n    mutate(\n      # Definition of benchmark weight\n      weight_benchmark = case_when(\n        value_weighting == TRUE ~ relative_mktcap,\n        value_weighting == FALSE ~ 1 / n\n      ),\n      # Parametric portfolio weights\n      weight_tilt = weight_benchmark + characteristic_tilt,\n      # Short-sell constraint\n      weight_tilt = case_when(\n        allow_short_selling == TRUE ~ weight_tilt,\n        allow_short_selling == FALSE ~ pmax(0, weight_tilt)\n      ),\n      # Weights sum up to 1\n      weight_tilt = weight_tilt / sum(weight_tilt)\n    ) |&gt;\n    ungroup()\n}\n\nIn the next step, we compute the portfolio weights for the arbitrary vector \\(\\theta_0\\). In the example below, we use the value-weighted portfolio as a benchmark and allow negative portfolio weights.\n\nweights_crsp &lt;- compute_portfolio_weights(\n  theta,\n  data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#portfolio-performance",
    "href": "r/parametric-portfolio-policies.html#portfolio-performance",
    "title": "Parametric Portfolio Policies",
    "section": "Portfolio Performance",
    "text": "Portfolio Performance\n Are the computed weights optimal in any way? Most likely not, as we picked \\(\\theta_0\\) arbitrarily. To evaluate the performance of an allocation strategy, one can think of many different approaches. In their original paper, Brandt, Santa-Clara, and Valkanov (2009) focus on a simple evaluation of the hypothetical utility of an agent equipped with a power utility function \\(u_\\gamma(r) = \\frac{(1 + r)^{(1-\\gamma)}}{1-\\gamma}\\), where \\(\\gamma\\) is the risk aversion factor.\n\npower_utility &lt;- function(r, gamma = 5) {\n  (1 + r)^(1 - gamma) / (1 - gamma)\n}\n\nWe want to note that Gehrig, Sögner, and Westerkamp (2020) warn that, in the leading case of constant relative risk aversion (CRRA), strong assumptions on the properties of the returns, the variables used to implement the parametric portfolio policy, and the parameter space are necessary to obtain a well-defined optimization problem.\nNo doubt, there are many other ways to evaluate a portfolio. The function below provides a summary of all kinds of interesting measures that can be considered relevant. Do we need all these evaluation measures? It depends: the original paper by Brandt, Santa-Clara, and Valkanov (2009) only cares about the expected utility to choose \\(\\theta\\). However, if you want to choose optimal values that achieve the highest performance while putting some constraints on your portfolio weights, it is helpful to have everything in one function.\n\nevaluate_portfolio &lt;- function(weights_crsp,\n                               capm_evaluation = TRUE,\n                               full_evaluation = TRUE,\n                               length_year = 12) {\n  \n  evaluation &lt;- weights_crsp |&gt;\n    group_by(date) |&gt;\n    summarize(\n      tilt = weighted.mean(ret_excess, weight_tilt),\n      benchmark = weighted.mean(ret_excess, weight_benchmark)\n    ) |&gt;\n    pivot_longer(\n      -date,\n      values_to = \"portfolio_return\",\n      names_to = \"model\"\n    ) \n  \n  evaluation_stats &lt;- evaluation |&gt;\n    group_by(model) |&gt;\n    left_join(factors_ff3_monthly, join_by(date)) |&gt;\n    summarize(tibble(\n      \"Expected utility\" = mean(power_utility(portfolio_return)),\n      \"Average return\" = 100 * mean(length_year * portfolio_return),\n      \"SD return\" = 100 * sqrt(length_year) * sd(portfolio_return),\n      \"Sharpe ratio\" = sqrt(length_year) * mean(portfolio_return) / sd(portfolio_return),\n\n    )) |&gt;\n    mutate(model = str_remove(model, \"return_\")) \n  \n  if (capm_evaluation) {\n    evaluation_capm &lt;- evaluation |&gt; \n      left_join(factors_ff3_monthly, join_by(date)) |&gt;\n      group_by(model) |&gt;\n      summarize(\n      \"CAPM alpha\" = coefficients(lm(portfolio_return ~ mkt_excess))[1],\n      \"Market beta\" = coefficients(lm(portfolio_return ~ mkt_excess))[2]\n      )\n    \n    evaluation_stats &lt;- evaluation_stats |&gt; \n      left_join(evaluation_capm, join_by(model))\n  }\n\n  if (full_evaluation) {\n    evaluation_weights &lt;- weights_crsp |&gt;\n      select(date, contains(\"weight\")) |&gt;\n      pivot_longer(-date, values_to = \"weight\", names_to = \"model\") |&gt;\n      group_by(model, date) |&gt;\n      mutate(\n        \"Absolute weight\" = abs(weight),\n        \"Max. weight\" = max(weight),\n        \"Min. weight\" = min(weight),\n        \"Avg. sum of negative weights\" = -sum(weight[weight &lt; 0]),\n        \"Avg. fraction of negative weights\" = sum(weight &lt; 0) / n(),\n        .keep = \"none\"\n      ) |&gt;\n      group_by(model) |&gt;\n      summarize(across(-date, ~ 100 * mean(.))) |&gt;\n      mutate(model = str_remove(model, \"weight_\")) \n    \n    evaluation_stats &lt;- evaluation_stats |&gt; \n      left_join(evaluation_weights, join_by(model))\n  }\n  \n  evaluation_output &lt;- evaluation_stats |&gt; \n    pivot_longer(cols = -model, names_to = \"measure\") |&gt; \n    pivot_wider(names_from = model)\n  \n  return(evaluation_output)\n}\n\n Let us take a look at the different portfolio strategies and evaluation measures.\n\nevaluate_portfolio(weights_crsp) |&gt;\n  print(n = Inf)\n\n# A tibble: 11 × 3\n   measure                            benchmark     tilt\n   &lt;chr&gt;                                  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Expected utility                  -0.249     -0.260  \n 2 Average return                     7.04       0.837  \n 3 SD return                         15.4       21.2    \n 4 Sharpe ratio                       0.457      0.0396 \n 5 CAPM alpha                         0.000105  -0.00479\n 6 Market beta                        0.994      0.947  \n 7 Absolute weight                    0.0249     0.0638 \n 8 Max. weight                        3.67       3.80   \n 9 Min. weight                        0.0000266 -0.144  \n10 Avg. sum of negative weights       0         77.9    \n11 Avg. fraction of negative weights  0         49.5    \n\n\nThe value-weighted portfolio delivers an annualized return of more than 6 percent and clearly outperforms the tilted portfolio, irrespective of whether we evaluate expected utility, the Sharpe ratio, or the CAPM alpha. We can conclude the market beta is close to one for both strategies (naturally almost identically 1 for the value-weighted benchmark portfolio). When it comes to the distribution of the portfolio weights, we see that the benchmark portfolio weight takes less extreme positions (lower average absolute weights and lower maximum weight). By definition, the value-weighted benchmark does not take any negative positions, while the tilted portfolio also takes short positions.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#optimal-parameter-choice",
    "href": "r/parametric-portfolio-policies.html#optimal-parameter-choice",
    "title": "Parametric Portfolio Policies",
    "section": "Optimal Parameter Choice",
    "text": "Optimal Parameter Choice\nNext, we move to a choice of \\(\\theta\\) that actually aims to improve some (or all) of the performance measures. We first define a helper function compute_objective_function(), which we then pass to an optimizer.\n\ncompute_objective_function &lt;- function(theta,\n                                       data,\n                                       objective_measure = \"Expected utility\",\n                                       value_weighting = TRUE,\n                                       allow_short_selling = TRUE) {\n  processed_data &lt;- compute_portfolio_weights(\n    theta,\n    data,\n    value_weighting,\n    allow_short_selling\n  )\n\n  objective_function &lt;- evaluate_portfolio(\n    processed_data,\n    capm_evaluation = FALSE,\n    full_evaluation = FALSE\n  ) |&gt;\n    filter(measure == objective_measure) |&gt;\n    pull(tilt)\n\n  return(-objective_function)\n}\n\nYou may wonder why we return the negative value of the objective function. This is simply due to the common convention for optimization procedures to search for minima as a default. By minimizing the negative value of the objective function, we get the maximum value as a result. In its most basic form, R optimization relies on the function optim(). As main inputs, the function requires an initial guess of the parameters and the objective function to minimize. Now, we are fully equipped to compute the optimal values of \\(\\hat\\theta\\), which maximize the hypothetical expected utility of the investor.\n\noptimal_theta &lt;- optim(\n  par = theta,\n  fn = compute_objective_function,\n  objective_measure = \"Expected utility\",\n  data = data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE,\n  method = \"Nelder-Mead\"\n)\n\noptimal_theta$par\n\nmomentum_lag     size_lag \n       0.269       -1.658 \n\n\nThe resulting values of \\(\\hat\\theta\\) are easy to interpret: intuitively, expected utility increases by tilting weights from the value-weighted portfolio toward smaller stocks (negative coefficient for size) and toward past winners (positive value for momentum). Both findings are in line with the well-documented size effect (Banz 1981) and the momentum anomaly (Jegadeesh and Titman 1993).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#more-model-specifications",
    "href": "r/parametric-portfolio-policies.html#more-model-specifications",
    "title": "Parametric Portfolio Policies",
    "section": "More Model Specifications",
    "text": "More Model Specifications\nHow does the portfolio perform for different model specifications? For this purpose, we compute the performance of a number of different modeling choices based on the entire CRSP sample. The next code chunk performs all the heavy lifting.\n\nevaluate_optimal_performance &lt;- function(data, \n                                         objective_measure,\n                                         value_weighting, \n                                         allow_short_selling) {\n  optimal_theta &lt;- optim(\n    par = theta,\n    fn = compute_objective_function,\n    data = data,\n    objective_measure = \"Expected utility\",\n    value_weighting = TRUE,\n    allow_short_selling = TRUE,\n    method = \"Nelder-Mead\"\n  )\n\n  processed_data = compute_portfolio_weights(\n    optimal_theta$par, \n    data,\n    value_weighting,\n    allow_short_selling\n  )\n  \n  portfolio_evaluation = evaluate_portfolio(\n    processed_data,\n    capm_evaluation = TRUE,\n    full_evaluation = TRUE\n  )\n \n  return(portfolio_evaluation) \n}\n\nspecifications &lt;- expand_grid(\n  data = list(data_portfolios),\n  objective_measure = \"Expected utility\",\n  value_weighting = c(TRUE, FALSE),\n  allow_short_selling = c(TRUE, FALSE)\n) |&gt; \n  mutate(\n    portfolio_evaluation = pmap(\n      .l = list(data, objective_measure, value_weighting, allow_short_selling),\n      .f = evaluate_optimal_performance\n    )\n)\n\nFinally, we can compare the results. The table below shows summary statistics for all possible combinations: equal- or value-weighted benchmark portfolio, with or without short-selling constraints, and tilted toward maximizing expected utility.\n\nperformance_table &lt;- specifications |&gt;\n  select(\n    value_weighting,\n    allow_short_selling,\n    portfolio_evaluation\n  ) |&gt;\n  unnest(portfolio_evaluation)\n\nperformance_table |&gt;\n  rename(\n    \" \" = benchmark,\n    Optimal = tilt\n  ) |&gt;\n  mutate(\n    value_weighting = case_when(\n      value_weighting == TRUE ~ \"VW\",\n      value_weighting == FALSE ~ \"EW\"\n    ),\n    allow_short_selling = case_when(\n      allow_short_selling == TRUE ~ \"\",\n      allow_short_selling == FALSE ~ \"(no s.)\"\n    )\n  ) |&gt;\n  pivot_wider(\n    names_from = value_weighting:allow_short_selling,\n    values_from = \" \":Optimal,\n    names_glue = \"{value_weighting} {allow_short_selling} {.value} \"\n  ) |&gt;\n  select(\n    measure,\n    `EW    `,\n    `VW    `,\n    sort(contains(\"Optimal\"))\n  ) |&gt;\n  print(n = 11)\n\n# A tibble: 11 × 7\n   measure     `EW    ` `VW    ` `VW  Optimal ` `VW (no s.) Optimal `\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Expected u… -0.251   -2.49e-1       -0.247                -0.248  \n 2 Average re…  9.96     7.04e+0       12.7                  12.0    \n 3 SD return   20.4      1.54e+1       19.2                  18.8    \n 4 Sharpe rat…  0.487    4.57e-1        0.662                 0.639  \n 5 CAPM alpha   0.00175  1.05e-4        0.00478               0.00403\n 6 Market beta  1.13     9.94e-1        1.01                  1.03   \n 7 Absolute w…  0.0249   2.49e-2        0.0340                0.0249 \n 8 Max. weight  0.0249   3.67e+0        3.52                  2.97   \n 9 Min. weight  0.0249   2.66e-5       -0.0262                0      \n10 Avg. sum o…  0        0             19.0                   0      \n11 Avg. fract…  0        0             36.5                   0      \n# ℹ 2 more variables: `EW  Optimal ` &lt;dbl&gt;,\n#   `EW (no s.) Optimal ` &lt;dbl&gt;\n\n\nThe results indicate that the average annualized Sharpe ratio of the equal-weighted portfolio exceeds the Sharpe ratio of the value-weighted benchmark portfolio. Nevertheless, starting with the weighted value portfolio as a benchmark and tilting optimally with respect to momentum and small stocks yields the highest Sharpe ratio across all specifications. Finally, imposing no short-sale constraints does not improve the performance of the portfolios in our application.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#key-takeaways",
    "href": "r/parametric-portfolio-policies.html#key-takeaways",
    "title": "Parametric Portfolio Policies",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nParametric portfolio policies estimate portfolio weights directly as functions of stock characteristics like momentum and size, avoiding the need to forecast expected returns or covariances.\nThis method, based on Brandt, Santa-Clara, and Valkanov (2009), is computationally efficient and scalable for large cross-sectional datasets such as CRSP.\nOptimization focuses on maximizing expected utility, and evaluation includes measures such as Sharpe ratio, CAPM alpha, and utility-based performance.\nResults highlight that tilting value-weighted portfolios toward small-cap and high-momentum stocks improves performance, aligning with known anomalies in finance.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#exercises",
    "href": "r/parametric-portfolio-policies.html#exercises",
    "title": "Parametric Portfolio Policies",
    "section": "Exercises",
    "text": "Exercises\n\nDefine momentum as the rolling 12-month cumulative returns skipping the most recent month. Calculate the correlation with the measure based on market capitalization from above and compare summary statistics. How do the two measures differ?\nHow do the estimated parameters \\(\\hat\\theta\\) and the portfolio performance change if your objective is to maximize the Sharpe ratio instead of the hypothetical expected utility?\nThe code above is very flexible in the sense that you can easily add new firm characteristics. Construct a new characteristic of your choice and evaluate the corresponding coefficient \\(\\hat\\theta_i\\).\nTweak the function optimal_theta() such that you can impose additional performance constraints in order to determine \\(\\hat\\theta\\), which maximizes expected utility under the constraint that the market beta is below 1.\nDoes the portfolio performance resemble a realistic out-of-sample backtesting procedure? Verify the robustness of the results by first estimating \\(\\hat\\theta\\) based on past data only. Then, use more recent periods to evaluate the actual portfolio performance.\nBy formulating the portfolio problem as a statistical estimation problem, you can easily obtain standard errors for the coefficients of the weight function. Brandt, Santa-Clara, and Valkanov (2009) provide the relevant derivations in their paper in Equation (10). Implement a small function that computes standard errors for \\(\\hat\\theta\\).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/wrds-dummy-data.html",
    "href": "r/wrds-dummy-data.html",
    "title": "WRDS Dummy Data",
    "section": "",
    "text": "Note\n\n\n\nThis appendix chapter is based on a blog post Dummy Data for Tidy Finance Readers without Access to WRDS by Christoph Scheuch.\nIn this appendix chapter, we alleviate the constraints of readers who do not have access to WRDS and hence cannot run the code that we provide. We show how to create a dummy database that contains the WRDS tables and corresponding columns such that all code chunks in this book can be executed with this dummy database. We do not create dummy data for tables of macroeconomic variables because they can be freely downloaded from the original sources; check out Accessing and Managing Financial Data.\nWe deliberately use the dummy label because the data is not meaningful in the sense that it allows readers to actually replicate the results of the book. For legal reasons, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books.\nTo generate the dummy database, we use the following packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nLet us initialize a SQLite database (tidy_finance_r.sqlite) or connect to your existing one. Be careful, if you already downloaded the data from WRDS, then the code in this chapter will overwrite your data!\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\nSince we draw random numbers for most of the columns, we also define a seed to ensure that the generated numbers are replicable. We also initialize vectors of dates of different frequencies over ten years that we then use to create yearly, monthly, and daily data, respectively.\nset.seed(1234)\n\nstart_date &lt;- as.Date(\"2003-01-01\")\nend_date &lt;- as.Date(\"2022-12-31\")\n\ntime_series_years &lt;- seq(year(start_date), year(end_date), 1)\ntime_series_months &lt;- seq(start_date, end_date, \"1 month\")\ntime_series_days &lt;- seq(start_date, end_date, \"1 day\")",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "r/wrds-dummy-data.html#create-stock-dummy-data",
    "href": "r/wrds-dummy-data.html#create-stock-dummy-data",
    "title": "WRDS Dummy Data",
    "section": "Create Stock Dummy Data",
    "text": "Create Stock Dummy Data\nLet us start with the core data used throughout the book: stock and firm characteristics. We first generate a table with a cross-section of stock identifiers with unique permno and gvkey values, as well as associated exchcd, exchange, industry, and siccd values. The generated data is based on the characteristics of stocks in the crsp_monthly table of the original database, ensuring that the generated stocks roughly reflect the distribution of industries and exchanges in the original data, but the identifiers and corresponding exchanges or industries do not reflect actual firms. Similarly, the permno-gvkey combinations are purely nonsensical and should not be used together with actual CRSP or Compustat data.\n\nnumber_of_stocks &lt;- 100\n\nindustries &lt;- tibble(\n  industry = c(\"Agriculture\", \"Construction\", \"Finance\", \n               \"Manufacturing\", \"Mining\", \"Public\", \"Retail\", \n               \"Services\", \"Transportation\", \"Utilities\", \n               \"Wholesale\"),\n  n = c(81, 287, 4682, 8584, 1287, 1974, 1571, 4277, 1249, \n        457, 904),\n  prob = c(0.00319, 0.0113, 0.185, 0.339, 0.0508, 0.0779, \n           0.0620, 0.169, 0.0493, 0.0180, 0.0357)\n)\n\nexchanges &lt;- exchanges &lt;- tibble(\n  exchange = c(\"AMEX\", \"NASDAQ\", \"NYSE\"),\n  n = c(2893, 17236, 5553),\n  prob = c(0.113, 0.671, 0.216)\n)\n\nstock_identifiers &lt;- 1:number_of_stocks |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        permno = x,\n        gvkey = as.character(x + 10000),\n        exchange = sample(exchanges$exchange, 1, \n                          prob = exchanges$prob),\n        industry = sample(industries$industry, 1, \n                          prob = industries$prob)\n      ) |&gt; \n        mutate(\n          exchcd = case_when(\n            exchange == \"NYSE\" ~ sample(c(1, 31), n()),\n            exchange == \"AMEX\" ~ sample(c(2, 32), n()),\n            exchange == \"NASDAQ\" ~ sample(c(3, 33), n())\n          ),\n          siccd = case_when(\n            industry == \"Agriculture\" ~ sample(1:999, n()),\n            industry == \"Mining\" ~ sample(1000:1499, n()),\n            industry == \"Construction\" ~ sample(1500:1799, n()),\n            industry == \"Manufacturing\" ~ sample(1800:3999, n()),\n            industry == \"Transportation\" ~ sample(4000:4899, n()),\n            industry == \"Utilities\" ~ sample(4900:4999, n()),\n            industry == \"Wholesale\" ~ sample(5000:5199, n()),\n            industry == \"Retail\" ~ sample(5200:5999, n()),\n            industry == \"Finance\" ~ sample(6000:6799, n()),\n            industry == \"Services\" ~ sample(7000:8999, n()),\n            industry == \"Public\" ~ sample(9000:9999, n())\n          )\n        )\n    }\n  )\n\nNext, we construct three panels of stock data with varying frequencies: yearly, monthly, and daily. We begin by creating the stock_panel_yearly panel. To achieve this, we combine the stock_identifiers table with a new table containing the variable year from dummy_years. The expand_grid() function ensures that we get all possible combinations of the two tables. After combining, we select only the gvkey and year columns for our final yearly panel.\nNext, we construct the stock_panel_monthly panel. Similar to the yearly panel, we use the expand_grid() function to combine stock_identifiers with a new table that has the date variable from dummy_months. After merging, we select the columns permno, gvkey, date, siccd, industry, exchcd, and exchange to form our monthly panel.\nLastly, we create the stock_panel_daily panel. We combine stock_identifiers with a table containing the date variable from dummy_days. After merging, we retain only the permno and date columns for our daily panel.\n\nstock_panel_yearly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(year = time_series_years)\n) |&gt; \n  select(gvkey, year)\n\nstock_panel_monthly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(date = time_series_months)\n) |&gt; \n  select(permno, gvkey, date, siccd, industry, exchcd, exchange)\n\nstock_panel_daily &lt;- expand_grid(\n  stock_identifiers, \n  tibble(date = time_series_days)\n)|&gt; \n  select(permno, date)\n\n\nDummy beta table\nWe then proceed to create dummy beta values for our stock_panel_monthly table. We generate monthly beta values beta_monthly using the rnorm() function with a mean and standard deviation of 1. For daily beta values beta_daily, we take the dummy monthly beta and add a small random noise to it. This noise is generated again using the rnorm() function, but this time we divide the random values by 100 to ensure they are small deviations from the monthly beta.\n\nbeta_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    beta_monthly = rnorm(n(), mean = 1, sd = 1),\n    beta_daily = beta_monthly + rnorm(n()) / 100\n  )\n\ndbWriteTable(\n  tidy_finance,\n  \"beta\", \n  beta_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy compustat table\nTo create dummy firm characteristics, we take all columns from the compustat table and create random numbers between 0 and 1. For simplicity, we set the datadate for each firm-year observation to the last day of the year, although it is empirically not the case. \n\nrelevant_columns &lt;- c(\n  \"seq\", \"ceq\", \"at\", \"lt\", \"txditc\", \"txdb\", \"itcb\", \n  \"pstkrv\", \"pstkl\", \"pstk\", \"capx\", \"oancf\", \"sale\", \n  \"cogs\", \"xint\", \"xsga\", \"be\", \"op\", \"at_lag\", \"inv\"\n)\n\ncommands &lt;- unlist(\n  map(\n    relevant_columns, \n    ~rlang::exprs(!!..1 := runif(n()))\n  )\n)\n\ncompustat_dummy &lt;- stock_panel_yearly |&gt; \n  mutate(\n    datadate = ymd(str_c(year, \"12\", \"31\")),\n    !!!commands\n  )\n\ndbWriteTable(\n  tidy_finance, \n  \"compustat\", \n  compustat_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_monthly table\nThe crsp_monthly table only lacks a few more columns compared to stock_panel_monthly: the returns ret drawn from a normal distribution, the excess returns ret_excess with small deviations from the returns, the shares outstanding shrout and the last price per month altprc both drawn from uniform distributions, and the market capitalization mktcap as the product of shrout and altprc. \n\ncrsp_monthly_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    ret = pmax(rnorm(n()), -1),\n    ret_excess = pmax(ret - runif(n(), 0, 0.0025), -1),\n    shrout = runif(n(), 1, 50) * 1000,\n    altprc = runif(n(), 0, 1000),\n    mktcap = shrout * altprc\n  ) |&gt; \n  group_by(permno) |&gt; \n  arrange(date) |&gt; \n  mutate(mktcap_lag = lag(mktcap)) |&gt; \n  ungroup()\n\ndbWriteTable(\n  tidy_finance, \n  \"crsp_monthly\",\n  crsp_monthly_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_daily table\nThe crsp_daily table only contains a date column and the daily excess returns ret_excess as additional columns to stock_panel_daily.\n\ncrsp_daily_dummy &lt;- stock_panel_daily |&gt; \n  mutate(\n    ret_excess = pmax(rnorm(n()), -1)\n  )\n\ndbWriteTable(\n  tidy_finance,\n  \"crsp_daily\",\n  crsp_daily_dummy, \n  overwrite = TRUE\n)",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "r/wrds-dummy-data.html#create-bond-dummy-data",
    "href": "r/wrds-dummy-data.html#create-bond-dummy-data",
    "title": "WRDS Dummy Data",
    "section": "Create Bond Dummy Data",
    "text": "Create Bond Dummy Data\nLastly, we move to the bond data that we use in our books.\n\nDummy fisd data\nTo create dummy data with the structure of Mergent FISD, we calculate the empirical probabilities of actual bonds for several variables: maturity, offering_amt, interest_frequency, coupon, and sic_code. We use these probabilities to sample a small cross-section of bonds with completely made up complete_cusip, issue_id, and issuer_id.\n\nnumber_of_bonds &lt;- 100\n\nfisd_dummy &lt;- 1:number_of_bonds |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        complete_cusip = str_to_upper(\n          str_c(\n            sample(c(letters, 0:9), 12, replace = TRUE), \n            collapse = \"\"\n          )\n        ),\n      )\n    }\n  ) |&gt; \n  mutate(\n    maturity = sample(time_series_days, n(), replace = TRUE),\n    offering_amt = sample(seq(1:100) * 100000, n(), replace = TRUE),\n    offering_date = maturity - sample(seq(1:25) * 365, n(),replace = TRUE),\n    dated_date = offering_date - sample(-10:10, n(), replace = TRUE),\n    interest_frequency = sample(c(0, 1, 2, 4, 12), n(), replace = TRUE),\n    coupon = sample(seq(0, 2, by = 0.1), n(), replace = TRUE),\n    last_interest_date = pmax(maturity, offering_date, dated_date),\n    issue_id = row_number(),\n    issuer_id = sample(1:250, n(), replace = TRUE),\n    sic_code = as.character(sample(seq(1:9)*1000, n(), replace = TRUE))\n  )\n  \ndbWriteTable(\n  tidy_finance, \n  \"fisd\", \n  fisd_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy trace_enhanced data\nFinally, we create a dummy bond transaction data for the fictional CUSIPs of the dummy fisd data. We take the date range that we also analyze in the book and ensure that we have at least five transactions per day to fulfill a filtering step in the book. \n\nstart_date &lt;- as.Date(\"2014-01-01\")\nend_date &lt;- as.Date(\"2016-11-30\")\n\nbonds_panel &lt;- expand_grid(\n  fisd_dummy |&gt; \n    select(cusip_id = complete_cusip),\n  tibble(\n    trd_exctn_dt = seq(start_date, end_date, \"1 day\")\n  )\n)\n\ntrace_enhanced_dummy &lt;- bind_rows(\n  bonds_panel, bonds_panel, \n  bonds_panel, bonds_panel, \n  bonds_panel) |&gt; \n  mutate(\n    trd_exctn_tm = str_c(\n      sample(0:24, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE)\n    ),\n    rptd_pr = runif(n(), 10, 200),\n    entrd_vol_qt = sample(1:20, n(), replace = TRUE) * 1000,\n    yld_pt = runif(n(), -10, 10),\n    rpt_side_cd = sample(c(\"B\", \"S\"), n(), replace = TRUE),\n    cntra_mp_id = sample(c(\"C\", \"D\"), n(), replace = TRUE)\n  ) \n  \ndbWriteTable(\n  tidy_finance, \n  \"trace_enhanced\", \n  trace_enhanced_dummy, \n  overwrite = TRUE\n)\n\nAs stated in the introduction, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout this book.",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html",
    "href": "r/wrds-crsp-and-compustat.html",
    "title": "WRDS, CRSP, and Compustat",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nThis chapter shows how to connect to Wharton Research Data Services (WRDS), a popular provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics, CRSP and Compustat. Unfortunately, this data is not freely available, but most students and researchers typically have access to WRDS through their university libraries. Assuming that you have access to WRDS, we show you how to prepare and merge the databases and store them in the SQLite-database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the WRDS database.\nIf you don’t have access to WRDS, but still want to run the code in this book, we refer to our blog post on Dummy Data for Tidy Finance Readers without Access to WRDS, where show how to create a dummy database that contains the WRDS tables and corresponding columns such that all code chunks in this book can be executed with this dummy database.\nFirst, we load the R packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(scales)\nlibrary(RSQLite)\nlibrary(dbplyr)\nWe use the same date range as in the previous chapter to ensure consistency.\nstart_date &lt;- ymd(\"1960-01-01\")\nend_date &lt;- ymd(\"2024-12-31\")",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#accessing-wrds",
    "href": "r/wrds-crsp-and-compustat.html#accessing-wrds",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Accessing WRDS",
    "text": "Accessing WRDS\nWRDS is the most widely used source for asset and firm-specific financial data used in academic settings. WRDS is a data platform that provides data validation, flexible delivery options, and access to many different data sources. The data at WRDS is also organized in an SQL database, although they use the PostgreSQL engine. This database engine is just as easy to handle with R as SQLite. We use the RPostgres package to establish a connection to the WRDS database (Wickham, Ooms, and Müller 2022). Note that you could also use the odbc package to connect to a PostgreSQL database, but then you need to install the appropriate drivers yourself. RPostgres already contains a suitable driver.\n\nlibrary(RPostgres)\n\nTo establish a connection to WRDS, you use the function dbConnect() with arguments that specify the WRDS server and your login credentials. We defined environment variables for the purpose of this book because we obviously do not want (and are not allowed) to share our credentials with the rest of the world. See Setting Up Your Environment for information about why and how to create an .Renviron-file. Alternatively, you can replace Sys.getenv(\"WRDS_USER\") and Sys.getenv(\"WRDS_PASSWORD\") with your own credentials (but be careful not to share them with others or the public).\nAdditionally, you have to use two-factor authentication since May 2023 when establishing a PostgreSQL or other remote connections. You have two choices to provide the additional identification. First, if you have Duo Push enabled for your WRDS account, you will receive a push notification on your mobile phone when trying to establish a connection with the code below. Upon accepting the notification, you can continue your work. Second, you can log in to a WRDS website that requires two-factor authentication with your username and the same IP address. Once you have successfully identified yourself on the website, your username-IP combination will be remembered for 30 days, and you can comfortably use the remote connection below.\n\nwrds &lt;- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"WRDS_USER\"),\n  password = Sys.getenv(\"WRDS_PASSWORD\")\n)\n\nYou can also use the tidyfinance package to set the login credentials and create a connection.\n\nset_wrds_credentials()\nwrds &lt;- get_wrds_connection()\n\nThe remote connection to WRDS is very useful. Yet, the database itself contains many different tables. You can check the WRDS homepage to identify the table’s name you are looking for (if you go beyond our exposition). Alternatively, you can also query the data structure with the function dbSendQuery(). If you are interested, there is an exercise below that is based on WRDS’ tutorial on “Querying WRDS Data using R”. Furthermore, the penultimate section of this chapter shows how to investigate the structure of databases.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "href": "r/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Downloading and Preparing CRSP",
    "text": "Downloading and Preparing CRSP\nThe Center for Research in Security Prices (CRSP) provides the most widely used data for US stocks. We use the wrds connection object that we just created to first access monthly CRSP return data.1 Actually, we need two tables to get the desired data: (i) the CRSP monthly security file,\n\nmsf_db &lt;- tbl(wrds, I(\"crsp.msf_v2\"))\n\nand (ii) the identifying information,\n\nstksecurityinfohist_db &lt;- tbl(wrds, I(\"crsp.stksecurityinfohist\"))\n\nWe use the two remote tables to fetch the data we want to put into our local database. Just as above, the idea is that we let the WRDS database do all the work and just download the data that we actually need. We apply common filters and data selection criteria to narrow down our data of interest: (i) we use only stock prices from NYSE, Amex, and NASDAQ (primaryexch %in% c(\"N\", \"A\", \"Q\")) when or after issuance (conditionaltype %in% c(\"RW\", \"NW\")) for actively traded stocks (tradingstatusflg == \"A\")2, (ii) we keep only data in the time windows of interest, (iii) we keep only US-listed stocks as identified via no special share types (sharetype = 'NS'), security type equity (securitytype = 'EQTY'), security sub type common stock (securitysubtype = 'COM'), issuers that are a corporation (issuertype %in% c(\"ACOR\", \"CORP\")), and (iv) we keep only months within permno-specific start dates (secinfostartdt) and end dates (secinfoenddt). As of July 2022, there is no need to additionally download delisting information since it is already contained in the most recent version of msf (see our blog post about CRSP 2.0 for more information). Additionally, the industry information in stksecurityinfohist records the historic industry and should be used instead of the one stored under same variable name in msf_v2.\n\ncrsp_monthly &lt;- msf_db |&gt;\n  filter(mthcaldt &gt;= start_date & mthcaldt &lt;= end_date) |&gt;\n  select(-c(siccd, primaryexch, conditionaltype, tradingstatusflg)) |&gt; \n  inner_join(\n    stksecurityinfohist_db |&gt;\n      filter(sharetype == \"NS\" & \n               securitytype == \"EQTY\" & \n               securitysubtype == \"COM\" & \n               usincflg == \"Y\" & \n               issuertype %in% c(\"ACOR\", \"CORP\") & \n               primaryexch %in% c(\"N\", \"A\", \"Q\") &\n               conditionaltype %in% c(\"RW\", \"NW\") &\n               tradingstatusflg == \"A\") |&gt; \n        select(permno, secinfostartdt, secinfoenddt,\n               primaryexch, siccd),\n      join_by(permno)\n  ) |&gt; \n  filter(mthcaldt &gt;= secinfostartdt & mthcaldt &lt;= secinfoenddt) |&gt;\n  mutate(date = floor_date(mthcaldt, \"month\")) |&gt;\n  select(\n    permno, # Security identifier\n    date, # Month of the observation\n    ret = mthret, # Return\n    shrout, # Shares outstanding (in thousands)\n    prc = mthprc, # Last traded price in a month\n    primaryexch, # Primary exchange code\n    siccd # Industry code\n  ) |&gt;\n  collect() |&gt;\n  mutate(\n    date = ymd(date),\n    shrout = shrout * 1000\n  )\n\nNow, we have all the relevant monthly return data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters.\nThe first additional variable we create is market capitalization (mktcap), which is the product of the number of outstanding shares (shrout) and the last traded price in a month (prc). Note that in contrast to returns (ret), these two variables are not adjusted ex-post for any corporate actions like stock splits. Therefore, if you want to use a stock’s price, you need to adjust it with a cumulative adjustment factor. We also keep the market cap in millions of USD just for convenience, as we do not want to print huge numbers in our figures and tables. In addition, we set zero market capitalization to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(\n    mktcap = shrout * prc / 10^6,\n    mktcap = na_if(mktcap, 0)\n  )\n\nThe next variable we frequently use is the one-month lagged market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly CRSP data.\n\nmktcap_lag &lt;- crsp_monthly |&gt;\n  mutate(date = date %m+% months(1)) |&gt;\n  select(permno, date, mktcap_lag = mktcap)\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(mktcap_lag, join_by(permno, date))\n\nIf you wonder why we do not use the lag() function, e.g., via crsp_monthly |&gt; group_by(permno) |&gt; mutate(mktcap_lag = lag(mktcap)), take a look at the Exercises.\nNext, we transform primary listing exchange codes to explicit exchange names.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(exchange = case_when(\n    primaryexch == \"N\" ~ \"NYSE\",\n    primaryexch == \"A\" ~ \"AMEX\",\n    primaryexch == \"Q\" ~ \"NASDAQ\",\n    .default = \"Other\"\n  ))\n\nSimilarly, we transform industry codes to industry descriptions following Bali, Engle, and Murray (2016). Notice that there are also other categorizations of industries (e.g., Fama and French 1997) that are commonly used.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(industry = case_when(\n    siccd &gt;= 1 & siccd &lt;= 999 ~ \"Agriculture\",\n    siccd &gt;= 1000 & siccd &lt;= 1499 ~ \"Mining\",\n    siccd &gt;= 1500 & siccd &lt;= 1799 ~ \"Construction\",\n    siccd &gt;= 2000 & siccd &lt;= 3999 ~ \"Manufacturing\",\n    siccd &gt;= 4000 & siccd &lt;= 4899 ~ \"Transportation\",\n    siccd &gt;= 4900 & siccd &lt;= 4999 ~ \"Utilities\",\n    siccd &gt;= 5000 & siccd &lt;= 5199 ~ \"Wholesale\",\n    siccd &gt;= 5200 & siccd &lt;= 5999 ~ \"Retail\",\n    siccd &gt;= 6000 & siccd &lt;= 6799 ~ \"Finance\",\n    siccd &gt;= 7000 & siccd &lt;= 8999 ~ \"Services\",\n    siccd &gt;= 9000 & siccd &lt;= 9999 ~ \"Public\",\n    .default = \"Missing\"\n  ))\n\nNext, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop the risk-free rate from our tibble. Note that we ensure excess returns are bounded by -1 from below as a return less than -100 percent makes no sense conceptually. Before we can adjust the returns, we have to connect to our database and load the table factors_ff3_monthly.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(date, rf) |&gt;\n  collect()\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(factors_ff3_monthly,\n    join_by(date)\n  ) |&gt;\n  mutate(\n    ret_excess = ret - rf,\n    ret_excess = pmax(ret_excess, -1)\n  ) |&gt;\n  select(-rf)\n\nThe tidyfinance package provides a shortcut to implement all these processing steps from above.\n\ncrsp_monthly &lt;- download_data(\n  type = \"wrds_crsp_monthly\", \n  start_date = start_date,\n  end_date = end_date\n)\n\nSince excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  drop_na(ret_excess, mktcap, mktcap_lag)\n\nFinally, we store the monthly CRSP file in our database.\n\ndbWriteTable(\n  tidy_finance,\n  \"crsp_monthly\",\n  value = crsp_monthly,\n  overwrite = TRUE\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "href": "r/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "title": "WRDS, CRSP, and Compustat",
    "section": "First Glimpse of the CRSP Sample",
    "text": "First Glimpse of the CRSP Sample\nBefore we move on to other data sources, let us look at some descriptive statistics of the CRSP sample, which is our main source for stock returns.\nFigure 1 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks. The number of stocks listed on AMEX decreased steadily over the last couple of decades. By the end of 2024, there were 2382 stocks with a primary listing on NASDAQ, 1256 on NYSE, and 155 on AMEX. \n\ncrsp_monthly |&gt;\n  count(exchange, date) |&gt;\n  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by listing exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\nFigure 1: Number of stocks in the CRSP sample listed at each of the US exchanges.\n\n\n\n\n\nNext, we look at the aggregate market capitalization grouped by the respective listing exchanges in Figure 2. To ensure that we look at meaningful data which is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange and plotting it just as if they were in memory. All values in Figure 2 are at the end of 2024 USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.\n\ntbl(tidy_finance, \"crsp_monthly\") |&gt;\n  left_join(tbl(tidy_finance, \"cpi_monthly\"), join_by(date)) |&gt;\n  group_by(date, exchange) |&gt;\n  summarize(\n    mktcap = sum(mktcap, na.rm = TRUE) / cpi,\n    .groups = \"drop\"\n  ) |&gt;\n  collect() |&gt;\n  mutate(date = ymd(date)) |&gt;\n  ggplot(aes(\n    x = date, y = mktcap / 1000,\n    color = exchange, linetype = exchange\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly market cap by listing exchange in billions of Dec 2024 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\nFigure 2: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the horizontal axis reflect the buying power of billion USD in December 2024.\n\n\n\n\n\nOf course, performing the computation in the database is not really meaningful because we can easily pull all the required data into our memory. The code chunk above is slower than performing the same steps on tables that are already in memory. However, we just want to illustrate that you can perform many things in the database before loading the data into your memory. Before we proceed, we load the monthly CPI data.\n\ncpi_monthly &lt;- tbl(tidy_finance, \"cpi_monthly\") |&gt;\n  collect()\n\nNext, we look at the same descriptive statistics by industry. Figure 3 plots the number of stocks in the sample for each of the SIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.\n\ncrsp_monthly_industry &lt;- crsp_monthly |&gt;\n  left_join(cpi_monthly, join_by(date)) |&gt;\n  group_by(date, industry) |&gt;\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap) / mean(cpi),\n    .groups = \"drop\"\n  )\n\ncrsp_monthly_industry |&gt;\n  ggplot(aes(\n    x = date,\n    y = securities,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\nFigure 3: Number of stocks in the CRSP sample associated with different industries.\n\n\n\n\n\nWe also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in Figure 4. All values are again in terms of billions of end of 2024 USD. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.\n\ncrsp_monthly_industry |&gt;\n  ggplot(aes(\n    x = date,\n    y = mktcap / 1000,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market cap by industry in billions as of Dec 2024 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\nFigure 4: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the y-axis reflect the buying power of billion USD in December 2024.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#daily-crsp-data",
    "href": "r/wrds-crsp-and-compustat.html#daily-crsp-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Daily CRSP Data",
    "text": "Daily CRSP Data\nBefore we turn to accounting data, we provide a proposal for downloading daily CRSP data with the same filters used for the monthly data (i.e., using information from stksecurityinfohist). While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can exceed 20 GB. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire dataset to your local database), and in our experience, the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your R session.\nThere is a solution to this challenge. As with many big data problems, you can split up the big task into several smaller tasks that are easier to handle. That is, instead of downloading data about all stocks at once, download the data in small batches of stocks consecutively. Such operations can be implemented in for-loops, where we download, prepare, and store the data for a small number of stocks in each iteration. This operation might nonetheless take around 5 minutes, depending on your internet connection. To keep track of the progress, we create ad-hoc progress updates using message(). Notice that we also use the function dbWriteTable() here with the option to append the new data to an existing table, when we process the second and all following batches. As for the monthly CRSP data, there is no need to adjust for delisting returns in the daily CRSP data since July 2022.\n\ndsf_db &lt;- tbl(wrds, I(\"crsp.dsf_v2\"))\nstksecurityinfohist_db &lt;- tbl(wrds, I(\"crsp.stksecurityinfohist\"))\n\nfactors_ff3_daily &lt;- tbl(tidy_finance, \"factors_ff3_daily\") |&gt;\n  collect()\n\npermnos &lt;- stksecurityinfohist_db |&gt;\n  distinct(permno) |&gt; \n  pull(permno)\n\nbatch_size &lt;- 500\nbatches &lt;- ceiling(length(permnos) / batch_size)\n\nfor (j in 1:batches) {\n  \n  permno_batch &lt;- permnos[\n    ((j - 1) * batch_size + 1):min(j * batch_size, length(permnos))\n  ]\n\n  crsp_daily_sub &lt;- dsf_db |&gt;\n    filter(permno %in% permno_batch) |&gt; \n    filter(dlycaldt &gt;= start_date & dlycaldt &lt;= end_date) |&gt; \n    inner_join(\n      stksecurityinfohist_db |&gt;\n        filter(sharetype == \"NS\" & \n                securitytype == \"EQTY\" & \n                securitysubtype == \"COM\" & \n                usincflg == \"Y\" & \n                issuertype %in% c(\"ACOR\", \"CORP\") & \n                primaryexch %in% c(\"N\", \"A\", \"Q\") &\n                conditionaltype %in% c(\"RW\", \"NW\") &\n                tradingstatusflg == \"A\") |&gt; \n        select(permno, secinfostartdt, secinfoenddt),\n      join_by(permno)\n    ) |&gt;\n    filter(dlycaldt &gt;= secinfostartdt & dlycaldt &lt;= secinfoenddt)  |&gt; \n    select(permno, date = dlycaldt, ret = dlyret) |&gt;\n    collect() |&gt;\n    drop_na()\n\n  if (nrow(crsp_daily_sub) &gt; 0) {\n    \n    crsp_daily_sub &lt;- crsp_daily_sub |&gt;\n      left_join(factors_ff3_daily |&gt;\n        select(date, rf), join_by(date)) |&gt;\n      mutate(\n        ret_excess = ret - rf,\n        ret_excess = pmax(ret_excess, -1)\n      ) |&gt;\n      select(permno, date, ret, ret_excess)\n\n    dbWriteTable(\n      tidy_finance,\n      \"crsp_daily\",\n      value = crsp_daily_sub,\n      overwrite = ifelse(j == 1, TRUE, FALSE),\n      append = ifelse(j != 1, TRUE, FALSE)\n    )\n  }\n\n  message(\"Batch \", j, \" out of \", batches, \" done (\", percent(j / batches), \")\\n\")\n}\n\nEventually, we end up with more than 71 million rows of daily return data. Note that we only store the identifying information that we actually need, namely permno and date alongside the excess returns. We thus ensure that our local database contains only the data that we actually use.\nTo download the daily CRSP data via the tidyfinance package, you can call:\n\ncrsp_daily &lt;- download_data(\n  type = \"wrds_crsp_daily\",\n  start_date = start_date,\n  end_date = end_date\n)\n\nNote that you need at least 16 GB of memory to hold all the daily CRSP returns in memory. We hence recommend to use loop the function over different date periods and store the results.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "href": "r/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Preparing Compustat Data",
    "text": "Preparing Compustat Data\nFirm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters. The commonly used source for firm financial information is Compustat provided by S&P Global Market Intelligence, which is a global data vendor that provides financial, statistical, and market information on active and inactive companies throughout the world. For US and Canadian companies, annual history is available back to 1950 and quarterly as well as monthly histories date back to 1962.\nTo access Compustat data, we can again tap WRDS, which hosts the funda table that contains annual firm-level information on North American companies.\n\nfunda_db &lt;- tbl(wrds, I(\"comp.funda\"))\n\nWe follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, which includes companies that are primarily involved in manufacturing, services, and other non-financial business activities,3 (ii) in the standard format (i.e., consolidated information in standard presentation), (iii) reported in USD,4 and (iv) only data in the desired time window.\n\ncompustat &lt;- funda_db |&gt;\n  filter(\n    indfmt == \"INDL\" &\n      datafmt == \"STD\" & \n      consol == \"C\" &\n      curcd == \"USD\" &\n      datadate &gt;= start_date & datadate &lt;= end_date\n  ) |&gt;\n  select(\n    gvkey, # Firm identifier\n    datadate, # Date of the accounting data\n    seq, # Stockholders' equity\n    ceq, # Total common/ordinary equity\n    at, # Total assets\n    lt, # Total liabilities\n    txditc, # Deferred taxes and investment tax credit\n    txdb, # Deferred taxes\n    itcb, # Investment tax credit\n    pstkrv, # Preferred stock redemption value\n    pstkl, # Preferred stock liquidating value\n    pstk, # Preferred stock par value\n    capx, # Capital investment\n    oancf, # Operating cash flow\n    sale,  # Revenue\n    cogs, # Costs of goods sold\n    xint, # Interest expense\n    xsga # Selling, general, and administrative expenses\n  ) |&gt;\n  collect()\n\nNext, we calculate the book value of preferred stock and equity be and the operating profitability op inspired by the variable definitions in Ken French’s data library. Note that we set negative or zero equity to missing which is a common practice when working with book-to-market ratios (see Fama and French 1992 for details).\n\ncompustat &lt;- compustat |&gt;\n  mutate(\n    be = coalesce(seq, ceq + pstk, at - lt) +\n      coalesce(txditc, txdb + itcb, 0) -\n      coalesce(pstkrv, pstkl, pstk, 0),\n    be = if_else(be &lt;= 0, NA, be),\n    op = (sale - coalesce(cogs, 0) - \n            coalesce(xsga, 0) - coalesce(xint, 0)) / be,\n  )\n\nWe keep only the last available information for each firm-year group. Note that datadate defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2022). Therefore, datadate is not the date when data was made available to the public. Check out the exercises for more insights into the peculiarities of datadate.\n\ncompustat &lt;- compustat |&gt;\n  mutate(year = year(datadate)) |&gt;\n  group_by(gvkey, year) |&gt;\n  filter(datadate == max(datadate)) |&gt;\n  ungroup()\n\nWe also compute the investment ratio inv according to Ken French’s variable definitions as the change in total assets from one fiscal year to another. Note that we again use the approach using joins as introduced with the CRSP data above to construct lagged assets.\n\ncompustat &lt;- compustat |&gt; \n  left_join(\n    compustat |&gt; \n      select(gvkey, year, at_lag = at) |&gt; \n      mutate(year = year + 1), \n    join_by(gvkey, year)\n  ) |&gt; \n  mutate(\n    inv = at / at_lag - 1,\n    inv = if_else(at_lag &lt;= 0, NA, inv)\n  )\n\nWith the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database.\n\ndbWriteTable(\n  tidy_finance,\n  \"compustat\",\n  value = compustat,\n  overwrite = TRUE\n)\n\nThe tidyfinance package provides a shortcut for these processing steps as well:\n\ncompustat &lt;- download_data(\n  type = \"wrds_compustat_annual\",\n  start_date = start_date,\n  end_date = end_date\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "href": "r/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Merging CRSP with Compustat",
    "text": "Merging CRSP with Compustat\nUnfortunately, CRSP and Compustat use different keys to identify stocks and firms. CRSP uses permno for stocks, while Compustat uses gvkey to identify firms. Fortunately, a curated matching table on WRDS allows us to merge CRSP and Compustat, so we create a connection to the CRSP-Compustat Merged table (provided by CRSP).\n\nccm_linking_table_db &lt;- tbl(wrds, I(\"crsp.ccmxpf_lnkhist\"))\n\nThe linking table contains links between CRSP and Compustat identifiers from various approaches. However, we need to make sure that we keep only relevant and correct links, again following the description outlined in Bali, Engle, and Murray (2016). Note also that currently active links have no end date, so we just enter the current date via today().\n\nccm_linking_table &lt;- ccm_linking_table_db |&gt;\n  filter(\n    linktype %in% c(\"LU\", \"LC\") &\n    linkprim %in% c(\"P\", \"C\")\n  ) |&gt;\n  select(permno = lpermno, gvkey, linkdt, linkenddt) |&gt;\n  collect() |&gt;\n  mutate(linkenddt = replace_na(linkenddt, today()))\n\nWe use these links to create a new table with a mapping between stock identifier, firm identifier, and month. We then add these links to the Compustat gvkey to our monthly stock data.\n\nccm_links &lt;- crsp_monthly |&gt;\n  inner_join(ccm_linking_table, \n             join_by(permno), relationship = \"many-to-many\") |&gt;\n  filter(!is.na(gvkey) & \n           (date &gt;= linkdt & date &lt;= linkenddt)) |&gt;\n  select(permno, gvkey, date)\n\nTo fetch these links via tidyfinance, you can call:\n\nccm_links &lt;- download_data(type = \"wrds_ccm_links\")\n\nAs the last step, we update the previously prepared monthly CRSP file with the linking information in our local database.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(ccm_links, join_by(permno, date))\n\ndbWriteTable(\n  tidy_finance,\n  \"crsp_monthly\",\n  value = crsp_monthly,\n  overwrite = TRUE\n)\n\nBefore we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, Figure 5 plots the share of securities with book equity values for each exchange. It turns out that the coverage is pretty bad for AMEX- and NYSE-listed stocks in the 1960s but hovers around 80 percent for all periods thereafter. We can ignore the erratic coverage of securities that belong to the other category since there is only a handful of them anyway in our sample.\n\ncrsp_monthly |&gt;\n  group_by(permno, year = year(date)) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  left_join(compustat, join_by(gvkey, year)) |&gt;\n  group_by(exchange, year) |&gt;\n  summarize(\n    share = n_distinct(permno[!is.na(be)]) / n_distinct(permno),\n    .groups = \"drop\"\n  ) |&gt;\n  ggplot(aes(\n    x = year, \n    y = share, \n    color = exchange,\n    linetype = exchange\n    )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Share of securities with book equity values by exchange\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\nFigure 5: End-of-year share of securities with book equity values by listing exchange.\n\n\n\n\n\nThe difference arises from the distinct coverage of the two databases. CRSP focuses on stock prices from the major US exchanges, capturing a narrower universe of firms. In contrast, Compustat provides financial statement data for a broader set of companies, including all US firms that file 10‑K reports, many Canadian firms, and those listed on major or regional exchanges, traded over‑the‑counter, or even companies with a notable amount of publicly issued debt.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#some-tricks-for-postgresql-databases",
    "href": "r/wrds-crsp-and-compustat.html#some-tricks-for-postgresql-databases",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Some Tricks for PostgreSQL Databases",
    "text": "Some Tricks for PostgreSQL Databases\nAs we mentioned above, the WRDS database runs on PostgreSQL rather than SQLite. Finding the right tables for your data needs can be tricky in the WRDS PostgreSQL instance, as the tables are organized in schemas. If you wonder what the purpose of schemas is, check out this documetation. For instance, if you want to find all tables that live in the crsp schema, you run\n\ndbListObjects(wrds, Id(schema = \"crsp\"))\n\nThis operation returns a list of all tables that belong to the crsp family on WRDS, e.g., &lt;Id&gt; schema = crsp, table = msenames. Similarly, you can fetch a list of all tables that belong to the comp family via\n\ndbListObjects(wrds, Id(schema = \"comp\"))\n\nIf you want to get all schemas, then run\n\ndbListObjects(wrds)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#key-takeaways",
    "href": "r/wrds-crsp-and-compustat.html#key-takeaways",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nWRDS provides secure access to essential financial databases like CRSP and Compustat, which are critical for empirical finance research.\nCRSP data provides return, market capitalization and industry data for US common stocks listed on NYSE, NASDAQ, or AMEX.\nCompustat provides firm-level accounting data such as book equity, profitability, and investment.\nThe tidyfinance R package streamlines all major data download and processing steps, making it easy to replicate and scale financial data analysis.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#exercises",
    "href": "r/wrds-crsp-and-compustat.html#exercises",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Exercises",
    "text": "Exercises\n\nCheck out the structure of the WRDS database by sending queries in the spirit of “Querying WRDS Data using R” and verify the output with dbListObjects(). How many tables are associated with CRSP? Can you identify what is stored within msp500?\nCompute mkt_cap_lag using lag(mktcap) rather than using joins as above. Filter out all the rows where the lag-based market capitalization measure is different from the one we computed above. Why are the two measures they different?\nPlot the average market capitalization of firms for each exchange and industry, respectively, over time. What do you find?\nIn the compustat table, datadate refers to the date to which the fiscal year of a corresponding firm refers. Count the number of observations in Compustat by month of this date variable. What do you find? What does the finding suggest about pooling observations with the same fiscal year?\nGo back to the original Compustat data in funda_db and extract rows where the same firm has multiple rows for the same fiscal year. What is the reason for these observations?\nKeep the last observation of crsp_monthly by year and join it with the compustat table. Create the following plots: (i) aggregate book equity by exchange over time and (ii) aggregate annual book equity by industry over time. Do you notice any different patterns to the corresponding plots based on market capitalization?\nRepeat the analysis of market capitalization for book equity, which we computed from the Compustat data. Then, use the matched sample to plot book equity against market capitalization. How are these two variables related?\nBefore merging the CRSP and Compustat datasets, calculate and plot the proportion of Compustat firms that have corresponding stock return data in CRSP for each year. How does the coverage evolve over time?",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#footnotes",
    "href": "r/wrds-crsp-and-compustat.html#footnotes",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe tbl() function creates a lazy table in our R session based on the remote WRDS database. To look up specific tables, we use the I(\"schema_name.table_name\") approach.↩︎\nThese three criteria jointly replicate the filter exchcd %in% c(1, 2, 3, 31, 32, 33) used for the legacy version of CRSP. If you do not want to include stocks at issuance, you can set the conditionaltype == \"RW\", which is equivalent to the restriction of exchcd %in% c(1, 2, 3) with the old CRSP format.↩︎\nCompanies that operate in the banking, insurance, or utilities sector typically report in different industry formats that reflect their specific regulatory requirements.↩︎\nCompustat also contains reports in CAD, which can lead a currency mismatch, e.g., when relating book equity to market equity.↩︎",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html",
    "href": "r/financial-statement-analysis.html",
    "title": "Financial Statement Analysis",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nFinancial statements and ratios are fundamental tools for understanding and evaluating companies. While we discuss how assets are priced in equilibrium in the previous chapter on the Capital Asset Pricing Model, this chapter examines how investors and analysts assess companies using accounting information. Financial statements serve as the primary source of standardized information about a company’s operations, financial position, and performance. Their standardization and legal requirements make them particularly valuable as all companies must file financial statements.\nBuilding on this standardized information, financial ratios transform raw accounting data into meaningful metrics that facilitate analysis across companies and over time. These ratios serve multiple purposes in both academic research and practical applications. They enable investors to benchmark companies against their peers, identify industry trends, and screen for investment opportunities. In academic finance, ratios play a crucial role in asset pricing models (e.g., the book-to-market ratio in the Fama-French three-factor model) and corporate finance (e.g., capital structure research). In many practical applications, ratios help assess a company’s financial health and performance.\nThis chapter demonstrates how to access, process, and analyze financial statements using R. We start by reviewing the financial statements balance sheet, income statement, and cash flow statement. Then, we download publicly available statements to calculate key financial ratios, implement common screening strategies, and evaluate companies. Our analysis combines theoretical frameworks with practical implementation, providing tools for both academic research and investment practice.\nFor the purpose of this chapter, we use financial statements provided by the US Securities and Exchange Commission (i.e., SEC). While the SEC provides a web interface to search filings, programmatic access to financial statements greatly facilitates systematic analyses as ours. The Financial Modeling Prep (FMP) API offers such programmatic access, which we can leverage through the R package fmpapi.\nThe FMP API’s free tier provides access to:\nNext to fmpapi, we use the following packages throughout this chapter:\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(scales)\nlibrary(ggrepel)\nlibrary(fmpapi)",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#balance-sheet-statements",
    "href": "r/financial-statement-analysis.html#balance-sheet-statements",
    "title": "Financial Statement Analysis",
    "section": "Balance Sheet Statements",
    "text": "Balance Sheet Statements\nThe balance sheet is one of the three primary financial statements capturing a company’s financial position at a specific moment in time. The statement lists all uses (assets) and sources (liabilities and equity) of funds, which result in the fundamental accounting equation:\n\\[\\text{Assets} = \\text{Liabilities} + \\text{Equity}\\]\nThis equation reflects a core principle of accounting: a company’s resources (assets) must equal its sources of funding, whether from creditors (liabilities) or investors (shareholders’ equity). Assets represent resources that the company controls and expects to generate future economic benefits, such as cash, inventory, or equipment. Liabilities encompass all obligations to external parties, from short-term payables to long-term debt. Shareholders’ equity represents the residual claim on assets after accounting for all liabilities.\nFigure 1 provides a stylized representation of a balance sheet’s structure. The visualization highlights how assets on the left side must equal the combined claims of creditors and shareholders on the right side.\n\n\n\n\n\n\nFigure 1: A stylized representation of a balance sheet statement.\n\n\n\nThe asset side of the balance sheet typically comprises three main categories, each serving different roles in the company’s operations:\n\nCurrent assets: These are assets expected to be converted into cash or used within one operating cycle (typically one year). They include, e.g., cash and cash equivalents, short-term investments, accounts receivable (money owed by customers), and inventory (raw materials, work in progress, and finished goods).\nNon-current assets: These long-term assets support the company’s operations beyond one year like, e.g., property, plant, and equipment (PP&E), long-term investments, and other long-term assets.\nIntangible assets: These non-physical assets often represent significant value in modern companies, e.g., patents and intellectual property, trademarks and brands, and goodwill from acquisitions (a premium paid on the book value of the acquired assets). Intangible assets are usually also considered long-term assets, which means that they are included in the group of non-current assets.\n\nFigure 2 illustrates this breakdown of assets, showing how companies classify their resources.\n\n\n\n\n\n\nFigure 2: A stylized representation of a balance sheet beakdown.\n\n\n\nFigure 2 also shows the breakdown of liabilities. The liability side similarly follows a temporal classification, dividing obligations based on when they come due:\n\nCurrent liabilities: Obligations due within one year such as accounts payable, short-term debt, current portion of long-term debt, and accrued expenses.\nNon-current liabilities: Long-term obligations such as long-term debt, bonds payable, deferred tax liabilities, and pension obligations.\n\nLastly, the equity section represents ownership claims and typically consists of:\n\nRetained earnings: Accumulated profits reinvested in the business.\nCommon stock: Par value and additional paid-in capital from share issuance.\nPreferred stock: Hybrid securities with characteristics of both debt and equity.\n\nFigure 2 also depicts this equity structure, showing how companies track different forms of ownership claims.\nTo illustrate these concepts in practice, Figure 3 presents Microsoft’s balance sheet from 2023. This real-world example demonstrates how one of the world’s largest technology companies structures its financial position, reflecting both traditional elements like PP&E and modern aspects like significant intangible assets.\n\n\n\n\n\n\nFigure 3: A screenshot of the balance sheet statement of Microsoft in 2023.\n\n\n\nWhile there are more details, the basic structure is exactly the same as in the introduction above. Importantly, the balance sheet obeys the fundamental accounting equation as assets are equal to the sum of liabilities and equity. In subsequent sections, we will explore how to analyze such statements using financial ratios, particularly focusing on measures of liquidity, solvency, and efficiency.\nLet us examine Microsoft’s balance sheet statements using the fmp_get() function. This function requires three main arguments: The type of financial data to retrieve (resource), the stock ticker symbol (symbol), and additional parameters like periodicity and number of periods (params).\n\nfmp_get(\n  resource = \"balance-sheet-statement\",\n  symbol = \"MSFT\",\n  params = list(period = \"annual\", limit = 5)\n)\n\n# A tibble: 5 × 61\n  date       symbol reported_currency cik        filing_date\n  &lt;date&gt;     &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;      &lt;date&gt;     \n1 2025-06-30 MSFT   USD               0000789019 2025-07-30 \n2 2024-06-30 MSFT   USD               0000789019 2024-07-30 \n3 2023-06-30 MSFT   USD               0000789019 2023-07-27 \n4 2022-06-30 MSFT   USD               0000789019 2022-07-28 \n5 2021-06-30 MSFT   USD               0000789019 2021-07-29 \n# ℹ 56 more variables: accepted_date &lt;dttm&gt;, fiscal_year &lt;chr&gt;,\n#   period &lt;chr&gt;, cash_and_cash_equivalents &lt;dbl&gt;,\n#   short_term_investments &lt;dbl&gt;,\n#   cash_and_short_term_investments &lt;dbl&gt;, net_receivables &lt;dbl&gt;,\n#   accounts_receivables &lt;dbl&gt;, other_receivables &lt;int&gt;,\n#   inventory &lt;dbl&gt;, prepaids &lt;int&gt;, other_current_assets &lt;dbl&gt;,\n#   total_current_assets &lt;dbl&gt;, …\n\n\nThe function returns a data frame containing detailed balance sheet information, with each row representing a different reporting period. This structured format makes it easy to analyze trends over time and calculate financial ratios. We can see how the data aligns with the balance sheet components we discussed earlier, from current assets like cash and receivables to long-term assets and various forms of liabilities and equity.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#income-statements",
    "href": "r/financial-statement-analysis.html#income-statements",
    "title": "Financial Statement Analysis",
    "section": "Income Statements",
    "text": "Income Statements\nWhile the balance sheet provides a snapshot of a company’s financial position at a point in time, the income statement (also called profit and loss statement or PnL) measures financial performance over a period, typically a quarter or year. It follows a hierarchical structure that progressively captures different levels of profitability:\n\nRevenue (Sales): The total income generated from goods or services sold.\nCost of goods sold (COGS): Direct costs associated with producing the goods or services (raw materials, labor, etc.).\nGross profit: Revenue minus COGS, showing the basic profitability from core operations.\nOperating expenses: Costs related to regular business operations (e.g., salaries, rent, and marketing).\nOperating income (EBIT): Earnings before interest and taxes (measures profitability from core operations before financing and tax costs), often also referred to as operating profit.\nInterest and taxes: The interest paid on debt is deducted for determining the taxable income.\nNet income: The “bottom line”, total profit after all expenses, interest, and taxes are subtracted from revenue.\n\nFigure 4 illustrates this progression from total revenue to net income, showing how various costs and expenses are subtracted to arrive at different measure of profit.\n\n\n\n\n\n\nFigure 4: A stylized representation of an income statement.\n\n\n\nConsider Microsoft’s 2023 income statement in Figure 5, which exemplifies how a leading technology company reports its financial performance:\n\n\n\n\n\n\nFigure 5: A screenshot of the income statement of Microsoft in 2023.\n\n\n\nWe can also access this data programmatically using the FMP API:\n\nfmp_get(\n  resource = \"income-statement\",\n  symbol = \"MSFT\",\n  params = list(period = \"annual\", limit = 5)\n)\n\n# A tibble: 5 × 39\n  date       symbol reported_currency cik        filing_date\n  &lt;date&gt;     &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;      &lt;date&gt;     \n1 2025-06-30 MSFT   USD               0000789019 2025-07-30 \n2 2024-06-30 MSFT   USD               0000789019 2024-07-30 \n3 2023-06-30 MSFT   USD               0000789019 2023-07-27 \n4 2022-06-30 MSFT   USD               0000789019 2022-07-28 \n5 2021-06-30 MSFT   USD               0000789019 2021-07-29 \n# ℹ 34 more variables: accepted_date &lt;dttm&gt;, fiscal_year &lt;chr&gt;,\n#   period &lt;chr&gt;, revenue &lt;dbl&gt;, cost_of_revenue &lt;dbl&gt;,\n#   gross_profit &lt;dbl&gt;, research_and_development_expenses &lt;dbl&gt;,\n#   general_and_administrative_expenses &lt;dbl&gt;,\n#   selling_and_marketing_expenses &lt;dbl&gt;,\n#   selling_general_and_administrative_expenses &lt;dbl&gt;,\n#   other_expenses &lt;int&gt;, operating_expenses &lt;dbl&gt;, …\n\n\nIn later sections, we will use income statement items to calculate important profitability ratios and examine how they compare across companies and industries. The income statement’s focus on performance complements the balance sheet’s position snapshot, providing a more complete picture of a company’s core business operations",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#cash-flow-statements",
    "href": "r/financial-statement-analysis.html#cash-flow-statements",
    "title": "Financial Statement Analysis",
    "section": "Cash Flow Statements",
    "text": "Cash Flow Statements\nThe cash flow statement complements the balance sheet and income statement by tracking the actual movement of cash through the business. While the income statement shows profitability and the balance sheet shows financial position, the cash flow statement reveals a company’s ability to generate and manage cash - a crucial aspect of every business. The statement is divided into three main categories:\n\nOperating activities: Cash generated from a company’s core business activities (i.e., net income adjusted for non-cash items like depreciation and changes in working capital).\nFinancing activities: Cash flows related to borrowing, repaying debt, issuing equity, or paying dividends.\nInvesting activities: Cash spent on or received from long-term investments, such as purchasing or selling property and equipment.\n\nFigure 6 illustrates these three categories of cash flows, which map into changes in the company’s cash balance.\n\n\n\n\n\n\nFigure 6: A stylized representation of a cash flow statement.\n\n\n\nThe statement reconciles accrual-based accounting (used in the income statement) with actual cash movements. This reconciliation is crucial because profitable companies can still face cash shortages, and unprofitable companies might maintain positive cash flow. We complement the brief introduction, by Microsoft’s 2023 cash flow statement in Figure 7.\n\n\n\n\n\n\nFigure 7: A screenshot of the cash flow statement of Microsoft in 2023.\n\n\n\nOf course, we can access this data through the FMP API:\n\nfmp_get(\n  resource = \"cash-flow-statement\",\n  symbol = \"MSFT\",\n  params = list(period = \"annual\", limit = 5)\n)\n\n# A tibble: 5 × 47\n  date       symbol reported_currency cik        filing_date\n  &lt;date&gt;     &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;      &lt;date&gt;     \n1 2025-06-30 MSFT   USD               0000789019 2025-07-30 \n2 2024-06-30 MSFT   USD               0000789019 2024-07-30 \n3 2023-06-30 MSFT   USD               0000789019 2023-07-27 \n4 2022-06-30 MSFT   USD               0000789019 2022-07-28 \n5 2021-06-30 MSFT   USD               0000789019 2021-07-29 \n# ℹ 42 more variables: accepted_date &lt;dttm&gt;, fiscal_year &lt;chr&gt;,\n#   period &lt;chr&gt;, net_income &lt;dbl&gt;,\n#   depreciation_and_amortization &lt;dbl&gt;, deferred_income_tax &lt;dbl&gt;,\n#   stock_based_compensation &lt;dbl&gt;, change_in_working_capital &lt;dbl&gt;,\n#   accounts_receivables &lt;dbl&gt;, inventory &lt;int&gt;,\n#   accounts_payables &lt;dbl&gt;, other_working_capital &lt;dbl&gt;,\n#   other_non_cash_items &lt;int&gt;, …\n\n\nIn subsequent sections, we will use cash flow data to calculate important cash flow ratios that help assess a company’s liquidity, capital allocation efficiency, and overall financial sustainability. The combination of all three financial statements - balance sheet, income statement, and cash flow statement - provides a comprehensive view of a company’s financial health and performance.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#download-financial-statements",
    "href": "r/financial-statement-analysis.html#download-financial-statements",
    "title": "Financial Statement Analysis",
    "section": "Download Financial Statements",
    "text": "Download Financial Statements\nWe now turn to downloading and processing statements for multiple companies. The next code chunk demonstrates how to retrieve financial data for selected stocks that are supported in the free tier of FMP.\n\nsample &lt;- c(\n  \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"NVDA\", \"META\", \"NFLX\", \"DIS\", \"NKE\",\n  \"WMT\", \"KO\", \"JPM\", \"BAC\", \"V\", \"XOM\", \"CVX\", \"JNJ\", \"PFE\", \"INTC\",\n  \"AMD\", \"SBUX\", \"BABA\", \"UBER\", \"CSCO\"\n)\n\nparams &lt;- list(period = \"annual\", limit = 5)\n\nbalance_sheet_statements &lt;- sample |&gt;\n  map_df(\n    \\(x) {\n      fmp_get(resource = \"balance-sheet-statement\", symbol = x, params = params)\n    }\n  )\n\nincome_statements &lt;- sample |&gt;\n  map_df(\n    \\(x) fmp_get(resource = \"income-statement\", symbol = x, params = params)\n  )\n\ncash_flow_statements &lt;- sample |&gt;\n  map_df(\n    \\(x) fmp_get(resource = \"cash-flow-statement\", symbol = x, params = params)\n  )\n\nThe resulting data sets provide a foundation for cross-sectional analyses of financial ratios and trends across major U.S. companies. In the following sections, we use these data sets to calculate various financial ratios and analyze patterns in corporate financial performance.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#liquidity-ratios",
    "href": "r/financial-statement-analysis.html#liquidity-ratios",
    "title": "Financial Statement Analysis",
    "section": "Liquidity Ratios",
    "text": "Liquidity Ratios\nLiquidity ratios assess a company’s ability to meet its short-term obligations and are typically calculated using balance sheet items. These ratios are particularly important for creditors and investors concerned about a company’s short-term financial health and ability to cover immediate obligations.\nThe Current Ratio is the most basic measure of liquidity, comparing all current assets to current liabilities:\n\\[\\text{Current Ratio} = \\frac{\\text{Current Assets}}{\\text{Total Liabilities}}\\]\nA ratio above one indicates that the company has enough current assets to cover its current liabilities, which are due within one year as discussed above.\nHowever, not all current assets are equally liquid, i.e., can be easily sold to meet a company’s obligations. This aspect is reflected in the Quick Ratio:\n\\[\\text{Quick Ratio} = \\frac{\\text{Current Assets - Inventory}}{\\text{Current Liabilities}}\\] The Quick Ratio provides a more stringent measure of liquidity by excluding inventory, which is typically the least liquid current asset. Furthermore, a company without inventory for production or sale will have a difficult operating position. A ratio above one suggests strong short-term solvency without relying on selling off inventory.\nThe most conservative liquidity measure is the Cash Ratio:\n\\[\\text{Cash Ratio} = \\frac{\\text{Cash and Cash Equivalents}}{\\text{Current Liabilities}}\\]\nThis ratio focuses solely on the most liquid assets - cash and cash equivalents. While a ratio of one indicates robust liquidity, most companies maintain lower cash ratios to avoid holding excessive non-productive assets. Afterall, all that cash could also be distributed to equity or pay down costly debt.\nNext, we calculate these ratios for all stocks, focusing on four major technology companies:\n\nselected_symbols &lt;- c(\"MSFT\", \"AAPL\", \"AMZN\", \"NVDA\")\n\nbalance_sheet_statements &lt;- balance_sheet_statements |&gt;\n  mutate(\n    fiscal_year = as.integer(fiscal_year),\n    current_ratio = total_current_assets / total_assets,\n    quick_ratio = (total_current_assets - inventory) /\n      total_current_liabilities,\n    cash_ratio = cash_and_cash_equivalents / total_current_liabilities,\n    label = if_else(symbol %in% selected_symbols, symbol, NA),\n  )\n\nFigure 8 compares the three liquidity ratios across Microsoft, Apple, and Amazon for 2023. We call such an analysis a cross-sectional comparison.\n\nbalance_sheet_statements |&gt;\n  filter(fiscal_year == 2023 & !is.na(label)) |&gt;\n  select(symbol, contains(\"ratio\")) |&gt;\n  pivot_longer(-symbol) |&gt;\n  mutate(name = str_to_title(str_replace_all(name, \"_\", \" \"))) |&gt;\n  ggplot(aes(x = value, y = name, fill = symbol)) +\n  geom_col(position = \"dodge\") +\n  scale_x_continuous(labels = percent) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = NULL,\n    title = \"Liquidity ratios for selected stocks for 2023\"\n  )\n\n\n\n\n\n\n\nFigure 8: Liquidity ratios are based on financial statements provided through the FMP API.\n\n\n\n\n\nWhile we are not commenting on the ratios in detail here, the liquidity ratios for Microsoft, Apple, and Amazon in 2023 reveal distinct patterns. Generally, higher liquidity ratios signal a more convervative approach by holding larger liquidity buffers in the company.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#leverage-ratios",
    "href": "r/financial-statement-analysis.html#leverage-ratios",
    "title": "Financial Statement Analysis",
    "section": "Leverage Ratios",
    "text": "Leverage Ratios\nLeverage ratios assess a company’s capital structure, in particular, its mix between debt and equity. These metrics are crucial for understanding the company’s financial risk and long-term solvency. We examine three key leverage measures.\nThe debt-to-equity ratio indicates how much a company is financing its operations through debt versus shareholders’ equity:\n\\[\\text{Debt-to-Equity} = \\frac{\\text{Total Debt}}{\\text{Total Equity}}\\]\nThe debt-to-asset ratio shows the percentage of assets financed through debt:\n\\[\\text{Debt-to-Asset} = \\frac{\\text{Total Debt}}{\\text{Total Assets}}\\]\nInterest coverage measures a company’s ability to meet interest payments:\n\\[\\text{Interest Coverage} = \\frac{\\text{EBIT}}{\\text{Interest Expense}}\\]\nLet’s calculate these ratios for our sample of companies:\n\nbalance_sheet_statements &lt;- balance_sheet_statements |&gt;\n  mutate(\n    debt_to_equity = total_debt / total_equity,\n    debt_to_asset = total_debt / total_assets\n  )\n\nincome_statements &lt;- income_statements |&gt;\n  mutate(\n    fiscal_year = as.integer(fiscal_year),\n    interest_coverage = operating_income / interest_expense,\n    label = if_else(symbol %in% selected_symbols, symbol, NA),\n  )\n\nFigure 9 tracks the evolution of debt-to-asset ratios for Microsoft, Apple, and Amazon over time:\n\nbalance_sheet_statements |&gt;\n  filter(symbol %in% selected_symbols) |&gt;\n  ggplot(aes(x = fiscal_year, y = debt_to_asset, color = symbol)) +\n  geom_line(linewidth = 1) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Debt-to-asset ratios of selected stocks between 2020 and 2024\"\n  )\n\n\n\n\n\n\n\nFigure 9: Debt-to-asset ratios are based on financial statements provided through the FMP API.\n\n\n\n\n\nThe evolution of debt-to-asset ratios among these major technology companies reveals distinct capital structure strategies and their changes over time. While Apple and Microsoft reduced leverage over time, Amazon has maintained its leverage level.\nFigure 10 provides a cross-sectional view of debt-to-asset ratios across our stock sample in 2023.\n\nselected_colors &lt;- c(\"#F21A00\", \"#EBCC2A\", \"#78B7C5\", \"#3B9AB2\", \"lightgrey\")\n\nbalance_sheet_statements |&gt;\n  filter(fiscal_year == 2023) |&gt;\n  ggplot(\n    aes(x = debt_to_asset, y = fct_reorder(symbol, debt_to_asset), fill = label)\n  ) +\n  geom_col() +\n  scale_x_continuous(labels = percent) +\n  scale_fill_manual(values = selected_colors) +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Debt-to-asset ratios of selected stocks in 2023\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 10: Debt-to-asset ratios are based on financial statements provided through the FMP API.\n\n\n\n\n\nFigure 11 reveals the relationship between companies’ debt levels and their ability to service that debt.\n\nincome_statements |&gt;\n  filter(fiscal_year == 2023) |&gt;\n  select(symbol, interest_coverage, fiscal_year) |&gt;\n  left_join(\n    balance_sheet_statements,\n    join_by(symbol, fiscal_year)\n  ) |&gt;\n  ggplot(aes(x = debt_to_asset, y = interest_coverage, color = label)) +\n  geom_point(size = 2) +\n  geom_label_repel(aes(label = label), seed = 42, box.padding = 0.75) +\n  scale_x_continuous(labels = percent) +\n  scale_y_continuous(labels = percent) +\n  scale_color_manual(values = selected_colors) +\n  labs(\n    x = \"Debt-to-Asset\",\n    y = \"Interest Coverage\",\n    title = \"Debt-to-asset ratios and interest coverages for selected stocks\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 11: Debt-to-asset ratios and interest coverages are based on financial statements provided through the FMP API.\n\n\n\n\n\nThe scatter plot suggests that companies with higher debt-to-asset ratios tend to have lower interest coverage ratios, though there’s considerable variation in this relation. Quantification of this relation is left as an exercise.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#efficiency-ratios",
    "href": "r/financial-statement-analysis.html#efficiency-ratios",
    "title": "Financial Statement Analysis",
    "section": "Efficiency Ratios",
    "text": "Efficiency Ratios\nEfficiency ratios measure how a company utilizes its assets and manages its operations. These metrics help us to understand operational performance and management effectiveness, particularly in how well a company converts its various assets into revenue and profit.\nAsset Turnover measures how efficiently a company uses its total assets to generate revenue:\n\\[\\text{Asset Turnover} = \\frac{\\text{Revenue}}{\\text{Total Assets}}\\]\nA higher ratio indicates more efficient use of assets in generating sales. However, this ratio typically varies significantly across industries - retail companies often have higher turnover ratios due to lower asset requirements, while manufacturing companies might show lower ratios due to substantial fixed asset investments. Such industry-specifics show the importance of cross-sectional comparisons, when making decisions based on data.\nInventory turnover indicates how many times a company’s inventory is sold and replaced over a period:\n\\[\\text{Inventory Turnover} = \\frac{\\text{COGS}}{\\text{Inventory}}\\]\nHigher inventory turnover suggests more efficient inventory management and working capital utilization. However, extremely high ratios might indicate potential stockouts, while very low ratios could suggest obsolete inventory or overinvestment in working capital.\nReceivables turnover measures how effectively a company collects payments from customers:\n\\[\\text{Receivables Turnover} = \\frac{\\text{Revenue}}{\\text{Accounts Receivable}}\\] A higher ratio indicates more efficient credit and collection processes, though this must be balanced against the potential impact on sales from overly restrictive credit policies.\nHere is how we can calculate these efficiency metrics across our sample of companies:\n\ncombined_statements &lt;- balance_sheet_statements |&gt;\n  select(\n    symbol,\n    fiscal_year,\n    label,\n    current_ratio,\n    quick_ratio,\n    cash_ratio,\n    debt_to_equity,\n    debt_to_asset,\n    total_assets,\n    total_equity\n  ) |&gt;\n  left_join(\n    income_statements |&gt;\n      select(\n        symbol,\n        fiscal_year,\n        interest_coverage,\n        revenue,\n        cost_of_revenue,\n        selling_general_and_administrative_expenses,\n        interest_expense,\n        gross_profit,\n        net_income\n      ),\n    join_by(symbol, fiscal_year)\n  ) |&gt;\n  left_join(\n    cash_flow_statements |&gt;\n      mutate(fiscal_year = as.integer(fiscal_year)) |&gt; \n      select(symbol, fiscal_year, inventory, accounts_receivables),\n    join_by(symbol, fiscal_year)\n  )\n\ncombined_statements &lt;- combined_statements |&gt;\n  mutate(\n    asset_turnover = revenue / total_assets,\n    inventory_turnover = cost_of_revenue / inventory,\n    receivables_turnover = revenue / accounts_receivables\n  )\n\nWe leave the visualization and interpretation of these figures as an exercise and move on the the last category of financial ratios.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#profitability-ratios",
    "href": "r/financial-statement-analysis.html#profitability-ratios",
    "title": "Financial Statement Analysis",
    "section": "Profitability Ratios",
    "text": "Profitability Ratios\nProfitability ratios evaluate a company’s ability to generate earnings relative to its revenue, assets, and equity. These metrics are fundamental to investment analysis as they directly measure a company’s operational efficiency and financial success.\nThe gross margin measures what percentage of revenue remains after accounting for the direct costs of producing goods or services:\n\\[\\text{Gross Margin} = \\frac{\\text{Gross Profit}}{\\text{Revenue}}\\]\nA higher gross margin indicates stronger pricing power or more efficient production processes. This metric is particularly useful for comparing companies within the same industry, as it reveals their relative efficiency in core operations before accounting for operating expenses and other costs.\nThe profit margin reveals what percentage of revenue ultimately becomes net income:\n\\[\\text{Profit Margin} = \\frac{\\text{Net Income}}{\\text{Revenue}}\\] This comprehensive profitability measure accounts for all costs, expenses, interest, and taxes. A higher profit margin suggests more effective overall cost management and stronger competitive position, though optimal margins vary significantly across industries.\nReturn on Equity (ROE) measures how efficiently a company uses shareholders’ investments to generate profits:\n\\[\\text{After-Tax ROE} = \\frac{\\text{Net Income}}{\\text{Total Equity}}\\] This metric is particularly important for investors as it directly measures the return on their invested capital (at least in terms of book value of equity). A higher ROE indicates more effective use of shareholders’ equity, though it must be considered alongside leverage ratios since high debt levels can artificially inflate ROE.\nThe next code chunk calculates these profitability metrics for our sample of companies, allowing us to analyze how different firms convert their revenue into various levels of profit and return on investment.\n\ncombined_statements &lt;- combined_statements |&gt;\n  mutate(\n    gross_margin = gross_profit / revenue,\n    profit_margin = net_income / revenue,\n    after_tax_roe = net_income / total_equity\n  )\n\nFigure 12 shows the patterns in gross margin trends among Microsoft, Apple, and Amazon between 2019 and 2023.\n\ncombined_statements |&gt;\n  filter(symbol %in% selected_symbols) |&gt;\n  ggplot(aes(x = fiscal_year, y = gross_margin, color = symbol)) +\n  geom_line() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Gross margins for selected stocks between 2019 and 2023\"\n  )\n\n\n\n\n\n\n\nFigure 12: Gross margins are based on financial statements provided through the FMP API.\n\n\n\n\n\nMicrosoft maintains the highest margins at 65-70%, reflecting its low-cost software business model, while Apple and Amazon show lower but improving margins from around 40% to 45-47%. This divergence highlights fundamental business model differences.\nFigure 13 illustrates the relationship between gross margins and profit margins across our sample of stocks in 2023\n\ncombined_statements |&gt;\n  filter(fiscal_year == 2023) |&gt;\n  ggplot(aes(x = gross_margin, y = profit_margin, color = label)) +\n  geom_point(size = 2) +\n  geom_label_repel(aes(label = label), seed = 42, box.padding = 0.75) +\n  scale_x_continuous(labels = percent) +\n  scale_y_continuous(labels = percent) +\n  scale_color_manual(values = selected_colors) +\n  labs(\n    x = \"Gross margin\",\n    y = \"Profit margin\",\n    title = \"Gross and profit margins for selected stocks in 2023\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 13: Gross and profit margins are based on financial statements provided through the FMP API.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#combining-financial-ratios",
    "href": "r/financial-statement-analysis.html#combining-financial-ratios",
    "title": "Financial Statement Analysis",
    "section": "Combining Financial Ratios",
    "text": "Combining Financial Ratios\nWhile individual financial ratios provide specific insights, combining them offers a more comprehensive view of company performance. By examining how companies rank across different ratio categories, we can better understand their overall financial position and identify potential strengths and weaknesses in their operations.\nFigure 14 compares Microsoft, Apple, and Amazon’s rankings across four key financial ratio categories among our stock sample. Rankings closer to 1 indicate better performance within each category.\n\nfinancial_ratios &lt;- combined_statements |&gt;\n  filter(fiscal_year == 2023) |&gt;\n  select(\n    symbol,\n    contains(c(\n      \"ratio\",\n      \"margin\",\n      \"roe\",\n      \"_to_\",\n      \"turnover\",\n      \"interest_coverage\"\n    ))\n  ) |&gt;\n  pivot_longer(cols = -symbol) |&gt;\n  mutate(\n    type = case_when(\n      name %in% c(\"current_ratio\", \"quick_ratio\", \"cash_ratio\") ~\n        \"Liquidity Ratios\",\n      name %in% c(\"debt_to_equity\", \"debt_to_asset\", \"interest_coverage\") ~\n        \"Leverage Ratios\",\n      name %in%\n        c(\"asset_turnover\", \"inventory_turnover\", \"receivables_turnover\") ~\n        \"Efficiency Ratios\",\n      name %in% c(\"gross_margin\", \"profit_margin\", \"after_tax_roe\") ~\n        \"Profitability Ratios\"\n    )\n  )\n\nfinancial_ratios |&gt;\n  group_by(type, name) |&gt;\n  arrange(desc(value)) |&gt;\n  mutate(rank = row_number()) |&gt;\n  group_by(symbol, type) |&gt;\n  summarize(rank = mean(rank), .groups = \"drop\") |&gt;\n  filter(symbol %in% selected_symbols) |&gt;\n  ggplot(aes(x = rank, y = type, color = symbol)) +\n  geom_point(shape = 17, size = 4) +\n  scale_color_manual(values = selected_colors) +\n  labs(\n    x = \"Average rank\",\n    y = NULL,\n    color = NULL,\n    title = \"Average rank among selected stocks\"\n  ) +\n  coord_cartesian(xlim = c(1, 30))\n\n\n\n\n\n\n\nFigure 14: Ranks are based on financial statements provided through the FMP API.\n\n\n\n\n\nThese combined rankings highlight how different business models and strategies lead to varying financial profiles. This analysis underscores the importance of considering multiple financial metrics together rather than in isolation when evaluating company performance.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#financial-ratios-in-asset-pricing",
    "href": "r/financial-statement-analysis.html#financial-ratios-in-asset-pricing",
    "title": "Financial Statement Analysis",
    "section": "Financial Ratios in Asset Pricing",
    "text": "Financial Ratios in Asset Pricing\nThe Fama-French five-factor model aims to explain stock returns by incorporating specific financial metrics ratios. We provide more details in Replicating Fama-French Factors, but here is an intuitive overview:\n\nSize: Calculated as the logarithm of a company’s market capitalization, which is the total market value of its outstanding shares. This factor captures the tendency for smaller firms to outperform larger ones over time.\nBook-to-market ratio: Determined by dividing the company’s book equity by its market capitalization. A higher ratio indicates a ‘value’ stock, while a lower ratio suggests a ‘growth’’’ stock. This metric helps differentiate between undervalued and overvalued companies.\nProfitability: Measured as the ratio of operating profit to book equity, where operating profit is calculated as revenue minus cost of goods sold (COGS), selling, general, and administrative expenses (SG&A), and interest expense. This factor assesses a company’s efficiency in generating profits from its equity base.\nInvestment: Calculated as the percentage change in total assets from the previous period. This factor reflects the company’s growth strategy, indicating whether it is investing aggressively or conservatively.\n\nWe can calculate these factors using the FMP API as follows. Since the free tier only supports historical data for the last couple of months, we use the earliest available data that is returned by default:\n\nmarket_cap &lt;- sample |&gt;\n  map_df(\n    \\(x) {\n      fmp_get(\n        resource = \"historical-market-capitalization\",\n        x\n      )\n    }\n  ) |&gt; \n  filter(date == min(date))\n\ncombined_statements_ff &lt;- combined_statements |&gt;\n  filter(fiscal_year == 2023) |&gt;\n  left_join(market_cap, join_by(symbol)) |&gt;\n  left_join(\n    balance_sheet_statements |&gt;\n      filter(fiscal_year == 2022) |&gt;\n      select(symbol, total_assets_lag = total_assets),\n    join_by(symbol)\n  ) |&gt;\n  mutate(\n    size = log(market_cap),\n    book_to_market = total_equity / market_cap,\n    operating_profitability = (revenue -\n      cost_of_revenue -\n      selling_general_and_administrative_expenses -\n      interest_expense) /\n      total_equity,\n    investment = total_assets / total_assets_lag\n  )\n\nFigure 15 shows the ranks of our selected stocks for ratios used in the Fama-French model. The ranks of Microsoft, Apple, and Amazon across Fama-French factors reveal interesting patterns in how these major technology companies align with established asset pricing factors.\n\ncombined_statements_ff |&gt;\n  select(\n    symbol,\n    Size = size,\n    `Book-to-Market` = book_to_market,\n    `Profitability` = operating_profitability,\n    Investment = investment\n  ) |&gt;\n  pivot_longer(-symbol) |&gt;\n  group_by(name) |&gt;\n  arrange(desc(value)) |&gt;\n  mutate(rank = row_number()) |&gt;\n  ungroup() |&gt;\n  filter(symbol %in% selected_symbols) |&gt;\n  ggplot(aes(x = rank, y = name, color = symbol)) +\n  geom_point(shape = 17, size = 4) +\n  scale_color_manual(values = selected_colors) +\n  labs(\n    x = \"Rank\",\n    y = NULL,\n    color = NULL,\n    title = \"Rank in Fama-French variables for selected stocks\"\n  ) +\n  coord_cartesian(xlim = c(1, 30))\n\n\n\n\n\n\n\nFigure 15: Ranks are based on financial statements and historical market capitalization provided through the FMP API.\n\n\n\n\n\nAs expected, all three tech giants rank among the largest firms by size. Apple shows the highest profitability among the three tech giants according to the new measure, while Microsoft ranks only in the middle. In terms of investment, however, Apple ranks in the lower third of the distribution. All three stocks exhibit relatively low book-to-market ratios—typical of growth stocks—but only when compared to other stocks in our stock sample.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#key-takeaways",
    "href": "r/financial-statement-analysis.html#key-takeaways",
    "title": "Financial Statement Analysis",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFinancial statements offer structured insights into a company’s financial health by summarizing its assets, liabilities, equity, revenues, expenses, and cash flows.\nLiquidity ratios, such as the current, quick, and cash ratios, help assess a company’s ability to meet short-term obligations using different levels of liquid assets.\nLeverage ratios, including debt-to-equity and debt-to-asset, measure how a company finances its operations and indicate long-term financial risk and capital structure.\nProfitability ratios, such as gross margin, profit margin, and return on equity, show how effectively a company turns revenues and investments into earnings.\nEfficiency ratios, including asset turnover and inventory turnover, highlight how well a company manages its assets and operations to generate sales.\nFinancial ratios also serve as key inputs in asset pricing models, such as the Fama-French five-factor model, linking corporate fundamentals to expected stock returns.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/financial-statement-analysis.html#exercises",
    "href": "r/financial-statement-analysis.html#exercises",
    "title": "Financial Statement Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the financial statements for Netflix (NFLX) using the FMP API. Calculate its current ratio, quick ratio, and cash ratio for the past three years. Create a line plot showing how these liquidity ratios have evolved over time. How do Netflix’s liquidity ratios compare to those of the technology companies discussed in this chapter?\nSelect three companies from different industries. Calculate their debt-to-equity ratios, debt-to-asset ratios, and interest coverage ratios. Create a visualization comparing these leverage metrics across the companies. Write a brief analysis explaining how and why leverage patterns differ across industries.\nFor all stocks in the sample above, calculate asset turnover, inventory turnover, and receivables turnover. Create a scatter plot showing the relationship between asset turnover and profitability. Identify any outliers and explain potential reasons for their unusual performance. Which industries tend to show higher efficiency ratios? Why might this be the case?\nRevisit the scatter plot of debt-to-asset ratios and interest coverages by adding a regression line and quantifying the relationship between the two variables. How can you describe their relationship?",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html",
    "href": "r/value-and-bivariate-sorts.html",
    "title": "Value and Bivariate Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we extend univariate portfolio analysis to bivariate sorts, which means we assign stocks to portfolios based on two characteristics. Bivariate sorts are regularly used in the academic asset pricing literature and are the basis for the factors in the Fama-French three-factor model. However, some scholars also use sorts with three grouping variables. Conceptually, portfolio sorts are easily applicable in higher dimensions.\nWe form portfolios on firm size and the book-to-market ratio. To calculate book-to-market ratios, accounting data is required, which necessitates additional steps during portfolio formation. In the end, we demonstrate how to form portfolios on two sorting variables using so-called independent and dependent portfolio sorts.\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#data-preparation",
    "href": "r/value-and-bivariate-sorts.html#data-preparation",
    "title": "Value and Bivariate Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we load the necessary data from our SQLite-database introduced in Accessing and Managing Financial Data. We conduct portfolio sorts based on the CRSP sample but keep only the necessary columns in our memory. We use the same data sources for firm size as in Size Sorts and P-Hacking.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n    select(\n    permno, gvkey, date, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |&gt; \n  collect() |&gt;\n  drop_na()\n\nFurther, we utilize accounting data. The most common source of accounting data is Compustat. We only need book equity data in this application, which we select from our database. Additionally, we convert the variable datadate to its monthly value, as we only consider monthly returns here and do not need to account for the exact date. To achieve this, we use the function floor_date().\n\nbook_equity &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n  select(gvkey, datadate, be) |&gt;\n  collect() |&gt;\n  drop_na() |&gt;\n  mutate(date = floor_date(ymd(datadate), \"month\"))",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#book-to-market-ratio",
    "href": "r/value-and-bivariate-sorts.html#book-to-market-ratio",
    "title": "Value and Bivariate Sorts",
    "section": "Book-to-Market Ratio",
    "text": "Book-to-Market Ratio\nA fundamental problem in handling accounting data is the look-ahead bias; we must not include data in forming a portfolio that is not public knowledge at the time. Of course, researchers have more information when looking into the past than agents had at that moment. However, abnormal excess returns from a trading strategy should not rely on an information advantage because the differential cannot be the result of informed agents’ trades. Hence, we have to lag accounting information.\nAs in the previous chapter, we continue to lag firm size by one month. Then, we compute the book-to-market ratio, which relates a firm’s book equity to its market equity. Firms with high (low) book-to-market ratio are called value (growth) firms. After matching the accounting and market equity information from the same month, we lag book-to-market by six months. This is a sufficiently conservative approach because accounting information is usually released well before six months pass. However, in the asset pricing literature, even longer lags are used as well.1\nHaving both variables, i.e., firm size lagged by one month and book-to-market lagged by six months, we merge these sorting variables to our returns using the sorting_date-column created for this purpose. The final step in our data preparation deals with differences in the frequency of our variables. Returns and firm size are recorded monthly. Yet the accounting information is only released on an annual basis. Hence, we only match book-to-market to one month per year and have eleven empty observations. To solve this frequency issue, we carry the latest book-to-market ratio of each firm to the subsequent months, i.e., we fill the missing observations with the most current report. This is done via the fill()-function after sorting by date and firm (which we identify by permno and gvkey) and on a firm basis (which we do by group_by() as usual). We filter out all observations with accounting data that is older than a year. As the last step, we remove all rows with missing entries because the returns cannot be matched to any annual report.\n\nsize &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = date %m+% months(1)) |&gt;\n  select(permno, sorting_date, size = mktcap)\n\nbm &lt;- book_equity |&gt;\n  inner_join(crsp_monthly, join_by(gvkey, date)) |&gt;\n  mutate(\n    bm = be / mktcap,\n    sorting_date = date %m+% months(6),\n    accounting_date = sorting_date\n  ) |&gt;\n  select(permno, gvkey, sorting_date, accounting_date, bm)\n\ndata_for_sorts &lt;- crsp_monthly |&gt;\n  left_join(\n    bm, join_by(permno, gvkey, date == sorting_date)\n  ) |&gt;\n  left_join(\n    size, join_by(permno, date == sorting_date)\n  ) |&gt;\n  select(\n    permno, gvkey, date, ret_excess,\n    mktcap_lag, size, bm, exchange, accounting_date\n  )\n\ndata_for_sorts &lt;- data_for_sorts |&gt;\n  arrange(permno, gvkey, date) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(bm, accounting_date) |&gt;\n  ungroup() |&gt; \n  filter(accounting_date &gt; date %m-% months(12)) |&gt;\n  select(-accounting_date) |&gt;\n  drop_na()\n\nThe last step of preparation for the portfolio sorts is the computation of breakpoints. We continue to use the same function allowing for the specification of exchanges to use for the breakpoints. Additionally, we reintroduce the argument sorting_variable into the function for defining different sorting variables.\n\nassign_portfolio &lt;- function(\n  data, \n  sorting_variable, \n  n_portfolios, \n  exchanges\n) {\n  breakpoints &lt;- data |&gt;\n    filter(exchange %in% exchanges) |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  assigned_portfolios\n}\n\nNote that the tidyfinance package also provides an assing_portfolio() function, albeit with more flexibility. For ease of exposition, we continue to use the function that we just defined.\nAfter these data preparation steps, we present bivariate portfolio sorts on an independent and dependent basis.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#independent-sorts",
    "href": "r/value-and-bivariate-sorts.html#independent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Independent Sorts",
    "text": "Independent Sorts\nBivariate sorts create portfolios within a two-dimensional space spanned by two sorting variables. It is then possible to assess the return impact of either sorting variable by the return differential from a trading strategy that invests in the portfolios at either end of the respective variables spectrum. We create a five-by-five matrix using book-to-market and firm size as sorting variables in our example below. We end up with 25 portfolios. Since we are interested in the value premium (i.e., the return differential between high and low book-to-market firms), we go long the five portfolios of the highest book-to-market firms and short the five portfolios of the lowest book-to-market firms. The five portfolios at each end are due to the size splits we employed alongside the book-to-market splits.\nTo implement the independent bivariate portfolio sort, we assign monthly portfolios for each of our sorting variables separately to create the variables portfolio_bm and portfolio_size, respectively. Then, these separate portfolios are combined to the final sort stored in portfolio_combined. After assigning the portfolios, we compute the average return within each portfolio for each month. Additionally, we keep the book-to-market portfolio as it makes the computation of the value premium easier. The alternative would be to disaggregate the combined portfolio in a separate step. Notice that we weigh the stocks within each portfolio by their market capitalization, i.e., we decide to value-weight our returns.\n\nvalue_portfolios &lt;- data_for_sorts |&gt;\n  group_by(date) |&gt;\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"bm\",\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_size = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"size\",\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    )) |&gt;\n  group_by(date, portfolio_bm, portfolio_size) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )\n\nEquipped with our monthly portfolio returns, we are ready to compute the value premium. However, we still have to decide how to invest in the five high and the five low book-to-market portfolios. The most common approach is to weigh these portfolios equally, but this is yet another researcher’s choice. Then, we compute the return differential between the high and low book-to-market portfolios and show the average value premium.\n\nvalue_premium &lt;- value_portfolios |&gt;\n  group_by(date, portfolio_bm) |&gt;\n  summarize(ret = mean(ret), .groups = \"drop_last\") |&gt;\n  summarize(\n    value_premium = ret[portfolio_bm == max(portfolio_bm)] -\n      ret[portfolio_bm == min(portfolio_bm)]\n  ) |&gt; \n  summarize(\n    value_premium = mean(value_premium)\n  )\n\nThe resulting monthly value premium is 0.41 percent with an annualized return of 5 percent.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#dependent-sorts",
    "href": "r/value-and-bivariate-sorts.html#dependent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Dependent Sorts",
    "text": "Dependent Sorts\nIn the previous exercise, we assigned the portfolios without considering the second variable in the assignment. This protocol is called independent portfolio sorts. The alternative, i.e., dependent sorts, creates portfolios for the second sorting variable within each bucket of the first sorting variable. In our example below, we sort firms into five size buckets, and within each of those buckets, we assign firms to five book-to-market portfolios. Hence, we have monthly breakpoints that are specific to each size group. The decision between independent and dependent portfolio sorts is another choice for the researcher. Notice that dependent sorts guarantee that portfolios have roughly equal numbers of stocks when breakpoints are computed from all exchanges. However, if breakpoints are based only on NYSE stocks, portfolio counts will generally be uneven — reflecting the large presence of small-cap stocks on NASDAQ and AMEX (see Exercise below).\nTo implement the dependent sorts, we first create the size portfolios by calling assign_portfolio() with sorting_variable = \"size\". Then, we group our data again by month and by the size portfolio before assigning the book-to-market portfolio. The rest of the implementation is the same as before. Finally, we compute the value premium.\n\nvalue_portfolios &lt;- data_for_sorts |&gt;\n  group_by(date) |&gt;\n  mutate(portfolio_size = assign_portfolio(\n    data = pick(everything()),\n    sorting_variable = \"size\",\n    n_portfolios = 5,\n    exchanges = c(\"NYSE\")\n  )) |&gt;\n  group_by(date, portfolio_size) |&gt;\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"bm\",\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    )) |&gt;\n  group_by(date, portfolio_size, portfolio_bm) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )\n\nvalue_premium &lt;- value_portfolios |&gt;\n  group_by(date, portfolio_bm) |&gt;\n  summarize(ret = mean(ret), .groups = \"drop_last\") |&gt;\n  summarize(\n    value_premium = ret[portfolio_bm == max(portfolio_bm)] -\n      ret[portfolio_bm == min(portfolio_bm)]\n  ) |&gt; \n  summarize(\n    value_premium = mean(value_premium)\n  )\n\nThe monthly value premium from dependent sorts is 0.35 percent, which translates to an annualized premium of 4.3 percent per year.\nOverall, we show how to conduct bivariate portfolio sorts in this chapter. In one case, we sort the portfolios independently of each other. Yet we also discuss how to create dependent portfolio sorts. Along the lines of Size Sorts and P-Hacking, we see how many choices a researcher has to make to implement portfolio sorts, and bivariate sorts increase the number of choices.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#key-takeaways",
    "href": "r/value-and-bivariate-sorts.html#key-takeaways",
    "title": "Value and Bivariate Sorts",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nBivariate portfolio sorts assign stocks based on two characteristics, such as firm size and book-to-market ratio, to better capture return patterns in asset pricing.\nIndependent sorts treat each variable separately, while dependent sorts condition the second sort on the first.\nProper handling of accounting data, especially lagging the book-to-market ratio, is essential to avoid look-ahead bias and ensure valid backtesting.\nValue premiums are derived by comparing returns of high versus low book-to-market portfolios, with results sensitive to sorting choices and weighting schemes.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#exercises",
    "href": "r/value-and-bivariate-sorts.html#exercises",
    "title": "Value and Bivariate Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nCalculate the number of stocks in each size–value portfolio under two scenarios: (i) breakpoints based on all exchanges (NYSE, AMEX, NASDAQ) and (ii) breakpoints based on NYSE stocks only. Compare the portfolio counts between the two methods and explain the differences.\nIn Size Sorts and P-Hacking, we examine the distribution of market equity. Repeat this analysis for book equity and the book-to-market ratio (alongside a plot of the breakpoints, i.e., deciles).\nWhen we investigate the portfolios, we focus on the returns exclusively. However, it is also of interest to understand the characteristics of the portfolios. Write a function to compute the average characteristics for size and book-to-market across the 25 independently and dependently sorted portfolios.\nAs for the size premium, also the value premium constructed here does not follow Fama and French (1993). Implement a p-hacking setup as in Size Sorts and P-Hacking to find a premium that comes closest to their HML premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#footnotes",
    "href": "r/value-and-bivariate-sorts.html#footnotes",
    "title": "Value and Bivariate Sorts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe definition of a time lag is another choice a researcher has to make, similar to breakpoint choices as we describe in Size Sorts and P-Hacking.↩︎",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html",
    "href": "r/factor-selection-via-machine-learning.html",
    "title": "Factor Selection via Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nThe aim of this chapter is twofold. From a data science perspective, we introduce tidymodels, a collection of packages for modeling and machine learning (ML) using tidyverse principles. tidymodels comes with a handy workflow for all sorts of typical prediction tasks. From a finance perspective, we address the notion of factor zoo (Cochrane 2011) using ML methods. We introduce Lasso and Ridge regression as a special case of penalized regression models. Then, we explain the concept of cross-validation for model tuning with Elastic Net regularization as a popular example. We implement and showcase the entire cycle from model specification, training, and forecast evaluation within the tidymodels universe. While the tools can generally be applied to an abundance of interesting asset pricing problems, we apply penalized regressions for identifying macroeconomic variables and asset pricing factors that help explain a cross-section of industry portfolios.\nIn previous chapters, we illustrate that stock characteristics such as size provide valuable pricing information in addition to the market beta. Such findings question the usefulness of the Capital Asset Pricing Model. In fact, during the last decades, financial economists discovered a plethora of additional factors which may be correlated with the marginal utility of consumption (and would thus deserve a prominent role in pricing applications). The search for factors that explain the cross-section of expected stock returns has produced hundreds of potential candidates, as noted more recently by Harvey, Liu, and Zhu (2016), Mclean and Pontiff (2016), and Hou, Xue, and Zhang (2020). Therefore, given the multitude of proposed risk factors, the challenge these days rather is: do we believe in the relevance of 300+ risk factors? During recent years, promising methods from the field of ML got applied to common finance applications. We refer to Mullainathan and Spiess (2017) for a treatment of ML from the perspective of an econometrician, Nagel (2021) for an excellent review of ML practices in asset pricing, Easley et al. (2020) for ML applications in (high-frequency) market microstructure, and Dixon, Halperin, and Bilokon (2020) for a detailed treatment of all methodological aspects.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "href": "r/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "title": "Factor Selection via Machine Learning",
    "section": "Brief Theoretical Background",
    "text": "Brief Theoretical Background\nThis is a book about doing empirical work in a tidy manner, and we refer to any of the many excellent textbook treatments of ML methods and especially penalized regressions for some deeper discussion. Excellent material is provided, for instance, by Hastie, Tibshirani, and Friedman (2009), Gareth et al. (2013), and De Prado (2018). Instead, we briefly summarize the idea of Lasso and Ridge regressions as well as the more general Elastic Net. Then, we turn to the fascinating question on how to implement, tune, and use such models with the tidymodels workflow.\nTo set the stage, we start with the definition of a linear model: suppose we have data \\((y_t, x_t), t = 1,\\ldots, T\\), where \\(x_t\\) is a \\((K \\times 1)\\) vector of regressors and \\(y_t\\) is the response for observation \\(t\\). The linear model takes the form \\(y_t = \\beta' x_t + \\varepsilon_t\\) with some error term \\(\\varepsilon_t\\) and has been studied in abundance. The well-known ordinary-least square (OLS) estimator for the \\((K \\times 1)\\) vector \\(\\beta\\) minimizes the sum of squared residuals and is then \\[\\hat{\\beta}^\\text{ols} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t.\\] \nWhile we are often interested in the estimated coefficient vector \\(\\hat\\beta^\\text{ols}\\), ML is about the predictive performance most of the time. For a new observation \\(\\tilde{x}_t\\), the linear model generates predictions such that \\[\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta^\\text{ols}{}' \\tilde x_t.\\] Is this the best we can do? Not really: instead of minimizing the sum of squared residuals, penalized linear models can improve predictive performance by choosing other estimators \\(\\hat{\\beta}\\) with lower variance than the estimator \\(\\hat\\beta^\\text{ols}\\). At the same time, it seems appealing to restrict the set of regressors to a few meaningful ones, if possible. In other words, if \\(K\\) is large (such as for the number of proposed factors in the asset pricing literature), it may be a desirable feature to select reasonable factors and set \\(\\hat\\beta^{\\text{ols}}_k = 0\\) for some redundant factors.\nIt should be clear that the promised benefits of penalized regressions, i.e., reducing the mean squared error (MSE), come at a cost. In most cases, reducing the variance of the estimator introduces a bias such that \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). What is the effect of such a bias-variance trade-off? To understand the implications, assume the following data-generating process for \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2)\\] We want to recover \\(f(x)\\), which denotes some unknown functional which maps the relationship between \\(x\\) and \\(y\\). While the properties of \\(\\hat\\beta^\\text{ols}\\) as an unbiased estimator may be desirable under some circumstances, they are certainly not if we consider predictive accuracy. Alternative predictors \\(\\hat{f}(x)\\) could be more desirable: For instance, the MSE depends on our model choice as follows: \\[\\begin{aligned}\nMSE &=E((y-\\hat{f}(x))^2)=E((f(x)+\\epsilon-\\hat{f}(x))^2)\\\\\n&= \\underbrace{E((f(x)-\\hat{f}(x))^2)}_{\\text{total quadratic error}}+\\underbrace{E(\\epsilon^2)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(x)^2\\right)+E\\left(f(x)^2\\right)-2E\\left(f(x)\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(x)^2\\right)+f(x)^2-2f(x)E\\left(\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(x)\\right)}_{\\text{variance of model}}+ \\underbrace{\\left(E(f(x)-\\hat{f}(x))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2.\n\\end{aligned}\\] While no model can reduce \\(\\sigma_\\varepsilon^2\\), a biased estimator with small variance may have a lower MSE than an unbiased estimator.\n\nRidge regression\n\nOne biased estimator is known as Ridge regression. Hoerl and Kennard (1970) propose to minimize the sum of squared errors while simultaneously imposing a penalty on the \\(L_2\\) norm of the parameters \\(\\hat\\beta\\). Formally, this means that for a penalty factor \\(\\lambda\\geq 0\\), the minimization problem takes the form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq c\\). Here \\(c\\geq 0\\) is a constant that depends on the choice of \\(\\lambda\\). The larger \\(\\lambda\\), the smaller \\(c\\) (technically speaking, there is a one-to-one relationship between \\(\\lambda\\), which corresponds to the Lagrangian of the minimization problem above and \\(c\\)). Here, \\(X = \\left(x_1  \\ldots  x_T\\right)'\\) and \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). A closed-form solution for the resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda I\\right)^{-1}X'y.\\] A couple of observations are worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta^\\text{ols}\\) for \\(\\lambda = 0\\) and \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) for \\(\\lambda\\rightarrow \\infty\\). Also for \\(\\lambda &gt; 0\\), \\(\\left(X'X + \\lambda I\\right)\\) is non-singular even if \\(X'X\\) is which means that \\(\\hat\\beta^\\text{ridge}\\) exists even if \\(\\hat\\beta\\) is not defined. However, note also that the Ridge estimator requires careful choice of the hyperparameter \\(\\lambda\\) which controls the amount of regularization: a larger value of \\(\\lambda\\) implies shrinkage of the regression coefficient toward 0, a smaller value of \\(\\lambda\\) reduces the bias of the resulting estimator.\n\nNote that \\(X\\) usually contains an intercept column with ones. As a general rule, the associated intercept coefficient is not penalized. In practice, this often implies that \\(y\\) is simply demeaned before computing \\(\\hat\\beta^\\text{ridge}\\).\n\nWhat about the statistical properties of the Ridge estimator? First, the bad news is that \\(\\hat\\beta^\\text{ridge}\\) is a biased estimator of \\(\\beta\\). However, the good news is that (under homoscedastic error terms) the variance of the Ridge estimator is guaranteed to be smaller than the variance of the OLS estimator. We encourage you to verify these two statements in the Exercises. As a result, we face a trade-off: The Ridge regression sacrifices some unbiasedness to achieve a smaller variance than the OLS estimator.\n\n\nLasso\n\nAn alternative to Ridge regression is the Lasso (least absolute shrinkage and selection operator). Similar to Ridge regression, the Lasso (Tibshirani 1996) is a penalized and biased estimator. The main difference to Ridge regression is that Lasso does not only shrink coefficients but effectively selects variables by setting coefficients for irrelevant variables to zero. Lasso implements a \\(L_1\\) penalization on the parameters such that: \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right)\\text{ s.t. } \\sum\\limits_{k=1}^K|\\beta_k| &lt; c(\\lambda).\\] There is no closed-form solution for \\(\\hat\\beta^\\text{Lasso}\\) in the above maximization problem, but efficient algorithms exist (e.g., the R package glmnet). Like for Ridge regression, the hyperparameter \\(\\lambda\\) has to be specified beforehand.\n\n\nElastic Net\nThe Elastic Net (Zou and Hastie 2005) combines \\(L_1\\) with \\(L_2\\) penalization and encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. This more general framework considers the following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2\\] Now, we have to choose two hyperparameters: the shrinkage factor \\(\\lambda\\) and the weighting parameter \\(\\rho\\). The Elastic Net resembles Lasso for \\(\\rho = 0\\) and Ridge regression for \\(\\rho = 1\\). While the R package glmnet provides efficient algorithms to compute the coefficients of penalized regressions, it is a good exercise to implement Ridge and Lasso estimation on your own before you use the glmnet package or the tidymodels back-end.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#data-preparation",
    "href": "r/factor-selection-via-machine-learning.html#data-preparation",
    "title": "Factor Selection via Machine Learning",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the required R packages and data. The main focus is on the workflow behind the tidymodels package collection (Kuhn and Wickham 2020). Kuhn and Silge (2018) provide a thorough introduction into all tidymodels components. glmnet (Simon et al. 2011) was developed and released in sync with Tibshirani (1996) and provides an R implementation of Elastic Net estimation. The package timetk (Dancho and Vaughan 2022) provides useful tools for time series data wrangling.\n\nlibrary(RSQLite)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(scales)\nlibrary(furrr)\nlibrary(glmnet)\nlibrary(timetk)\n\nIn this analysis, we use four different data sources that we load from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. We start with two different sets of factor portfolio returns which have been suggested as representing practical risk factor exposure and thus should be relevant when it comes to asset pricing applications.\n\nThe standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big, and high-minus-low book-to-market valuation sorts) defined in Fama and French (1992) and Fama and French (1993)\nMonthly q-factor returns from Hou, Xue, and Zhang (2014). The factors contain the size factor, the investment factor, the return-on-equity factor, and the expected growth factor\n\nNext, we include macroeconomic predictors which may predict the general stock market economy. Macroeconomic variables effectively serve as conditioning information such that their inclusion hints at the relevance of conditional models instead of unconditional asset pricing. We refer the interested reader to Cochrane (2009) on the role of conditioning information.\n\nOur set of macroeconomic predictors comes from Welch and Goyal (2008). The data has been updated by the authors until 2021 and contains monthly variables that have been suggested as good predictors for the equity premium. Some of the variables are the dividend price ratio, earnings price ratio, stock variance, net equity expansion, treasury bill rate, and inflation\n\nFinally, we need a set of test assets. The aim is to understand which of the plenty factors and macroeconomic variable combinations prove helpful in explaining our test assets’ cross-section of returns. In line with many existing papers, we use monthly portfolio returns from 10 different industries according to the definition from Kenneth French’s homepage as test assets.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  collect() |&gt;\n  rename_with(~ str_c(\"factor_ff_\", .), -date)\n\nfactors_q_monthly &lt;- tbl(tidy_finance, \"factors_q_monthly\") |&gt;\n  collect() |&gt;\n  rename_with(~ str_c(\"factor_q_\", .), -date)\n\nmacro_predictors &lt;- tbl(tidy_finance, \"macro_predictors\") |&gt;\n  collect() |&gt;\n  rename_with(~ str_c(\"macro_\", .), -date) |&gt;\n  select(-macro_rp_div)\n\nindustries_ff_monthly &lt;- tbl(tidy_finance, \"industries_ff_monthly\") |&gt;\n  collect() |&gt;\n  pivot_longer(-date, names_to = \"industry\", values_to = \"ret\") |&gt;\n  arrange(desc(industry)) |&gt; \n  mutate(industry = as_factor(industry))\n\nWe combine all the monthly observations into one dataframe.\n\ndata &lt;- industries_ff_monthly |&gt;\n  left_join(factors_ff3_monthly, join_by(date)) |&gt;\n  left_join(factors_q_monthly, join_by(date)) |&gt;\n  left_join(macro_predictors, join_by(date)) |&gt;\n  mutate(\n    ret = ret - factor_ff_rf\n  ) |&gt;\n  select(date, industry, ret_excess = ret, everything()) |&gt;\n  drop_na()\n\nOur data contains 26 columns of regressors with the 13 macro-variables and 12 factor returns for each month. Figure 1 provides summary statistics for the 10 monthly industry excess returns in percent.\n\ndata |&gt;\n  group_by(industry) |&gt;\n  ggplot(aes(x = industry, y = ret_excess)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Excess return distributions by industry in percent\"\n  ) +\n  scale_y_continuous(\n    labels = percent\n  )\n\n\n\n\n\n\n\nFigure 1: The box plots show the monthly dispersion of returns for 10 different industries.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#the-tidymodels-workflow",
    "href": "r/factor-selection-via-machine-learning.html#the-tidymodels-workflow",
    "title": "Factor Selection via Machine Learning",
    "section": "The tidymodels Workflow",
    "text": "The tidymodels Workflow\nTo illustrate penalized linear regressions, we employ the tidymodels collection of packages for modeling and ML using tidyverse principles. You can simply use install.packages(\"tidymodels\") to get access to all the related packages. We recommend checking out the work of Kuhn and Silge (2018): They continuously write on their great book ‘Tidy Modeling with R’ using tidy principles.\nThe tidymodels workflow encompasses the main stages of the modeling process: pre-processing of data, model fitting, and post-processing of results. As we demonstrate below, tidymodels provides efficient workflows that you can update with low effort.\nUsing the ideas of Ridge and Lasso regressions, the following example guides you through (i) pre-processing the data (data split and variable mutation), (ii) building models, (iii) fitting models, and (iv) tuning models to create the “best” possible predictions.\nTo start, we restrict our analysis to just one industry: Manufacturing. We first split the sample into a training and a test set. For that purpose, tidymodels provides the function initial_time_split() from the rsample package (Silge et al. 2022). The split takes the last 20% of the data as a test set, which is not used for any model tuning. We use this test set to evaluate the predictive accuracy in an out-of-sample scenario.\n\nsplit &lt;- initial_time_split(\n  data |&gt;\n    filter(industry == \"manuf\") |&gt;\n    select(-industry),\n  prop = 4 / 5\n)\nsplit\n\n&lt;Training/Testing/Total&gt;\n&lt;546/137/683&gt;\n\n\nThe object split simply keeps track of the observations of the training and the test set. We can call the training set with training(split), while we can extract the test set with testing(split).\n\nPre-process data\nRecipes help you pre-process your data before training your model. Recipes are a series of pre-processing steps such as variable selection, transformation, or conversion of qualitative predictors to indicator variables. Each recipe starts with a formula that defines the general structure of the dataset and the role of each variable (regressor or dependent variable). For our dataset, our recipe contains the following steps before we fit any model:\n\nOur formula defines that we want to explain excess returns with all available predictors. The regression equation thus takes the form \\[r_{t} = \\alpha_0 + \\left(\\tilde f_t \\otimes \\tilde z_t\\right)B + \\varepsilon_t \\] where \\(r_t\\) is the vector of industry excess returns at time \\(t\\) and \\(\\tilde f_t\\) and \\(\\tilde z_t\\) are the (standardized) vectors of factor portfolio returns and macroeconomic variables\nWe exclude the column month from the analysis\nWe include all interaction terms between factors and macroeconomic predictors\nWe demean and scale each regressor such that the standard deviation is one\n\n\nrec &lt;- recipe(ret_excess ~ ., data = training(split)) |&gt;\n  step_rm(date) |&gt;\n  step_interact(terms = ~ contains(\"factor\"):contains(\"macro\")) |&gt;\n  step_normalize(all_predictors())\n\nA table of all available recipe steps can be found in the tidymodels documentation. As of 2025, more than 150 different processing steps are available! One important point: The definition of a recipe does not trigger any calculations yet but rather provides a description of the tasks to be applied. As a result, it is very easy to reuse recipes for different models and thus make sure that the outcomes are comparable as they are based on the same input. In the example above, it does not make a difference whether you use the input data = training(split) or data = testing(split). All that matters at this early stage are the column names and types.\nWe can apply the recipe to any data with a suitable structure. The code below combines two different functions: prep() estimates the required parameters from a training set that can be applied to other datasets later. bake() applies the processed computations to new data.\n\ndata_prep &lt;- prep(rec, training(split))\n\nThe object data_prep contains information related to the different preprocessing steps applied to the training data: E.g., it is necessary to compute sample means and standard deviations to center and scale the variables.\n\ndata_bake &lt;- bake(data_prep,\n  new_data = testing(split)\n)\ndata_bake\n\n# A tibble: 137 × 182\n  factor_ff_mkt_excess factor_ff_smb factor_ff_hml factor_ff_rf\n                 &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1               0.0709       -0.940         -0.150        -1.71\n2               0.448         0.0895         0.295        -1.67\n3               0.489         0.0737         0.430        -1.67\n4              -0.478        -0.413          1.09         -1.67\n5               0.0687        0.121         -0.426        -1.67\n# ℹ 132 more rows\n# ℹ 178 more variables: factor_q_risk_free &lt;dbl&gt;,\n#   factor_q_mkt_excess &lt;dbl&gt;, factor_q_year &lt;dbl&gt;,\n#   factor_q_month &lt;dbl&gt;, factor_q_me &lt;dbl&gt;, factor_q_ia &lt;dbl&gt;,\n#   factor_q_roe &lt;dbl&gt;, factor_q_eg &lt;dbl&gt;, macro_dp &lt;dbl&gt;,\n#   macro_dy &lt;dbl&gt;, macro_ep &lt;dbl&gt;, macro_de &lt;dbl&gt;,\n#   macro_svar &lt;dbl&gt;, macro_bm &lt;dbl&gt;, macro_ntis &lt;dbl&gt;, …\n\n\nNote that the resulting data contains the 132 observations from the test set and 126 columns. Why so many? Recall that the recipe states to compute every possible interaction term between the factors and predictors, which increases the dimension of the data matrix substantially.\nYou may ask at this stage: why should I use a recipe instead of simply using the data wrangling commands such as mutate() or select()? tidymodels beauty is that a lot is happening under the hood. Recall, that for the simple scaling step, you actually have to compute the standard deviation of each column, then store this value, and apply the identical transformation to a different dataset, e.g., testing(split). A prepped recipe stores these values and hands them on once you bake() a novel dataset. Easy as pie with tidymodels, isn’t it?\n\n\nBuild a model\n Next, we can build an actual model based on our pre-processed data. In line with the definition above, we estimate regression coefficients of a Lasso regression such that we get \\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned}\\] We want to emphasize that the tidymodels workflow for any model is very similar, irrespective of the specific model. As you will see further below, it is straightforward to fit Ridge regression coefficients and, later, Neural networks or Random forests with basically the same code. The structure is always as follows: create a so-called workflow() and use the fit() function. A table with all available model APIs is available here. For now, we start with the linear regression model with a given value for the penalty factor \\(\\lambda\\). In the setup below, mixture denotes the value of \\(\\rho\\), hence setting mixture = 1 implies the Lasso.\n\nlm_model &lt;- linear_reg(\n  penalty = 0.0001,\n  mixture = 1\n) |&gt;\n  set_engine(\"glmnet\", intercept = FALSE)\n\nThat’s it - we are done! The object lm_model contains the definition of our model with all required information. Note that set_engine(\"glmnet\") indicates the API character of the tidymodels workflow: Under the hood, the package glmnet is doing the heavy lifting, while linear_reg() provides a unified framework to collect the inputs. The workflow ends with combining everything necessary for the serious data science workflow, namely, a recipe and a model.\n\nlm_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(lm_model)\nlm_fit\n\n══ Workflow ═════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ─────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_rm()\n• step_interact()\n• step_normalize()\n\n── Model ────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-04\n  mixture = 1\n\nEngine-Specific Arguments:\n  intercept = FALSE\n\nComputational engine: glmnet \n\n\n\n\nFit a model\nWith the workflow from above, we are ready to use fit(). Typically, we use training data to fit the model. The training data is pre-processed according to our recipe steps, and the Lasso regression coefficients are computed. First, we focus on the predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) Figure 2 illustrates the projections for the entire time series of the manufacturing industry portfolio returns. The grey area indicates the out-of-sample period, which we did not use to fit the model.\n\npredicted_values &lt;- lm_fit |&gt;\n  fit(data = training(split)) |&gt;\n  augment(data |&gt; filter(industry == \"manuf\")) |&gt;\n  select(date, \n         \"Fitted value\" = .pred,\n         \"Realization\" = ret_excess\n  ) |&gt;\n  pivot_longer(-date, names_to = \"Variable\")\n\n\npredicted_values |&gt;\n  ggplot(aes(\n    x = date, \n    y = value, \n    color = Variable,\n    linetype = Variable\n    )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"Monthly realized and fitted manufacturing industry risk premia\"\n  ) +\n  scale_x_date(\n    breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"5 years\"\n      )\n    },\n    minor_breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"1 years\"\n      )\n    },\n    expand = c(0, 0),\n    labels = date_format(\"%Y\")\n  ) +\n  scale_y_continuous(\n    labels = percent\n  ) +\n  annotate(\"rect\",\n    xmin = testing(split) |&gt; pull(date) |&gt; min(),\n    xmax = testing(split) |&gt; pull(date) |&gt; max(),\n    ymin = -Inf, ymax = Inf,\n    alpha = 0.5, fill = \"grey70\"\n  )\n\n\n\n\n\n\n\nFigure 2: The grey area corresponds to the out of sample period.\n\n\n\n\n\nWhat do the estimated coefficients look like? To analyze these values and to illustrate the difference between the tidymodels workflow and the underlying glmnet package, it is worth computing the coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. The code below estimates the coefficients for the Lasso and Ridge regression for the processed training data sample. Note that glmnet actually takes a vector y and the matrix of regressors \\(X\\) as input. Moreover, glmnet requires choosing the penalty parameter \\(\\alpha\\), which corresponds to \\(\\rho\\) in the notation above. When using the tidymodels model API, such details do not need consideration.\n\nx &lt;- data_bake |&gt;\n  select(-ret_excess) |&gt;\n  as.matrix()\ny &lt;- data_bake |&gt; pull(ret_excess)\n\nfit_lasso &lt;- glmnet(\n  x = x,\n  y = y,\n  alpha = 1,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nfit_ridge &lt;- glmnet(\n  x = x,\n  y = y,\n  alpha = 0,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nThe objects fit_lasso and fit_ridge contain an entire sequence of estimated coefficients for multiple values of the penalty factor \\(\\lambda\\). Figure 3 illustrates the trajectories of the regression coefficients as a function of the penalty factor. Both Lasso and Ridge coefficients converge to zero as the penalty factor increases.\n\nbind_rows(\n  tidy(fit_lasso) |&gt; mutate(Model = \"Lasso\"),\n  tidy(fit_ridge) |&gt; mutate(Model = \"Ridge\")\n) |&gt;\n  rename(\"Variable\" = term) |&gt;\n  ggplot(aes(x = lambda, y = estimate, color = Variable)) +\n  geom_line() +\n  scale_x_log10() +\n  facet_wrap(~ Model, scales = \"free_x\") +\n  labs(\n    x = \"Penalty factor (lambda)\", y = NULL,\n    title = \"Estimated coefficient paths for different penalty factors\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 3: The penalty parameters are chosen iteratively to resemble the path from no penalization to a model that excludes all variables.\n\n\n\n\n\n\nOne word of caution: The package glmnet computes estimates of the coefficients \\(\\hat\\beta\\) based on numerical optimization procedures. As a result, the estimated coefficients for the special case with no regularization (\\(\\lambda = 0\\)) can deviate from the standard OLS estimates.\n\n\n\nTune a model\nTo compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , we simply imposed a value for the penalty hyperparameter \\(\\lambda\\). Model tuning is the process of optimally selecting such hyperparameters. tidymodels provides extensive tuning options based on so-called cross-validation. Again, we refer to any treatment of cross-validation to get a more detailed discussion of the statistical underpinnings. Here we focus on the general idea and the implementation with tidymodels.\nThe goal for choosing \\(\\lambda\\) (or any other hyperparameter, e.g., \\(\\rho\\) for the Elastic Net) is to find a way to produce predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, the MSPE is not directly observable. We can only compute an estimate because our data is random and because we do not observe the entire population.\nObviously, if we train an algorithm on the same data that we use to compute the error, our estimate \\(\\text{MSPE}\\) would indicate way better predictive accuracy than what we can expect in real out-of-sample data. The result is called overfitting.\nCross-validation is a technique that allows us to alleviate this problem. We approximate the true MSPE as the average of many MSPE obtained by creating predictions for \\(K\\) new random samples of the data, none of them used to train the algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). In practice, this is done by carving out a piece of our data and pretending it is an independent sample. We again divide the data into a training set and a test set. The MSPE on the test set is our measure for actual predictive ability, while we use the training set to fit models with the aim to find the optimal hyperparameter values. To do so, we further divide our training sample into (several) subsets, fit our model for a grid of potential hyperparameter values (e.g., \\(\\lambda\\)), and evaluate the predictive accuracy on an independent sample. This works as follows:\n\nSpecify a grid of hyperparameters\nObtain predictors \\(\\hat{y}_i(\\lambda)\\) to denote the predictors for the used parameters \\(\\lambda\\)\nCompute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2\n\\] With K-fold cross-validation, we do this computation \\(K\\) times. Simply pick a validation set with \\(M=T/K\\) observations at random and think of these as random samples \\(y_1^k, \\dots, y_{\\tilde{T}}^k\\), with \\(k=1\\)\n\nHow should you pick \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original data. However, larger values of \\(K\\) will have much higher computation time. tidymodels provides all required tools to conduct \\(K\\)-fold cross-validation. We just have to update our model specification and let tidymodels know which parameters to tune. In our case, we specify the penalty factor \\(\\lambda\\) as well as the mixing factor \\(\\rho\\) as free parameters. Note that it is simple to change an existing workflow with update_model().\n\nlm_model &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) |&gt;\n  set_engine(\"glmnet\")\n\nlm_fit &lt;- lm_fit |&gt;\n  update_model(lm_model)\n\nFor our sample, we consider a time-series cross-validation sample. This means that we tune our models with 20 samples of length five years with a validation period of four years. For a grid of possible hyperparameters, we then fit the model for each fold and evaluate \\(\\hat{\\text{MSPE}}\\) in the corresponding validation set. Finally, we select the model specification with the lowest MSPE in the validation set. First, we define the cross-validation folds based on our training data only.\n\ndata_folds &lt;- time_series_cv(\n  data        = training(split),\n  date_var    = date,\n  initial     = \"5 years\",\n  assess      = \"48 months\",\n  cumulative  = FALSE,\n  slice_limit = 20\n)\ndata_folds\n\n# Time Series Cross Validation Plan \n# A tibble: 20 × 2\n  splits          id     \n  &lt;list&gt;          &lt;chr&gt;  \n1 &lt;split [60/48]&gt; Slice01\n2 &lt;split [60/48]&gt; Slice02\n3 &lt;split [60/48]&gt; Slice03\n4 &lt;split [60/48]&gt; Slice04\n5 &lt;split [60/48]&gt; Slice05\n# ℹ 15 more rows\n\n\nThen, we evaluate the performance for a grid of different penalty values. tidymodels provides functionalities to construct a suitable grid of hyperparameters with grid_regular. The code chunk below creates a \\(10 \\times 3\\) hyperparameters grid. Then, the function tune_grid() evaluates all the models for each fold.\n\nlm_tune &lt;- lm_fit |&gt;\n  tune_grid(\n    resample = data_folds,\n    grid = grid_regular(penalty(), mixture(), levels = c(20, 3)),\n    metrics = metric_set(rmse)\n  )\n\nAfter the tuning process, we collect the evaluation metrics (the root mean-squared error in our example) to identify the optimal model. Figure 4 illustrates the average validation set’s root mean-squared error for each value of \\(\\lambda\\) and \\(\\rho\\).\n\nautoplot(lm_tune) + \n  aes(linetype = `Proportion of Lasso Penalty`) + \n  guides(linetype = \"none\") +\n  labs(\n    x = \"Penalty factor (lambda)\",\n    y = \"Root MSPE\",\n    title = \"Root MSPE for different penalty factors\"\n  ) + \n  scale_x_log10()\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\n\n\n\nFigure 4: Evaluation of manufacturing excess returns for different penalty factors (lambda) and proportions of Lasso penalty (rho). 1.0 indicates Lasso, 0.5 indicates Elastic Net, and 0.0 indicates Ridge.\n\n\n\n\n\nFigure 4 shows that the cross-validated MSPE drops for Lasso and Elastic Net and spikes afterward. For Ridge regression, the MSPE increases above a certain threshold. Recall that the larger the regularization, the more restricted the model becomes. Thus, we would choose the model with the lowest MSPE.\n\n\nParallelized workflow\nOur starting point was the question: Which factors determine industry returns? While Avramov et al. (2023) provide a Bayesian analysis related to the research question above, we choose a simplified approach: To illustrate the entire workflow, we now run the penalized regressions for all ten industries. We want to identify relevant variables by fitting Lasso models for each industry returns time series. More specifically, we perform cross-validation for each industry to identify the optimal penalty factor \\(\\lambda\\). Then, we use the set of finalize_*()-functions that take a list or tibble of tuning parameter values and update objects with those values. After determining the best model, we compute the final fit on the entire training set and analyze the estimated coefficients.\nFirst, we define the Lasso model with one tuning parameter.\n\nlasso_model &lt;- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) |&gt;\n  set_engine(\"glmnet\")\n\nlm_fit &lt;- lm_fit |&gt;\n  update_model(lasso_model)\n\nThe following task can be easily parallelized to reduce computing time substantially. We use the parallelization capabilities of furrr. Note that we can also just recycle all the steps from above and collect them in a function.\n\nselect_variables &lt;- function(input) {\n  # Split into training and testing data\n  split &lt;- initial_time_split(input, prop = 4 / 5)\n\n  # Data folds for cross-validation\n  data_folds &lt;- time_series_cv(\n    data = training(split),\n    date_var = date,\n    initial = \"5 years\",\n    assess = \"48 months\",\n    cumulative = FALSE,\n    slice_limit = 20\n  )\n\n  # Model tuning with the Lasso model\n  lm_tune &lt;- lm_fit |&gt;\n    tune_grid(\n      resample = data_folds,\n      grid = grid_regular(penalty(), levels = c(10)),\n      metrics = metric_set(rmse)\n    )\n\n  # Identify the best model and fit with the training data\n  lasso_lowest_rmse &lt;- lm_tune |&gt; select_by_one_std_err(\"rmse\")\n  lasso_final &lt;- finalize_workflow(lm_fit, lasso_lowest_rmse)\n  lasso_final_fit &lt;- last_fit(lasso_final, split, metrics = metric_set(rmse))\n\n  # Extract the estimated coefficients\n  estimated_coefficients &lt;- lasso_final_fit |&gt;\n    extract_fit_parsnip() |&gt;\n    tidy() |&gt;\n    mutate(\n      term = str_remove_all(term, \"factor_|macro_|industry_\")\n    )\n\n  return(estimated_coefficients)\n}\n\n# Parallelization\nplan(multisession, workers = availableCores())\n\n# Computation by industry\nselected_factors &lt;- data |&gt;\n  nest(data = -industry) |&gt;\n  mutate(selected_variables = future_map(\n    data, select_variables,\n    .options = furrr_options(seed = TRUE)\n  ))\n\nLoading required package: scales\n\n\n\nAttaching package: ‘scales’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    discard\n\n\nLoading required package: scales\n\n\n\nAttaching package: ‘scales’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    discard\n\n\nLoading required package: scales\n\n\n\nAttaching package: ‘scales’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    discard\n\n\nLoading required package: scales\n\n\n\nAttaching package: ‘scales’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    discard\n\n\nLoading required package: scales\n\n\n\nAttaching package: ‘scales’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    discard\n\n\nLoading required package: scales\n\n\n\nAttaching package: ‘scales’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    discard\n\n\nLoading required package: scales\n\n\n\nAttaching package: ‘scales’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    discard\n\n\nLoading required package: scales\n\n\n\nAttaching package: ‘scales’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    discard\n\n\nWarning: There were 10 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `selected_variables = future_map(data,\n  select_variables, .options = furrr_options(seed = TRUE))`.\nCaused by warning in `select_by_one_std_err()`:\n! No value of `metric` was given; \"rmse\" will be used.\nℹ Run `dplyr::last_dplyr_warnings()` to see the 9 remaining\n  warnings.\n\n\nWhat has just happened? In principle, exactly the same as before but instead of computing the Lasso coefficients for one industry, we did it for ten in parallel. The final option seed = TRUE is required to make the cross-validation process reproducible. Now, we just have to do some housekeeping and keep only variables that Lasso does not set to zero. We illustrate the results in a heat map in Figure 5.\n\nselected_factors |&gt;\n  unnest(selected_variables) |&gt;\n  filter(\n    term != \"(Intercept)\",\n    estimate != 0\n  ) |&gt;\n  add_count(term) |&gt;\n  mutate(\n    term = str_remove_all(term, \"NA|ff_|q_\"),\n    term = str_replace_all(term, \"_x_\", \" \"),\n    term = fct_reorder(as_factor(term), n),\n    term = fct_lump_min(term, min = 2),\n    selected = 1\n  ) |&gt;\n  filter(term != \"Other\") |&gt;\n  mutate(term = fct_drop(term)) |&gt;\n  complete(industry, term, fill = list(selected = 0)) |&gt;\n  ggplot(aes(industry,\n    term,\n    fill = as_factor(selected)\n  )) +\n  geom_tile() +\n  scale_x_discrete(guide = guide_axis(angle = 70)) +\n  scale_fill_manual(values = c(\"white\", \"grey30\")) +\n  theme(legend.position = \"None\") +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Selected variables for different industries\"\n  )\n\n\n\n\n\n\n\nFigure 5: The figure shows selected variables for different industries. Dark areas indicate that the estimated Lasso regression coefficient is not set to zero. White fields show which variables get assigned a value of exactly zero.\n\n\n\n\n\nThe heat map in Figure 5 conveys two main insights. First, we see a lot of white, which means that many factors, macroeconomic variables, and interaction terms are not relevant for explaining the cross-section of returns across the industry portfolios. In fact, only the market factor and the return-on-equity factor play a role for several industries. Second, there seems to be quite some heterogeneity across different industries. While barely any variable is selected by Lasso for Utilities, many factors are selected for, e.g., High-Tech and Durable, but they do not coincide at all. In other words, there seems to be a clear picture that we do not need many factors, but Lasso does not provide a factor that consistently provides pricing abilities across industries.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#key-takeaways",
    "href": "r/factor-selection-via-machine-learning.html#key-takeaways",
    "title": "Factor Selection via Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe tidymodels framework in R enables a clean, modular workflow for pre-processing financial data, fitting models, and tuning hyperparameters through cross-validation.\nLasso regression is especially useful in high-dimensional settings, as it performs automatic variable selection by setting irrelevant coefficients to zero, offering insights into which factors truly matter.\nApplying these methods to real-world data shows that only a few factors consistently explain industry portfolio returns, and the relevant predictors vary across industries.\nThe analysis demonstrates practical tools to handle overfitting, model complexity, and interpretability in empirical asset pricing.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#exercises",
    "href": "r/factor-selection-via-machine-learning.html#exercises",
    "title": "Factor Selection via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and lambda and then returns the Ridge estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_2\\) penalty.\nCompute the \\(L_2\\) norm (\\(\\beta'\\beta\\)) for the regression coefficients based on the predictive regression from the previous exercise for a range of \\(\\lambda\\)’s and illustrate the effect of penalization in a suitable figure.\nNow, write a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and ’lambda` and then returns the Lasso estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_1\\) penalty.\nAfter you understand what Ridge and Lasso regressions are doing, familiarize yourself with the glmnet() package’s documentation. It is a thoroughly tested and well-established package that provides efficient code to compute the penalized regression coefficients for Ridge and Lasso and for combinations, commonly called Elastic Nets.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html",
    "href": "r/option-pricing-via-machine-learning.html",
    "title": "Option Pricing via Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nThis chapter covers machine learning methods in option pricing. First, we briefly introduce regression trees, random forests, and neural networks; these methods are advocated as highly flexible universal approximators, capable of recovering highly non-linear structures in the data. As the focus is on implementation, we leave a thorough treatment of the statistical underpinnings to other textbooks from authors with a real comparative advantage on these issues. We show how to implement random forests and deep neural networks with tidy principles using tidymodels and the torch package for more complicated network structures.\nMachine learning (ML) is seen as a part of artificial intelligence. ML algorithms build a model based on training data in order to make predictions or decisions without being explicitly programmed to do so. While ML can be specified along a vast array of different branches, this chapter focuses on so-called supervised learning for regressions. The basic idea of supervised learning algorithms is to build a mathematical model for data that contains both the inputs and the desired outputs. In this chapter, we apply well-known methods such as random forests and neural networks to a simple application in option pricing. More specifically, we create an artificial dataset of option prices for different values based on the Black-Scholes pricing equation for call options. Then, we train different models to learn how to price call options without prior knowledge of the theoretical underpinnings of the famous option pricing equation by Black and Scholes (1973).\nThroughout this chapter, we need the following R packages.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(torch)\nlibrary(brulee)\nlibrary(hardhat)\nlibrary(ranger)\nlibrary(glmnet)\nThe package torch (Falbel et al. 2023) provides functionality to define and train neural networks and is based on PyTorch (Paszke et al. 2019), while brulee (Kuhn and Falbel 2023) provides several basic modeling functions that use the torch infrastructure. The package ranger (Wright and Ziegler 2017) provides a fast implementation for random forests and hardhat (Vaughan and Kuhn 2022) is a helper function to for robust data preprocessing at fit time and prediction time.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "href": "r/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "title": "Option Pricing via Machine Learning",
    "section": "Regression Trees and Random Forests",
    "text": "Regression Trees and Random Forests\nRegression trees are a popular ML approach for incorporating multiway predictor interactions. In Finance, regression trees are gaining popularity, also in the context of asset pricing (see, e.g., Bryzgalova, Pelger, and Zhu 2022). Trees possess a logic that departs markedly from traditional regressions. Trees are designed to find groups of observations that behave similarly to each other. A tree grows in a sequence of steps. At each step, a new branch sorts the data leftover from the preceding step into bins based on one of the predictor variables. This sequential branching slices the space of predictors into partitions and approximates the unknown function \\(f(x)\\) which yields the relation between the predictors \\(x\\) and the outcome variable \\(y\\) with the average value of the outcome variable within each partition. For a more thorough treatment of regression trees, we refer to Coqueret and Guida (2020).\nFormally, we partition the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). For any predictor \\(x\\) that falls within region \\(R_j\\), we estimate \\(f(x)\\) with the average of the training observations, \\(\\hat y_i\\), for which the associated predictor \\(x_i\\) is also in \\(R_j\\). Once we select a partition \\(x\\) to split in order to create the new partitions, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, called \\(R_1(j,s)\\) and \\(R_2(j,s)\\), which split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\): \\[R_1(j,s) = \\{x \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{x \\mid x_j \\geq s\\}.\\] To pick \\(j\\) and \\(s\\), we find the pair that minimizes the residual sum of square (RSS): \\[\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\\] As in Factor Selection via Machine Learning in the context of penalized regressions, the first relevant question is: what are the hyperparameter decisions? Instead of a regularization parameter, trees are fully determined by the number of branches used to generate a partition (sometimes one specifies the minimum number of observations in each final branch instead of the maximum number of branches).\nModels with a single tree may suffer from high predictive variance. Random forests address these shortcomings of decision trees. The goal is to improve the predictive performance and reduce instability by averaging multiple decision trees. A forest basically implies creating many regression trees and averaging their predictions. To assure that the individual trees are not the same, we use a bootstrap to induce randomness. More specifically, we build \\(B\\) decision trees \\(T_1, \\ldots, T_B\\) using the training sample. For that purpose, we randomly select features to be included in the building of each tree. For each observation in the test set, we then form a prediction \\(\\hat{y} = \\frac{1}{B}\\sum\\limits_{i=1}^B\\hat{y}_{T_i}\\).",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#neural-networks",
    "href": "r/option-pricing-via-machine-learning.html#neural-networks",
    "title": "Option Pricing via Machine Learning",
    "section": "Neural Networks",
    "text": "Neural Networks\nRoughly speaking, neural networks propagate information from an input layer, through one or multiple hidden layers, to an output layer. While the number of units (neurons) in the input layer is equal to the dimension of the predictors, the output layer usually consists of one neuron (for regression) or multiple neurons for classification. The output layer predicts the future data, similar to the fitted value in a regression analysis. Neural networks have theoretical underpinnings as universal approximators for any smooth predictive association (Hornik 1991). Their complexity, however, ranks neural networks among the least transparent, least interpretable, and most highly parameterized ML tools. In finance, applications of neural networks can be found in many different contexts, e.g., Avramov, Cheng, and Metzker (2022), Chen, Pelger, and Zhu (2023), and Gu, Kelly, and Xiu (2020).\nEach neuron applies a non-linear activation function \\(f\\) to its aggregated signal before sending its output to the next layer \\[x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right)\\] Here, \\(\\theta\\) are the parameters to fit, \\(N^l\\) denotes the number of units (a hyperparameter to tune), and \\(z_j\\) are the input variables which can be either the raw data or, in the case of multiple chained layers, the outcome from a previous layer \\(z_j = x_k-1\\). While the easiest case with \\(f(x) = \\alpha + \\beta x\\) resembles linear regression, typical activation functions are sigmoid (i.e., \\(f(x) = (1+e^{-x})^{-1}\\)) or ReLu (i.e., \\(f(x) = max(x, 0)\\)).\nNeural networks gain their flexibility from chaining multiple layers together. Naturally, this imposes many degrees of freedom on the network architecture for which no clear theoretical guidance exists. The specification of a neural network requires, at a minimum, a stance on depth (number of hidden layers), the activation function, the number of neurons, the connection structure of the units (dense or sparse), and the application of regularization techniques to avoid overfitting. Finally, learning means to choose optimal parameters relying on numerical optimization, which often requires specifying an appropriate learning rate. Despite these computational challenges, implementation in R is not tedious at all because we can use the API to torch.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#option-pricing",
    "href": "r/option-pricing-via-machine-learning.html#option-pricing",
    "title": "Option Pricing via Machine Learning",
    "section": "Option Pricing",
    "text": "Option Pricing\nTo apply ML methods in a relevant field of finance, we focus on option pricing. The application in its core is taken from Hull (2020). In its most basic form, call options give the owner the right but not the obligation to buy a specific stock (the underlying) at a specific price (the strike price \\(K\\)) at a specific date (the exercise date \\(T\\)). The Black–Scholes price (Black and Scholes 1973) of a call option for a non-dividend-paying underlying stock is given by \\[\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_1 - \\sigma\\sqrt{T})Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left(\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right)\n\\end{aligned}\n\\] where \\(C(S, T)\\) is the price of the option as a function of today’s stock price of the underlying, \\(S\\), with time to maturity \\(T\\), \\(r_f\\) is the risk-free interest rate, and \\(\\sigma\\) is the volatility of the underlying stock return. \\(\\Phi\\) is the cumulative distribution function of a standard normal random variable.\nThe Black-Scholes equation provides a way to compute the arbitrage-free price of a call option once the parameters \\(S, K, r_f, T\\), and \\(\\sigma\\) are specified (arguably, in a realistic context, all parameters are easy to specify except for \\(\\sigma\\) which has to be estimated). A simple R function allows computing the price as we do below.\n\nblack_scholes_price &lt;- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  \n  d1 &lt;- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  d2 &lt;- d1 - sigma * sqrt(T)\n  price &lt;- S * pnorm(d1) - K * exp(-r * T) * pnorm(d2)\n  \n  return(price)\n}",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#learning-black-scholes",
    "href": "r/option-pricing-via-machine-learning.html#learning-black-scholes",
    "title": "Option Pricing via Machine Learning",
    "section": "Learning Black-Scholes",
    "text": "Learning Black-Scholes\nWe illustrate the concept of ML by showing how ML methods learn the Black-Scholes equation after observing some different specifications and corresponding prices without us revealing the exact pricing equation.\n\nData simulation\nTo that end, we start with simulated data. We compute option prices for call options for a grid of different combinations of times to maturity (T), risk-free rates (r), volatilities (sigma), strike prices (K), and current stock prices (S). In the code below, we add an idiosyncratic error term to each observation such that the prices considered do not exactly reflect the values implied by the Black-Scholes equation.\nIn order to keep the analysis reproducible, we use set.seed(). A random seed specifies the start point when a computer generates a random number sequence and ensures that our simulated data is the same across different machines.\n\nset.seed(420)\n\noption_prices &lt;- expand_grid(\n  S = 40:60,\n  K = 20:90,\n  r = seq(from = 0, to = 0.05, by = 0.01),\n  T = seq(from = 3 / 12, to = 2, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.8, by = 0.1)\n) |&gt;\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map_dbl(\n      black_scholes,\n      function(x) x + rnorm(1, sd = 0.15)\n    )\n  )\n\nThe code above generates more than 1.5 million random parameter constellations. For each of these values, two observed prices reflecting the Black-Scholes prices are given and a random innovation term pollutes the observed prices. The intuition of this application is simple: the simulated data provides many observations of option prices - by using the Black-Scholes equation we can evaluate the actual predictive performance of a ML method, which would be hard in a realistic context were the actual arbitrage-free price would be unknown.\nNext, we split the data into a training set (which contains 1% of all the observed option prices) and a test set that will only be used for the final evaluation. Note that the entire grid of possible combinations contains 1574496 different specifications. Thus, the sample to learn the Black-Scholes price contains only 31,489 observations and is therefore relatively small.\n\nsplit &lt;- initial_split(option_prices, prop = 1 / 100)\n\nWe process the training dataset further before we fit the different ML models. We define a recipe() that defines all processing steps for that purpose. For our specific case, we want to explain the observed price by the five variables that enter the Black-Scholes equation. The true prices (stored in column black_scholes) should obviously not be used to fit the model. The recipe also reflects that we standardize all predictors to ensure that each variable exhibits a sample average of zero and a sample standard deviation of one.\n\nrec &lt;- recipe(observed_price ~ .,\n  data = option_prices\n) |&gt;\n  step_rm(black_scholes) |&gt;\n  step_normalize(all_predictors())\n\n\n\nSingle layer networks and random forests\nNext, we show how to fit a neural network to the data. Note that this requires that torch is installed on your local machine. The function mlp() from the package parsnip provides the functionality to initialize a single layer, feed-forward neural network. The specification below defines a single layer feed-forward neural network with 10 hidden units. We set the number of training iterations to epochs = 500. The option set_mode(\"regression\") specifies a linear activation function for the output layer.\n\nnnet_model &lt;- mlp(\n  epochs = 500,\n  hidden_units = 10,\n  activation = \"sigmoid\",\n  penalty = 0.0001\n) |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"brulee\", verbose = FALSE)\n\nThe verbose = FALSE argument prevents logging the results to the console. We can follow the straightforward tidymodel workflow as in Factor Selection via Machine Learning: define a workflow, equip it with the recipe, and specify the associated model. Finally, fit the model with the training data.\n\nnn_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(nnet_model) |&gt;\n  fit(data = training(split))\n\nOnce you are familiar with the tidymodels workflow, it is a piece of cake to fit other models from the parsnip family. For instance, the model below initializes a random forest with 50 trees contained in the ensemble, where we require at least 2000 observations in a node. The random forests are trained using the package ranger, which is required to be installed in order to run the code below.\n\nrf_model &lt;- rand_forest(\n  trees = 50,\n  min_n = 2000\n) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\nFitting the model follows exactly the same convention as for the neural network before.\n\nrf_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_model) |&gt;\n  fit(data = training(split))\n\n\n\nDeep neural networks\nA deep neural network is a neural network with multiple layers between the input and output layers. By chaining multiple layers together, more complex structures can be represented with fewer parameters than simple shallow (one-layer) networks as the one implemented above. For instance, image or text recognition are typical tasks where deep neural networks are used (for applications of deep neural networks in finance, see, for instance, Jiang, Kelly, and Xiu 2023; Jensen et al. 2022).\nNote that while the tidymodels workflow is extremely convenient, these more sophisticated multi-layer (so-called deep) neural networks are not supported by tidymodels yet (as of September 2022). Instead, an implementation of a deep neural network in R requires additional computational tools. For that reason, the code snippet below illustrates how to initialize a sequential model with three hidden layers ith 10 units per layer. The brulee package provides a convenient interface to torch and is flexible enough to handle different activation functions.\n\ndeep_nnet_model &lt;- mlp(\n  epochs = 500,\n  hidden_units = c(10, 10, 10),\n  activation = \"sigmoid\",\n  penalty = 0.0001\n) |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"brulee\", verbose = FALSE)\n\nTo train the neural network, we provide the inputs (x) and the variable to predict (y) and then fit the parameters. Note the slightly tedious use of the method extract_mold(nn_fit). Instead of simply using the raw data, we fit the neural network with the same processed data that is used for the single-layer feed-forward network. What is the difference to simply calling x = training(data) |&gt; select(-observed_price, -black_scholes)? Recall that the recipe standardizes the variables such that all columns have unit standard deviation and zero mean. Further, it adds consistency if we ensure that all models are trained using the same recipe such that a change in the recipe is reflected in the performance of any model. A final note on a potentially irritating observation: fit() alters the model - this is one of the few instances, where a function in R alters the input such that after the function call the object model is not same anymore!\n\ndeep_nn_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(deep_nnet_model) |&gt;\n  fit(data = training(split))\n\n\n\nUniversal approximation\nBefore we evaluate the results, we implement one more model. In principle, any non-linear function can also be approximated by a linear model containing the input variables’ polynomial expansions. To illustrate this, we first define a new recipe, rec_linear, which processes the training data even further. We include polynomials up to the fifth degree of each predictor and then add all possible pairwise interaction terms. The final recipe step, step_lincomb(), removes potentially redundant variables (for instance, the interaction between \\(r^2\\) and \\(r^3\\) is the same as the term \\(r^5\\)). We fit a Lasso regression model with a pre-specified penalty term (consult Factor Selection via Machine Learning on how to tune the model hyperparameters).\n\nrec_linear &lt;- rec |&gt;\n  step_poly(all_predictors(),\n    degree = 5,\n    options = list(raw = TRUE)\n  ) |&gt;\n  step_interact(terms = ~ all_predictors():all_predictors()) |&gt;\n  step_lincomb(all_predictors())\n\nlm_model &lt;- linear_reg(penalty = 0.01) |&gt;\n  set_engine(\"glmnet\")\n\nlm_fit &lt;- workflow() |&gt;\n  add_recipe(rec_linear) |&gt;\n  add_model(lm_model) |&gt;\n  fit(data = training(split))",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#prediction-evaluation",
    "href": "r/option-pricing-via-machine-learning.html#prediction-evaluation",
    "title": "Option Pricing via Machine Learning",
    "section": "Prediction Evaluation",
    "text": "Prediction Evaluation\nFinally, we collect all predictions to compare the out-of-sample prediction error evaluated on 10,000 new data points. Note that for the evaluation, we use the call to extract_mold() to ensure that we use the same pre-processing steps for the testing data across each model. We also use the somewhat advanced functionality in forge(), which provides an easy, consistent, and robust pre-processor at prediction time.\n\nout_of_sample_data &lt;- testing(split) |&gt;\n  slice_sample(n = 10000)\n\npredictive_performance &lt;- deep_nn_fit |&gt;\n  predict(out_of_sample_data)|&gt;\n  rename(\"Deep NN\" = .pred) |&gt;\n  bind_cols(nn_fit |&gt;\n    predict(out_of_sample_data)) |&gt;\n  rename(\"Single layer\" = .pred) |&gt;\n  bind_cols(lm_fit |&gt; predict(out_of_sample_data)) |&gt;\n  rename(\"Lasso\" = .pred) |&gt;\n  bind_cols(rf_fit |&gt; predict(out_of_sample_data)) |&gt;\n  rename(\"Random forest\" = .pred) |&gt;\n  bind_cols(out_of_sample_data) |&gt;\n  pivot_longer(\"Deep NN\":\"Random forest\", names_to = \"Model\") |&gt;\n  mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\n\nIn the lines above, we use each of the fitted models to generate predictions for the entire test dataset of option prices. We evaluate the absolute pricing error as one possible measure of pricing accuracy, defined as the absolute value of the difference between predicted option price and the theoretical correct option price from the Black-Scholes model. We show the results graphically in Figure 1.\n\npredictive_performance |&gt;\n  ggplot(aes(\n    x = moneyness, \n    y = pricing_error, \n    color = Model,\n    linetype = Model\n    )) +\n  geom_jitter(alpha = 0.05) +\n  geom_smooth(se = FALSE, method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  facet_wrap(~Model, ncol = 2) + \n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Absolut prediction error (USD)\",\n    title = \"Prediction errors of call option prices for different models\",\n    linetype = NULL\n  )\n\n\n\n\n\n\n\nFigure 1: Absolut prediction error in USD for the different fitted methods. The prediction error is evaluated on a sample of call options that were not used for training.\n\n\n\n\n\nThe results can be summarized as follows:\n\nAll ML methods seem to be able to price call options after observing the training test set.\nThe average prediction errors increase for far in-the-money options.\nRandom forest and the Lasso seem to perform consistently worse in predicting option prices than the neural networks.\nThe complexity of the deep neural network relative to the single-layer neural network does not result in better out-of-sample predictions.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#key-takeaways",
    "href": "r/option-pricing-via-machine-learning.html#key-takeaways",
    "title": "Option Pricing via Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nMachine learning methods like random forests and neural networks can be used to estimate call option prices in R without relying on the Black-Scholes formula.\nSimulating noisy option price data and applying supervised learning models via the tidymodels framework provides a clean, reproducible analysis.\nDeep neural networks do not consistently outperform single-layer networks, underscoring the trade-off between model complexity and prediction performance.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#exercises",
    "href": "r/option-pricing-via-machine-learning.html#exercises",
    "title": "Option Pricing via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that takes y and a matrix of predictors X as inputs and returns a characterization of the relevant parameters of a regression tree with 1 branch.\nCreate a function that creates predictions for a new matrix of predictors newX based on the estimated regression tree.\nUse the package rpart to grow a tree based on the training data and use the illustration tools in rpart to understand which characteristics the tree deems relevant for option pricing.\nMake use of a training and a test set to choose the optimal depth (number of sample splits) of the tree.\nUse brulee to initialize a sequential neural network that can take the predictors from the training dataset as input, contains at least one hidden layer, and generates continuous predictions. This sounds harder than it is: see a simple regression example here. How many parameters does the neural network you aim to fit have?\nCompile the object from the previous exercise. It is important that you specify a loss function. Illustrate the difference in predictive accuracy for different architecture choices.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html",
    "href": "r/fama-macbeth-regressions.html",
    "title": "Fama-MacBeth Regressions",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we present a simple implementation of Fama and MacBeth (1973), a regression approach commonly called Fama-MacBeth regressions. Fama-MacBeth regressions are widely used in empirical asset pricing studies. We use individual stocks as test assets to estimate the risk premium associated with the three factors included in Fama and French (1993).\nResearchers use the two-stage regression approach to estimate risk premiums in various markets, but predominately in the stock market. Essentially, the two-step Fama-MacBeth regressions exploit a linear relationship between expected returns and exposure to (priced) risk factors. The basic idea of the regression approach is to project asset returns on factor exposures or characteristics that resemble exposure to a risk factor in the cross-section in each time period. Then, in the second step, the estimates are aggregated across time to test if a risk factor is priced. In principle, Fama-MacBeth regressions can be used in the same way as portfolio sorts introduced in previous chapters.\nThe Fama-MacBeth procedure is a simple two-step approach: The first step uses the exposures (characteristics) as explanatory variables in \\(T\\) cross-sectional regressions. For example, if \\(r_{i,t+1}\\) denote the excess returns of asset \\(i\\) in month \\(t+1\\), then the famous Fama-French three-factor model implies the following return generating process (see also Campbell et al. 1998): \\[\\begin{aligned}r_{i,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{i,t}  + \\lambda^{SMB}_t \\beta^{SMB}_{i,t} + \\lambda^{HML}_t \\beta^{HML}_{i,t} + \\epsilon_{i,t}.\\end{aligned}\\] Here, we are interested in the compensation \\(\\lambda^{f}_t\\) for the exposure to each risk factor \\(\\beta^{f}_{i,t}\\) at each time point, i.e., the risk premium. Note the terminology: \\(\\beta^{f}_{i,t}\\) is a asset-specific characteristic, e.g., a factor exposure or an accounting variable. If there is a linear relationship between expected returns and the characteristic in a given month, we expect the regression coefficient to reflect the relationship, i.e., \\(\\lambda_t^{f}\\neq0\\).\nIn the second step, the time-series average \\(\\frac{1}{T}\\sum_{t=1}^T \\hat\\lambda^{f}_t\\) of the estimates \\(\\hat\\lambda^{f}_t\\) can then be interpreted as the risk premium for the specific risk factor \\(f\\). We follow Zaffaroni and Zhou (2022) and consider the standard cross-sectional regression to predict future returns. If the characteristics are replaced with time \\(t+1\\) variables, then the regression approach captures risk attributes rather than risk premiums.\nBefore we move to the implementation, we want to highlight that the characteristics, e.g., \\(\\hat\\beta^{f}_{i}\\), are often estimated in a separate step before applying the actual Fama-MacBeth methodology. You can think of this as a step 0. You might thus worry that the errors of \\(\\hat\\beta^{f}_{i}\\) impact the risk premiums’ standard errors. Measurement error in \\(\\hat\\beta^{f}_{i}\\) indeed affects the risk premium estimates, i.e., they lead to biased estimates. The literature provides adjustments for this bias (see, e.g., Shanken 1992; Kim 1995; Chen, Lee, and Lee 2015, among others) but also shows that the bias goes to zero as \\(T \\to \\infty\\). We refer to Gagliardini, Ossola, and Scaillet (2016) for an in-depth discussion also covering the case of time-varying betas. Moreover, if you plan to use Fama-MacBeth regressions with individual stocks, Hou, Xue, and Zhang (2020) advocates using weighted-least squares to estimate the coefficients such that they are not biased toward small firms. Without this adjustment, the high number of small firms would drive the coefficient estimates.\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(sandwich)\nlibrary(broom)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#data-preparation",
    "href": "r/fama-macbeth-regressions.html#data-preparation",
    "title": "Fama-MacBeth Regressions",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe illustrate Fama and MacBeth (1973) with the monthly CRSP sample and use three characteristics to explain the cross-section of returns: market capitalization, the book-to-market ratio, and the CAPM beta (i.e., the covariance of the excess stock returns with the market excess returns). We collect the data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, gvkey, date, ret_excess, mktcap) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n  select(datadate, gvkey, be) |&gt;\n  collect()\n\nbeta &lt;- tbl(tidy_finance, \"beta\") |&gt; \n  filter(return_type == \"monthly\") |&gt; \n  select(permno, date, beta) |&gt;\n  collect()\n\nWe use the Compustat and CRSP data to compute the book-to-market ratio and the (log) market capitalization. Furthermore, we also use the CAPM betas based on monthly returns we computed in the previous chapters.\n\ncharacteristics &lt;- compustat |&gt;\n  mutate(date = floor_date(ymd(datadate), \"month\")) |&gt;\n  left_join(crsp_monthly, join_by(gvkey, date)) |&gt;\n  left_join(beta, join_by(permno, date)) |&gt;\n  transmute(\n    gvkey,\n    bm = be / mktcap,\n    log_mktcap = log(mktcap),\n    beta = beta,\n    sorting_date = date %m+% months(6)\n  )\n\ndata_fama_macbeth &lt;- crsp_monthly |&gt;\n  left_join(characteristics, join_by(gvkey, date == sorting_date)) |&gt;\n  group_by(permno) |&gt;\n  arrange(date) |&gt;\n  fill(c(beta, bm, log_mktcap), .direction = \"down\") |&gt;\n  ungroup() |&gt;\n  left_join(crsp_monthly |&gt;\n    select(permno, date, ret_excess_lead = ret_excess) |&gt;\n    mutate(date = date %m-% months(1)),\n  join_by(permno, date)\n  ) |&gt;\n  select(permno, date, ret_excess_lead, beta, log_mktcap, bm) |&gt;\n  drop_na()",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#cross-sectional-regression",
    "href": "r/fama-macbeth-regressions.html#cross-sectional-regression",
    "title": "Fama-MacBeth Regressions",
    "section": "Cross-Sectional Regression",
    "text": "Cross-Sectional Regression\nNext, we run the cross-sectional regressions with the characteristics as explanatory variables for each month. We regress the returns of the test assets at a particular time point on the characteristics of each asset. By doing so, we get an estimate of the risk premiums \\(\\hat\\lambda^{f}_t\\) for each point in time. \n\nrisk_premiums &lt;- data_fama_macbeth |&gt;\n  nest(data = c(ret_excess_lead, beta, log_mktcap, bm, permno)) |&gt;\n  mutate(estimates = map(\n    data,\n    \\(x) tidy(lm(ret_excess_lead ~ beta + log_mktcap + bm, data = x))\n  )) |&gt;\n  unnest(estimates)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#time-series-aggregation",
    "href": "r/fama-macbeth-regressions.html#time-series-aggregation",
    "title": "Fama-MacBeth Regressions",
    "section": "Time-Series Aggregation",
    "text": "Time-Series Aggregation\nNow that we have the risk premiums’ estimates for each period, we can average across the time-series dimension to get the expected risk premium for each characteristic. Similarly, we manually create the \\(t\\)-test statistics for each regressor, which we can then compare to usual critical values of 1.96 or 2.576 for two-tailed significance tests.\n\nprice_of_risk &lt;- risk_premiums |&gt;\n  group_by(factor = term) |&gt;\n  summarize(\n    risk_premium = mean(estimate),\n    t_statistic = mean(estimate) / sd(estimate) * sqrt(n())\n  )\n\nIt is common to adjust for autocorrelation when reporting standard errors of risk premiums. As in Univariate Portfolio Sorts, the typical procedure for this is computing Whitney K. Newey and West (1987) standard errors. We again recommend the data-driven approach of Whitney K. Newey and West (1994) using the NeweyWest() function, but note that you can enforce the typical 6 lag settings via NeweyWest(., lag = 6, prewhite = FALSE).\n\nregressions_for_newey_west &lt;- risk_premiums |&gt;\n  select(date, factor = term, estimate) |&gt;\n  nest(data = c(date, estimate)) |&gt;\n  mutate(\n    model = map(data, \\(x) lm(estimate ~ 1, x)),\n    mean = map(model, tidy)\n  )\n\nprice_of_risk_newey_west &lt;- regressions_for_newey_west |&gt;\n  mutate(newey_west_se = map_dbl(model, \\(x) sqrt(NeweyWest(x)))) |&gt;\n  unnest(mean) |&gt;\n  mutate(t_statistic_newey_west = estimate / newey_west_se) |&gt;\n  select(factor,\n    risk_premium = estimate,\n    t_statistic_newey_west\n  )\n\nleft_join(price_of_risk,\n  price_of_risk_newey_west |&gt;\n    select(factor, t_statistic_newey_west),\n  join_by(factor)\n)\n\n# A tibble: 4 × 4\n  factor      risk_premium t_statistic t_statistic_newey_west\n  &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;                  &lt;dbl&gt;\n1 (Intercept)    0.0118         4.70                   3.95  \n2 beta          -0.0000460     -0.0454                -0.0410\n3 bm             0.00141        3.19                   2.71  \n4 log_mktcap    -0.000969      -2.80                  -2.46  \n\n\nFinally, let us interpret the results. Stocks with higher book-to-market ratios earn higher expected future returns, which is in line with the value premium. The negative value for log market capitalization reflects the size premium for smaller stocks. Consistent with results from earlier chapters, we detect no relation between beta and future stock returns.\nYou can also replicate the results using the tidyfinace package via the estimate_fama_macbeth() function:\n\nlibrary(tidyfinance)\n\nestimate_fama_macbeth(\n  data = data_fama_macbeth, \n  model = \"ret_excess_lead ~ beta + bm + log_mktcap\",\n  vcov = \"newey-west\"\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#key-takeaways",
    "href": "r/fama-macbeth-regressions.html#key-takeaways",
    "title": "Fama-MacBeth Regressions",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFama-MacBeth regressions provide a two-step approach to estimate risk premiums by running time-series averages of cross-sectional regressions on asset characteristics.\nFama-MacBeth regressions are commonly used in empirical asset pricing to test whether factors like size, value, or market beta are priced in the cross-section of stock returns.\nMeasurement error in factor exposures, especially when estimated beforehand, can bias results, but corrections such as Newey-West standard errors and weighted regressions can improve accuracy.\nThe tidyfinance R package provides a user-friendly estimate_fama_macbeth() function that simplifies the Fama-MacBeth estimation pipeline.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#exercises",
    "href": "r/fama-macbeth-regressions.html#exercises",
    "title": "Fama-MacBeth Regressions",
    "section": "Exercises",
    "text": "Exercises\n\nEstimate stock-specific value and size risk factors to similar to the CAPM-beta using rolling estimation based on Fama-French 3 Factors. Use these estimates instead of the stock characteristics in the Fama-MacBeth regression from above. How do the coefficient estimates differ?\nDownload the 49 Industry Portfolios from Ken French data library. Use these industry portfolios instead of the stocks to estimate the three rolling risk-factors (beta, value, size). Replicate the Fama-MacBeth regression from above. Are the coefficient estimates similar?\nUse individual stocks with weighted-least squares based on a firm’s size as suggested by Hou, Xue, and Zhang (2020). Then, repeat the Fama-MacBeth regressions without the weighting scheme adjustment but drop the smallest 20 percent of firms each month. Compare the results of the three approaches.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/other-data-providers.html",
    "href": "r/other-data-providers.html",
    "title": "Other Data Providers",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn the previous chapters, we introduced many ways to get financial data that researchers regularly use. We showed how to load data into R from Yahoo Finance and commonly used file types, such as comma-separated or Excel files. Then, we introduced remotely connecting to WRDS and downloading data from there. However, this is only a subset of the vast amounts of data available these days.\nIn this short chapter, we aim to provide an overview of common alternative data providers for which direct access via R packages exists. Such a list requires constant adjustments because both data providers and access methods change. However, we want to emphasize two main insights: First, the number of R packages that provide access to (financial) data is large. Too large actually to survey here exhaustively. Instead, we can only cover the tip of the iceberg. Second, R provides the functionalities to access basically any form of files or data available online. Thus, even if a desired data source does not come with a well-established R package, chances are high that data can be retrieved by establishing your own API connection or by scrapping the content.\nIn our non-exhaustive list below, we restrict ourselves to listing data sources accessed through easy-to-use R packages. For further inspiration on potential data sources, we recommend reading the R task view empirical finance. Further inspiration (on more general social sciences) can be found here.\nApart from the list below, we want to advertise some amazing data compiled by others. First, there is Open Source Asset Pricing related to Chen and Zimmermann (2022). They provide return data for over 200 trading strategies with different time periods and specifications. The authors also provide signals and explanations of the factor construction. Moreover, in the same spirit, Global factor data provides the data related to Jensen2022b. They provide return data for characteristic-managed portfolios from around the world. The database includes factors for 153 characteristics in 13 themes, using data from 93 countries.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "r/other-data-providers.html#key-takeaways",
    "href": "r/other-data-providers.html#key-takeaways",
    "title": "Other Data Providers",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nAccess a wide range of financial and macroeconomic data through R packages tailored to specific providers.\nEven if a specific R package doesn’t exist, R can often retrieve data through APIs or web scraping methods.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "r/other-data-providers.html#exercises",
    "href": "r/other-data-providers.html#exercises",
    "title": "Other Data Providers",
    "section": "Exercises",
    "text": "Exercises\n\nSelect one of the data sources in the table above and retrieve some data. Browse the homepage of the data provider or the package documentation to find inspiration on which type of data is available to you and how to download the data into your R session.\nGenerate summary statistics of the data you retrieved and provide some useful visualization. The possibilities are endless: Maybe there is some interesting economic event you want to analyze, such as stock market responses to Twitter activity.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "contribute.html",
    "href": "contribute.html",
    "title": "Contribute to Tidy Finance",
    "section": "",
    "text": "Join our mission to support reproducible finance by contributing to the Tidy Finance Blog. The blog follows our endeavors to increase transparency in financial economics and opens a new channel for you. We actively encourage the finance community to share their insights on coding.\nTidy Finance is an open-source project to augment how research is conducted. We hope to change the way people think about sharing their code. The Tidy Finance Blog is open to everybody who wants to share their code, discuss paper replication, preparation of typical datasets or highlight novel empirical applications in financial economics. The blog is an excellent tool to promote your research and share your thought processes with the community.\nWe strive to make Tidy Finance a trusted, up-to-date resource for all topics in financial economics. Therefore, we commit our time to ensure high-quality content - a commitment that we extend to the Tidy Finance Blog by providing editorial-like processes. While we aim to be as open as possible, we will also carefully assess all contributions."
  },
  {
    "objectID": "contribute.html#who-should-contribute-to-the-tidy-finance-blog",
    "href": "contribute.html#who-should-contribute-to-the-tidy-finance-blog",
    "title": "Contribute to Tidy Finance",
    "section": "Who should contribute to the Tidy Finance Blog?",
    "text": "Who should contribute to the Tidy Finance Blog?\nEverybody should consider writing a blog post on Tidy Finance. We do not exclude anybody from the finance community. We always appreciate relevant contributions that help the target audiences of Tidy Finance (i.e., researchers, students, and professionals). Let us move forward together."
  },
  {
    "objectID": "contribute.html#how-can-i-contribute",
    "href": "contribute.html#how-can-i-contribute",
    "title": "Contribute to Tidy Finance",
    "section": "How can I contribute?",
    "text": "How can I contribute?\nIf you want to contribute to the Tidy Finance Blog, we provide a simple three-step process for your blog post.\n\nSend a proposal for your blog post to blog@tidy-finance.org.\nAfter our confirmation, write and submit your blog post.\nWork with us on potential revisions before reading your published post.\n\nWe are grateful for every contribution. This process is our attempt to provide fairness to all parties. From the perspective of the readers, the other authors, and the whole project, ensuring the quality and relevance of all contributions is paramount. Hence, we cannot publish all blog posts without thoroughly reviewing the submissions.\nWe ask you to submit a proposal before you write an entire post to ensure your efforts are not wasted. In this proposal, we want to learn about your idea and how it will contribute to Tidy Finance. Therefore, do not hesitate to submit a proposal. We guarantee full anonymity and an open attitude to your suggestion. Nevertheless, we do not make promises regarding a positive decision.\nOnce you receive our thumbs-up, you can start working on your contribution. The target format is a quarto document we can render as part of our website repository. When you have a first draft ready, send us an HTML output, and we will provide timely feedback.\nFinally, we will publish your post on the Tidy Finance blog with your name added to the list of contributors to reproducible finance. Unfortunately, we cannot provide you with a cape. Then again, not all heroes wear capes."
  },
  {
    "objectID": "contribute.html#part-of-the-future",
    "href": "contribute.html#part-of-the-future",
    "title": "Contribute to Tidy Finance",
    "section": "Part of the future",
    "text": "Part of the future\nThe Tidy Finance Blog is the place to contribute stand-alone applications in financial economics with guaranteed quality and relevance. While you publish under an open-source license, you retain sole authorship of your work. Furthermore, in future editions of “Tidy Finance with R,” we will acknowledge external contributions directly by referring to the Tidy Finance blog and citing your work."
  },
  {
    "objectID": "contribute.html#what-if-i-do-not-use-the-tidyverse-or-r",
    "href": "contribute.html#what-if-i-do-not-use-the-tidyverse-or-r",
    "title": "Contribute to Tidy Finance",
    "section": "What if I do not use the tidyverse or R?",
    "text": "What if I do not use the tidyverse or R?\nTidy Finance is strongly connected to the tidyverse and R. Yet, we do not rule out extending our horizon and learning new tricks. You can make non-tidyverse and non-R suggestions. However, we need a sufficient reason to break with our tradition."
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html",
    "href": "python/univariate-portfolio-sorts.html",
    "title": "Univariate Portfolio Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we dive into portfolio sorts, one of the most widely used statistical methodologies in empirical asset pricing (e.g., Bali, Engle, and Murray 2016). The key application of portfolio sorts is to examine whether one or more variables can predict future excess returns. In general, the idea is to sort individual stocks into portfolios, where the stocks within each portfolio are similar with respect to a sorting variable, such as firm size. The different portfolios then represent well-diversified investments that differ in the level of the sorting variable. You can then attribute the differences in the return distribution to the impact of the sorting variable. We start by introducing univariate portfolio sorts (which sort based on only one characteristic) and tackle bivariate sorting in Value and Bivariate Sorts.\nA univariate portfolio sort considers only one sorting variable \\(x_{i,t-1}\\). Here, \\(i\\) denotes the stock and \\(t-1\\) indicates that the characteristic is observable by investors at time \\(t\\). The objective is to assess the cross-sectional relation between \\(x_{i,t-1}\\) and, typically, stock excess returns \\(r_{i,t}\\) at time \\(t\\) as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas from the previous chapter as our sorting variable.\nThe current chapter relies on the following set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.api as sm\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom regtabletotext import prettify_result",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#data-preparation",
    "href": "python/univariate-portfolio-sorts.html#data-preparation",
    "title": "Univariate Portfolio Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start with loading the required data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. In particular, we use the monthly CRSP sample as our asset universe. Once we form our portfolios, we use the Fama-French market factor returns to compute the risk-adjusted performance (i.e., alpha). beta is the dataframe with market betas computed in the previous chapter.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=\"SELECT permno, date, ret_excess, mktcap_lag FROM crsp_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nbeta = pd.read_sql_query(\n  sql=\"SELECT permno, date, beta FROM beta WHERE return_type = 'monthly'\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "href": "python/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "title": "Univariate Portfolio Sorts",
    "section": "Sorting by Market Beta",
    "text": "Sorting by Market Beta\nNext, we merge our sorting variable with the return data. We use the one-month lagged betas as a sorting variable to ensure that the sorts rely only on information available when we create the portfolios. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month \\(t\\) information is available in month \\(t+1\\). You may be tempted to simply use a call such as crsp_monthly['beta_lag'] = crsp_monthly.groupby('permno')['beta'].shift(1) instead. This procedure, however, does not work correctly if there are implicit missing values in the time series.\n\nbeta_lag = (beta\n  .assign(date=lambda x: x[\"date\"]+pd.DateOffset(months=1))\n  .get([\"permno\", \"date\", \"beta\"])\n  .rename(columns={\"beta\": \"beta_lag\"})\n  .dropna()\n)\n\ndata_for_sorts = (crsp_monthly\n  .merge(beta_lag, how=\"inner\", on=[\"permno\", \"date\"])\n)\n\nThe first step to conduct portfolio sorts is to calculate periodic breakpoints that you can use to group the stocks into portfolios. For simplicity, we start with the median lagged market beta as the single breakpoint. We then compute the value-weighted returns for each of the two resulting portfolios, which means that the lagged market capitalization determines the weight in np.average().\n\nbeta_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: (x.assign(\n      portfolio=pd.qcut(\n        x[\"beta_lag\"], q=[0, 0.5, 1], labels=[\"low\", \"high\"]))\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"portfolio\",\"date\"])\n  .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n  .reset_index(name=\"ret\")\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#performance-evaluation",
    "href": "python/univariate-portfolio-sorts.html#performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "Performance Evaluation",
    "text": "Performance Evaluation\nWe can construct a long-short strategy based on the two portfolios: buy the high-beta portfolio and, at the same time, short the low-beta portfolio. Thereby, the overall position in the market is net-zero, i.e., you do not need to invest money to realize this strategy in the absence of frictions.\n\nbeta_longshort = (beta_portfolios\n  .pivot_table(index=\"date\", columns=\"portfolio\", values=\"ret\")\n  .reset_index()\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n)\n\nWe compute the average return and the corresponding standard error to test whether the long-short portfolio yields on average positive or negative excess returns. In the asset pricing literature, one typically adjusts for autocorrelation by using Newey and West (1987) \\(t\\)-statistics to test the null hypothesis that average portfolio excess returns are equal to zero. One necessary input for Newey-West standard errors is a chosen bandwidth based on the number of lags employed for the estimation. Researchers often default to choosing a pre-specified lag length of six months (which is not a data-driven approach). We do so in the fit() function by indicating the cov_type as HAC and providing the maximum lag length through an additional keywords dictionary.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\",\n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept       0.0       0.001         0.25    0.803\n\nSummary statistics:\n- Number of observations: 708\n- R-squared: 0.000, Adjusted R-squared: 0.000\n- F-statistic not available\n\n\n\nThe results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM yields that the high-beta stocks should yield higher expected returns. Our portfolio sort implicitly mimics an investment strategy that finances high-beta stocks by shorting low-beta stocks. Therefore, one should expect that the average excess returns yield a return that is above the risk-free rate.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "href": "python/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "title": "Univariate Portfolio Sorts",
    "section": "Functional Programming for Portfolio Sorts",
    "text": "Functional Programming for Portfolio Sorts\nNow, we take portfolio sorts to the next level. We want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming is very handy: we define a function that gives us flexibility concerning which variable to use for the sorting, denoted by sorting_variable. We use np.quantile() to compute breakpoints for n_portfolios. Then, we assign portfolios to stocks using the pd.cut() function. The output of the following function is a new column that contains the number of the portfolio to which a stock belongs.\nIn some applications, the variable used for the sorting might be clustered (e.g., at a lower bound of 0). Then, multiple breakpoints may be identical, leading to empty portfolios. Similarly, some portfolios might have a very small number of stocks at the beginning of the sample. Cases where the number of portfolio constituents differs substantially due to the distribution of the characteristics require careful consideration and, depending on the application, might require customized sorting approaches.\n\ndef assign_portfolio(data, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolios to a bin between breakpoints.\"\"\"\n    \n    breakpoints = np.quantile(\n      data[sorting_variable].dropna(), \n      np.linspace(0, 1, n_portfolios + 1), \n      method=\"linear\"\n    )\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nWe can use the above function to sort stocks into ten portfolios each month using lagged betas and compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.\n\nbeta_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio=assign_portfolio(x, \"beta_lag\", 10)\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"portfolio\", \"date\"])\n  .apply(lambda x: x.assign(\n      ret=np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    )\n  )\n  .reset_index(drop=True)\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#more-performance-evaluation",
    "href": "python/univariate-portfolio-sorts.html#more-performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "More Performance Evaluation",
    "text": "More Performance Evaluation\nIn the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns.\n\nbeta_portfolios_summary = (beta_portfolios\n  .groupby(\"portfolio\")\n  .apply(lambda x: x.assign(\n      alpha=sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[0],\n      beta=sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[1],\n      ret=x[\"ret\"].mean()\n    ).tail(1)\n  )\n  .reset_index(drop=True)\n  .get([\"portfolio\", \"alpha\", \"beta\", \"ret\"])\n)\n\nFigure 1 illustrates the CAPM alphas of beta-sorted portfolios. It shows that low-beta portfolios tend to exhibit positive alphas, while high-beta portfolios exhibit negative alphas.\n\nbeta_portfolios_figure = (\n  ggplot(\n    beta_portfolios_summary, \n    aes(x=\"portfolio\", y=\"alpha\", fill=\"portfolio\")\n  )\n  + geom_bar(stat=\"identity\")\n  + labs(\n      x=\"Portfolio\", y=\"CAPM alpha\", fill=\"Portfolio\",\n      title=\"CAPM alphas of beta-sorted portfolios\"\n    )\n  + scale_y_continuous(labels=percent_format())\n  + theme(legend_position=\"none\")\n)\nbeta_portfolios_figure.show()\n\n\n\n\n\n\n\nFigure 1: The figure shows CAPM alphas of beta-sorted portfolios. Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the entire CRSP period.\n\n\n\n\n\nThese results suggest a negative relation between beta and future stock returns, which contradicts the predictions of the CAPM. According to the CAPM, returns should increase with beta across the portfolios and risk-adjusted returns should be statistically indistinguishable from zero.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#security-market-line-and-beta-portfolios",
    "href": "python/univariate-portfolio-sorts.html#security-market-line-and-beta-portfolios",
    "title": "Univariate Portfolio Sorts",
    "section": "Security Market Line and Beta Portfolios",
    "text": "Security Market Line and Beta Portfolios\nThe CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time. Figure 2 illustrates the security market line: We see that (not surprisingly) the high-beta portfolio returns have a high correlation with the market returns. However, it seems like the average excess returns for high-beta stocks are lower than what the security market line implies would be an “appropriate” compensation for the high market risk.\n\nsml_capm = (sm.OLS.from_formula(\n    formula=\"ret ~ 1 + beta\", \n    data=beta_portfolios_summary\n  )\n  .fit()\n  .params\n)\n\nsml_figure = (\n  ggplot(\n    beta_portfolios_summary,\n    aes(x=\"beta\", y=\"ret\", color=\"portfolio\")\n  )\n  + geom_point()\n  + geom_abline(\n      intercept=0, slope=factors_ff3_monthly[\"mkt_excess\"].mean(), linetype=\"solid\"\n    )\n  + geom_abline(\n      intercept=sml_capm[\"Intercept\"], slope=sml_capm[\"beta\"], linetype=\"dashed\"\n    )\n  + labs(\n      x=\"Beta\", y=\"Excess return\", color=\"Portfolio\",\n      title=\"Average portfolio excess returns and beta estimates\"\n    )\n  + scale_x_continuous(limits=(0, 2))\n  + scale_y_continuous(\n      labels=percent_format(),\n      limits=(0, factors_ff3_monthly[\"mkt_excess\"].mean()*2)\n    )\n)\nsml_figure.show()\n\n\n\n\n\n\n\nFigure 2: The figure shows average portfolio excess returns and beta estimates. Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\n\nTo provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio.\n\nbeta_longshort = (beta_portfolios\n  .assign(\n    portfolio=lambda x: (\n      x[\"portfolio\"].apply(\n        lambda y: \"high\" if y == x[\"portfolio\"].max()\n        else (\"low\" if y == x[\"portfolio\"].min()\n        else y)\n      )\n    )\n  )\n  .query(\"portfolio in ['low', 'high']\")\n  .pivot_table(index=\"date\", columns=\"portfolio\", values=\"ret\")\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n)\n\nAgain, the resulting long-short strategy does not exhibit statistically significant returns.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept     0.002       0.003        0.743    0.458\n\nSummary statistics:\n- Number of observations: 708\n- R-squared: -0.000, Adjusted R-squared: -0.000\n- F-statistic not available\n\n\n\nHowever, controlling for the effect of beta, the long-short portfolio yields a statistically significant negative CAPM-adjusted alpha, although the average excess stock returns should be zero according to the CAPM. The results thus provide no evidence in support of the CAPM. The negative value has been documented as the so-called betting-against-beta factor (Frazzini and Pedersen 2014). Betting-against-beta corresponds to a strategy that shorts high-beta stocks and takes a (levered) long position in low-beta stocks. If borrowing constraints prevent investors from taking positions on the security market line they are instead incentivized to buy high-beta stocks, which leads to a relatively higher price (and therefore lower expected returns than implied by the CAPM) for such high-beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital-constrained investors with lower risk aversion.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1 + mkt_excess\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1 + mkt_excess\n\nCoefficients:\n            Estimate  Std. Error  t-Statistic  p-Value\nIntercept     -0.004       0.002       -1.783    0.075\nmkt_excess     1.137       0.068       16.627    0.000\n\nSummary statistics:\n- Number of observations: 708\n- R-squared: 0.429, Adjusted R-squared: 0.429\n- F-statistic: 276.449 on 1 and 706 DF, p-value: 0.000\n\n\n\nFigure 3 shows the annual returns of the extreme beta portfolios we are mainly interested in. The figure illustrates no consistent striking patterns over the last years; each portfolio exhibits periods with positive and negative annual returns.\n\nbeta_longshort_year = (beta_longshort\n  .assign(year=lambda x: x[\"date\"].dt.year)\n  .groupby(\"year\")\n  .aggregate(\n    low=(\"low\", lambda x: (1+x).prod()-1),\n    high=(\"high\", lambda x: (1+x).prod()-1),\n    long_short=(\"long_short\", lambda x: (1+x).prod()-1)\n  )\n  .reset_index()\n  .melt(id_vars=\"year\", var_name=\"name\", value_name=\"value\")\n)\n\nbeta_longshort_figure = (\n  ggplot(\n    beta_longshort_year, \n    aes(x=\"year\", y=\"value\", fill=\"name\")\n  )\n  + geom_col(position=\"dodge\")\n  + facet_wrap(\"~name\", ncol=1)\n  + labs(x=\"\", y=\"\", title=\"Annual returns of beta portfolios\")\n  + scale_y_continuous(labels=percent_format())\n  + theme(legend_position=\"none\")\n)\nbeta_longshort_figure.show()\n\n\n\n\n\n\n\nFigure 3: The figure shows annual returns of beta portfolios. We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.\n\n\n\n\n\nOverall, this chapter shows how functional programming can be leveraged to form an arbitrary number of portfolios using any sorting variable and how to evaluate the performance of the resulting portfolios. In the next chapter, we dive deeper into the many degrees of freedom that arise in the context of portfolio analysis.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#key-takeaways",
    "href": "python/univariate-portfolio-sorts.html#key-takeaways",
    "title": "Univariate Portfolio Sorts",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nUnivariate portfolio sorts assess whether a single firm characteristic, like lagged market beta, can predict future excess returns.\nPortfolios are formed each month using quantile breakpoints, with returns computed using value-weighted averages to reflect realistic investment strategies.\nA long-short strategy based on beta-sorted portfolios fails to generate significant positive excess returns, contradicting CAPM predictions that higher beta should yield higher returns.\nThe analysis highlights the “betting against beta” anomaly, where low-beta portfolios deliver higher alphas than high-beta portfolios, providing evidence against the CAPM\nThe functional programming capabilities of Python enable scalable and flexible portfolio sorting, making it easy to analyze multiple characteristics and portfolio configurations.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#exercises",
    "href": "python/univariate-portfolio-sorts.html#exercises",
    "title": "Univariate Portfolio Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nTake the two long-short beta strategies based on different numbers of portfolios and compare the returns. Is there a significant difference in returns? How do the Sharpe ratios compare between the strategies? Find one additional portfolio evaluation statistic and compute it.\nWe plotted the alphas of the ten beta portfolios above. Write a function that tests these estimates for significance. Which portfolios have significant alphas?\nThe analysis here is based on betas from monthly returns. However, we also computed betas from daily returns. Re-run the analysis and point out differences in the results.\nGiven the results in this chapter, can you define a long-short strategy that yields positive abnormal returns (i.e., alphas)? Plot the cumulative excess return of your strategy and the market excess return for comparison.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html",
    "href": "python/constrained-optimization-and-backtesting.html",
    "title": "Constrained Optimization and Backtesting",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with Rhere.\nIn this chapter, we conduct portfolio backtesting in a realistic setting by including transaction costs and investment constraints such as no-short-selling rules. We start with standard mean-variance efficient portfolios and introduce constraints in a step-by-step manner. To do so, we rely on numerical optimization procedures in Python. We conclude the chapter by providing an out-of-sample backtesting procedure for the different strategies that we introduce in this chapter.\nThroughout this chapter, we use the following Python packages:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom itertools import product\nfrom scipy.stats import expon\nfrom scipy.optimize import minimize\nCompared to previous chapters, we introduce expon from scipy.stats to calculate exponential continuous random variables.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#data-preparation",
    "href": "python/constrained-optimization-and-backtesting.html#data-preparation",
    "title": "Constrained Optimization and Backtesting",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start by loading the required data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. For simplicity, we restrict our investment universe to the monthly Fama-French industry portfolio returns in the following application. \n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nindustry_returns = (pd.read_sql_query(\n    sql=\"SELECT * FROM industries_ff_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .drop(columns=[\"date\"])\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "href": "python/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Recap of Portfolio Choice",
    "text": "Recap of Portfolio Choice\nA common objective for portfolio optimization is to find mean-variance efficient portfolio weights, i.e., the allocation that delivers the lowest possible return variance for a given minimum level of expected returns. In the most extreme case, where the investor is only concerned about portfolio variance, they may choose to implement the minimum variance portfolio (MVP) weights which are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\tag{1}\\] where \\(\\Sigma\\) is the \\((N \\times N)\\) covariance matrix of the returns. The optimal weights \\(\\omega_\\text{mvp}\\) can be found analytically and are \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). In terms of code, the math is equivalent to the following chunk. \n\nn_industries = industry_returns.shape[1]\n\nmu = np.array(industry_returns.mean()).T\nsigma = np.array(industry_returns.cov())\nw_mvp = np.linalg.inv(sigma) @ np.ones(n_industries)\nw_mvp = w_mvp/w_mvp.sum()\n\nweights_mvp = pd.DataFrame({\n  \"Industry\": industry_returns.columns.tolist(),\n  \"Minimum variance\": w_mvp\n})\nweights_mvp.round(3)\n\n\n\n\n\n\n\n\nIndustry\nMinimum variance\n\n\n\n\n0\nnodur\n0.274\n\n\n1\ndurbl\n0.010\n\n\n2\nmanuf\n0.023\n\n\n3\nenrgy\n0.087\n\n\n4\nhitec\n0.021\n\n\n5\ntelcm\n0.231\n\n\n6\nshops\n0.087\n\n\n7\nhlth\n0.170\n\n\n8\nutils\n0.461\n\n\n9\nother\n-0.364\n\n\n\n\n\n\n\nNext, consider an investor who aims to achieve minimum variance given a required expected portfolio return \\(\\bar{\\mu}\\) such that she chooses \\[\\omega_\\text{eff}({\\bar{\\mu}}) =\\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}. \\tag{2}\\] We leave it as an exercise below to show that the portfolio choice problem can equivalently be formulated for an investor with mean-variance preferences and risk aversion factor \\(\\gamma\\). That means the investor aims to choose portfolio weights as the solution to \\[ \\omega^*_\\gamma = \\arg\\max \\omega' \\mu - \\frac{\\gamma}{2}\\omega'\\Sigma \\omega\\quad \\text{ s.t. } \\omega'\\iota = 1. \\tag{3}\\] The solution to the optimal portfolio choice problem is: \\[\\omega^*_{\\gamma} = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota. \\tag{4}\\] To proof this statement, we refer to the derivations in Proofs. Empirically, this classical solution imposes many problems. In particular, the estimates of \\(\\mu\\) are noisy over short horizons, the (\\(N \\times N\\)) matrix \\(\\Sigma\\) contains \\(N(N-1)/2\\) distinct elements and thus, estimation error is huge. Seminal papers on the effect of ignoring estimation uncertainty, among others, are Brown (1976), Jobson and Korkie (1980), Jorion (1986), and Chopra and Ziemba (1993).\nEven worse, if the asset universe contains more assets than available time periods \\((N &gt; T)\\), the sample covariance matrix is no longer positive definite such that the inverse \\(\\Sigma^{-1}\\) does not exist anymore. To address estimation issues for vast-dimensional covariance matrices, regularization techniques (see, e.g., Ledoit and Wolf 2003, 2004, 2012; Fan, Fan, and Lv 2008) and the parametric approach from the previous chapter are popular tools.\nWhile the uncertainty associated with estimated parameters is challenging, the data-generating process is also unknown to the investor. In other words, model uncertainty reflects that it is ex-ante not even clear which parameters require estimation (for instance, if returns are driven by a factor model, selecting the universe of relevant factors imposes model uncertainty). Wang (2005) and Garlappi, Uppal, and Wang (2007) provide theoretical analysis on optimal portfolio choice under model and estimation uncertainty. In the most extreme case, Pflug, Pichler, and Wozabal (2012) shows that the naive portfolio, which allocates equal wealth to all assets, is the optimal choice for an investor averse to model uncertainty.\nOn top of the estimation uncertainty, transaction costs are a major concern. Rebalancing portfolios is costly, and, therefore, the optimal choice should depend on the investor’s current holdings. In the presence of transaction costs, the benefits of reallocating wealth may be smaller than the costs associated with turnover. This aspect has been investigated theoretically, among others, for one risky asset by Magill and Constantinides (1976) and Davis and Norman (1990). Subsequent extensions to the case with multiple assets have been proposed by Balduzzi and Lynch (1999) and Balduzzi and Lynch (2000). More recent papers on empirical approaches that explicitly account for transaction costs include Gârleanu and Pedersen (2013), DeMiguel, Nogales, and Uppal (2014), and DeMiguel, Martín-Utrera, and Nogales (2015).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "href": "python/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "title": "Constrained Optimization and Backtesting",
    "section": "Estimation Uncertainty and Transaction Costs",
    "text": "Estimation Uncertainty and Transaction Costs\nThe empirical evidence regarding the performance of a mean-variance optimization procedure in which you simply plug in some sample estimates \\(\\hat \\mu\\) and \\(\\hat \\Sigma\\) can be summarized rather briefly: mean-variance optimization performs poorly! The literature discusses many proposals to overcome these empirical issues. For instance, one may impose some form of regularization of \\(\\Sigma\\), rely on Bayesian priors inspired by theoretical asset pricing models (Kan and Zhou 2007), or use high-frequency data to improve forecasting (Hautsch, Kyj, and Malec 2015). One unifying framework that works easily, effectively (even for large dimensions), and is purely inspired by economic arguments is an ex-ante adjustment for transaction costs (Hautsch and Voigt 2019).\nAssume that returns are from a multivariate normal distribution with mean \\(\\mu\\) and variance-covariance matrix \\(\\Sigma\\), \\(N(\\mu,\\Sigma)\\). Additionally, we assume quadratic transaction costs which penalize rebalancing such that \\[\n\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) = \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\\end{aligned} \\tag{5}\\] with cost parameter \\(\\beta&gt;0\\) and \\(\\omega_{t^+} = {\\omega_t \\circ (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\), where \\(\\circ\\) is the element-wise Hadamard product. \\(\\omega_{t^+}\\) denotes the portfolio weights just before rebalancing. Note that \\(\\omega_{t^+}\\) differs mechanically from \\(\\omega_t\\) due to the returns in the past period. Intuitively, transaction costs penalize portfolio performance when the portfolio is shifted from the current holdings \\(\\omega_{t^+}\\) to a new allocation \\(\\omega_{t+1}\\). In this setup, transaction costs do not increase linearly. Instead, larger rebalancing is penalized more heavily than small adjustments. Then, the optimal portfolio choice for an investor with mean variance preferences is \\[\\begin{aligned}\\omega_{t+1} ^* &= \\arg\\max \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\text{ s.t. } \\iota'\\omega = 1\\\\\n&=\\arg\\max\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t.} \\iota'\\omega=1,\\end{aligned} \\tag{6}\\] where \\[\\mu^*=\\mu+\\beta \\omega_{t^+} \\quad \\text{and} \\quad \\Sigma^*=\\Sigma + \\frac{\\beta}{\\gamma} I_N. \\tag{7}\\] As a result, adjusting for transaction costs implies a standard mean-variance optimal portfolio choice with adjusted return parameters \\(\\Sigma^*\\) and \\(\\mu^*\\): \\[\\omega^*_{t+1} = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^* + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota. \\tag{8}\\]\nAn alternative formulation of the optimal portfolio can be derived as follows: \\[\\omega_{t+1} ^*=\\arg\\max\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t. } \\iota'\\omega=1. \\tag{9}\\] The optimal weights correspond to a mean-variance portfolio, where the vector of expected returns is such that assets that currently exhibit a higher weight are considered as delivering a higher expected return.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "href": "python/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Optimal Portfolio Choice",
    "text": "Optimal Portfolio Choice\nThe function below implements the efficient portfolio weights in its general form, allowing for transaction costs (conditional on the holdings before reallocation). For \\(\\beta=0\\), the computation resembles the standard mean-variance efficient framework. gamma denotes the coefficient of risk aversion \\(\\gamma\\), beta is the transaction cost parameter \\(\\beta\\) and w_prev are the weights before rebalancing \\(\\omega_{t^+}\\).\n\ndef compute_efficient_weight(sigma, \n                             mu, \n                             gamma=2, \n                             beta=0,\n                             w_prev=np.ones(sigma.shape[1])/sigma.shape[1]):\n    \"\"\"Compute efficient portfolio weights.\"\"\"\n    \n    n = sigma.shape[1]\n    iota = np.ones(n)\n    sigma_processed = sigma+(beta/gamma)*np.eye(n)\n    mu_processed = mu+beta*w_prev\n\n    sigma_inverse = np.linalg.inv(sigma_processed)\n\n    w_mvp = sigma_inverse @ iota\n    w_mvp = w_mvp/np.sum(w_mvp)\n    w_opt = w_mvp+(1/gamma)*\\\n        (sigma_inverse-np.outer(w_mvp, iota) @ sigma_inverse) @ mu_processed\n        \n    return w_opt\n\nw_efficient = compute_efficient_weight(sigma, mu)\n\nweights_efficient = pd.DataFrame({\n  \"Industry\": industry_returns.columns.tolist(),\n  \"Efficient portfolio\": w_efficient\n})\nweights_efficient.round(3)\n\n\n\n\n\n\n\n\nIndustry\nEfficient portfolio\n\n\n\n\n0\nnodur\n1.192\n\n\n1\ndurbl\n0.239\n\n\n2\nmanuf\n-1.666\n\n\n3\nenrgy\n0.616\n\n\n4\nhitec\n0.453\n\n\n5\ntelcm\n-0.437\n\n\n6\nshops\n0.755\n\n\n7\nhlth\n0.359\n\n\n8\nutils\n-0.047\n\n\n9\nother\n-0.464\n\n\n\n\n\n\n\nThe portfolio weights above indicate the efficient portfolio for an investor with risk aversion coefficient \\(\\gamma=2\\) in the absence of transaction costs. Some of the positions are negative, which implies short-selling, and most of the positions are rather extreme. For instance, a position of \\(-1\\) implies that the investor takes a short position worth their entire wealth to lever long positions in other assets. What is the effect of transaction costs or different levels of risk aversion on the optimal portfolio choice? The following few lines of code analyze the distance between the minimum variance portfolio and the portfolio implemented by the investor for different values of the transaction cost parameter \\(\\beta\\) and risk aversion \\(\\gamma\\).\n\ngammas = [2, 4, 8, 20]\nbetas = 20*expon.ppf(np.arange(1, 100)/100, scale=1)\n\ntransaction_costs = (pd.DataFrame(\n    list(product(gammas, betas)), \n    columns=[\"gamma\", \"beta\"]\n  )\n  .assign(\n    weights=lambda x: x.apply(lambda y:\n      compute_efficient_weight(\n        sigma, mu, gamma=y[\"gamma\"], beta=y[\"beta\"]/10000, w_prev=w_mvp), \n      axis=1\n    ),\n    concentration=lambda x: x[\"weights\"].apply(\n      lambda x: np.sum(np.abs(x-w_mvp))\n    )\n  )\n)\n\nThe code chunk above computes the optimal weight in the presence of transaction cost for different values of \\(\\beta\\) and \\(\\gamma\\) but with the same initial allocation, the theoretical optimal minimum variance portfolio. Starting from the initial allocation, the investor chooses their optimal allocation along the efficient frontier to reflect their own risk preferences. If transaction costs were absent, the investor would simply implement the mean-variance efficient allocation. If transaction costs make it costly to rebalance, their optimal portfolio choice reflects a shift toward the efficient portfolio, whereas their current portfolio anchors their investment.\n\nrebalancing_figure = (\n    ggplot(\n      transaction_costs, \n      aes(x=\"beta\", y=\"concentration\", \n          color=\"factor(gamma)\", linetype=\"factor(gamma)\")\n    )\n    + geom_line()\n    + guides(linetype=None)\n    + labs(\n        x=\"Transaction cost parameter\", y=\"Distance from MVP\",\n        color=\"Risk aversion\", \n        title=\"Portfolio weights for different risk aversion and transaction cost\"\n      )\n)\nrebalancing_figure.show()\n\n\n\n\n\n\n\nFigure 1: The figure shows portfolio weights for different risk aversion and transaction cost. The horizontal axis indicates the distance from the empirical minimum variance portfolio weight, measured by the sum of the absolute deviations of the chosen portfolio from the benchmark.\n\n\n\n\n\nFigure 1 shows rebalancing from the initial portfolio (which we always set to the minimum variance portfolio weights in this example). The higher the transaction costs parameter \\(\\beta\\), the smaller is the rebalancing from the initial portfolio. In addition, if risk aversion \\(\\gamma\\) increases, the efficient portfolio is closer to the minimum variance portfolio weights such that the investor desires less rebalancing from the initial holdings.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#constrained-optimization",
    "href": "python/constrained-optimization-and-backtesting.html#constrained-optimization",
    "title": "Constrained Optimization and Backtesting",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\nNext, we introduce constraints to the above optimization procedure. Very often, typical constraints such as short-selling restrictions prevent analytical solutions for optimal portfolio weights (short-selling restrictions simply imply that negative weights are not allowed such that we require that \\(w_i \\geq 0\\,\\forall i\\)). However, numerical optimization allows computing the solutions to such constrained problems.\nWe rely on the powerful scipy.optimize package, which provides a common interface to a number of different optimization routines. In particular, we employ the Sequential Least-Squares Quadratic Programming (SLSQP) algorithm of Kraft (1994) because it is able to handle multiple equality and inequality constraints at the same time and is typically used for problems where the objective function and the constraints are twice continuously differentiable. We have to provide the algorithm with the objective function and its gradient, as well as the constraints and their Jacobian.\nWe illustrate the use of minimize() by replicating the analytical solutions for the minimum variance and efficient portfolio weights from above. Note that the equality constraint for both solutions is given by the requirement that the weights must sum up to one. In addition, we supply a vector of equal weights as an initial value for the algorithm in all applications. We verify that the output is equal to the above solution. Note that np.allclose() is a safe way to compare two vectors for pairwise equality. The alternative == is sensitive to small differences that may occur due to the representation of floating points on a computer, while np.allclose() has a built-in tolerance. It returns True if both are equal, which is the case in both applications below.\n\nw_initial = np.ones(n_industries)/n_industries\n\ndef objective_mvp(w):\n    return 0.5*w.T @ sigma @ w\n  \ndef gradient_mvp(w):\n    return sigma @ w\n\ndef equality_constraint(w):\n    return np.sum(w)-1\n\ndef jacobian_equality(w):\n    return np.ones_like(w)\n\nconstraints = (\n  {\"type\": \"eq\", \"fun\": equality_constraint, \"jac\": jacobian_equality}\n)\n\noptions = {\n  \"tol\":1e-20,\n  \"maxiter\": 10000,\n  \"method\":\"SLSQP\"\n}\n\nw_mvp_numerical = minimize(\n  x0=w_initial,\n  fun=objective_mvp,\n  jac=gradient_mvp,\n  constraints=constraints,\n  tol=options[\"tol\"],\n  options={\"maxiter\": options[\"maxiter\"]},\n  method=options[\"method\"]\n)\n\nnp.allclose(w_mvp, w_mvp_numerical.x, atol=1e-3)\n\ndef objective_efficient(w):\n    return 2*0.5*w.T @ sigma @ w-(1+mu) @ w\n\ndef gradient_efficient(w):\n    return 2*sigma @ w-(1+mu)\n\nw_efficient_numerical = minimize(\n  x0=w_initial,\n  fun=objective_efficient,\n  jac=gradient_efficient,\n  constraints=constraints,\n  tol=options[\"tol\"],\n  options={\"maxiter\": options[\"maxiter\"]},\n  method=options[\"method\"]\n)\n\nnp.allclose(w_efficient, w_efficient_numerical.x, atol = 1e-3)\n\nThe result above shows that the numerical procedure indeed recovered the optimal weights for a scenario where we already know the analytic solution.\nNext, we approach problems where no analytical solutions exist. First, we additionally impose short-sale constraints, which implies \\(N\\) inequality constraints of the form \\(\\omega_i &gt;=0\\). We can implement the short-sale constraints by imposing a vector of lower bounds lb = rep(0, n_industries).\n\nw_no_short_sale = minimize(\n  x0=w_initial,\n  fun=objective_efficient,\n  jac=gradient_efficient,\n  constraints=constraints,\n  bounds=((0, None), )*n_industries,\n  tol=options[\"tol\"],\n  options={\"maxiter\": options[\"maxiter\"]},\n  method=options[\"method\"]\n)\n\nweights_no_short_sale = pd.DataFrame({\n  \"Industry\": industry_returns.columns.tolist(),\n  \"No short-sale\": w_no_short_sale.x\n})\nweights_no_short_sale.round(3)\n\n\n\n\n\n\n\n\nIndustry\nNo short-sale\n\n\n\n\n0\nnodur\n0.280\n\n\n1\ndurbl\n0.000\n\n\n2\nmanuf\n0.000\n\n\n3\nenrgy\n0.198\n\n\n4\nhitec\n0.000\n\n\n5\ntelcm\n0.000\n\n\n6\nshops\n0.339\n\n\n7\nhlth\n0.182\n\n\n8\nutils\n0.000\n\n\n9\nother\n0.000\n\n\n\n\n\n\n\nAs expected, the resulting portfolio weights are all positive (up to numerical precision). Typically, the holdings in the presence of short-sale constraints are concentrated among way fewer assets than in the unrestricted case. You can verify that np.sum(w_no_short_sale.x) returns 1. In other words, minimize() provides the numerical solution to a portfolio choice problem for a mean-variance investor with risk aversion gamma = 2, where negative holdings are forbidden.\nminimize() can also handle more complex problems. As an example, we show how to compute optimal weights, subject to the so-called Regulation-T constraint, which requires that the sum of all absolute portfolio weights is smaller than 1.5, that is \\(\\sum_{i=1}^N |\\omega_i| \\leq 1.5\\). The constraint enforces that a maximum of 50 percent of the allocated wealth can be allocated to short positions, thus implying an initial margin requirement of 50 percent. Imposing such a margin requirement reduces portfolio risks because extreme portfolio weights are not attainable anymore. The implementation of Regulation-T rules is numerically interesting because the margin constraints imply a non-linear constraint on the portfolio weights. \n\nreg_t = 1.5\n\ndef inequality_constraint(w):\n    return reg_t-np.sum(np.abs(w))\n\ndef jacobian_inequality(w):\n    return -np.sign(w)\n\ndef objective_reg_t(w):\n    return -w @ (1+mu)+2*0.5*w.T @ sigma @ w\n\ndef gradient_reg_t(w):\n    return -(1+mu)+2*np.dot(sigma, w)\n\nconstraints = (\n  {\"type\": \"eq\", \"fun\": equality_constraint, \"jac\": jacobian_equality},\n  {\"type\": \"ineq\", \"fun\": inequality_constraint, \"jac\": jacobian_inequality}\n)\n\nw_reg_t = minimize(\n  x0=w_initial,\n  fun=objective_reg_t,\n  jac=gradient_reg_t,\n  constraints=constraints,\n  tol=options[\"tol\"],\n  options={\"maxiter\": options[\"maxiter\"]},\n  method=options[\"method\"]\n)\n\nweights_reg_t = pd.DataFrame({\n  \"Industry\": industry_returns.columns.tolist(),\n  \"Regulation-T\": w_reg_t.x\n})\nweights_reg_t.round(3)\n\n\n\n\n\n\n\n\nIndustry\nRegulation-T\n\n\n\n\n0\nnodur\n0.359\n\n\n1\ndurbl\n0.000\n\n\n2\nmanuf\n-0.250\n\n\n3\nenrgy\n0.245\n\n\n4\nhitec\n0.051\n\n\n5\ntelcm\n-0.000\n\n\n6\nshops\n0.401\n\n\n7\nhlth\n0.195\n\n\n8\nutils\n-0.000\n\n\n9\nother\n-0.000\n\n\n\n\n\n\n\nFigure 2 shows the optimal allocation weights across all python len(industry_returns.columns) industries for the four different strategies considered so far: minimum variance, efficient portfolio with \\(\\gamma\\) = 2, efficient portfolio with short-sale constraints, and the Regulation-T constrained portfolio.\n\nweights = (weights_mvp\n  .merge(weights_efficient)\n  .merge(weights_no_short_sale)\n  .merge(weights_reg_t)\n  .melt(id_vars=\"Industry\", var_name=\"Strategy\", value_name=\"weights\")\n)\n\nweights_figure = (\n  ggplot(\n    weights, \n    aes(x=\"Industry\", y=\"weights\", fill=\"Strategy\")\n  )\n  + geom_bar(stat=\"identity\", position=\"dodge\", width=0.7)\n  + coord_flip()\n  + labs(\n      y=\"Allocation weight\", fill=\"\",\n      title=\"Optimal allocations for different strategies\"\n    )\n  + scale_y_continuous(labels=percent_format())\n)\nweights_figure.show()\n\n\n\n\n\n\n\nFigure 2: The figure shows optimal allocation weights for the ten industry portfolios and the four different allocation strategies.\n\n\n\n\n\nThe results clearly indicate the effect of imposing additional constraints: the extreme holdings the investor implements if they follow the (theoretically optimal) efficient portfolio vanish under, e.g., the Regulation-T constraint. You may wonder why an investor would deviate from what is theoretically the optimal portfolio by imposing potentially arbitrary constraints. The short answer is: the efficient portfolio is only efficient if the true parameters of the data-generating process correspond to the estimated parameters \\(\\hat\\Sigma\\) and \\(\\hat\\mu\\). Estimation uncertainty may thus lead to inefficient allocations. By imposing restrictions, we implicitly shrink the set of possible weights and prevent extreme allocations, which could result from error-maximization due to estimation uncertainty (Jagannathan and Ma 2003).\nBefore we move on, we want to propose a final allocation strategy, which reflects a somewhat more realistic structure of transaction costs instead of the quadratic specification used above. The function below computes efficient portfolio weights while adjusting for transaction costs of the form \\(\\beta\\sum_{i=1}^N |(\\omega_{i, t+1} - \\omega_{i, t^+})|\\). No closed-form solution exists, and we rely on non-linear optimization procedures.\n\ndef compute_efficient_weight_L1_TC(mu, sigma, gamma, beta, initial_weights):\n    \"\"\"Compute efficient portfolio weights with L1 constraint.\"\"\"       \n    \n    def objective(w):\n      return (gamma*0.5*w.T @ sigma @ w-(1+mu) @ w\n               +(beta/10000)/2*np.sum(np.abs(w-initial_weights)))\n\n    def gradient(w):\n      return (-mu+gamma*sigma @ w \n              +(beta/10000)*0.5*np.sign(w-initial_weights))\n      \n    constraints = (\n      {\"type\": \"eq\", \"fun\": equality_constraint, \"jac\": jacobian_equality}\n    )\n    \n    result = minimize(\n        x0=initial_weights,\n        fun=objective,\n        jac=gradient,\n        constraints=constraints,\n        tol=options[\"tol\"],\n        options={\"maxiter\": options[\"maxiter\"]},\n        method=options[\"method\"]\n    )\n    \n    return result.x",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "href": "python/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "title": "Constrained Optimization and Backtesting",
    "section": "Out-of-Sample Backtesting",
    "text": "Out-of-Sample Backtesting\nFor the sake of simplicity, we committed one fundamental error in computing portfolio weights above: we used the full sample of the data to determine the optimal allocation (Arnott, Harvey, and Markowitz 2019). To implement this strategy at the beginning of the 2000s, you will need to know how the returns will evolve until 2021. While interesting from a methodological point of view, we cannot evaluate the performance of the portfolios in a reasonable out-of-sample fashion. We do so next in a backtesting application for three strategies. For the backtest, we recompute optimal weights just based on past available data.\nThe few lines below define the general setup. We consider 120 periods from the past to update the parameter estimates before recomputing portfolio weights. Then, we update portfolio weights, which is costly and affects the performance. The portfolio weights determine the portfolio return. A period later, the current portfolio weights have changed and form the foundation for transaction costs incurred in the next period. We consider three different competing strategies: the mean-variance efficient portfolio, the mean-variance efficient portfolio with ex-ante adjustment for transaction costs, and the naive portfolio, which allocates wealth equally across the different assets.\n\nwindow_length = 120\nperiods = industry_returns.shape[0]-window_length\n\nbeta = 50\ngamma = 2\n\nperformance_values = np.empty((periods, 3))\nperformance_values[:] = np.nan\nperformance_values = {\n  \"MV (TC)\": performance_values.copy(), \n  \"Naive\": performance_values.copy(), \n  \"MV\": performance_values.copy()\n}\n\nn_industries = industry_returns.shape[1]\nw_prev_1 = w_prev_2 = w_prev_3 = np.ones(n_industries)/n_industries\n\nWe also define two helper functions: One to adjust the weights due to returns and one for performance evaluation, where we compute realized returns net of transaction costs.\n\ndef adjust_weights(w, next_return):\n    w_prev = 1+w*next_return\n    return np.array(w_prev/np.sum(np.array(w_prev)))\n\ndef evaluate_performance(w, w_previous, next_return, beta=50):\n    \"\"\"Calculate portfolio evaluation measures.\"\"\"  \n    \n    raw_return = np.dot(next_return, w)\n    turnover = np.sum(np.abs(w-w_previous))\n    net_return = raw_return-beta/10000*turnover\n    \n    return np.array([raw_return, turnover, net_return])\n\nThe following code chunk performs a rolling-window estimation, which we implement in a loop. In each period, the estimation window contains the returns available up to the current period. Note that we use the sample variance-covariance matrix and ignore the estimation of \\(\\hat\\mu\\) entirely, but you might use more advanced estimators in practice.\n\nfor p in range(periods):\n    returns_window = industry_returns.iloc[p:(p+window_length-1), :]\n    next_return = industry_returns.iloc[p+window_length, :]\n\n    sigma_window = np.array(returns_window.cov())\n    mu = 0*np.array(returns_window.mean())\n\n    # Transaction-cost adjusted portfolio\n    w_1 = compute_efficient_weight_L1_TC(\n      mu=mu, sigma=sigma_window, \n      beta=beta, \n      gamma=gamma, \n      initial_weights=w_prev_1\n    )\n\n    performance_values[\"MV (TC)\"][p, :] = evaluate_performance(\n      w_1, w_prev_1, next_return, beta=beta\n    )\n    w_prev_1 = adjust_weights(w_1, next_return)\n\n    # Naive portfolio\n    w_2 = np.ones(n_industries)/n_industries\n    performance_values[\"Naive\"][p, :] = evaluate_performance(\n      w_2, w_prev_2, next_return\n    )\n    w_prev_2 = adjust_weights(w_2, next_return)\n\n    # Mean-variance efficient portfolio (w/o transaction costs)\n    w_3 = compute_efficient_weight(sigma=sigma_window, mu=mu, gamma=gamma)\n    performance_values[\"MV\"][p, :] = evaluate_performance(\n      w_3, w_prev_3, next_return\n    )\n    w_prev_3 = adjust_weights(w_3, next_return)\n\nFinally, we get to the evaluation of the portfolio strategies net-of-transaction costs. Note that we compute annualized returns and standard deviations. \n\nperformance = pd.DataFrame()\nfor i in enumerate(performance_values.keys()):\n    tmp_data = pd.DataFrame(\n      performance_values[i[1]], \n      columns=[\"raw_return\", \"turnover\", \"net_return\"]\n    )\n    tmp_data[\"strategy\"] = i[1]\n    performance = pd.concat([performance, tmp_data], axis=0)\n\nlength_year = 12\n\nperformance_table = (performance\n  .groupby(\"strategy\")\n  .aggregate(\n    mean=(\"net_return\", lambda x: length_year*100*x.mean()),\n    sd=(\"net_return\", lambda x: np.sqrt(length_year)*100*x.std()),\n    sharpe_ratio=(\"net_return\", lambda x: (\n      (length_year*100*x.mean())/(np.sqrt(length_year)*100*x.std()) \n        if x.mean() &gt; 0 else np.nan)\n    ),\n    turnover=(\"turnover\", lambda x: 100*x.mean())\n  )\n  .reset_index()\n)\nperformance_table.round(3)\n\n\n\n\n\n\n\n\nstrategy\nmean\nsd\nsharpe_ratio\nturnover\n\n\n\n\n0\nMV\n-1.041\n12.556\nNaN\n210.020\n\n\n1\nMV (TC)\n12.069\n15.129\n0.798\n0.019\n\n\n2\nNaive\n12.052\n15.132\n0.796\n0.236\n\n\n\n\n\n\n\nThe results clearly speak against mean-variance optimization. Turnover is huge when the investor only considers their portfolio’s expected return and variance. Effectively, the mean-variance portfolio generates a negative annualized return after adjusting for transaction costs. At the same time, the naive portfolio turns out to perform very well. In fact, the performance gains of the transaction-cost adjusted mean-variance portfolio are small. The out-of-sample Sharpe ratio is slightly higher than for the naive portfolio. Note the extreme effect of turnover penalization on turnover: MV (TC) effectively resembles a buy-and-hold strategy which only updates the portfolio once the estimated parameters \\(\\hat\\mu_t\\) and \\(\\hat\\Sigma_t\\) indicate that the current allocation is too far away from the optimal theoretical portfolio.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#key-takeaways",
    "href": "python/constrained-optimization-and-backtesting.html#key-takeaways",
    "title": "Constrained Optimization and Backtesting",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe scipy Python package can be used to solve constrained portfolio optimization problems that cannot be addressed analytically, including margin and regulatory constraints.\nTransaction costs can be modeled in both quadratic and absolute terms, showing how rebalancing penalties influence portfolio allocations and reduce excessive turnover.\nAn out-of-sample backtesting framework demonstrates that naive portfolios often outperform classical mean-variance strategies once transaction costs are considered.\nThe findings highlight the practical trade-offs between theoretical optimality and robust, implementable investment strategies under uncertainty.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#exercises",
    "href": "python/constrained-optimization-and-backtesting.html#exercises",
    "title": "Constrained Optimization and Backtesting",
    "section": "Exercises",
    "text": "Exercises\n\nConsider the portfolio choice problem for transaction-cost adjusted certainty equivalent maximization with risk aversion parameter \\(\\gamma\\) \\[\\omega_{t+1} ^* = \\arg\\max_{\\omega \\in \\mathbb{R}^N, \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\tag{10}\\] where \\(\\Sigma\\) and \\(\\mu\\) are (estimators of) the variance-covariance matrix of the returns and the vector of expected returns. Assume for now that transaction costs are quadratic in rebalancing and proportional to stock illiquidity such that \\[\\nu_t\\left(\\omega, B\\right) = \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right) \\tag{11}\\] where \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) is a diagonal matrix, where \\(ill_1, \\ldots, ill_N\\). Derive a closed-form solution for the mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based on the transaction cost specification above. Discuss the effect of illiquidity \\(ill_i\\) on the individual portfolio weights relative to an investor that myopically ignores transaction costs in their decision.\nUse the solution from the previous exercise to update the function compute_efficient_weight() such that you can compute optimal weights conditional on a matrix \\(B\\) with illiquidity measures.\nIllustrate the evolution of the optimal weights from the naive portfolio to the efficient portfolio in the mean-standard deviation diagram.\nIs it always optimal to choose the same \\(\\beta\\) in the optimization problem than the value used in evaluating the portfolio performance? In other words, can it be optimal to choose theoretically sub-optimal portfolios based on transaction cost considerations that do not reflect the actual incurred costs? Evaluate the out-of-sample Sharpe ratio after transaction costs for a range of different values of imposed \\(\\beta\\) values.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/colophon.html",
    "href": "python/colophon.html",
    "title": "Colophon",
    "section": "",
    "text": "In this appendix chapter, we provide details on the package versions used in this edition. The book was built with Python (Python Software Foundation 2023) version 3.10.11 and the following packages:\n\n\n\n\n\n\nPackage\nVersion\n\n\n\n\n0\nanyio\n4.0.0\n\n\n1\nappdirs\n1.4.4\n\n\n2\nappnope\n0.1.3\n\n\n3\nargon2-cffi\n23.1.0\n\n\n4\nargon2-cffi-bindings\n21.2.0\n\n\n5\narrow\n1.3.0\n\n\n6\nastor\n0.8.1\n\n\n7\nasttokens\n2.4.0\n\n\n8\nasync-lru\n2.0.4\n\n\n9\nattrs\n23.1.0\n\n\n10\nBabel\n2.13.0\n\n\n11\nbackcall\n0.2.0\n\n\n12\nbeautifulsoup4\n4.12.2\n\n\n13\nbleach\n6.1.0\n\n\n14\ncertifi\n2023.7.22\n\n\n15\ncffi\n1.16.0\n\n\n16\ncharset-normalizer\n3.3.0\n\n\n17\nclick\n8.1.7\n\n\n18\ncomm\n0.1.4\n\n\n19\ncontourpy\n1.1.1\n\n\n20\ncycler\n0.12.1\n\n\n21\nCython\n3.0.3\n\n\n22\ndebugpy\n1.8.0\n\n\n23\ndecorator\n5.1.1\n\n\n24\ndefusedxml\n0.7.1\n\n\n25\net-xmlfile\n1.1.0\n\n\n26\nexceptiongroup\n1.1.3\n\n\n27\nexecuting\n2.0.0\n\n\n28\nfastjsonschema\n2.18.1\n\n\n29\nfonttools\n4.43.1\n\n\n30\nformulaic\n0.6.6\n\n\n31\nfqdn\n1.5.1\n\n\n32\nfrozendict\n2.3.8\n\n\n33\nhtml5lib\n1.1\n\n\n34\nhttpimport\n1.3.1\n\n\n35\nidna\n3.4\n\n\n36\nimportlib-metadata\n6.8.0\n\n\n37\ninterface-meta\n1.3.0\n\n\n38\nipykernel\n6.25.2\n\n\n39\nipython\n8.16.1\n\n\n40\nipython-genutils\n0.2.0\n\n\n41\nipywidgets\n8.1.1\n\n\n42\nisoduration\n20.11.0\n\n\n43\njedi\n0.18.2\n\n\n44\nJinja2\n3.1.2\n\n\n45\njoblib\n1.3.2\n\n\n46\njson5\n0.9.14\n\n\n47\njsonpointer\n2.4\n\n\n48\njsonschema\n4.19.1\n\n\n49\njsonschema-specifications\n2023.7.1\n\n\n50\njupyter\n1.0.0\n\n\n51\njupyter-cache\n0.6.1\n\n\n52\njupyter-console\n6.6.3\n\n\n53\njupyter-events\n0.7.0\n\n\n54\njupyter-lsp\n2.2.0\n\n\n55\njupyter_client\n8.3.1\n\n\n56\njupyter_core\n5.4.0\n\n\n57\njupyter_server\n2.7.3\n\n\n58\njupyter_server_terminals\n0.4.4\n\n\n59\njupyterlab\n4.0.6\n\n\n60\njupyterlab-pygments\n0.2.2\n\n\n61\njupyterlab-widgets\n3.0.9\n\n\n62\njupyterlab_server\n2.25.0\n\n\n63\nkiwisolver\n1.4.5\n\n\n64\nlinearmodels\n5.3\n\n\n65\nlxml\n4.9.3\n\n\n66\nMarkupSafe\n2.1.3\n\n\n67\nmatplotlib\n3.8.0\n\n\n68\nmatplotlib-inline\n0.1.6\n\n\n69\nmistune\n3.0.2\n\n\n70\nmizani\n0.9.3\n\n\n71\nmultitasking\n0.0.11\n\n\n72\nmypy-extensions\n1.0.0\n\n\n73\nnbclient\n0.7.4\n\n\n74\nnbconvert\n7.9.2\n\n\n75\nnbformat\n5.9.2\n\n\n76\nnest-asyncio\n1.5.8\n\n\n77\nnotebook\n7.0.4\n\n\n78\nnotebook_shim\n0.2.3\n\n\n79\nnumpy\n1.26.0\n\n\n80\nopenpyxl\n3.1.2\n\n\n81\noverrides\n7.4.0\n\n\n82\npackaging\n23.2\n\n\n83\npandas\n2.1.1\n\n\n84\npandas-datareader\n0.10.0\n\n\n85\npandocfilters\n1.5.0\n\n\n86\nparso\n0.8.3\n\n\n87\npatsy\n0.5.3\n\n\n88\npeewee\n3.16.3\n\n\n89\npexpect\n4.8.0\n\n\n90\npickleshare\n0.7.5\n\n\n91\nPillow\n10.0.1\n\n\n92\nplatformdirs\n3.11.0\n\n\n93\nplotnine\n0.12.3\n\n\n94\nprometheus-client\n0.17.1\n\n\n95\nprompt-toolkit\n3.0.39\n\n\n96\npsutil\n5.9.5\n\n\n97\npsycopg2-binary\n2.9.9\n\n\n98\nptyprocess\n0.7.0\n\n\n99\npure-eval\n0.2.2\n\n\n100\npycparser\n2.21\n\n\n101\nPygments\n2.16.1\n\n\n102\npyhdfe\n0.2.0\n\n\n103\npyparsing\n3.1.1\n\n\n104\npython-dateutil\n2.8.2\n\n\n105\npython-dotenv\n1.0.0\n\n\n106\npython-json-logger\n2.0.7\n\n\n107\npytz\n2023.3.post1\n\n\n108\nPyYAML\n6.0.1\n\n\n109\npyzmq\n25.1.1\n\n\n110\nqtconsole\n5.4.4\n\n\n111\nQtPy\n2.4.0\n\n\n112\nreferencing\n0.30.2\n\n\n113\nregtabletotext\n0.0.11\n\n\n114\nrequests\n2.31.0\n\n\n115\nrfc3339-validator\n0.1.4\n\n\n116\nrfc3986-validator\n0.1.1\n\n\n117\nrpds-py\n0.10.4\n\n\n118\nscikit-learn\n1.3.1\n\n\n119\nscipy\n1.11.3\n\n\n120\nSend2Trash\n1.8.2\n\n\n121\nsetuptools-scm\n7.1.0\n\n\n122\nsix\n1.16.0\n\n\n123\nsniffio\n1.3.0\n\n\n124\nsoupsieve\n2.5\n\n\n125\nSQLAlchemy\n2.0.21\n\n\n126\nstack-data\n0.6.3\n\n\n127\nstatsmodels\n0.14.0\n\n\n128\ntabulate\n0.9.0\n\n\n129\nterminado\n0.17.1\n\n\n130\nthreadpoolctl\n3.2.0\n\n\n131\ntinycss2\n1.2.1\n\n\n132\ntomli\n2.0.1\n\n\n133\ntornado\n6.3.3\n\n\n134\ntraitlets\n5.11.2\n\n\n135\ntypes-python-dateutil\n2.8.19.14\n\n\n136\ntyping_extensions\n4.8.0\n\n\n137\ntzdata\n2023.3\n\n\n138\nuri-template\n1.3.0\n\n\n139\nurllib3\n2.0.6\n\n\n140\nwcwidth\n0.2.8\n\n\n141\nwebcolors\n1.13\n\n\n142\nwebencodings\n0.5.1\n\n\n143\nwebsocket-client\n1.6.4\n\n\n144\nwidgetsnbextension\n4.0.9\n\n\n145\nwrapt\n1.15.0\n\n\n146\nyfinance\n0.2.31\n\n\n147\nzipp\n3.17.0\n\n\n\n\n\n\n\n\n\nReferences\n\nPython Software Foundation. 2023. “Python Language Reference, Version 3.10.11.”",
    "crumbs": [
      "R",
      "Appendix",
      "Colophon"
    ]
  },
  {
    "objectID": "python/modern-portfolio-theory.html",
    "href": "python/modern-portfolio-theory.html",
    "title": "Modern Portfolio Theory",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn the previous chapter, we show how to download and analyze stock market data with figures and summary statistics. Now, we move to a typical question in finance: How should an investor allocate their wealth across assets that differ in expected returns, variance, and correlations to optimize their portfolio’s performance? Modern Portfolio Theory (MPT), introduced by Markowitz (1952), revolutionized the way how we think about such investment decisions by formalizing the trade-off between risk and expected return. Markowitz’s framework laid the foundation for much of modern finance, also earning him the Sveriges Riksbank Prize in Economic Sciences in 1990.\nMPT relies on the fact that portfolio risk depends on individual asset volatilities as well as on the correlations between asset returns. This insight highlights the power of diversification: Combining assets with low or negative correlations with a given portfolio reduces the overall portfolio risk. This principle is often illustrated with the analogy of a fruit basket: If all you have are apples and they spoil, you lose everything. With a variety of fruits, some fruits may spoil, but others will stay fresh.\nAt the heart of MPT is mean-variance analysis, which evaluates portfolios based on two dimensions: expected return and risk, defined as the variance of the portfolio returns. By balancing these two components, investors can construct portfolios that either maximize their expected return for a given level of risk or minimize their taken risk for a desired level of return. In this chapter, we first derive the optimal portfolio decisions and implement the mean-variance approach in R.\nWe use the following packages throughout this chapter:\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text\nWe introduce the adjustText package for adding text labels to the figures in this chapter (Flyamer 2024).",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "python/modern-portfolio-theory.html#the-asset-universe",
    "href": "python/modern-portfolio-theory.html#the-asset-universe",
    "title": "Modern Portfolio Theory",
    "section": "The Asset Universe",
    "text": "The Asset Universe\nSuppose that \\(N\\) different risky assets are available to the investor. Each asset \\(i\\) delivers expected returns \\(\\mu_i\\), representing the anticipated profit from holding the asset for one period. The investor can allocate their wealth across these assets by choosing the portfolio weights \\(\\omega_i\\) for each asset \\(i\\). We impose that the portfolio weights sum up to one to ensure that the investor is fully invested. There is no outside option, such as keeping your money under a mattress. The overarching question of this chapter is: How should the investor allocate their wealth across these assets to optimize their portfolio’s performance?\nAccording to Markowitz (1952), portfolio selection involves two stages: First, forming expectations about future security performance based on observations and experience. Second, using these expectations to choose a portfolio. In practice, these two steps cannot be separated. You need historical data or other considerations to generate estimates of the distribution of future returns. Only then can one proceed proceed to optimal decision-making conditional on your estimation.\nTo keep things conceptually simple, we focus on the latter part for now and assume that the actual distribution of the asset returns is known. In later chapters, we discuss the issues that arise once we take estimation uncertainty into account. To provide some meaningful illustrations, we rely on historical data to compute reasonable proxies for the expected returns and the variance-covariance of the assets returns, but we will work under the assumption that these are the true parameters of the return distribution.\nThus, leveraging the approach introduced in Working with Stock Returns, we download the constituents of the Dow Jones Industrial Average as an example portfolio as well as their daily adjusted close prices.\n\nsymbols = tf.download_data(\n  domain=\"constituents\", \n  index=\"Dow Jones Industrial Average\"\n)\n\nprices_daily = tf.download_data(\n  domain=\"stock_prices\", \n  symbols=symbols[\"symbol\"].tolist(),\n  start_date=\"2000-01-01\", \n  end_date=\"2023-12-31\"\n)\n\nTo have a stable stock universe and to keep the analysis simple, we ensure that all stocks were traded over the whole sample period:\n\nprices_daily = (prices_daily\n  .groupby(\"symbol\")\n  .apply(lambda x: x.assign(counts=x[\"adjusted_close\"].dropna().count()))\n  .reset_index(drop=True)\n  .query(\"counts == counts.max()\")\n)\n\nWe compute the sample average returns as \\(\\frac{1}{T} \\sum_{t=1}^{T} r_{i,t},\\) where \\(r_{i,t}\\) is the return of asset \\(i\\) in period \\(t\\), and \\(T\\) is the total number of periods. As noted above, we treat the vector of sample averages as the true expected returns of the assets. For simplicity and easier interpretation, we focus on monthly returns going forward.\n\nreturns_monthly = (prices_daily\n  .assign(\n    date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n  )\n  .groupby([\"symbol\", \"date\"], as_index=False)\n  .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n  .assign(\n    ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n  )\n)\n\nIndividual asset risk in MPT is typically quantified using variance (i.e., \\(\\sigma^2_i\\)) or volatilities (i.e., \\(\\sigma_i\\)).1 We suppose that the true volatilities of the assets are also given by the sample standard deviation.\nWe compute the sample standard deviation for each asset by using the std() function.\n\nassets = (returns_monthly\n  .groupby(\"symbol\", as_index=False)\n  .agg(\n    mu=(\"ret\", \"mean\"),\n    sigma=(\"ret\", \"std\")\n  )\n)\n\nWe can illustrate the resulting distribution of the asset returns in Figure 1, showing the volatility on the horizontal axis and the expected return on the vertical axis.\n\nassets_figure = (\n  ggplot(\n    assets, \n    aes(x=\"sigma\", y=\"mu\", label=\"symbol\")\n  )\n  + geom_point()\n  + geom_text(adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility\", y=\"Expected return\",\n      title=\"Expected returns and volatilities of Dow Jones index constituents\"\n  )\n)\nassets_figure.show()\n\n\n\n\n\n\n\nFigure 1: Expected returns and volatilities based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nAs highlighted above, a key innovation of MPT is to consider interactions between assets. The variance-covariance matrix collects this information. Again, we proxy the true variance-covariance matrix \\(\\Sigma\\) of the returns by the sample covariance.\nThe interpretation of the covariance is straightforward: While a positive covariance between assets indicates that these assets tend to move in the same direction, a negative covariance indicates that the assets move in opposite directions.\nWe can use the cov() function that takes a matrix of returns as inputs. We thus need to transform the returns from a data frame into a \\((T \\times N)\\) matrix with one column for each of the \\(N\\) symbols and one row for each of the \\(T\\) trading days. We achieve this by using pivot_wider() with the new column names from the symbol-column and setting the values to ret.\n\nreturns_wide = (returns_monthly\n  .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n  .reset_index()\n)\n\nsigma = (returns_wide\n  .drop(columns=[\"date\"])\n  .cov()\n)\n\nFigure Figure 2 illustrates the resulting variance-covariance matrix.\n\nsigma_long = (sigma\n  .reset_index()\n  .melt(id_vars=\"symbol\", var_name=\"symbol_b\", value_name=\"value\")\n)\n\nsigma_long[\"symbol_b\"] = pd.Categorical(\n  sigma_long[\"symbol_b\"], \n  categories=sigma_long[\"symbol_b\"].unique()[::-1],\n  ordered=True\n)\n\nsigma_figure = (\n  ggplot(\n    sigma_long, \n    aes(x=\"symbol\", y=\"symbol_b\", fill=\"value\")\n  )\n  + geom_tile()\n  + labs(\n      x=\"\", y=\"\", fill=\"(Co-)Variance\",\n      title=\"Sample Variance-covariance matrix of Dow Jones index constituents\"\n    )\n  + scale_fill_continuous(labels=percent_format())\n  + theme(axis_text_x=element_text(angle=45, hjust=1))\n)\nsigma_figure.show()\n\n\n\n\n\n\n\nFigure 2: Variances and covariances based on monthly returns adjusted for dividend payments and stock splits.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "python/modern-portfolio-theory.html#the-minimum-variance-framework",
    "href": "python/modern-portfolio-theory.html#the-minimum-variance-framework",
    "title": "Modern Portfolio Theory",
    "section": "The Minimum-Variance Framework",
    "text": "The Minimum-Variance Framework\nSuppose now the investor allocates their wealth in a portfolio given by the weight vector \\(\\omega\\). The resulting portfolio returns \\(\\omega^\\prime r\\) have an expected return \\(\\mu_\\omega = \\omega^{\\prime}\\mu = \\sum_{i=1}^N \\omega_i \\mu_i\\). The variance of the portfolio returns is \\(\\sigma^2_\\omega = \\omega^{\\prime}\\Sigma\\omega = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\omega_i \\omega_j \\sigma_{ij}\\) where \\(\\omega_i\\) and \\(\\omega_j\\) are the weights of assets \\(i\\) and \\(j\\) in the portfolio, respectively, and \\(\\sigma_{ij}\\) is the covariance between returns of assets \\(i\\) and \\(j\\).\nWe first consider an investor who wants to invest in a portfolio that delivers the lowest possible variance as a reference point. Thus, the optimization problem of the minimum-variance investor is given by\n\\[\\min_{\\omega_1, ... \\omega_n} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\omega_i \\omega_j \\sigma_{ij} = \\min_\\omega \\omega^{\\prime}\\Sigma\\omega.\\]\nWhile staying fully invested across all available assets \\(N\\), \\(\\sum_{i=1}^{N} \\omega_i = 1\\). The analytical solution for the minimum-variance portfolio is\n\\[\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota^{\\prime}\\Sigma^{-1}\\iota},\\]\nwhere \\(\\iota\\) is a vector of 1’s and \\(\\Sigma^{-1}\\) is the inverse of the variance-covariance matrix \\(\\Sigma\\). See Proofs in the Appendix for details on the derivation. In the following code chunk, we calculate the weights of the minimum-variance portfolio:\n\niota = np.ones(sigma.shape[0])\nsigma_inv = np.linalg.inv(sigma.values)\nomega_mvp = (sigma_inv @ iota) / (iota @ sigma_inv @ iota)\n\nFigure Figure 3 shows the resulting portfolio weights.\n\nassets = assets.assign(omega_mvp=omega_mvp)\n\nassets[\"symbol\"] = pd.Categorical(\n  assets[\"symbol\"],\n  categories=assets.sort_values(\"omega_mvp\")[\"symbol\"],\n  ordered=True\n)\n\nomega_figure = (\n  ggplot(\n    assets,\n    aes(y=\"omega_mvp\", x=\"symbol\", fill=\"omega_mvp&gt;0\")\n  )\n  + geom_col()\n  + coord_flip()\n  + scale_y_continuous(labels=percent_format())\n  + labs(x=\"\", y=\"\", title=\"Minimum-variance portfolio weights\")\n  + theme(legend_position=\"none\")\n)\nomega_figure.show()\n\n\n\n\n\n\n\nFigure 3: Weights are based on historical moments of monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nBefore we move on to other portfolios, we collect the return and volatility of the minimum-variance portfolio in a data frame:\n\nmu = assets[\"mu\"].values\nmu_mvp = omega_mvp @ mu\nsigma_mvp = np.sqrt(omega_mvp @ sigma.values @ omega_mvp)\nsummary_mvp = pd.DataFrame({\n  \"mu\": [mu_mvp],\n  \"sigma\": [sigma_mvp],\n  \"type\": [\"Minimum-Variance Portfolio\"]\n})\nsummary_mvp\n\n\n\n\n\n\n\n\nmu\nsigma\ntype\n\n\n\n\n0\n0.008356\n0.032144\nMinimum-Variance Portfolio",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "python/modern-portfolio-theory.html#efficient-portfolios",
    "href": "python/modern-portfolio-theory.html#efficient-portfolios",
    "title": "Modern Portfolio Theory",
    "section": "Efficient Portfolios",
    "text": "Efficient Portfolios\nIn many instances, earning the lowest possible variance may not be the desired outcome. Instead, we generalize the concept of efficient portfolios, where, in addition to minimizing portfolio variance, the investor aims to earn a minimum expected return \\(\\omega^{\\prime}\\mu \\geq \\bar{\\mu}.\\) In other words, when \\(\\bar\\mu\\geq \\omega_\\text{mvp}^{\\prime}\\mu\\), the investor is willing to accept a higher portfolio variance in return for earning a higher expected return.\nSuppose, for instance, the investor wants to earn at least the historical average return of the asset that delivered the highest average returns in the past:\n\nmu_bar = assets[\"mu\"].max()\n\nFormally, the optimization problem is given by\n\\[\\min_\\omega \\omega^{\\prime}\\Sigma\\omega \\text{ s.t. } \\omega^{\\prime}\\iota = 1 \\text{ and } \\omega^{\\prime}\\mu\\geq\\bar\\mu.\\]\nThe analytic solution for the efficient portfolio can be derived as:\n\\[\\omega_{efp} = \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right),\\]\nwhere \\(\\lambda^* = 2\\frac{\\bar\\mu - D/C}{E-D^2/C}\\), \\(C = \\iota'\\Sigma^{-1}\\iota\\), \\(D=\\iota'\\Sigma^{-1}\\mu\\), and \\(E=\\mu'\\Sigma^{-1}\\mu\\). For details, we again refer to the Proofs in the Appendix.\nThe code below implements the analytic solution to this optimization problem and collects the resulting portfolio return and risk in a data frame.\n\nC = iota @ sigma_inv @ iota\nD = iota @ sigma_inv @ mu\nE = mu @ sigma_inv @ mu\nlambda_tilde = 2 * (mu_bar - D / C) / (E - (D ** 2) / C)\nomega_efp = omega_mvp + (lambda_tilde / 2) * (sigma_inv @ mu - D * omega_mvp)\nmu_efp = omega_efp @ mu\nsigma_efp = np.sqrt(omega_efp @ sigma.values @ omega_efp)\n\nsummary_efp = pd.DataFrame({\n  \"mu\": [mu_efp],\n  \"sigma\": [sigma_efp],\n  \"type\": [\"Efficient Portfolio\"]\n})\n\nFigure Figure 4 shows the average return and volatility of the minimum-variance and the efficient portfolio relative to the index constituents. As expected, the efficient portfolio has a higher expected return at the cost of higher volatility compared to the minimum-variance portfolio.\n\nsummaries = pd.concat(\n  [assets, summary_mvp, summary_efp], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=summaries.query(\"type.isna()\"))\n  + geom_point(data=summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3)\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility\", y=\"Expected return\",\n      title=\"Efficient & minimum-variance portfolios\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 4: The big dots indicate the location of the minimum-variance and the efficient portfolio that delivers the expected return of the stock with the higehst return, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe figure illustrates the substantial diversification benefits: Instead of allocating all wealth into one asset that delivered a high average return in the past (at a substantial volatility), the efficient portfolio promises exactly the same expected returns but at a much lower volatility.\nIt should be noted that the level of desired returns \\(\\bar\\mu\\) reflects the risk-aversion of the investor. Less risk-averse investors may require a higher level of desired returns. In contrast, more risk-averse investors may only choose \\(\\bar\\mu\\) closer to the expected return of the minimum-variance portfolio. Very often, the mean-variance framework is instead derived as the optimal decision framework of an investor with a mean-variance utility function with a coefficient of relative risk aversion \\(\\gamma\\). In the Proofs in the Appendix, we show that there is a one-to-one mapping from \\(\\gamma\\) to the desired level of expected returns \\(\\bar\\mu\\), which implies that the resulting efficient portfolios are equivalent and do not depend on the way the optimization problem is formulated.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "python/modern-portfolio-theory.html#the-efficient-frontier",
    "href": "python/modern-portfolio-theory.html#the-efficient-frontier",
    "title": "Modern Portfolio Theory",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n The set of portfolios that satisfies the condition that no other portfolio exists with a higher expected return for a given level of volatility is called the efficient frontier, see, e.g., Merton (1972). . To derive the portfolios that span the efficient frontier, the mutual fund separation theorem proves very helpful. In short, the theorem states that as soon as we have two efficient portfolios (such as the minimum-variance portfolio \\(\\omega_\\text{mvp}\\) and the efficient portfolio for a higher required level of expected returns \\(\\omega_\\bar\\mu\\), we can characterize the entire efficient frontier by combining these two portfolios. That is, any linear combination of the two portfolio weights will again represent an efficient portfolio. In other words, the efficient frontier can be characterized by the following equation:\n\\[\\omega_{a\\mu_1 + (1-a)\\mu_2} = a \\cdot \\omega_{\\mu_1} + (1-a) \\cdot\\omega_{\\mu_2},\\]\nwhere \\(a\\) is a scalar between 0 and 1, \\(\\omega_{\\mu_i}\\) is an efficient portfolio that delivers the expected return \\(\\mu_i\\). It is straightforward to prove the theorem. Consider the analytical solution for the efficient portfolio, which delivers expected returns \\(\\mu_i\\), implying:\n\\[a \\cdot \\omega_{\\mu_1} + (1-a) \\cdot\\omega_{\\mu_2} = \\left(\\frac{a\\mu_1 + (1-a)\\mu_2- D/C }{E-D^2/C}\\right)\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right),\\]\nwhich corresponds to the efficient portfolio earning \\(a\\mu_1 + (1-a)\\mu_2\\) in expectation.\nThe code below implements the construction of this efficient frontier, which characterizes the highest expected return achievable at each level of risk.\n\nefficient_frontier = (\n  pd.DataFrame({\n    \"a\": np.arange(-1, 2.01, 0.01)\n  })\n  .assign(\n    omega=lambda x: x[\"a\"].map(lambda x: x * omega_efp + (1 - x) * omega_mvp)\n  )\n  .assign(\n    mu=lambda x: x[\"omega\"].map(lambda x: x @ mu),\n    sigma=lambda x: x[\"omega\"].map(lambda x: np.sqrt(x @ sigma @ x))\n  )\n)\n\nFinally, it is simple to visualize the efficient frontier alongside the two efficient portfolios in a figure using ggplot (see Figure 5). We also add the individual stocks in the same plot.\n\nsummaries = pd.concat(\n  [summaries, efficient_frontier], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(\n      data = summaries.query(\"type.isna()\")\n    )\n  + geom_point(\n      data = summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3\n    )\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility\", y=\"Expected return\",\n      title=\"Efficient & minimum-variance portfolios\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 5: The big dots indicate the location of the minimum-variance and the efficient portfolio that delivers three times the expected return of the minimum-variance portfolio, respectively. The small dots indicate the location of the individual constituents.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "python/modern-portfolio-theory.html#key-takeaways",
    "href": "python/modern-portfolio-theory.html#key-takeaways",
    "title": "Modern Portfolio Theory",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nModern Portfolio Theory provides a an intuitive framework to allocate capital by balancing expected return against risk.\nMean-variance optimization allows investors to construct portfolios that either minimize risk or maximize return based on historical return and volatility data.\nPortfolio risk is not only determined by individual asset volatility but also by the correlation between assets, which highlights the value of diversification.\nThe minimum-variance portfolio identifies the asset allocation that yields the lowest possible volatility while remaining fully invested.\nEfficient portfolios are those that deliver the highest expected return for a given level of risk.\nThe efficient frontier visually represents the set of optimal portfolios, helping investors understand the trade-off between risk and return.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "python/modern-portfolio-theory.html#exercises",
    "href": "python/modern-portfolio-theory.html#exercises",
    "title": "Modern Portfolio Theory",
    "section": "Exercises",
    "text": "Exercises\n\nWe restricted our sample to all assets trading every day since 2000-01-01. Discuss why this restriction might introduce survivorship bias and how it could affect inferences about future expected portfolio performance.\nThe efficient frontier characterizes portfolios with the highest expected return for different levels of risk. Identify the portfolio with the highest expected return per unit of standard deviation (risk). Which famous performance measure corresponds to the ratio of average returns to standard deviation?\nAnalyze how varying the correlation coefficients between asset returns influences the shape of the efficient frontier. Use hypothetical data for a small number of assets to visualize and interpret these changes.",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "python/modern-portfolio-theory.html#footnotes",
    "href": "python/modern-portfolio-theory.html#footnotes",
    "title": "Modern Portfolio Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlternative approaches include value-at-risk, expected shortfall, or higher-order moments such as skewness and kurtosis.↩︎",
    "crumbs": [
      "R",
      "Getting Started",
      "Modern Portfolio Theory"
    ]
  },
  {
    "objectID": "python/capital-asset-pricing-model.html",
    "href": "python/capital-asset-pricing-model.html",
    "title": "The Capital Asset Pricing Model",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nThe Capital Asset Pricing Model (CAPM) is one of the most influential theories in finance and builds on the foundation laid by Modern Portfolio Theory (MPT). It was simultaneously developed by Sharpe (1964), Lintner (1965), and Mossin (1966). While MPT shows how to construct efficient portfolios, the CAPM extends this framework to explain how assets should be priced in equilibrium when all investors follow MPT principles. The CAPM is the simplest model that aims to explain equilibrium asset prices and hence the cornerstone for a myriad of extensions. In this chapter, we derive the CAPM, illustrate the theoretical underpinnings and show how to estimate the coefficients of the CAPM. For this final purpose, we download stock market data, estimate betas using regression analysis, and evaluate asset performance.\nWe use the following packages throughout this chapter:\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "python/capital-asset-pricing-model.html#asset-returns-volatilities",
    "href": "python/capital-asset-pricing-model.html#asset-returns-volatilities",
    "title": "The Capital Asset Pricing Model",
    "section": "Asset Returns & Volatilities",
    "text": "Asset Returns & Volatilities\nBuilding on our analysis from the previous chapter on Modern Portfolio Theory, we again examine the Dow Jones Industrial Average constituents as an exemplary asset universe. We download and prepare our monthly return data:\n\nsymbols = tf.download_data(\n  domain=\"constituents\", \n  index=\"Dow Jones Industrial Average\"\n)\n\nprices_daily = tf.download_data(\n  domain=\"stock_prices\", \n  symbols=symbols[\"symbol\"].tolist(),\n  start_date=\"2000-01-01\", \n  end_date=\"2023-12-31\"\n)\n\nprices_daily = (prices_daily\n  .groupby(\"symbol\")\n  .apply(lambda x: x.assign(counts=x[\"adjusted_close\"].dropna().count()))\n  .reset_index(drop=True)\n  .query(\"counts == counts.max()\")\n)\n\nreturns_monthly = (prices_daily\n  .assign(\n    date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n  )\n  .groupby([\"symbol\", \"date\"], as_index=False)\n  .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n  .assign(\n    ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n  )\n)\n\nThe relationship between risk and return is central to the CAPM. Intuitively, one may expect that assets with higher volatility should also deliver higher expected returns. Instead, the CAPM’s key insight states that not all risks are rewarded in equilibrium. Only so-called systematic risk of an asset will determine the assets expected return. To understand this, we need to distinguish between systematic and idiosyncratic risk.\nCompany-specific events might affect individual stock prices, e.g., CEO resignations, product launches, and earnings reports. These idiosyncratic events don’t necessarily impact the overall market and this asset-specific risk can be eliminated through diversification. Therefore, we call this risk idiosyncratic. Systematic risk, on the other hand, affects all assets in the market at the same time and investors really dislike it because it cannot be diversified away.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "python/capital-asset-pricing-model.html#portfolio-return-variance",
    "href": "python/capital-asset-pricing-model.html#portfolio-return-variance",
    "title": "The Capital Asset Pricing Model",
    "section": "Portfolio Return & Variance",
    "text": "Portfolio Return & Variance\nWhile we covered efficient portfolio choice in detail in the previous chapter, CAPM relies on the introduction of a crucial new element: a risk-free asset. This additional asset fundamentally changes the investment opportunity set and leads to powerful conclusions about efficient portfolio choice.\nTo recap, we considered a portfolio weight vector \\(\\omega\\in\\mathbb{R}^N\\) which denotes investments into the available \\(N\\) risky assets. So far we assumed \\(\\sum_i^N \\omega_i=1\\), indicating that all available wealth is invested across the asset universe without any outside option. Now, we relax this assumption and instead assume that all the remaining wealth, \\(1-\\iota'\\omega\\), is invested in a risk-free asset which pays a constant interest \\(r_f\\). The expected portfolio return for the portfolio of risky assets \\(\\omega\\) is then\n\\[\\mu_\\omega = \\omega^{\\prime}\\mu + (1-\\iota^{\\prime}\\omega)r_f = r_f + \\omega^{\\prime}\\underbrace{(\\mu-r_f)}_{\\tilde\\mu}.\\]\nwhere \\(\\mu\\) is the the vector of expected return of assets and \\(r_f\\) is the risk-free rate. In what follows, we refer to \\(\\tilde\\mu\\) as the vector of expected excess returns.\nBy assumption, the risk-free asset has zero volatility. Therefore, the volatility of the portfolio is given by\n\\[\\sigma_\\omega = \\sqrt{\\omega^{\\prime}\\Sigma\\omega}\\]\nwhere \\(\\Sigma\\) is the variance-covariance matrix of the returns. Restating the optimal decision problem of an investor who wants to earn a desired level of expected portfolio (excess) returns (\\(\\bar\\mu-r_f\\)) with the lowest possible variance leads to the optimization problem\n\\[\\min_\\omega Z(\\omega) = \\min_\\omega \\omega^{\\prime}\\Sigma\\omega - \\lambda \\left(\\omega^{\\prime}\\tilde\\mu-\\bar\\mu\\right).\\]\nThe first-order conditions for this optimization problem yield:\n\\[\\frac{\\partial Z}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda \\tilde\\mu = 0 \\\\\\Leftrightarrow \\omega^* = \\frac{\\lambda}{2}\\Sigma^{-1}\\tilde\\mu\\]\nThe constraint \\(\\omega^{\\prime}\\tilde\\mu_\\omega\\geq \\bar\\mu\\) delivers\n\\[\\bar\\mu = \\tilde\\mu^{\\prime}\\omega^* = \\frac{\\lambda}{2}\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu \\\\\n\\Rightarrow \\lambda = \\frac{2\\bar\\mu}{\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu}.\\] Thus, the optimal portfolio weights are given by \\[\\omega^* = \\frac{\\bar\\mu}{\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu}\\Sigma^{-1}\\tilde\\mu.\\]\nNote that \\(\\omega^*\\) does not necessarily sum up to one as, mechanically, the remainder \\(1-\\iota^{\\prime}\\omega^*\\) is invested in the risk-free asset. However, scaling \\(\\omega^*\\) delivers the portfolio of risky assets \\(\\omega_\\text{tan}\\) such that\n\\[\\omega_\\text{tan} := \\frac{\\omega^*}{\\iota'\\omega^*}= \\frac{\\Sigma^{-1}(\\mu-r_f)}{\\iota^{\\prime}\\Sigma^{-1}(\\mu-r_f)}.\\]\nImportantly, \\(\\omega_\\text{tan}\\) is independent from \\(\\bar\\mu\\)! Thus, irrespective of the desired level of expected returns (or the investors’ risk aversion), everybody chooses the same portfolio of risky assets. The only variation arises from the amount of wealth invested into the risk-free asset. Some investors may even choose to lever their risky position by borrowing at the risk-free rate and investing more than their actual wealth into the portfolio \\(\\omega_\\text{tan}\\).\nThe portfolio\n\\[\\omega_{tan}=\\frac{\\Sigma^{-1}\\tilde\\mu}{\\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu}\\]\nis central to the CAPM and is typically called the efficient tangency portfolio.\nFor illustrative purposes, we compute the efficient tangency portfolio for our hypothetical asset universe. As a realistic proxy for the risk-free rate, we use the average13-week T-bill rate (traded with the symbol ^IRX). Since the prices are quoted in annualized percentage yields, we have to divide them by 100 and convert them to monthly rates.\n\nrisk_free_monthly = (\n  tf.download_data(\"stock_prices\", symbols=\"^IRX\", start_date=\"2019-10-01\", end_date=\"2024-09-30\")\n  .assign(\n    risk_free=lambda x: (1 + x[\"adjusted_close\"] / 100)**(1/12) - 1\n  )\n  .dropna()\n)\n\nrf = risk_free_monthly[\"risk_free\"].mean()\n\nNext, we define the core parameters governing the distribution of asset returns, \\(\\Sigma\\) and \\(\\tilde\\mu\\).\n\nassets = (returns_monthly\n  .groupby(\"symbol\", as_index=False)\n  .agg(\n    mu=(\"ret\", \"mean\"),\n    sigma=(\"ret\", \"std\")\n  )\n)\n\nsigma = (returns_monthly\n  .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n  .cov()\n)\n\nmu = (returns_monthly\n  .groupby(\"symbol\")[\"ret\"]\n  .mean()\n  .values\n)\n\nWe are ready to illustrate the resulting efficient frontier. Every investor chooses to allocate a fraction of her wealth in the efficient tangency portfolio \\(\\omega_\\text{tan}\\) and the remainder in the risk-free asset. The optimal allocation depends on the investor’s risk aversion. As the risk-free asset, by definition, has a zero volatility, the efficient frontier is a straight line connecting the risk-free asset with the tangency portfolio. The slope of the line connecting the risk-free asset and the tangency portfolio is\n\\[\\frac{\\omega^{\\prime}_\\text{tan}\\mu-r_f}{\\omega_\\text{tan}^{\\prime}\\Sigma\\omega_\\text{tan}^{\\prime}}.\\]\nTypically, the excess return of an asset scaled by its volatility, \\(\\frac{\\tilde\\mu_i}{\\sigma_i}\\), is called the Sharpe ratio of the asset. Thus, the slope of the efficient frontier corresponds to the Sharpe ratio of the tangency portfolio returns. By construction, the tangency portfolio is the maximum Sharpe ratio portfolio.1\n\nw_tan = np.linalg.solve(sigma, mu - rf)\nw_tan = w_tan / np.sum(w_tan)\n\nmu_w = w_tan.T @ mu\nsigma_w = np.sqrt(w_tan.T @ sigma @ w_tan)\n\nefficient_portfolios = pd.DataFrame([\n  {\"symbol\": \"\\omega_{tan}\", \"mu\": mu_w, \"sigma\": sigma_w},\n  {\"symbol\": \"r_f\", \"mu\": rf, \"sigma\": 0}\n])\n\nsharpe_ratio = (mu_w - rf) / sigma_w\n\nFigure 1 shows the resulting efficient frontiert with the efficient portfolio and a risk-free asset.\n\nefficient_portfolios_figure = (\n  ggplot(\n    efficient_portfolios, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=assets)\n  + geom_point(data=efficient_portfolios, color=\"blue\")\n  + geom_label(\n      aes(label=\"symbol\"), parse=True, \n      adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility\", y=\"Expected return\",\n      title=\"The efficient frontier with a risk-free asset and Dow index constituents\"\n    )\n  + geom_abline(aes(intercept=rf, slope=sharpe_ratio), linetype=\"dotted\")\n)\nefficient_portfolios_figure.show()\n\n\n\n\n\n\n\nFigure 1: Expected returns and volatilities based on monthly returns adjusted for dividend payments and stock splits.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "python/capital-asset-pricing-model.html#the-capital-market-line",
    "href": "python/capital-asset-pricing-model.html#the-capital-market-line",
    "title": "The Capital Asset Pricing Model",
    "section": "The Capital Market Line",
    "text": "The Capital Market Line\nTaking another look at the efficient tangency portfolio \\(\\omega_\\text{tan}\\) reveals that expected asset excess returns \\(\\tilde\\mu\\) cannot be arbitrarily large or small. From the first order condition of the optimization problem above we get, via simple rearranging,\n\\[\\frac{\\partial Z}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda \\tilde\\mu =0 \\\\\\Leftrightarrow \\tilde\\mu = \\frac{2}{\\lambda}\\Sigma\\omega^* = \\frac{2}{\\lambda}\\underbrace{\\iota'\\omega^*}_{=\\frac{\\lambda}{2}\\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu}\\Sigma\\omega_\\text{tan} \\\\ = \\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu\\Sigma\\omega_\\text{tan}\\]\nNow, note the following three simple derivations:\n\nThe expected excess return of the efficient tangency portfolio, \\(\\tilde\\mu_\\text{tan}\\) is given by \\(E(\\omega_\\text{tan}^{\\prime} r - r_f) = \\omega_\\text{tan} \\tilde\\mu = \\frac{\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu}{\\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu}\\).\nThe variance of the returns of the efficient tangency portfolio \\(\\sigma_\\text{tan}^2\\) is given by \\(\\text{Var}(\\omega_\\text{tan}^{\\prime} r) = \\omega_\\text{tan}^{\\prime} \\Sigma \\omega_\\text{tan}^{\\prime} = \\frac{\\tilde\\mu^{\\prime}\\Sigma^{-1}\\tilde\\mu}{(\\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu)^2}\\). It follows that \\(\\frac{\\tilde\\mu_\\text{tan}}{\\sigma_\\text{tan}^2} = \\iota^{\\prime}\\Sigma^{-1}\\tilde\\mu\\)\nThe \\(i\\)-th element of the vector \\(\\Sigma\\omega_\\text{tan}\\) is \\(\\sum\\limits_{j=1}^N \\sigma_{ij}\\omega_{j,\\text{tan}}= \\text{Cov}\\left(r_i, \\sum\\limits_{j=1}^N r_i'\\omega_{j,\\text{tan}}\\right) = \\text{Cov}\\left(r_i, r_\\text{tan}\\right)\\), which is the covariance of assets \\(i\\) returns with the returns of the efficient tangency portfolio.\n\nPutting everything together yields for the expected excess return of asset \\(i\\):\n\\[\\tilde{\\mu}_i = \\frac{E(\\omega_\\text{tan}^{\\prime}r - r_f)}{\\sigma_{\\text{tan}}^2} \\text{Cov}\\left(r_i,r_\\text{tan}\\right) = \\beta_i \\tilde{\\mu}_\\text{tan}.\\]\nThe equation above is the famous CAPM equation and central to asset pricing. It states that an assets expected excess return is proportional to the assets return covariance with the efficient portfolio excess returns. The price of risk is the excess return of the efficient tangency portfolio. An asset with 0 beta has the same expected return as the risk-free rate. An asset with a beta of 1 has the same expected return as the efficient tangency portfolio. An asset with a negative beta has expected returns lower than the risk-free asset - the very definition of an insurance. Therefore, the CAPM equation explains why some assets may have the same volatility but different returns: Because their systematic risk (\\(\\beta_i\\)) is different.\nWe derived the CAPM equation as a consequence of efficient wealth allocation. Suppose an asset delivers high expected returns. The investor will increase her holdings of the assets in order to benefit from the high promised returns. As a consequence, the covariance of the asset with the efficient tangency portfolio will increase (mechanically, as the asset gradually becomes a larger part of the efficient tangency portfolio). At some point, the weight of the asset is so high that gain of \\(\\tilde\\mu_i\\) of marginally increasing the holding does not offset the implied increase in systematic risk. The investor will stop increasing her holdings and the asset’s expected return will be proportional to its systematic risk.\nWe can illustrate the relationship between systematic risk and expected returns in the so-called security market line.\n\nbetas = (sigma @ w_tan) / (w_tan.T @ sigma @ w_tan)\nassets[\"beta\"] = betas.values\n\nprice_of_risk = float(w_tan.T @ mu - rf)\n\nassets_figure = (\n  ggplot(\n    assets, \n    aes(x=\"beta\", y=\"mu\")\n  )\n  + geom_point()\n  + geom_abline(intercept=rf, slope=price_of_risk)\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Beta\", y=\"Expected return\",\n      title=\"Security market line\"\n    ) \n  + annotate(\"text\", x=0, y=rf, label=\"Risk-free\")\n)\nassets_figure.show()\n\n\n\n\n\n\n\n\nThe security market line shows the relationship between systematic risk (\\(\\beta_i\\)) and the expected return of an asset. The slope of the line is the price of risk, which is the expected return of the efficient tangency portfolio. The risk-free asset is represented by the intercept with the vertical axis. The CAPM equation states that an asset’s expected return is proportional to its beta, with the efficient tangency portfolio’s expected return as the price of risk.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "python/capital-asset-pricing-model.html#asset-pricing-and-the-market-portfolio",
    "href": "python/capital-asset-pricing-model.html#asset-pricing-and-the-market-portfolio",
    "title": "The Capital Asset Pricing Model",
    "section": "Asset Pricing and the Market Portfolio",
    "text": "Asset Pricing and the Market Portfolio\nSo far, we focused on the optimal decision problem of an individual investor. The capital asset pricing model goes an important step further and considers the equilibrium in which all investors in the market choose to hold efficient portfolios.\nBased on the previous derivations, it should be clear that\n\nAll rational investors prefer efficient portfolios to individual assets or any other portfolios.\nThe tangency portfolio \\(\\omega_\\text{tan}\\) serves as the optimal risky portfolio for all investors.\n\nThis leads to a powerful conclusion: In equilibrium, all investors hold some combination of the risk-free asset and the tangency portfolio, regardless of their risk preferences. Their only choice is how much to allocate to each of the two funds. Aggregating all capital allocated to risky assets yields the total market capitalization of all risky assets. Given that everybody holds the same portfolio of risky assets, the market capitalization of each asset is proportional to its weight in the efficient tangency portfolio. The market portfolio is the sum of all risky assets weighted by their market capitalization. For practical purposes the insight that the market portfolio is the efficient tangency portfolio, has substantial value: Instead of having to derive the vector of expected asset returns and the variance-covariance matrix, we can use the market portfolio as a proxy for the efficient tangency portfolio. Market capitalization is readily observable and the market portfolio is easy to replicate.2\nEmpirically, the CAPM-equation boils down to a linear regression:\n\\[r_{t,i}-r_f =  \\beta_i (r_{m,t} - r_f) + \\varepsilon_{i,t}.\\]\nThus, a naive, but straightforward way of ‘estimating’ the CAPM is to run a linear regression of asset excess returns on market excess returns. The slope of the regression line is then the asset’s beta.\nThe potential intercept is the asset’s alpha, which measures the asset’s risk-adjusted performance. If the CAPM holds, the alpha should be statistically indistinguishable from zero for all assets. Alpha provides a risk-adjusted performance measure. A positive alpha indicates that the asset outperformed its risk-adjusted benchmark, while a negative alpha suggests underperformance.\nIn practice, we hence estimate both alpha and beta using regression analysis. The empirical model is:\n\\[r_{i,t}  - r_{f,t}  = \\alpha_i + \\beta_i \\cdot (r_{m,t} - r_{f,t} ) + \\varepsilon_{i,t}, \\] where \\(r_{i,t}\\) and \\(r_{m,t}\\) are the realized returns of the asset and market portfolio on day \\(t\\), respectively. The error term \\(\\varepsilon_{i,t}\\) captures the asset’s idiosyncratic risk.\nLet’s turn to estimating CAPM parameters using real market data. Instead of using our previously constructed tangency portfolio, we employ the Fama-French market excess returns, which provide a widely accepted proxy for the market portfolio. These returns are already adjusted to represent excess returns over the risk-free rate, simplifying our analysis.\n\nimport pandas_datareader as pdr\n\nfactors_raw = pdr.DataReader(\n  name=\"F-F_Research_Data_5_Factors_2x3\",\n  data_source=\"famafrench\", \n  start=\"2000-01-01\", \n  end=\"2024-09-30\")[0]\n\nfactors = (factors_raw\n  .divide(100)\n  .reset_index(names=\"date\")\n  .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns={\"mkt-rf\": \"mkt_excess\"})\n)\n\nFor our regression analysis, we first prepare the data by calculating excess returns for each stock. We join our monthly returns with the Fama-French factors and subtract the risk-free rate to obtain excess returns. The estimate_capm() function then implements the regression equation we previously discussed. We leverage nested dataframes to efficiently run these regressions for all assets simultaneously. The map() function applies our regression to each nested dataset and extracts the coefficients, giving us a clean data frame of assets and their corresponding betas.\n\nimport statsmodels.formula.api as smf\n\nreturns_excess_monthly = (returns_monthly\n  .merge(factors, on=\"date\", how=\"left\")\n  .assign(ret_excess=lambda x: x[\"ret\"] - x[\"rf\"])\n)\n\ndef estimate_capm(data):\n  model = smf.ols(\"ret_excess ~ mkt_excess\", data=data).fit()\n  result = pd.DataFrame({\n    \"coefficient\": [\"alpha\", \"beta\"],\n    \"estimate\": model.params.values,\n    \"t_statistic\": model.tvalues.values\n  })\n  return result\n\ncapm_results = (returns_excess_monthly\n  .groupby(\"symbol\", group_keys=True)\n  .apply(estimate_capm)\n  .reset_index()\n)\n\nThe results are particularly interesting when we visualize the alphas across our sample of Dow Jones constituents. Figure 2 reveals the cross-sectional distribution of risk-adjusted performance, with positive values indicating outperformance and negative values indicating underperformance relative to what CAPM would predict. Statistical significance is indicated through color coding, showing which alphas are statistically different from zero at the 95% confidence level.\n\nalphas = (capm_results\n  .query(\"coefficient=='alpha'\")\n  .assign(is_significant=lambda x: np.abs(x[\"t_statistic\"]) &gt;= 1.96)\n)\n\nalphas[\"symbol\"] = pd.Categorical(\n  alphas[\"symbol\"],\n  categories=alphas.sort_values(\"estimate\")[\"symbol\"],\n  ordered=True\n)\n\nalphas_figure = (\n  ggplot(\n    alphas, \n    aes(y=\"estimate\", x=\"symbol\", fill=\"is_significant\")\n  )\n  + geom_col()\n  + scale_y_continuous(labels=percent_format())\n  + coord_flip()\n  + labs(\n      x=\"Estimated asset alphas\", y=\"\", fill=\"Significant at 95%?\",\n      title=\"Estimated CAPM alphas for Dow index constituents\"\n    )\n)\nalphas_figure.show()\n\n\n\n\n\n\n\nFigure 2: Estimates are based on returns adjusted for dividend payments and stock splits and using the Fama-French market excess returns as a measure for the market.\n\n\n\n\n\nMost notably, our analysis shows that only very few assets exhibit a statistically significant positive alpha during our sample period. This finding aligns with the exceptional performance of technology stocks, particularly those involved in AI and chip manufacturing, but suggests that most Dow components’ returns can be explained by their market exposure alone.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "python/capital-asset-pricing-model.html#shortcomings-extensions",
    "href": "python/capital-asset-pricing-model.html#shortcomings-extensions",
    "title": "The Capital Asset Pricing Model",
    "section": "Shortcomings & Extensions",
    "text": "Shortcomings & Extensions\nWhile the CAPM’s elegance and simplicity have made it a cornerstone of modern finance, the model faces several important challenges in practice. A fundamental challenge lies in the identification of the market portfolio. The CAPM theory requires a truly universal market portfolio that includes all investable assets – not just stocks, but also real estate, private businesses, human capital, and even intangible assets. In practice, we must rely on proxies like the S&P 500, DAX, or TOPIX. The choice of market proxy can significantly impact our estimates and may need to be tailored to specific contexts. For instance, a U.S.-focused investor might use the S&P 500, while a Japanese investor might prefer the TOPIX.\nAnother crucial limitation concerns the stability of beta over time. The model assumes that an asset’s systematic risk remains constant, but this rarely holds in practice. Companies undergo significant changes that can affect their market sensitivity: they may alter their capital structure, enter new markets, face new competitors, or transform their business models. Consider how tech companies’ betas might change as they mature from growth startups to established enterprises, or how a retailer’s beta might shift as it expands its online presence.\nPerhaps most importantly, empirical evidence suggests that systematic risk alone cannot fully explain asset returns. Numerous studies have documented patterns in stock returns that CAPM cannot explain. Small-cap stocks tend to outperform large-cap stocks, and value stocks (those with high book-to-market ratios) tend to outperform growth stocks, even after adjusting for market risk. These “anomalies” suggest that investors may care about multiple dimensions of risk beyond market sensitivity.\nThese limitations have spawned a rich literature of alternative and extended models. The Fama-French three-factor model (@ Fama and French 1992) represents a seminal extension, adding two factors to capture the size and value effects:\n\nThe SMB (Small Minus Big) factor captures the tendency of small stocks to outperform large stocks, as we discuss in our chapter Size Sorts and P-Hacking.\nThe HML (High Minus Low) factor captures the tendency of value stocks to outperform growth stocks, as we show in Value and Bivariate Sorts.\n\nBuilding on this framework, the Fama-French five-factor model (Fama and French 2015) adds two more dimensions, which we later discuss in Replicating Fama-French Factors:\n\nThe RMW (Robust Minus Weak) factor captures the outperformance of companies with strong operating profitability\nThe CMA (Conservative Minus Aggressive) factor reflects the tendency of companies with conservative investment policies to outperform those with aggressive investment policies\n\nThe field continues to evolve with various theoretical and empirical innovations. The Consumption CAPM links asset prices to macroeconomic risks through aggregate consumption. The Conditional CAPM (Jagannathan and Wang 1996) allows risk premiums and betas to vary with the business cycle. The Carhart four-factor model (Carhart 1997) adds momentum to the three-factor framework, while the Q-factor model and investment CAPM (Hou, Xue, and Zhang 2014) provide alternative theoretical foundations rooted in corporate finance.\nDespite its limitations, the CAPM remains valuable as a conceptual framework and practical tool. Its core insight – that only systematic risk should be priced in equilibrium – continues to influence how we think about risk and return. Understanding both its strengths and weaknesses helps us apply it more effectively and appreciate the contributions of newer models that build upon its foundation.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "python/capital-asset-pricing-model.html#key-takeaways",
    "href": "python/capital-asset-pricing-model.html#key-takeaways",
    "title": "The Capital Asset Pricing Model",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe CAPM explains asset prices in equilibrium based on systematic risk and investor preferences under ideal market conditions.\nIn equilibrium, all investors invest in a mix of the market portfolio and a risk-free asset, with individual risk tolerance determining the allocation between the two.\nExpected returns are linearly related to systematic risk, meaning assets with higher beta values should offer higher expected returns to compensate for undiversifiable risk.\nBeta is a measure of an asset’s sensitivity to overall market movements and is estimated using regression analysis of excess asset returns on excess market returns.\nDespite its simplifying assumptions, the CAPM remains foundational in finance and highlights the critical distinction between systematic and idiosyncratic risk.\nEmpirical limitations of the CAPM, such as instability of beta and unexplained return anomalies, have led to the development of multifactor models like the Fama-French and Carhart models.",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "python/capital-asset-pricing-model.html#exercises",
    "href": "python/capital-asset-pricing-model.html#exercises",
    "title": "The Capital Asset Pricing Model",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily returns for a German stock of your choice and the S&P 500 index for the past five years. Calculate the stock’s beta and interpret its meaning. How does your estimate change if you use daily instead of monthly returns?\nCompare the betas of stocks estimated using different market proxies (e.g., S&P 500, Russell 3000, MSCI World). How do the differences in market definition affect your conclusions about systematic risk?\nSelect a mutual fund and estimate its alpha and beta relative to its benchmark index. Is the fund’s performance statistically significant after accounting for market risk? How do your conclusions change if you use a different benchmark?\nCompare betas of multinational companies using both local and global market indices. How do the estimates differ? What might explain these differences?",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "python/capital-asset-pricing-model.html#footnotes",
    "href": "python/capital-asset-pricing-model.html#footnotes",
    "title": "The Capital Asset Pricing Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe proof in the Appendix Proofs that the efficient tangency portfolio \\(\\omega_\\text{tan}\\) can also be derived as a solution to the optimization problem \\(\\max_{\\omega} \\frac{\\omega^{\\prime}\\tilde\\mu}{\\sqrt{\\omega^{\\prime}\\Sigma\\omega}} \\text{ s.t. } \\omega^{\\prime}\\iota=1.\\)↩︎\nObviously, the market portfolio is the efficient tangency portfolio only if the strict assumptions of the CAPM hold: The model describes equilibrium in a single-period economy, markets are frictionless, with no transaction costs or taxes, all investors can borrow and lend at the risk-free rate, investors share the same expectations about returns and risks, investors are rational, seeking to maximize returns for a given level of risk.↩︎",
    "crumbs": [
      "R",
      "Getting Started",
      "The Capital Asset Pricing Model"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html",
    "href": "python/parametric-portfolio-policies.html",
    "title": "Parametric Portfolio Policies",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we apply different portfolio performance measures to evaluate and compare portfolio allocation strategies. For this purpose, we introduce a direct way to estimate optimal portfolio weights for large-scale cross-sectional applications. More precisely, the approach of Brandt, Santa-Clara, and Valkanov (2009) proposes to parametrize the optimal portfolio weights as a function of stock characteristics instead of estimating the stock’s expected return, variance, and covariances with other stocks in a prior step. We choose weights as a function of characteristics that maximize the expected utility of the investor. This approach is feasible for large portfolio dimensions (such as the entire CRSP universe) and has been proposed by Brandt, Santa-Clara, and Valkanov (2009). See the review paper by Brandt (2010) for an excellent treatment of related portfolio choice methods.\nThe current chapter relies on the following set of Python packages:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n\nfrom itertools import product, starmap\nfrom scipy.optimize import minimize\nCompared to previous chapters, we introduce the scipy.optimize module from the scipy (Virtanen et al. 2020) for solving optimization problems.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#data-preparation",
    "href": "python/parametric-portfolio-policies.html#data-preparation",
    "title": "Parametric Portfolio Policies",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the monthly CRSP file, which forms our investment universe. We load the data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=(\"SELECT permno, date, ret_excess, mktcap, mktcap_lag \"\n         \"FROM crsp_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .dropna()\n)\n\nTo evaluate the performance of portfolios, we further use monthly market returns as a benchmark to compute CAPM alphas.\n\nfactors_ff_monthly = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nNext, we retrieve some stock characteristics that have been shown to have an effect on the expected returns or expected variances (or even higher moments) of the return distribution. In particular, we record the lagged one-year return momentum (momentum_lag), defined as the compounded return between months \\(t-13\\) and \\(t-2\\) for each firm, which we calculate using market capitalization for simplicity. In finance, momentum is the empirically observed tendency for rising asset prices to rise further and falling prices to keep falling (Jegadeesh and Titman 1993). We refer to the exercise for a more elaborate measure of momentum. The second characteristic is the firm’s market equity (size_lag), defined as the log of the price per share times the number of shares outstanding (Banz 1981). To construct the correct lagged values, we use the approach introduced in WRDS, CRSP, and Compustat.\n\ncrsp_monthly_lags = (crsp_monthly\n  .assign(date=lambda x: x[\"date\"]+pd.DateOffset(months=13))\n  .get([\"permno\", \"date\", \"mktcap\"])\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(crsp_monthly_lags, \n         how=\"inner\", on=[\"permno\", \"date\"], suffixes=[\"\", \"_13\"])\n)\n\ndata_portfolios = (crsp_monthly\n  .assign(\n    momentum_lag=lambda x: x[\"mktcap_lag\"]/x[\"mktcap_13\"],\n    size_lag=lambda x: np.log(x[\"mktcap_lag\"])\n  )\n  .dropna(subset=[\"momentum_lag\", \"size_lag\"])\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "href": "python/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "title": "Parametric Portfolio Policies",
    "section": "Parametric Portfolio Policies",
    "text": "Parametric Portfolio Policies\nThe basic idea of parametric portfolio weights is as follows. Suppose that at each date \\(t\\), we have \\(N_t\\) stocks in the investment universe, where each stock \\(i\\) has a return of \\(r_{i, t+1}\\) and is associated with a vector of firm characteristics \\(x_{i, t}\\) such as time-series momentum or the market capitalization. The investor’s problem is to choose portfolio weights \\(w_{i,t}\\) to maximize the expected utility of the portfolio return: \\[\\begin{aligned}\n\\max_{\\omega} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{i=1}^{N_t}\\omega_{i,t}\\cdot r_{i,t+1}\\right)\\right]\n\\end{aligned} \\tag{1}\\] where \\(u(\\cdot)\\) denotes the utility function.\nWhere do the stock characteristics show up? We parameterize the optimal portfolio weights as a function of the stock characteristic \\(x_{i,t}\\) with the following linear specification for the portfolio weights: \\[\\omega_{i,t} = \\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}, \\tag{2}\\] where \\(\\bar{\\omega}_{i,t}\\) is a stock’s weight in a benchmark portfolio (we use the value-weighted or naive portfolio in the application below), \\(\\theta\\) is a vector of coefficients which we are going to estimate, and \\(\\hat{x}_{i,t}\\) are the characteristics of stock \\(i\\), cross-sectionally standardized to have zero mean and unit standard deviation.\nIntuitively, the portfolio strategy is a form of active portfolio management relative to a performance benchmark. Deviations from the benchmark portfolio are derived from the individual stock characteristics. Note that by construction, the weights sum up to one as \\(\\sum_{i=1}^{N_t}\\hat{x}_{i,t} = 0\\) due to the standardization. Moreover, the coefficients are constant across assets and over time. The implicit assumption is that the characteristics fully capture all aspects of the joint distribution of returns that are relevant for forming optimal portfolios.\nWe first implement cross-sectional standardization for the entire CRSP universe. We also keep track of (lagged) relative market capitalization relative_mktcap, which will represent the value-weighted benchmark portfolio, while n denotes the number of traded assets \\(N_t\\), which we use to construct the naive portfolio benchmark.\n\ndata_portfolios = (data_portfolios\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      relative_mktcap=x[\"mktcap_lag\"]/x[\"mktcap_lag\"].sum()\n    )\n  )\n  .reset_index(drop=True)\n  .set_index(\"date\")\n  .groupby(level=\"date\")\n  .transform(\n    lambda x: (x-x.mean())/x.std() if x.name.endswith(\"lag\") else x\n  )\n  .reset_index()\n  .drop([\"mktcap_lag\"], axis=1)\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#computing-portfolio-weights",
    "href": "python/parametric-portfolio-policies.html#computing-portfolio-weights",
    "title": "Parametric Portfolio Policies",
    "section": "Computing Portfolio Weights",
    "text": "Computing Portfolio Weights\nNext, we move on to identify optimal choices of \\(\\theta\\). We rewrite the optimization problem together with the weight parametrization and can then estimate \\(\\theta\\) to maximize the objective function based on our sample \\[\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{i=1}^{N_t}\\left(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\right)r_{i,t+1}\\right).\n\\end{aligned} \\tag{3}\\] The allocation strategy is straightforward because the number of parameters to estimate is small. Instead of a tedious specification of the \\(N_t\\) dimensional vector of expected returns and the \\(N_t(N_t+1)/2\\) free elements of the covariance matrix, all we need to focus on in our application is the vector \\(\\theta\\). \\(\\theta\\) contains only two elements in our application: the relative deviation from the benchmark due to size and momentum.\nTo get a feeling for the performance of such an allocation strategy, we start with an arbitrary initial vector \\(\\theta_0\\). The next step is to choose \\(\\theta\\) optimally to maximize the objective function. We automatically detect the number of parameters by counting the number of columns with lagged values. Note that the value for \\(\\theta\\) of 1.5 is an arbitrary choice.\n\nlag_columns = [i for i in data_portfolios.columns if \"lag\" in i]\nn_parameters = len(lag_columns)\ntheta = pd.DataFrame({\"theta\": [1.5]*n_parameters}, index=lag_columns)\n\nThe function compute_portfolio_weights() below computes the portfolio weights \\(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\) according to our parametrization for a given value \\(\\theta_0\\). Everything happens within a single pipeline. Hence, we provide a short walk-through.\nWe first compute characteristic_tilt, the tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{i, t}\\) which resemble the deviation from the benchmark portfolio. Next, we compute the benchmark portfolio weight_benchmark, which can be any reasonable set of portfolio weights. In our case, we choose either the value or equal-weighted allocation. weight_tilt completes the picture and contains the final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt, which deviate from the benchmark portfolio depending on the stock characteristics.\nThe final few lines go a bit further and implement a simple version of a no-short sale constraint. While it is generally not straightforward to ensure portfolio weight constraints via parameterization, we simply normalize the portfolio weights such that they are enforced to be positive. Finally, we make sure that the normalized weights sum up to one again: \\[\\omega_{i,t}^+ = \\frac{\\max(0, \\omega_{i,t})}{\\sum_{j=1}^{N_t}\\max(0, \\omega_{i,t})}. \\tag{4}\\]\nThe following function computes the optimal portfolio weights in the way just described.\n\ndef compute_portfolio_weights(theta, \n                              data,\n                              value_weighting=True,\n                              allow_short_selling=True):\n    \"\"\"Compute portfolio weights for different strategies.\"\"\"\n    \n    lag_columns = [i for i in data.columns if \"lag\" in i]\n    theta = pd.DataFrame(theta, index=lag_columns)\n\n    data = (data\n      .groupby(\"date\")\n      .apply(lambda x: x.assign(\n          characteristic_tilt=x[theta.index] @ theta / x.shape[0]\n        )\n      )\n      .reset_index(drop=True)\n      .assign(\n        weight_benchmark=lambda x: \n          x[\"relative_mktcap\"] if value_weighting else 1/x.shape[0],\n        weight_tilt=lambda x: \n          x[\"weight_benchmark\"] + x[\"characteristic_tilt\"]\n      )\n      .drop(columns=[\"characteristic_tilt\"])\n    )\n\n    if not allow_short_selling:\n        data = (data\n          .assign(weight_tilt=lambda x: np.maximum(0, x[\"weight_tilt\"]))\n        )\n\n    data = (data\n      .groupby(\"date\")\n      .apply(lambda x: x.assign(\n        weight_tilt=lambda x: x[\"weight_tilt\"]/x[\"weight_tilt\"].sum()))\n      .reset_index(drop=True)\n    )\n\n    return data\n\nIn the next step, we compute the portfolio weights for the arbitrary vector \\(\\theta_0\\). In the example below, we use the value-weighted portfolio as a benchmark and allow negative portfolio weights.\n\nweights_crsp = compute_portfolio_weights(\n  theta,\n  data_portfolios,\n  value_weighting=True,\n  allow_short_selling=True\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#portfolio-performance",
    "href": "python/parametric-portfolio-policies.html#portfolio-performance",
    "title": "Parametric Portfolio Policies",
    "section": "Portfolio Performance",
    "text": "Portfolio Performance\n Are the computed weights optimal in any way? Most likely not, as we picked \\(\\theta_0\\) arbitrarily. To evaluate the performance of an allocation strategy, one can think of many different approaches. In their original paper, Brandt, Santa-Clara, and Valkanov (2009) focus on a simple evaluation of the hypothetical utility of an agent equipped with a power utility function \\[u_\\gamma(r) = \\frac{(1 + r)^{(1-\\gamma)}}{1-\\gamma}, \\tag{5}\\] where \\(\\gamma\\) is the risk aversion factor.\n\ndef power_utility(r, gamma=5):\n    \"\"\"Calculate power utility for given risk aversion.\"\"\"\n    \n    utility = ((1+r)**(1-gamma))/(1-gamma)\n    \n    return utility\n\nWe want to note that Gehrig, Sögner, and Westerkamp (2020) warn that, in the leading case of constant relative risk aversion (CRRA), strong assumptions on the properties of the returns, the variables used to implement the parametric portfolio policy, and the parameter space are necessary to obtain a well-defined optimization problem.\nNo doubt, there are many other ways to evaluate a portfolio. The function below provides a summary of all kinds of interesting measures that can be considered relevant. Do we need all these evaluation measures? It depends: The original paper by Brandt, Santa-Clara, and Valkanov (2009) only cares about the expected utility to choose \\(\\theta\\). However, if you want to choose optimal values that achieve the highest performance while putting some constraints on your portfolio weights, it is helpful to have everything in one function.\n\ndef evaluate_portfolio(weights_data,\n                       full_evaluation=True,\n                       capm_evaluation=True,\n                       length_year=12):\n    \"\"\"Calculate portfolio evaluation measures.\"\"\"\n    evaluation = (weights_data\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series(\n          np.average(x[[\"ret_excess\", \"ret_excess\"]],\n                     weights=x[[\"weight_tilt\", \"weight_benchmark\"]],\n                     axis=0),\n          [\"return_tilt\", \"return_benchmark\"])\n        )\n        .reset_index()\n        .melt(id_vars=\"date\", var_name=\"model\",\n              value_vars=[\"return_tilt\", \"return_benchmark\"],\n              value_name=\"portfolio_return\")\n        .assign(model=lambda x: x[\"model\"].str.replace(\"return_\", \"\"))\n    )\n\n    evaluation_stats = (evaluation\n        .groupby(\"model\")[\"portfolio_return\"]\n        .aggregate([\n          (\"Expected utility\", lambda x: np.mean(power_utility(x))),\n          (\"Average return\", lambda x: np.mean(length_year*x)*100),\n          (\"SD return\", lambda x: np.std(x)*np.sqrt(length_year)*100),\n          (\"Sharpe ratio\", lambda x: (np.mean(x)/np.std(x)* \n                                        np.sqrt(length_year)))\n        ])\n    )\n\n    if capm_evaluation:\n        evaluation_capm = (evaluation\n            .merge(factors_ff_monthly, how=\"left\", on=\"date\")\n            .groupby(\"model\")\n            .apply(lambda x: \n              smf.ols(formula=\"portfolio_return ~ 1 + mkt_excess\", data=x)\n              .fit().params\n            )\n            .rename(columns={\"const\": \"CAPM alpha\",\n                             \"mkt_excess\": \"Market beta\"})\n            )\n        evaluation_stats = (evaluation_stats\n          .merge(evaluation_capm, how=\"left\", on=\"model\")\n        )\n\n    if full_evaluation:\n        evaluation_weights = (weights_data\n          .melt(id_vars=\"date\", var_name=\"model\",\n                value_vars=[\"weight_benchmark\", \"weight_tilt\"],\n                value_name=\"weight\")\n          .groupby([\"model\", \"date\"])[\"weight\"]\n          .aggregate([\n            (\"Mean abs. weight\", lambda x: np.mean(abs(x))),\n            (\"Max. weight\", lambda x: max(x)),\n            (\"Min. weight\", lambda x: min(x)),\n            (\"Avg. sum of neg. weights\", lambda x: -np.sum(x[x &lt; 0])),\n            (\"Avg. share of neg. weights\", lambda x: np.mean(x &lt; 0))\n          ])\n          .reset_index()\n          .drop(columns=[\"date\"])\n          .groupby([\"model\"])\n          .aggregate(lambda x: np.average(x)*100)\n          .reset_index()\n          .assign(model=lambda x: x[\"model\"].str.replace(\"weight_\", \"\"))\n        )\n        \n        evaluation_stats = (evaluation_stats\n          .merge(evaluation_weights, how=\"left\", on=\"model\")\n          .set_index(\"model\")\n        )\n        \n    evaluation_stats = (evaluation_stats\n      .transpose()\n      .rename_axis(columns=None)\n    )\n\n    return evaluation_stats\n\n Let us take a look at the different portfolio strategies and evaluation measures.\n\nevaluate_portfolio(weights_crsp).round(2)\n\n\n\n\n\n\n\n\nbenchmark\ntilt\n\n\n\n\nExpected utility\n-0.25\n-0.26\n\n\nAverage return\n6.87\n0.53\n\n\nSD return\n15.46\n21.18\n\n\nSharpe ratio\n0.44\n0.03\n\n\nIntercept\n0.00\n-0.00\n\n\nMarket beta\n0.99\n0.94\n\n\nMean abs. weight\n0.03\n0.08\n\n\nMax. weight\n4.09\n4.25\n\n\nMin. weight\n0.00\n-0.17\n\n\nAvg. sum of neg. weights\n0.00\n78.13\n\n\nAvg. share of neg. weights\n0.00\n49.06\n\n\n\n\n\n\n\nThe value-weighted portfolio delivers an annualized return of more than six percent and clearly outperforms the tilted portfolio, irrespective of whether we evaluate expected utility, the Sharpe ratio, or the CAPM alpha. We can conclude the market beta is close to one for both strategies (naturally almost identically one for the value-weighted benchmark portfolio). When it comes to the distribution of the portfolio weights, we see that the benchmark portfolio weight takes less extreme positions (lower average absolute weights and lower maximum weight). By definition, the value-weighted benchmark does not take any negative positions, while the tilted portfolio also takes short positions.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#optimal-parameter-choice",
    "href": "python/parametric-portfolio-policies.html#optimal-parameter-choice",
    "title": "Parametric Portfolio Policies",
    "section": "Optimal Parameter Choice",
    "text": "Optimal Parameter Choice\nNext, we move to a choice of \\(\\theta\\) that actually aims to improve some (or all) of the performance measures. We first define the helper function compute_objective_function(), which we then pass to an optimizer.\n\ndef objective_function(theta,\n                       data,\n                       objective_measure=\"Expected utility\",\n                       value_weighting=True,\n                       allow_short_selling=True):\n    \"\"\"Define portfolio objective function.\"\"\"\n    \n    processed_data = compute_portfolio_weights(\n      theta, data, value_weighting, allow_short_selling\n    )\n\n    objective_function = evaluate_portfolio(\n      processed_data, \n      capm_evaluation=False, \n      full_evaluation=False\n    )\n\n    objective_function = -objective_function.loc[objective_measure, \"tilt\"]\n\n    return objective_function\n\nYou may wonder why we return the negative value of the objective function. This is simply due to the common convention for optimization procedures to search for minima as a default. By minimizing the negative value of the objective function, we get the maximum value as a result. In its most basic form, Python optimization uses the function minimize(). As main inputs, the function requires an initial guess of the parameters and the objective function to minimize. Now, we are fully equipped to compute the optimal values of \\(\\hat\\theta\\), which maximize the hypothetical expected utility of the investor.\n\noptimal_theta = minimize(\n  fun=objective_function,\n  x0=[1.5]*n_parameters,\n  args=(data_portfolios, \"Expected utility\", True, True),\n  method=\"Nelder-Mead\",\n  tol=1e-2\n)\n\n(pd.DataFrame(\n  optimal_theta.x,\n  columns=[\"Optimal theta\"],\n  index=[\"momentum_lag\", \"size_lag\"]).T.round(3)\n)\n\n\n\n\n\n\n\n\nmomentum_lag\nsize_lag\n\n\n\n\nOptimal theta\n0.304\n-1.704\n\n\n\n\n\n\n\nThe resulting values of \\(\\hat\\theta\\) are easy to interpret: intuitively, expected utility increases by tilting weights from the value-weighted portfolio toward smaller stocks (negative coefficient for size) and toward past winners (positive value for momentum). Both findings are in line with the well-documented size effect (Banz 1981) and the momentum anomaly (Jegadeesh and Titman 1993).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#more-model-specifications",
    "href": "python/parametric-portfolio-policies.html#more-model-specifications",
    "title": "Parametric Portfolio Policies",
    "section": "More Model Specifications",
    "text": "More Model Specifications\nHow does the portfolio perform for different model specifications? For this purpose, we compute the performance of a number of different modeling choices based on the entire CRSP sample. The next code chunk performs all the heavy lifting.\n\ndef evaluate_optimal_performance(data,\n                                 objective_measure=\"Expected utility\",\n                                 value_weighting=True,\n                                 allow_short_selling=True):\n    \"\"\"Calculate optimal portfolio performance.\"\"\"\n    \n    optimal_theta = minimize(\n      fun=objective_function,\n      x0=[1.5]*n_parameters,\n      args=(data, objective_measure, value_weighting, allow_short_selling),\n      method=\"Nelder-Mead\",\n      tol=10e-2\n    ).x\n\n    processed_data = compute_portfolio_weights(\n      optimal_theta, data, \n      value_weighting, allow_short_selling\n    )\n\n    portfolio_evaluation = evaluate_portfolio(processed_data)\n\n    weight_text = \"VW\" if value_weighting else \"EW\"\n    short_text = \"\" if allow_short_selling else \" (no s.)\"\n\n    strategy_name_dict = {\n      \"benchmark\": weight_text,\n      \"tilt\": f\"{weight_text} Optimal{short_text}\"\n    }\n\n    portfolio_evaluation.columns = [\n      strategy_name_dict[i] for i in portfolio_evaluation.columns\n    ]\n    \n    return(portfolio_evaluation)\n\nFinally, we can compare the results. The table below shows summary statistics for all possible combinations: equal- or value-weighted benchmark portfolio, with or without short-selling constraints, and tilted toward maximizing expected utility.\n\ndata = [data_portfolios]\nvalue_weighting = [True, False]\nallow_short_selling = [True, False]\nobjective_measure = [\"Expected utility\"]\n\npermutations = product(\n  data, objective_measure,\n  value_weighting, allow_short_selling\n)\nresults = list(starmap(\n  evaluate_optimal_performance, \n  permutations\n))\nperformance_table = (pd.concat(results, axis=1)\n  .T.drop_duplicates().T.round(3)\n)\nperformance_table.get([\"EW\", \"VW\"])\n\n\n\n\n\n\n\n\nEW\nVW\n\n\n\n\nExpected utility\n-0.251\n-0.250\n\n\nAverage return\n10.012\n6.867\n\n\nSD return\n20.467\n15.461\n\n\nSharpe ratio\n0.489\n0.444\n\n\nIntercept\n0.002\n0.000\n\n\nMarket beta\n1.130\n0.994\n\n\nMean abs. weight\n0.000\n0.030\n\n\nMax. weight\n0.000\n4.091\n\n\nMin. weight\n0.000\n0.000\n\n\nAvg. sum of neg. weights\n0.000\n0.000\n\n\nAvg. share of neg. weights\n0.000\n0.000\n\n\n\n\n\n\n\n\nperformance_table.get([\"EW Optimal\", \"VW Optimal\"])\n\n\n\n\n\n\n\n\nEW Optimal\nVW Optimal\n\n\n\n\nExpected utility\n-4.630\n-0.261\n\n\nAverage return\n-4891.585\n0.532\n\n\nSD return\n14402.492\n21.176\n\n\nSharpe ratio\n-0.340\n0.025\n\n\nIntercept\n-3.614\n-0.005\n\n\nMarket beta\n-81.790\n0.944\n\n\nMean abs. weight\n60.120\n0.077\n\n\nMax. weight\n1009.558\n4.254\n\n\nMin. weight\n-213.264\n-0.173\n\n\nAvg. sum of neg. weights\n75807.814\n78.128\n\n\nAvg. share of neg. weights\n51.744\n49.064\n\n\n\n\n\n\n\n\nperformance_table.get([\"EW Optimal (no s.)\", \"VW Optimal (no s.)\"])\n\n\n\n\n\n\n\n\nEW Optimal (no s.)\nVW Optimal (no s.)\n\n\n\n\nExpected utility\n-0.252\n-0.250\n\n\nAverage return\n7.970\n7.414\n\n\nSD return\n19.148\n16.705\n\n\nSharpe ratio\n0.416\n0.444\n\n\nIntercept\n0.000\n0.000\n\n\nMarket beta\n1.137\n1.056\n\n\nMean abs. weight\n0.030\n0.030\n\n\nMax. weight\n1.307\n2.351\n\n\nMin. weight\n0.000\n0.000\n\n\nAvg. sum of neg. weights\n0.000\n0.000\n\n\nAvg. share of neg. weights\n0.000\n0.000\n\n\n\n\n\n\n\nThe results indicate that the average annualized Sharpe ratio of the equal-weighted portfolio exceeds the Sharpe ratio of the value-weighted benchmark portfolio. Nevertheless, starting with the weighted value portfolio as a benchmark and tilting optimally with respect to momentum and small stocks yields the highest Sharpe ratio across all specifications. Finally, imposing no short-sale constraints does not improve the performance of the portfolios in our application.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#key-takeaways",
    "href": "python/parametric-portfolio-policies.html#key-takeaways",
    "title": "Parametric Portfolio Policies",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nParametric portfolio policies estimate portfolio weights directly as functions of stock characteristics like momentum and size, avoiding the need to forecast expected returns or covariances.\nThis method, based on Brandt, Santa-Clara, and Valkanov (2009), is computationally efficient and scalable for large cross-sectional datasets such as CRSP.\nOptimization focuses on maximizing expected utility, and evaluation includes measures such as Sharpe ratio, CAPM alpha, and utility-based performance.\nResults highlight that tilting value-weighted portfolios toward small-cap and high-momentum stocks improves performance, aligning with known anomalies in finance.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#exercises",
    "href": "python/parametric-portfolio-policies.html#exercises",
    "title": "Parametric Portfolio Policies",
    "section": "Exercises",
    "text": "Exercises\n\nDefine momentum as the rolling 12-month cumulative returns skipping the most recent month. Calculate the correlation with the measure based on market capitalization from above and compare summary statistics. How do the two measures differ?\nHow do the estimated parameters \\(\\hat\\theta\\) and the portfolio performance change if your objective is to maximize the Sharpe ratio instead of the hypothetical expected utility?\nThe code above is very flexible in the sense that you can easily add new firm characteristics. Construct a new characteristic of your choice and evaluate the corresponding coefficient \\(\\hat\\theta_i\\).\nTweak the function optimal_theta() such that you can impose additional performance constraints in order to determine \\(\\hat\\theta\\), which maximizes expected utility under the constraint that the market beta is below 1.\nDoes the portfolio performance resemble a realistic out-of-sample backtesting procedure? Verify the robustness of the results by first estimating \\(\\hat\\theta\\) based on past data only. Then, use more recent periods to evaluate the actual portfolio performance.\nBy formulating the portfolio problem as a statistical estimation problem, you can easily obtain standard errors for the coefficients of the weight function. Brandt, Santa-Clara, and Valkanov (2009) provide the relevant derivations in their paper in Equation (10). Implement a small function that computes standard errors for \\(\\hat\\theta\\).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/wrds-dummy-data.html",
    "href": "python/wrds-dummy-data.html",
    "title": "WRDS Dummy Data",
    "section": "",
    "text": "In this appendix chapter, we alleviate the constraints of readers who don’t have access to WRDS and hence cannot run the code that we provide. We show how to create a dummy database that contains the WRDS tables and corresponding columns such that all code chunks in this book can be executed with this dummy database. We do not create dummy data for tables of macroeconomic variables because they can be freely downloaded from the original sources; check out Accessing and Managing Financial Data.\nWe deliberately use the dummy label because the data is not meaningful in the sense that it allows readers to actually replicate the results of the book. For legal reasons, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books.\nTo generate the dummy database, we use the following packages:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport string\nLet us initialize a SQLite database (tidy_finance_python.sqlite) or connect to your existing one. Be careful, if you already downloaded the data from WRDS, then the code in this chapter will overwrite your data!\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\nSince we draw random numbers for most of the columns, we also define a seed to ensure that the generated numbers are replicable. We also initialize vectors of dates of different frequencies over ten years that we then use to create yearly, monthly, and daily data, respectively.\nnp.random.seed(1234)\n\nstart_date = pd.Timestamp(\"2003-01-01\")\nend_date = pd.Timestamp(\"2022-12-31\")\n\ndummy_years = np.arange(start_date.year, end_date.year+1, 1)\ndummy_months = pd.date_range(start_date, end_date, freq=\"MS\") \ndummy_days = pd.date_range(start_date, end_date, freq=\"D\")",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "python/wrds-dummy-data.html#create-stock-dummy-data",
    "href": "python/wrds-dummy-data.html#create-stock-dummy-data",
    "title": "WRDS Dummy Data",
    "section": "Create Stock Dummy Data",
    "text": "Create Stock Dummy Data\nLet us start with the core data used throughout the book: stock and firm characteristics. We first generate a table with a cross-section of stock identifiers with unique permno and gvkey values, as well as associated exchcd, exchange, industry, and siccd values. The generated data is based on the characteristics of stocks in the crsp_monthly table of the original database, ensuring that the generated stocks roughly reflect the distribution of industries and exchanges in the original data, but the identifiers and corresponding exchanges or industries do not reflect actual firms. Similarly, the permno-gvkey combinations are purely nonsensical and should not be used together with actual CRSP or Compustat data.\n\nnumber_of_stocks = 100\n\nindustries = pd.DataFrame({\n  \"industry\": [\"Agriculture\", \"Construction\", \"Finance\",\n               \"Manufacturing\", \"Mining\", \"Public\", \"Retail\", \n               \"Services\", \"Transportation\", \"Utilities\", \"Wholesale\"],\n  \"n\": [81, 287, 4682, 8584, 1287, 1974, 1571, 4277, 1249, 457, 904],\n  \"prob\": [0.00319, 0.0113, 0.185, 0.339, 0.0508, 0.0779, \n           0.0620, 0.169, 0.0493, 0.0180, 0.03451]\n})\n\nexchanges = pd.DataFrame({\n  \"exchange\": [\"AMEX\", \"NASDAQ\", \"NYSE\"],\n  \"n\": [2893, 17236, 5553],\n  \"prob\": [0.113, 0.671, 0.216]\n})\n\nstock_identifiers_list = []\nfor x in range(1, number_of_stocks+1):\n  exchange = np.random.choice(exchanges[\"exchange\"], p=exchanges[\"prob\"])\n  industry = np.random.choice(industries[\"industry\"], p=industries[\"prob\"])\n\n  exchcd_mapping = {\n    \"NYSE\": np.random.choice([1, 31]),\n    \"AMEX\": np.random.choice([2, 32]),\n    \"NASDAQ\": np.random.choice([3, 33])\n  }\n\n  siccd_mapping = {\n    \"Agriculture\": np.random.randint(1, 1000),\n    \"Mining\": np.random.randint(1000, 1500),\n    \"Construction\": np.random.randint(1500, 1800),\n    \"Manufacturing\": np.random.randint(1800, 4000),\n    \"Transportation\": np.random.randint(4000, 4900),\n    \"Utilities\": np.random.randint(4900, 5000),\n    \"Wholesale\": np.random.randint(5000, 5200),\n    \"Retail\": np.random.randint(5200, 6000),\n    \"Finance\": np.random.randint(6000, 6800),\n    \"Services\": np.random.randint(7000, 9000),\n    \"Public\": np.random.randint(9000, 10000)\n  }\n\n  stock_identifiers_list.append({\n    \"permno\": x,\n    \"gvkey\": str(x+10000),\n    \"exchange\": exchange,\n    \"industry\": industry,\n    \"exchcd\": exchcd_mapping[exchange],\n    \"siccd\": siccd_mapping[industry]\n  })\n\nstock_identifiers = pd.DataFrame(stock_identifiers_list)\n\nNext, we construct three panels of stock data with varying frequencies: yearly, monthly, and daily. We begin by creating the stock_panel_yearly panel. To achieve this, we combine the stock_identifiers table with a new table containing the variable year from dummy_years. The expand_grid() function ensures that we get all possible combinations of the two tables. After combining, we select only the gvkey and year columns for our final yearly panel.\nNext, we construct the stock_panel_monthly panel. Similar to the yearly panel, we use the expand_grid() function to combine stock_identifiers with a new table that has the date variable from dummy_months. After merging, we select the columns permno, gvkey, date, siccd, industry, exchcd, and exchange to form our monthly panel.\nLastly, we create the stock_panel_daily panel. We combine stock_identifiers with a table containing the date variable from dummy_days. After merging, we retain only the permno and date columns for our daily panel.\n\nstock_panel_yearly = pd.DataFrame({\n  \"gvkey\": np.tile(stock_identifiers[\"gvkey\"], len(dummy_years)),\n  \"year\": np.repeat(dummy_years, len(stock_identifiers))\n})\n\nstock_panel_monthly = pd.DataFrame({\n  \"permno\": np.tile(stock_identifiers[\"permno\"], len(dummy_months)),\n  \"gvkey\": np.tile(stock_identifiers[\"gvkey\"], len(dummy_months)),\n  \"date\": np.repeat(dummy_months, len(stock_identifiers)),\n  \"siccd\": np.tile(stock_identifiers[\"siccd\"], len(dummy_months)),\n  \"industry\": np.tile(stock_identifiers[\"industry\"], len(dummy_months)),\n  \"exchcd\": np.tile(stock_identifiers[\"exchcd\"], len(dummy_months)),\n  \"exchange\": np.tile(stock_identifiers[\"exchange\"], len(dummy_months))\n})\n\nstock_panel_daily = pd.DataFrame({\n  \"permno\": np.tile(stock_identifiers[\"permno\"], len(dummy_days)),\n  \"date\": np.repeat(dummy_days, len(stock_identifiers))\n})\n\n\nDummy beta table\nWe then proceed to create dummy beta values for our stock_panel_monthly table. We generate monthly beta values beta_monthly using the rnorm() function with a mean and standard deviation of 1. For daily beta values beta_daily, we take the dummy monthly beta and add a small random noise to it. This noise is generated again using the rnorm() function, but this time we divide the random values by 100 to ensure they are small deviations from the monthly beta.\n\nbeta_dummy = (stock_panel_monthly\n  .assign(\n    beta_monthly=np.random.normal(\n      loc=1, scale=1, size=len(stock_panel_monthly)\n    ),\n    beta_daily=lambda x: (\n      x[\"beta_monthly\"]+np.random.normal(scale=0.01, size=len(x))\n    )\n  )\n)\n\n(beta_dummy\n  .to_sql(name=\"beta\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n\n\n\nDummy compustat table\nTo create dummy firm characteristics, we take all columns from the compustat table and create random numbers between 0 and 1. For simplicity, we set the datadate for each firm-year observation to the last day of the year, although it is empirically not the case. \n\nrelevant_columns = [\n  \"seq\", \"ceq\", \"at\", \"lt\", \"txditc\", \"txdb\", \"itcb\", \n  \"pstkrv\", \"pstkl\", \"pstk\", \"capx\", \"oancf\", \"sale\", \n  \"cogs\", \"xint\", \"xsga\", \"be\", \"op\", \"at_lag\", \"inv\"\n]\n\ncommands = {\n  col: np.random.rand(len(stock_panel_yearly)) for col in relevant_columns\n}\n\ncompustat_dummy = (\n  stock_panel_yearly\n  .assign(\n    datadate=lambda x: pd.to_datetime(x[\"year\"].astype(str)+\"-12-31\")\n  )\n  .assign(**commands)\n)\n\n(compustat_dummy\n  .to_sql(name=\"compustat\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\n\n\nDummy crsp_monthly table\nThe crsp_monthly table only lacks a few more columns compared to stock_panel_monthly: the returns ret drawn from a normal distribution, the excess returns ret_excess with small deviations from the returns, the shares outstanding shrout and the last price per month altprc both drawn from uniform distributions, and the market capitalization mktcap as the product of shrout and altprc. \n\ncrsp_monthly_dummy = (stock_panel_monthly\n  .assign(\n    ret=lambda x: np.fmax(np.random.normal(size=len(x)), -1),\n    ret_excess=lambda x: (\n      np.fmax(x[\"ret\"]-np.random.uniform(0, 0.0025, len(x)), -1)\n    ),\n    shrout=1000*np.random.uniform(1, 50, len(stock_panel_monthly)),\n    altprc=np.random.uniform(0, 1000, len(stock_panel_monthly)))\n  .assign(mktcap=lambda x: x[\"shrout\"]*x[\"altprc\"])\n  .sort_values(by=[\"permno\", \"date\"])\n  .assign(\n    mktcap_lag=lambda x: (x.groupby(\"permno\")[\"mktcap\"].shift(1))\n  )\n  .reset_index(drop=True)\n)\n\n(crsp_monthly_dummy\n  .to_sql(name=\"crsp_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\n\n\nDummy crsp_daily table\nThe crsp_daily table only contains a date column and the daily excess returns ret_excess as additional columns to stock_panel_daily.\n\ncrsp_daily_dummy = (stock_panel_daily\n  .assign(\n    ret_excess=lambda x: np.fmax(np.random.normal(size=len(x)), -1)\n  )\n  .reset_index(drop=True)\n)\n\n(crsp_daily_dummy\n  .to_sql(name=\"crsp_daily\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "python/wrds-dummy-data.html#create-bond-dummy-data",
    "href": "python/wrds-dummy-data.html#create-bond-dummy-data",
    "title": "WRDS Dummy Data",
    "section": "Create Bond Dummy Data",
    "text": "Create Bond Dummy Data\nLastly, we move to the bond data that we use in our books.\n\nDummy fisd data\nTo create dummy data with the structure of Mergent FISD, we calculate the empirical probabilities of actual bonds for several variables: maturity, offering_amt, interest_frequency, coupon, and sic_code. We use these probabilities to sample a small cross-section of bonds with completely made up complete_cusip, issue_id, and issuer_id.\n\nnumber_of_bonds = 100\n\ndef generate_cusip():\n  \"\"\"Generate cusip.\"\"\"\n  \n  characters = list(string.ascii_uppercase+string.digits)  # Convert to list\n  cusip = (\"\".join(np.random.choice(characters, size=12))).upper()\n    \n  return cusip\n\nfisd_dummy = (pd.DataFrame({\n    \"complete_cusip\": [generate_cusip() for _ in range(number_of_bonds)]\n  })\n  .assign(\n    maturity=lambda x: np.random.choice(dummy_days, len(x), replace=True),\n    offering_amt=lambda x: np.random.choice(\n      np.arange(1, 101)*100000, len(x), replace=True\n    )\n  )\n  .assign(\n    offering_date=lambda x: (\n      x[\"maturity\"]-pd.to_timedelta(\n        np.random.choice(np.arange(1, 26)*365, len(x), replace=True), \n        unit=\"D\"\n      )\n    )\n  )\n  .assign(\n    dated_date=lambda x: (\n      x[\"offering_date\"]-pd.to_timedelta(\n        np.random.choice(np.arange(-10, 11), len(x), replace=True), \n        unit=\"D\"\n      )\n    ),\n    interest_frequency=lambda x: np.random.choice(\n      [0, 1, 2, 4, 12], len(x), replace=True\n    ),\n    coupon=lambda x: np.random.choice(\n      np.arange(0, 2.1, 0.1), len(x), replace=True\n    )\n  )\n  .assign(\n    last_interest_date=lambda x: (\n      x[[\"maturity\", \"offering_date\", \"dated_date\"]].max(axis=1)\n    ),\n    issue_id=lambda x: x.index+1,\n    issuer_id=lambda x: np.random.choice(\n      np.arange(1, 251), len(x), replace=True\n    ),\n    sic_code=lambda x: (np.random.choice(\n      np.arange(1, 10)*1000, len(x), replace=True)\n    ).astype(str)\n  )\n)\n\n(fisd_dummy\n  .to_sql(name=\"fisd\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\n\n\nDummy trace_enhanced data\nFinally, we create a dummy bond transaction data for the fictional CUSIPs of the dummy fisd data. We take the date range that we also analyze in the book and ensure that we have at least five transactions per day to fulfill a filtering step in the book. \n\nnumber_of_bonds = 100\nstart_date = pd.Timestamp(\"2014-01-01\")\nend_date = pd.Timestamp(\"2016-11-30\")\n\nbonds_panel = pd.DataFrame({\n  \"cusip_id\": np.tile(\n    fisd_dummy[\"complete_cusip\"], \n    (end_date-start_date).days+1\n  ),\n  \"trd_exctn_dt\": np.repeat(\n    pd.date_range(start_date, end_date), len(fisd_dummy)\n  )\n})\n\ntrace_enhanced_dummy = (pd.concat([bonds_panel]*5)\n  .assign(\n    trd_exctn_tm = lambda x: pd.to_datetime(\n      x[\"trd_exctn_dt\"].astype(str)+\" \" +\n      np.random.randint(0, 24, size=len(x)).astype(str)+\":\" +\n      np.random.randint(0, 60, size=len(x)).astype(str)+\":\" +\n      np.random.randint(0, 60, size=len(x)).astype(str)\n    ),\n    rptd_pr=np.random.uniform(10, 200, len(bonds_panel)*5),\n    entrd_vol_qt=1000*np.random.choice(\n      range(1,21), len(bonds_panel)*5, replace=True\n    ),\n    yld_pt=np.random.uniform(-10, 10, len(bonds_panel)*5),\n    rpt_side_cd=np.random.choice(\n      [\"B\", \"S\"], len(bonds_panel)*5, replace=True\n    ),\n    cntra_mp_id=np.random.choice(\n      [\"C\", \"D\"], len(bonds_panel)*5, replace=True\n    )\n  )\n  .reset_index(drop=True)\n)\n\n(trace_enhanced_dummy\n  .to_sql(name=\"trace_enhanced\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\nAs stated in the introduction, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout this book.",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html",
    "href": "python/wrds-crsp-and-compustat.html",
    "title": "WRDS, CRSP, and Compustat",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nThis chapter shows how to connect to Wharton Research Data Services (WRDS), a popular provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics, CRSP and Compustat. Unfortunately, this data is not freely available, but most students and researchers typically have access to WRDS through their university libraries. Assuming that you have access to WRDS, we show you how to prepare and merge the databases and store them in the SQLite database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the WRDS database.\nIf you don’t have access to WRDS but still want to run the code in this book, we refer to WRDS Dummy Data, where we show how to create a dummy database that contains the WRDS tables and corresponding columns. With this database at hand, all code chunks in this book can be executed with this dummy database.\nFirst, we load the Python packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them. The last two packages are used for plotting.\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\nimport sqlite3\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format, percent_format\nfrom datetime import datetime\nWe use the same date range as in the previous chapter to ensure consistency. However, we have to use the date format that the WRDS database expects.\nstart_date = \"01/01/1960\"\nend_date = \"12/31/2024\"",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#accessing-wrds",
    "href": "python/wrds-crsp-and-compustat.html#accessing-wrds",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Accessing WRDS",
    "text": "Accessing WRDS\nWRDS is the most widely used source for asset and firm-specific financial data used in academic settings. WRDS is a data platform that provides data validation, flexible delivery options, and access to many different data sources. The data at WRDS is also organized in an SQL database, although they use the PostgreSQL engine. This database engine is just as easy to handle with Python as SQL. We use the sqlalchemy package to establish a connection to the WRDS database because it already contains a suitable driver.1\n\nfrom sqlalchemy import create_engine\n\nTo establish a connection to WRDS, you use the function create_engine() with a connection string that specifies the WRDS server and your login credentials. We defined environment variables for the purpose of this book because we obviously do not want (and are not allowed) to share our credentials with the rest of the world. See Setting Up Your Environment for information about why and how to create an .env-file that can be loaded with load_dotenv(). Alternatively, you can replace os.getenv('WRDS_USER) and os.getenv('WRDS_PASSWORD') with your own credentials (but be careful not to share them with others or the public).\nAdditionally, you have to use two-factor authentication since May 2023 when establishing a remote connection to WRDS. You have two choices to provide the additional identification. First, if you have Duo Push enabled for your WRDS account, you will receive a push notification on your mobile phone when trying to establish a connection with the code below. Upon accepting the notification, you can continue your work. Second, you can log in to a WRDS website that requires two-factor authentication with your username and the same IP address. Once you have successfully identified yourself on the website, your username-IP combination will be remembered for 30 days, and you can comfortably use the remote connection below.\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nconnection_string = (\n  \"postgresql+psycopg2://\"\n f\"{os.getenv('WRDS_USER')}:{os.getenv('WRDS_PASSWORD')}\"\n  \"@wrds-pgdata.wharton.upenn.edu:9737/wrds\"\n)\n\nwrds = create_engine(connection_string, pool_pre_ping=True)\n\nYou can also use the tidyfinance package to set the login credentials and create a connection.\n\ntf.set_wrds_credentials()\nwrds = tf.get_wrds_connection()\n\nThe remote connection to WRDS is very useful. Yet, the database itself contains many different tables. You can check the WRDS homepage to identify the table’s name you are looking for (if you go beyond our exposition).",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "href": "python/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Downloading and Preparing CRSP",
    "text": "Downloading and Preparing CRSP\nThe Center for Research in Security Prices (CRSP) provides the most widely used data for US stocks. We use the wrds engine object that we just created to first access monthly CRSP return data. Actually, we need two tables to get the desired data: (i) the CRSP monthly security file (msf), and (ii) the historical identifying information (stksecurityinfohist).\nWe use the two remote tables to fetch the data we want to put into our local database. Just as above, the idea is that we let the WRDS database do all the work and just download the data that we actually need. We apply common filters and data selection criteria to narrow down our data of interest: (i) we use only stock prices from NYSE, Amex, and NASDAQ (primaryexch %in% c(\"N\", \"A\", \"Q\")) when or after issuance (conditionaltype %in% c(\"RW\", \"NW\")) for actively traded stocks (tradingstatusflg == \"A\")2, (ii) we keep only data in the time windows of interest, (iii) we keep only US-listed stocks as identified via no special share types (sharetype = 'NS'), security type equity (securitytype = 'EQTY'), security sub type common stock (securitysubtype = 'COM'), issuers that are a corporation (issuertype %in% c(\"ACOR\", \"CORP\")), and (iv) we keep only months within permno-specific start dates (secinfostartdt) and end dates (secinfoenddt). As of July 2022, there is no need to additionally download delisting information since it is already contained in the most recent version of msf (see our blog post about CRSP 2.0 for more information). Additionally, the industry information in stksecurityinfohist records the historic industry and should be used instead of the one stored under same variable name in msf_v2.\n\ncrsp_monthly_query = (\n  \"SELECT msf.permno, date_trunc('month', msf.mthcaldt)::date AS date, \"\n         \"msf.mthret AS ret, msf.shrout, msf.mthprc AS altprc, \"\n         \"ssih.primaryexch, ssih.siccd \"\n    \"FROM crsp.msf_v2 AS msf \"\n    \"INNER JOIN crsp.stksecurityinfohist AS ssih \"\n    \"ON msf.permno = ssih.permno AND \"\n       \"ssih.secinfostartdt &lt;= msf.mthcaldt AND \"\n       \"msf.mthcaldt &lt;= ssih.secinfoenddt \"\n   f\"WHERE msf.mthcaldt BETWEEN '{start_date}' AND '{end_date}' \"\n          \"AND ssih.sharetype = 'NS' \"\n          \"AND ssih.securitytype = 'EQTY' \"  \n          \"AND ssih.securitysubtype = 'COM' \" \n          \"AND ssih.usincflg = 'Y' \" \n          \"AND ssih.issuertype in ('ACOR', 'CORP') \" \n          \"AND ssih.primaryexch in ('N', 'A', 'Q') \"\n          \"AND ssih.conditionaltype in ('RW', 'NW') \"\n          \"AND ssih.tradingstatusflg = 'A'\"\n)\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=crsp_monthly_query,\n    con=wrds,\n    dtype={\"permno\": int, \"siccd\": int},\n    parse_dates={\"date\"})\n  .assign(shrout=lambda x: x[\"shrout\"]*1000)\n)\n\nNow, we have all the relevant monthly return data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters.\nThe first additional variable we create is market capitalization (mktcap), which is the product of the number of outstanding shares (shrout) and the last traded price in a month (prc). Note that in contrast to returns (ret), these two variables are not adjusted ex-post for any corporate actions like stock splits. Therefore, if you want to use a stock’s price, you need to adjust it with a cumulative adjustment factor. We also keep the market cap in millions of USD just for convenience, as we do not want to print huge numbers in our figures and tables. In addition, we set zero market capitalization to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).\n\ncrsp_monthly = (crsp_monthly\n  .assign(mktcap=lambda x: x[\"shrout\"]*x[\"altprc\"]/1000000)\n  .assign(mktcap=lambda x: x[\"mktcap\"].replace(0, np.nan))\n)\n\nThe next variable we frequently use is the one-month lagged market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly CRSP data.\n\nmktcap_lag = (crsp_monthly\n  .assign(\n    date=lambda x: x[\"date\"]+pd.DateOffset(months=1),\n    mktcap_lag=lambda x: x[\"mktcap\"]\n  )\n  .get([\"permno\", \"date\", \"mktcap_lag\"])\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(mktcap_lag, how=\"left\", on=[\"permno\", \"date\"])\n)\n\nNext, we transform primary listing exchange codes to explicit exchange names.\n\ndef assign_exchange(primaryexch):\n    if primaryexch == \"N\":\n        return \"NYSE\"\n    elif primaryexch == \"A\":\n        return \"AMEX\"\n    elif primaryexch == \"Q\":\n        return \"NASDAQ\"\n    else:\n        return \"Other\"\n\ncrsp_monthly[\"exchange\"] = (crsp_monthly[\"primaryexch\"]\n  .apply(assign_exchange)\n)\n\nSimilarly, we transform industry codes to industry descriptions following Bali, Engle, and Murray (2016). Notice that there are also other categorizations of industries (e.g., Fama and French 1997) that are commonly used.\n\ndef assign_industry(siccd):\n    if 1 &lt;= siccd &lt;= 999:\n        return \"Agriculture\"\n    elif 1000 &lt;= siccd &lt;= 1499:\n        return \"Mining\"\n    elif 1500 &lt;= siccd &lt;= 1799:\n        return \"Construction\"\n    elif 2000 &lt;= siccd &lt;= 3999:\n        return \"Manufacturing\"\n    elif 4000 &lt;= siccd &lt;= 4899:\n        return \"Transportation\"\n    elif 4900 &lt;= siccd &lt;= 4999:\n        return \"Utilities\"\n    elif 5000 &lt;= siccd &lt;= 5199:\n        return \"Wholesale\"\n    elif 5200 &lt;= siccd &lt;= 5999:\n        return \"Retail\"\n    elif 6000 &lt;= siccd &lt;= 6799:\n        return \"Finance\"\n    elif 7000 &lt;= siccd &lt;= 8999:\n        return \"Services\"\n    elif 9000 &lt;= siccd &lt;= 9999:\n        return \"Public\"\n    else:\n        return \"Missing\"\n\ncrsp_monthly[\"industry\"] = (crsp_monthly[\"siccd\"]\n  .apply(assign_industry)\n)\n\nNext, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop the risk-free rate from our data frame. Note that we ensure excess returns are bounded by -1 from below as a return less than -100% makes no sense conceptually. Before we can adjust the returns, we have to connect to our database and load the data frame factors_ff3_monthly.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT date, rf FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n  \ncrsp_monthly = (crsp_monthly\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n  .assign(ret_excess=lambda x: x[\"ret\"]-x[\"rf\"])\n  .assign(ret_excess=lambda x: x[\"ret_excess\"].clip(lower=-1))\n  .drop(columns=[\"rf\"])\n)\n\nThe tidyfinance package provides a shortcut to implement all these processing steps from above:\n\ncrsp_monthly = tf.download_data(\n  domain=\"wrds\",\n  dataset=\"crsp_monthly\",\n  start_date=start_date,\n  end_date=end_date\n)\n\nSince excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization.\n\ncrsp_monthly = (crsp_monthly\n  .dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n)\n\nFinally, we store the monthly CRSP file in our database.\n\n(crsp_monthly\n  .to_sql(name=\"crsp_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "href": "python/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "title": "WRDS, CRSP, and Compustat",
    "section": "First Glimpse of the CRSP Sample",
    "text": "First Glimpse of the CRSP Sample\nBefore we move on to other data sources, let us look at some descriptive statistics of the CRSP sample, which is our main source for stock returns.\nFigure 1 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks. The number of stocks listed on AMEX decreased steadily over the last couple of decades. By the end of {python} crsp_monthly['date'].max().year, there were {python} len(crsp_monthly.loc[(crsp_monthly['date'] == crsp_monthly['date'].max()) & (crsp_monthly['exchange'] == 'NASDAQ'), 'permno'].unique()) stocks with a primary listing on NASDAQ, {python} len(crsp_monthly.loc[(crsp_monthly['date'] == crsp_monthly['date'].max()) & (crsp_monthly['exchange'] == 'NYSE'), 'permno'].unique()) on NYSE, and {python} len(crsp_monthly.loc[(crsp_monthly['date'] == crsp_monthly['date'].max()) & (crsp_monthly['exchange'] == 'AMEX'), 'permno'].unique()) on AMEX. \n\nsecurities_per_exchange = (crsp_monthly\n  .groupby([\"exchange\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nsecurities_per_exchange_figure = (\n  ggplot(\n    securities_per_exchange, \n    aes(x=\"date\", y=\"n\", color=\"exchange\", linetype=\"exchange\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly number of securities by listing exchange\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\nsecurities_per_exchange_figure.show()\n\n\n\n\n\n\n\nFigure 1: The figure shows the monthly number of stocks in the CRSP sample listed at each of the US exchanges.\n\n\n\n\n\nNext, we look at the aggregate market capitalization grouped by the respective listing exchanges in Figure 2. To ensure that we look at meaningful data that is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange. All values in Figure 2 are in terms of the end of 2024 USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.\n\ncpi_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM cpi_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nmarket_cap_per_exchange = (crsp_monthly\n  .merge(cpi_monthly, how=\"left\", on=\"date\")\n  .groupby([\"date\", \"exchange\"])\n  .apply(\n    lambda group: pd.Series({\n      \"mktcap\": group[\"mktcap\"].sum()/group[\"cpi\"].mean()\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_exchange_figure = (\n  ggplot(\n    market_cap_per_exchange, \n    aes(x=\"date\", y=\"mktcap/1000\", color=\"exchange\", linetype=\"exchange\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly market cap by listing exchange in billions of Dec 2024 USD\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\nmarket_cap_per_exchange_figure.show()\n\n\n\n\n\n\n\nFigure 2: The figure shows the monthly market capitalization by listing exchange. Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the horizontal axis reflect the buying power of billion USD in December 2024.\n\n\n\n\n\nNext, we look at the same descriptive statistics by industry. Figure 3 plots the number of stocks in the sample for each of the SIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.\n\nsecurities_per_industry = (crsp_monthly\n  .groupby([\"industry\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nlinetypes = [\"-\", \"--\", \"-.\", \":\"]\nn_industries = securities_per_industry[\"industry\"].nunique()\n\nsecurities_per_industry_figure = (\n  ggplot(\n    securities_per_industry, \n    aes(x=\"date\", y=\"n\", color=\"industry\", linetype=\"industry\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly number of securities by industry\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n  + scale_linetype_manual(\n      values=[linetypes[l % len(linetypes)] for l in range(n_industries)]\n    ) \n)\nsecurities_per_industry_figure.show()\n\n\n\n\n\n\n\nFigure 3: The figure shows the monthly number of stocks in the CRSP sample associated with different industries.\n\n\n\n\n\nWe also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in Figure 4. All values are again in terms of billions of end of 2024 USD. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.\n\nmarket_cap_per_industry = (crsp_monthly\n  .merge(cpi_monthly, how=\"left\", on=\"date\")\n  .groupby([\"date\", \"industry\"])\n  .apply(\n    lambda group: pd.Series({\n      \"mktcap\": (group[\"mktcap\"].sum()/group[\"cpi\"].mean())\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_industry_figure = (\n  ggplot(\n    market_cap_per_industry, \n    aes(x=\"date\", y=\"mktcap/1000\", color=\"industry\", linetype=\"industry\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly market cap by industry in billions of Dec 2024 USD\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n  + scale_linetype_manual(\n      values=[linetypes[l % len(linetypes)] for l in range(n_industries)]\n    ) \n)\nmarket_cap_per_industry_figure.show()\n\n\n\n\n\n\n\nFigure 4: The figure shows the total Market capitalization in billion USD, adjusted for consumer price index changes such that the values on the y-axis reflect the buying power of billion USD in December 2024.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#daily-crsp-data",
    "href": "python/wrds-crsp-and-compustat.html#daily-crsp-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Daily CRSP Data",
    "text": "Daily CRSP Data\nBefore we turn to accounting data, we provide a proposal for downloading daily CRSP data with the same filters used for the monthly data (i.e., using information from stksecurityinfohist). While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can exceed 20 GB. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire dataset to your local database), and in our experience, the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your Python session.\nThere is a solution to this challenge. As with many big data problems, you can split up the big task into several smaller tasks that are easier to handle. That is, instead of downloading data about all stocks at once, download the data in small batches of stocks consecutively. Such operations can be implemented in for-loops, where we download, prepare, and store the data for a small number of stocks in each iteration. This operation might nonetheless take around 5 minutes, depending on your internet connection. To keep track of the progress, we create ad-hoc progress updates using print(). Notice that we also use the method to_sql() here with the option to append the new data to an existing table, when we process the second and all following batches. As for the monthly CRSP data, there is no need to adjust for delisting returns in the daily CRSP data since July 2022.\n\nfactors_ff3_daily = pd.read_sql(\n  sql=\"SELECT * FROM factors_ff3_daily\", \n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\npermnos = pd.read_sql(\n  sql=\"SELECT DISTINCT permno FROM crsp.stksecurityinfohist\", \n  con=wrds,\n  dtype={\"permno\": int}\n)\n\npermnos = list(permnos[\"permno\"].astype(str))\n  \nbatch_size = 500\nbatches = np.ceil(len(permnos)/batch_size).astype(int)\n\nfor j in range(1, batches+1):  \n    \n  permno_batch = permnos[\n    ((j-1)*batch_size):(min(j*batch_size, len(permnos)))\n  ]\n  \n  permno_batch_formatted = (\n    \", \".join(f\"'{permno}'\" for permno in permno_batch)\n  )\n  permno_string = f\"({permno_batch_formatted})\"\n  \n  crsp_daily_sub_query = (\n    \"SELECT dsf.permno, dlycaldt AS date, dlyret AS ret \"\n      \"FROM crsp.dsf_v2 AS dsf \"\n      \"INNER JOIN crsp.stksecurityinfohist AS ssih \"\n      \"ON dsf.permno = ssih.permno AND \"\n         \"ssih.secinfostartdt &lt;= dsf.dlycaldt AND \"\n         \"dsf.dlycaldt &lt;= ssih.secinfoenddt \"\n      f\"WHERE dsf.permno IN {permno_string} \"\n           f\"AND dlycaldt BETWEEN '{start_date}' AND '{end_date}' \"\n            \"AND ssih.sharetype = 'NS' \"\n            \"AND ssih.securitytype = 'EQTY' \"  \n            \"AND ssih.securitysubtype = 'COM' \" \n            \"AND ssih.usincflg = 'Y' \" \n            \"AND ssih.issuertype in ('ACOR', 'CORP') \" \n            \"AND ssih.primaryexch in ('N', 'A', 'Q') \"\n            \"AND ssih.conditionaltype in ('RW', 'NW') \"\n            \"AND ssih.tradingstatusflg = 'A'\"\n  )\n    \n  crsp_daily_sub = (pd.read_sql_query(\n      sql=crsp_daily_sub_query,\n      con=wrds,\n      dtype={\"permno\": int},\n      parse_dates={\"date\"}\n    )\n    .dropna()\n   )\n\n  if not crsp_daily_sub.empty:\n    \n      crsp_daily_sub = (crsp_daily_sub\n        .merge(factors_ff3_daily[[\"date\", \"rf\"]], \n               on=\"date\", how=\"left\")\n        .assign(\n          ret_excess = lambda x: \n            ((x[\"ret\"] - x[\"rf\"]).clip(lower=-1))\n        )\n        .get([\"permno\", \"date\", \"ret_excess\"])\n      )\n        \n      if j == 1:\n        if_exists_string = \"replace\"\n      else:\n        if_exists_string = \"append\"\n\n      crsp_daily_sub.to_sql(\n        name=\"crsp_daily\", \n        con=tidy_finance, \n        if_exists=if_exists_string, \n        index=False\n      )\n            \n  print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")\n\nEventually, we end up with more than 72 million rows of daily return data. Note that we only store the identifying information that we actually need, namely permno and date alongside the excess returns. We thus ensure that our local database contains only the data that we actually use.\nTo download the daily CRSP data via the tidyfinance package, you can call:\n\ncrsp_daily = tf.download_data(\n  domain=\"wrds\",\n  dataset=\"crsp_daily\",\n  start_date = start_date,\n  end_date = end_date\n)\n\nNote that you need at least 16 GB of memory to hold all the daily CRSP returns in memory. We hence recommend to use loop the function over different date periods and store the results.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "href": "python/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Preparing Compustat Data",
    "text": "Preparing Compustat Data\nFirm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters. The commonly used source for firm financial information is Compustat provided by S&P Global Market Intelligence, which is a global data vendor that provides financial, statistical, and market information on active and inactive companies throughout the world. For US and Canadian companies, annual history is available back to 1950 and quarterly as well as monthly histories date back to 1962.\nTo access Compustat data, we can again tap WRDS, which hosts the funda table that contains annual firm-level information on North American companies. We follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, which includes companies that are primarily involved in manufacturing, services, and other non-financial business activities,3, (ii) in the standard format (i.e., consolidated information in standard presentation), (iii) reported in USD,4 and (iv) only data in the desired time window.\n\ncompustat_query = (\n  \"SELECT gvkey, datadate, seq, ceq, at, lt, txditc, txdb, itcb,  pstkrv, \"\n         \"pstkl, pstk, capx, oancf, sale, cogs, xint, xsga \"\n    \"FROM comp.funda \"\n    \"WHERE indfmt = 'INDL' \"\n          \"AND datafmt = 'STD' \"\n          \"AND consol = 'C' \"\n          \"AND curcd = 'USD' \"\n         f\"AND datadate BETWEEN '{start_date}' AND '{end_date}'\"\n)\n\ncompustat = pd.read_sql_query(\n  sql=compustat_query,\n  con=wrds,\n  dtype={\"gvkey\": str},\n  parse_dates={\"datadate\"}\n)\n\nNext, we calculate the book value of preferred stock and equity be and the operating profitability op inspired by the variable definitions in Kenneth French’s data library. Note that we set negative or zero equity to missing, which is a common practice when working with book-to-market ratios (see Fama and French 1992 for details).\n\ncompustat = (compustat\n  .assign(\n    be=lambda x: \n      (x[\"seq\"].combine_first(x[\"ceq\"]+x[\"pstk\"])\n       .combine_first(x[\"at\"]-x[\"lt\"])+\n       x[\"txditc\"].combine_first(x[\"txdb\"]+x[\"itcb\"]).fillna(0)-\n       x[\"pstkrv\"].combine_first(x[\"pstkl\"])\n       .combine_first(x[\"pstk\"]).fillna(0))\n  )\n  .assign(\n    be=lambda x: x[\"be\"].apply(lambda y: np.nan if y &lt;= 0 else y)\n  )\n  .assign(\n    op=lambda x: \n      ((x[\"sale\"]-x[\"cogs\"].fillna(0)- \n        x[\"xsga\"].fillna(0)-x[\"xint\"].fillna(0))/x[\"be\"])\n  )\n)\n\nWe keep only the last available information for each firm-year group (by using the tail(1) pandas function for each group). Note that datadate defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2022). Therefore, datadate is not the date when data was made available to the public. Check out the Exercises for more insights into the peculiarities of datadate.\n\ncompustat = (compustat\n  .assign(year=lambda x: pd.DatetimeIndex(x[\"datadate\"]).year)\n  .sort_values(\"datadate\")\n  .groupby([\"gvkey\", \"year\"])\n  .tail(1)\n  .reset_index()\n)\n\nWe also compute the investment ratio (inv) according to Kenneth French’s variable definitions as the change in total assets from one fiscal year to another. Note that we again use the approach using joins as introduced with the CRSP data above to construct lagged assets.\n\ncompustat_lag = (compustat\n  .get([\"gvkey\", \"year\", \"at\"])\n  .assign(year=lambda x: x[\"year\"]+1)\n  .rename(columns={\"at\": \"at_lag\"})\n)\n\ncompustat = (compustat\n  .merge(compustat_lag, how=\"left\", on=[\"gvkey\", \"year\"])\n  .assign(inv=lambda x: x[\"at\"]/x[\"at_lag\"]-1)\n  .assign(inv=lambda x: np.where(x[\"at_lag\"] &lt;= 0, np.nan, x[\"inv\"]))\n)\n\nWith the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database.\n\n(compustat\n  .to_sql(name=\"compustat\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\nThe tidyfinance package provides a shortcut for these processing steps as well:\n\ncompustat = tf.download_data(\n  domain=\"wrds\",\n  dataset=\"compustat_annual\",\n  start_date = start_date,\n  end_date = end_date\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "href": "python/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Merging CRSP with Compustat",
    "text": "Merging CRSP with Compustat\nUnfortunately, CRSP and Compustat use different keys to identify stocks and firms. CRSP uses permno for stocks, while Compustat uses gvkey to identify firms. Fortunately, a curated matching table on WRDS allows us to merge CRSP and Compustat, so we create a connection to the CRSP-Compustat Merged table (provided by CRSP). The linking table contains links between CRSP and Compustat identifiers from various approaches. However, we need to make sure that we keep only relevant and correct links, again following the description outlined in Bali, Engle, and Murray (2016). Note also that currently active links have no end date, so we just enter the current date via the SQL verb CURRENT_DATE.\n\nccm_linking_table_query = (\n  \"SELECT lpermno AS permno, gvkey, linkdt, \"\n         \"COALESCE(linkenddt, CURRENT_DATE) AS linkenddt \"\n    \"FROM crsp.ccmxpf_linktable \"\n    \"WHERE linktype IN ('LU', 'LC') \"\n          \"AND linkprim IN ('P', 'C')\"\n)\n\nccm_linking_table = pd.read_sql_query(\n  sql=ccm_linking_table_query,\n  con=wrds,\n  dtype={\"permno\": int, \"gvkey\": str},\n  parse_dates={\"linkdt\", \"linkenddt\"}\n)\n\nTo fetch these links via tidyfinance, you can call:\n\nccm_links = tf.download_data(domain=\"wrds\", dataset=\"ccm_links\")\n\nWe use these links to create a new table with a mapping between stock identifier, firm identifier, and month. We then add these links to the Compustat gvkey to our monthly stock data.\n\nccm_links = (crsp_monthly\n  .merge(ccm_linking_table, how=\"inner\", on=\"permno\")\n  .query(\"~gvkey.isnull() & (date &gt;= linkdt) & (date &lt;= linkenddt)\")\n  .get([\"permno\", \"gvkey\", \"date\"])\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(ccm_links, how=\"left\", on=[\"permno\", \"date\"])\n)\n\nAs the last step, we update the previously prepared monthly CRSP file with the linking information in our local database.\n\n(crsp_monthly\n  .to_sql(name=\"crsp_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\nBefore we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, Figure 5 plots the share of securities with book equity values for each exchange. It turns out that the coverage is pretty bad for AMEX- and NYSE-listed stocks in the 1960s but hovers around 80 percent for all periods thereafter. We can ignore the erratic coverage of securities that belong to the other category since there is only a handful of them anyway in our sample.\n\nshare_with_be = (crsp_monthly\n  .assign(year=lambda x: pd.DatetimeIndex(x[\"date\"]).year)\n  .sort_values(\"date\")\n  .groupby([\"permno\", \"year\"])\n  .tail(1)\n  .reset_index()\n  .merge(compustat, how=\"left\", on=[\"gvkey\", \"year\"])\n  .groupby([\"exchange\", \"year\"])\n  .apply(\n    lambda x: pd.Series({\n    \"share\": x[\"permno\"][~x[\"be\"].isnull()].nunique()/x[\"permno\"].nunique()\n    })\n  )\n  .reset_index()\n)\n\nshare_with_be_figure = (\n  ggplot(\n    share_with_be, \n    aes(x=\"year\", y=\"share\", color=\"exchange\", linetype=\"exchange\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Share of securities with book equity values by exchange\"\n    )\n  + scale_y_continuous(labels=percent_format())\n  + coord_cartesian(ylim=(0, 1))\n)\nshare_with_be_figure.show()\n\n\n\n\n\n\n\nFigure 5: The figure shows the end-of-year share of securities with book equity values by listing exchange.\n\n\n\n\n\nThe difference arises from the distinct coverage of the two databases. CRSP focuses on stock prices from the major US exchanges, capturing a narrower universe of firms. In contrast, Compustat provides financial statement data for a broader set of companies, including all US firms that file 10‑K reports, many Canadian firms, and those listed on major or regional exchanges, traded over‑the‑counter, or even companies with a notable amount of publicly issued debt.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#key-takeaways",
    "href": "python/wrds-crsp-and-compustat.html#key-takeaways",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nWRDS provides secure access to essential financial databases like CRSP and Compustat, which are critical for empirical finance research.\nCRSP data provides return, market capitalization and industry data for US common stocks listed on NYSE, NASDAQ, or AMEX.\nCompustat provides firm-level accounting data such as book equity, profitability, and investment.\nThe tidyfinance Python package streamlines all major data download and processing steps, making it easy to replicate and scale financial data analysis.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#exercises",
    "href": "python/wrds-crsp-and-compustat.html#exercises",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Exercises",
    "text": "Exercises\n\nCompute mkt_cap_lag using shift() rather than using joins as above. Filter out all the rows where the lag-based market capitalization measure is different from the one we computed above. Why are the two measures different?\nPlot the average market capitalization of firms for each exchange and industry, respectively, over time. What do you find?\nIn the compustat table, datadate refers to the date to which the fiscal year of a corresponding firm refers. Count the number of observations in Compustat by month of this date variable. What do you find? What does the finding suggest about pooling observations with the same fiscal year?\nGo back to the original Compustat and extract rows where the same firm has multiple rows for the same fiscal year. What is the reason for these observations?\nKeep the last observation of crsp_monthly by year and join it with the compustat table. Create the following plots: (i) aggregate book equity by exchange over time and (ii) aggregate annual book equity by industry over time. Do you notice any different patterns to the corresponding plots based on market capitalization?\nRepeat the analysis of market capitalization for book equity, which we computed from the Compustat data. Then, use the matched sample to plot book equity against market capitalization. How are these two variables related?\nBefore merging the CRSP and Compustat datasets, calculate and plot the proportion of Compustat firms that have corresponding stock return data in CRSP for each year. How does the coverage evolve over time?",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#footnotes",
    "href": "python/wrds-crsp-and-compustat.html#footnotes",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn alternative to establish a connection to WRDS is to use the WRDS-Py library. We chose to work with sqlalchemy (Bayer 2012) to show how to access PostgreSQL engines in general.↩︎\nThese three criteria jointly replicate the filter exchcd %in% c(1, 2, 3, 31, 32, 33) used for the legacy version of CRSP. If you do not want to include stocks at issuance, you can set the conditionaltype == \"RW\", which is equivalent to the restriction of exchcd %in% c(1, 2, 3) with the old CRSP format.↩︎\nCompanies that operate in the banking, insurance, or utilities sector typically report in different industry formats that reflect their specific regulatory requirements.↩︎\nCompustat also contains reports in CAD, which can lead a currency mismatch, e.g., when relating book equity to market equity.↩︎",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html",
    "href": "python/financial-statement-analysis.html",
    "title": "Financial Statement Analysis",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nFinancial statements and ratios are fundamental tools for understanding and evaluating companies. While we discuss how assets are priced in equilibrium in the previous chapter on the Capital Asset Pricing Model, this chapter examines how investors and analysts assess companies using accounting information. Financial statements serve as the primary source of standardized information about a company’s operations, financial position, and performance. Their standardization and legal requirements make them particularly valuable as all companies must file financial statements.\nBuilding on this standardized information, financial ratios transform raw accounting data into meaningful metrics that facilitate analysis across companies and over time. These ratios serve multiple purposes in both academic research and practical applications. They enable investors to benchmark companies against their peers, identify industry trends, and screen for investment opportunities. In academic finance, ratios play a crucial role in asset pricing models (e.g., the book-to-market ratio in the Fama-French three-factor model) and corporate finance (e.g., capital structure research). In many practical applications, ratios help assess a company’s financial health and performance.\nThis chapter demonstrates how to access, process, and analyze financial statements using R. We start by reviewing the financial statements balance sheet, income statement, and cash flow statement. Then, we download publicly available statements to calculate key financial ratios, implement common screening strategies, and evaluate companies. Our analysis combines theoretical frameworks with practical implementation, providing tools for both academic research and investment practice.\nFor the purpose of this chapter, we use financial statements provided by the US Securities and Exchange Commission (i.e., SEC). While the SEC provides a web interface to search filings, programmatic access to financial statements greatly facilitates systematic analyses as ours. The Financial Modeling Prep (FMP) API offers such programmatic access, which we can leverage through the R package fmpapi.\nThe FMP API’s free tier provides access to:\nNext to fmpapi, we use the following packages throughout this chapter:\nimport pandas as pd\nimport numpy as np\n\nfrom fmpapi import fmp_get\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#balance-sheet-statements",
    "href": "python/financial-statement-analysis.html#balance-sheet-statements",
    "title": "Financial Statement Analysis",
    "section": "Balance Sheet Statements",
    "text": "Balance Sheet Statements\nThe balance sheet is one of the three primary financial statements capturing a company’s financial position at a specific moment in time. The statement lists all uses (assets) and sources (liabilities and equity) of funds, which result in the fundamental accounting equation:\n\\[\\text{Assets} = \\text{Liabilities} + \\text{Equity}\\]\nThis equation reflects a core principle of accounting: a company’s resources (assets) must equal its sources of funding, whether from creditors (liabilities) or investors (shareholders’ equity). Assets represent resources that the company controls and expects to generate future economic benefits, such as cash, inventory, or equipment. Liabilities encompass all obligations to external parties, from short-term payables to long-term debt. Shareholders’ equity represents the residual claim on assets after accounting for all liabilities.\nFigure 1 provides a stylized representation of a balance sheet’s structure. The visualization highlights how assets on the left side must equal the combined claims of creditors and shareholders on the right side.\n\n\n\n\n\n\nFigure 1: A stylized representation of a balance sheet statement.\n\n\n\nThe asset side of the balance sheet typically comprises three main categories, each serving different roles in the company’s operations:\n\nCurrent assets: These are assets expected to be converted into cash or used within one operating cycle (typically one year). They include, e.g., cash and cash equivalents, short-term investments, accounts receivable (money owed by customers), and inventory (raw materials, work in progress, and finished goods).\nNon-current assets: These long-term assets support the company’s operations beyond one year like, e.g., property, plant, and equipment (PP&E), long-term investments, and other long-term assets.\nIntangible assets: These non-physical assets often represent significant value in modern companies, e.g., patents and intellectual property, trademarks and brands, and goodwill from acquisitions (a premium paid on the book value of the acquired assets). Intangible assets are usually also considered long-term assets, which means that they are included in the group of non-current assets.\n\nFigure 2 illustrates this breakdown of assets, showing how companies classify their resources.\n\n\n\n\n\n\nFigure 2: A stylized representation of a balance sheet beakdown.\n\n\n\nFigure 2 also shows the breakdown of liabilities. The liability side similarly follows a temporal classification, dividing obligations based on when they come due:\n\nCurrent liabilities: Obligations due within one year such as accounts payable, short-term debt, current portion of long-term debt, and accrued expenses.\nNon-current liabilities: Long-term obligations such as long-term debt, bonds payable, deferred tax liabilities, and pension obligations.\n\nLastly, the equity section represents ownership claims and typically consists of:\n\nRetained earnings: Accumulated profits reinvested in the business.\nCommon stock: Par value and additional paid-in capital from share issuance.\nPreferred stock: Hybrid securities with characteristics of both debt and equity.\n\nFigure 2 also depicts this equity structure, showing how companies track different forms of ownership claims.\nTo illustrate these concepts in practice, Figure 3 presents Microsoft’s balance sheet from 2023. This real-world example demonstrates how one of the world’s largest technology companies structures its financial position, reflecting both traditional elements like PP&E and modern aspects like significant intangible assets.\n\n\n\n\n\n\nFigure 3: A screenshot of the balance sheet statement of Microsoft in 2023.\n\n\n\nWhile there are more details, the basic structure is exactly the same as in the introduction above. Importantly, the balance sheet obeys the fundamental accounting equation as assets are equal to the sum of liabilities and equity. In subsequent sections, we will explore how to analyze such statements using financial ratios, particularly focusing on measures of liquidity, solvency, and efficiency.\nLet us examine Microsoft’s balance sheet statements using the fmp_get() function. This function requires three main arguments: The type of financial data to retrieve (resource), the stock ticker symbol (symbol), and additional parameters like periodicity and number of periods (params).\n\nfmp_get(\n  resource=\"balance-sheet-statement\", \n  symbol=\"MSFT\", \n  params={\"period\": \"annual\", \"limit\": 5},\n  to_pandas=True\n)\n\n\n\n\n\n\n\n\ndate\nsymbol\nreported_currency\ncik\nfiling_date\naccepted_date\nfiscal_year\nperiod\ncash_and_cash_equivalents\nshort_term_investments\n...\nadditional_paid_in_capital\naccumulated_other_comprehensive_income_loss\nother_total_stockholders_equity\ntotal_stockholders_equity\ntotal_equity\nminority_interest\ntotal_liabilities_and_total_equity\ntotal_investments\ntotal_debt\nnet_debt\n\n\n\n\n0\n2025-06-30\nMSFT\nUSD\n0000789019\n2025-07-30\n2025-07-30 16:11:40\n2025\nFY\n30242000000\n64323000000\n...\n0\n-3347000000\n0\n343479000000\n343479000000\n0\n619003000000\n79728000000\n60588000000\n30346000000\n\n\n1\n2024-06-30\nMSFT\nUSD\n0000789019\n2024-07-30\n2024-07-30 16:06:22\n2024\nFY\n18315000000\n57216000000\n...\n0\n-5590000000\n0\n268477000000\n268477000000\n0\n512163000000\n71816000000\n67127000000\n48812000000\n\n\n2\n2023-06-30\nMSFT\nUSD\n0000789019\n2023-07-27\n2023-07-27 16:01:56\n2023\nFY\n34704000000\n76552000000\n...\n0\n-6343000000\n0\n206223000000\n206223000000\n0\n411976000000\n86431000000\n59965000000\n25261000000\n\n\n3\n2022-06-30\nMSFT\nUSD\n0000789019\n2022-07-28\n2022-07-28 16:06:19\n2022\nFY\n13931000000\n90818000000\n...\n0\n-4678000000\n0\n166542000000\n166542000000\n0\n364840000000\n97709000000\n61270000000\n47339000000\n\n\n4\n2021-06-30\nMSFT\nUSD\n0000789019\n2021-07-29\n2021-07-29 16:21:55\n2021\nFY\n14224000000\n116032000000\n...\n0\n1822000000\n0\n141988000000\n141988000000\n0\n333779000000\n122016000000\n67775000000\n53551000000\n\n\n\n\n5 rows × 61 columns\n\n\n\nThe function returns a data frame containing detailed balance sheet information, with each row representing a different reporting period. This structured format makes it easy to analyze trends over time and calculate financial ratios. We can see how the data aligns with the balance sheet components we discussed earlier, from current assets like cash and receivables to long-term assets and various forms of liabilities and equity.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#income-statements",
    "href": "python/financial-statement-analysis.html#income-statements",
    "title": "Financial Statement Analysis",
    "section": "Income Statements",
    "text": "Income Statements\nWhile the balance sheet provides a snapshot of a company’s financial position at a point in time, the income statement (also called profit and loss statement or PnL) measures financial performance over a period, typically a quarter or year. It follows a hierarchical structure that progressively captures different levels of profitability:\n\nRevenue (Sales): The total income generated from goods or services sold.\nCost of goods sold (COGS): Direct costs associated with producing the goods or services (raw materials, labor, etc.).\nGross profit: Revenue minus COGS, showing the basic profitability from core operations.\nOperating expenses: Costs related to regular business operations (e.g., salaries, rent, and marketing).\nOperating income (EBIT): Earnings before interest and taxes (measures profitability from core operations before financing and tax costs), often also referred to as operating profit.\nInterest and taxes: The interest paid on debt is deducted for determining the taxable income.\nNet income: The “bottom line”, total profit after all expenses, interest, and taxes are subtracted from revenue.\n\nFigure 4 illustrates this progression from total revenue to net income, showing how various costs and expenses are subtracted to arrive at different measure of profit.\n\n\n\n\n\n\nFigure 4: A stylized representation of an income statement.\n\n\n\nConsider Microsoft’s 2023 income statement in Figure 5, which exemplifies how a leading technology company reports its financial performance:\n\n\n\n\n\n\nFigure 5: A screenshot of the income statement of Microsoft in 2023.\n\n\n\nWe can also access this data programmatically using the FMP API:\n\nfmp_get(\n  resource=\"income-statement\", \n  symbol=\"MSFT\", \n  params={\"period\": \"annual\", \"limit\": 5},\n  to_pandas=True\n)\n\n\n\n\n\n\n\n\ndate\nsymbol\nreported_currency\ncik\nfiling_date\naccepted_date\nfiscal_year\nperiod\nrevenue\ncost_of_revenue\n...\nnet_income_from_continuing_operations\nnet_income_from_discontinued_operations\nother_adjustments_to_net_income\nnet_income\nnet_income_deductions\nbottom_line_net_income\neps\neps_diluted\nweighted_average_shs_out\nweighted_average_shs_out_dil\n\n\n\n\n0\n2025-06-30\nMSFT\nUSD\n0000789019\n2025-07-30\n2025-07-30 16:11:40\n2025\nFY\n281724000000\n87831000000\n...\n101832000000\n0\n0\n101832000000\n0\n101832000000\n13.70\n13.64\n7433000000\n7465000000\n\n\n1\n2024-06-30\nMSFT\nUSD\n0000789019\n2024-07-30\n2024-07-30 16:06:22\n2024\nFY\n245122000000\n74114000000\n...\n88136000000\n0\n0\n88136000000\n0\n88136000000\n11.86\n11.80\n7431000000\n7469000000\n\n\n2\n2023-06-30\nMSFT\nUSD\n0000789019\n2023-07-27\n2023-07-27 16:01:56\n2023\nFY\n211915000000\n65863000000\n...\n72361000000\n0\n0\n72361000000\n0\n72361000000\n9.72\n9.68\n7446000000\n7472000000\n\n\n3\n2022-06-30\nMSFT\nUSD\n0000789019\n2022-07-28\n2022-07-28 16:06:19\n2022\nFY\n198270000000\n62650000000\n...\n72738000000\n0\n0\n72738000000\n0\n72738000000\n9.70\n9.65\n7496000000\n7540000000\n\n\n4\n2021-06-30\nMSFT\nUSD\n0000789019\n2021-07-29\n2021-07-29 16:21:55\n2021\nFY\n168088000000\n52232000000\n...\n61271000000\n0\n0\n61271000000\n0\n61271000000\n8.12\n8.05\n7547000000\n7608000000\n\n\n\n\n5 rows × 39 columns\n\n\n\nIn later sections, we will use income statement items to calculate important profitability ratios and examine how they compare across companies and industries. The income statement’s focus on performance complements the balance sheet’s position snapshot, providing a more complete picture of a company’s core business operations",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#cash-flow-statements",
    "href": "python/financial-statement-analysis.html#cash-flow-statements",
    "title": "Financial Statement Analysis",
    "section": "Cash Flow Statements",
    "text": "Cash Flow Statements\nThe cash flow statement complements the balance sheet and income statement by tracking the actual movement of cash through the business. While the income statement shows profitability and the balance sheet shows financial position, the cash flow statement reveals a company’s ability to generate and manage cash - a crucial aspect of every business. The statement is divided into three main categories:\n\nOperating activities: Cash generated from a company’s core business activities (i.e., net income adjusted for non-cash items like depreciation and changes in working capital).\nFinancing activities: Cash flows related to borrowing, repaying debt, issuing equity, or paying dividends.\nInvesting activities: Cash spent on or received from long-term investments, such as purchasing or selling property and equipment.\n\nFigure 6 illustrates these three categories of cash flows, which map into changes in the company’s cash balance.\n\n\n\n\n\n\nFigure 6: A stylized representation of a cash flow statement.\n\n\n\nThe statement reconciles accrual-based accounting (used in the income statement) with actual cash movements. This reconciliation is crucial because profitable companies can still face cash shortages, and unprofitable companies might maintain positive cash flow. We complement the brief introduction, by Microsoft’s 2023 cash flow statement in Figure 7.\n\n\n\n\n\n\nFigure 7: A screenshot of the cash flow statement of Microsoft in 2023.\n\n\n\nOf course, we can access this data through the FMP API:\n\nfmp_get(\n  resource=\"cash-flow-statement\", \n  symbol=\"MSFT\", \n  params={\"period\": \"annual\", \"limit\": 5},\n  to_pandas=True\n)\n\n\n\n\n\n\n\n\ndate\nsymbol\nreported_currency\ncik\nfiling_date\naccepted_date\nfiscal_year\nperiod\nnet_income\ndepreciation_and_amortization\n...\nnet_cash_provided_by_financing_activities\neffect_of_forex_changes_on_cash\nnet_change_in_cash\ncash_at_end_of_period\ncash_at_beginning_of_period\noperating_cash_flow\ncapital_expenditure\nfree_cash_flow\nincome_taxes_paid\ninterest_paid\n\n\n\n\n0\n2025-06-30\nMSFT\nUSD\n0000789019\n2025-07-30\n2025-07-30 16:11:40\n2025\nFY\n101832000000\n34153000000\n...\n-51699000000\n63000000\n11927000000\n30242000000\n18315000000\n136162000000\n-64551000000\n71611000000\n0\n0\n\n\n1\n2024-06-30\nMSFT\nUSD\n0000789019\n2024-07-30\n2024-07-30 16:06:22\n2024\nFY\n88136000000\n22287000000\n...\n-37757000000\n-210000000\n-16389000000\n18315000000\n34704000000\n118548000000\n-44477000000\n74071000000\n0\n0\n\n\n2\n2023-06-30\nMSFT\nUSD\n0000789019\n2023-07-27\n2023-07-27 16:01:56\n2023\nFY\n72361000000\n13861000000\n...\n-43935000000\n-194000000\n20773000000\n34704000000\n13931000000\n87582000000\n-28107000000\n59475000000\n0\n0\n\n\n3\n2022-06-30\nMSFT\nUSD\n0000789019\n2022-07-28\n2022-07-28 16:06:19\n2022\nFY\n72738000000\n14460000000\n...\n-58876000000\n-141000000\n-293000000\n13931000000\n14224000000\n89035000000\n-23886000000\n65149000000\n0\n0\n\n\n4\n2021-06-30\nMSFT\nUSD\n0000789019\n2021-07-29\n2021-07-29 16:21:55\n2021\nFY\n61271000000\n11686000000\n...\n-48486000000\n-29000000\n648000000\n14224000000\n13576000000\n76740000000\n-20622000000\n56118000000\n0\n0\n\n\n\n\n5 rows × 47 columns\n\n\n\nIn subsequent sections, we will use cash flow data to calculate important cash flow ratios that help assess a company’s liquidity, capital allocation efficiency, and overall financial sustainability. The combination of all three financial statements - balance sheet, income statement, and cash flow statement - provides a comprehensive view of a company’s financial health and performance.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#download-financial-statements",
    "href": "python/financial-statement-analysis.html#download-financial-statements",
    "title": "Financial Statement Analysis",
    "section": "Download Financial Statements",
    "text": "Download Financial Statements\nWe now turn to downloading and processing statements for multiple companies. The next code chunk demonstrates how to retrieve financial data for selected stocks that are supported in the free tier of FMP.\n\nsample = [\n  \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"NVDA\", \"META\", \"NFLX\", \"DIS\", \"NKE\",\n  \"WMT\", \"KO\", \"JPM\", \"BAC\", \"V\", \"XOM\", \"CVX\", \"JNJ\", \"PFE\", \"INTC\",\n  \"AMD\", \"SBUX\", \"BABA\", \"UBER\", \"CSCO\"\n]\n\nparams = {\"period\": \"annual\", \"limit\": 5}\n\nbalance_sheet_statements = pd.concat(\n  [fmp_get(\n      resource=\"balance-sheet-statement\", symbol=x, params=params, to_pandas=True\n    ) for x in sample],\n  ignore_index=True\n)\n\nincome_statements = pd.concat(\n  [fmp_get(\n      resource=\"income-statement\", symbol=x, params=params, to_pandas=True\n    ) for x in sample],\n  ignore_index=True\n)\n\ncash_flow_statements = pd.concat(\n  [fmp_get(\n      resource=\"cash-flow-statement\", symbol=x, params=params, to_pandas=True\n    ) for x in sample],\n  ignore_index=True\n)\n\nThe resulting data sets provide a foundation for cross-sectional analyses of financial ratios and trends across major U.S. companies. In the following sections, we use these data sets to calculate various financial ratios and analyze patterns in corporate financial performance.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#liquidity-ratios",
    "href": "python/financial-statement-analysis.html#liquidity-ratios",
    "title": "Financial Statement Analysis",
    "section": "Liquidity Ratios",
    "text": "Liquidity Ratios\nLiquidity ratios assess a company’s ability to meet its short-term obligations and are typically calculated using balance sheet items. These ratios are particularly important for creditors and investors concerned about a company’s short-term financial health and ability to cover immediate obligations.\nThe Current Ratio is the most basic measure of liquidity, comparing all current assets to current liabilities:\n\\[\\text{Current Ratio} = \\frac{\\text{Current Assets}}{\\text{Total Liabilities}}\\]\nA ratio above one indicates that the company has enough current assets to cover its current liabilities, which are due within one year as discussed above.\nHowever, not all current assets are equally liquid, i.e., can be easily sold to meet a company’s obligations. This aspect is reflected in the Quick Ratio:\n\\[\\text{Quick Ratio} = \\frac{\\text{Current Assets - Inventory}}{\\text{Current Liabilities}}\\] The Quick Ratio provides a more stringent measure of liquidity by excluding inventory, which is typically the least liquid current asset. Furthermore, a company without inventory for production or sale will have a difficult operating position. A ratio above one suggests strong short-term solvency without relying on selling off inventory.\nThe most conservative liquidity measure is the Cash Ratio:\n\\[\\text{Cash Ratio} = \\frac{\\text{Cash and Cash Equivalents}}{\\text{Current Liabilities}}\\]\nThis ratio focuses solely on the most liquid assets - cash and cash equivalents. While a ratio of one indicates robust liquidity, most companies maintain lower cash ratios to avoid holding excessive non-productive assets. Afterall, all that cash could also be distributed to equity or pay down costly debt.\nNext, we calculate these ratios for all stocks, focusing on three major technology companies:\n\nselected_symbols = [\"MSFT\", \"AAPL\", \"AMZN\", \"NVDA\"]\n\nbalance_sheet_statements = (balance_sheet_statements\n  .assign(\n    fiscal_year=lambda x: x[\"fiscal_year\"].astype(int),\n    current_ratio=lambda x: x[\"total_current_assets\"] / x[\"total_assets\"],\n    quick_ratio=lambda x: (x[\"total_current_assets\"] - x[\"inventory\"]) / x[\"total_current_liabilities\"],\n    cash_ratio=lambda x: x[\"cash_and_cash_equivalents\"] / x[\"total_current_liabilities\"],\n    label=lambda x: np.where(x[\"symbol\"].isin(selected_symbols), x[\"symbol\"], np.nan)\n  )\n)\n\nFigure 8 compares the three liquidity ratios across Microsoft, Apple, and Amazon for 2023. We call such an analysis a cross-sectional comparison.\n\nliquidity_ratios = (balance_sheet_statements\n  .query(\"fiscal_year == 2023 & label.notna()\")\n  .get([\"symbol\", \"current_ratio\", \"quick_ratio\", \"cash_ratio\"])\n  .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n  .assign(\n    name=lambda x: x[\"name\"].str.replace(\"_\", \" \").str.title()\n  )\n)\n\nliquidity_ratios_figure = (\n  ggplot(\n    liquidity_ratios, \n    aes(y=\"value\", x=\"name\", fill=\"symbol\")\n  )\n  + geom_col(position=\"dodge\")\n  + coord_flip()\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", y=\"\", fill=\"\",\n      title=\"Liquidity ratios for selected stocks for 2023\"\n    )\n)\nliquidity_ratios_figure.show()\n\n\n\n\n\n\n\nFigure 8: Liquidity ratios are based on financial statements provided through the FMP API.\n\n\n\n\n\nWhile we are not commenting on the ratios in detail here, the liquidity ratios for Microsoft, Apple, and Amazon in 2023 reveal distinct patterns. Generally, higher liquidity ratios signal a more convervative approach by holding larger liquidity buffers in the company.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#leverage-ratios",
    "href": "python/financial-statement-analysis.html#leverage-ratios",
    "title": "Financial Statement Analysis",
    "section": "Leverage Ratios",
    "text": "Leverage Ratios\nLeverage ratios assess a company’s capital structure, in particular, its mix between debt and equity. These metrics are crucial for understanding the company’s financial risk and long-term solvency. We examine three key leverage measures.\nThe debt-to-equity ratio indicates how much a company is financing its operations through debt versus shareholders’ equity:\n\\[\\text{Debt-to-Equity} = \\frac{\\text{Total Debt}}{\\text{Total Equity}}\\]\nThe debt-to-asset ratio shows the percentage of assets financed through debt:\n\\[\\text{Debt-to-Asset} = \\frac{\\text{Total Debt}}{\\text{Total Assets}}\\]\nInterest coverage measures a company’s ability to meet interest payments:\n\\[\\text{Interest Coverage} = \\frac{\\text{EBIT}}{\\text{Interest Expense}}\\]\nLet’s calculate these ratios for our sample of companies:\n\nbalance_sheet_statements = balance_sheet_statements.assign(\n  debt_to_equity=lambda x: x[\"total_debt\"] / x[\"total_equity\"],\n  debt_to_asset=lambda x: x[\"total_debt\"] / x[\"total_assets\"]\n)\n\nincome_statements = income_statements.assign(\n  fiscal_year=lambda x: x[\"fiscal_year\"].astype(int),\n  interest_coverage=lambda x: x[\"operating_income\"] / x[\"interest_expense\"],\n  label=lambda x: np.where(x[\"symbol\"].isin(selected_symbols), x[\"symbol\"], np.nan)\n)\n\nFigure 9 tracks the evolution of debt-to-asset ratios for Microsoft, Apple, and Amazon over time:\n\ndebt_to_asset = (balance_sheet_statements\n  .query(\"symbol in @selected_symbols\")\n)\n\ndebt_to_asset_figure = (\n  ggplot(\n    debt_to_asset,\n    aes(x=\"fiscal_year\", y=\"debt_to_asset\", color=\"symbol\")\n  )\n  + geom_line(size=1)\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", y=\"\", color=\"\",\n      title=\"Debt-to-asset ratios of selected stocks between 2020 and 2024\"\n    ) \n)\ndebt_to_asset_figure.show()\n\n\n\n\n\n\n\nFigure 9: Debt-to-asset ratios are based on financial statements provided through the FMP API.\n\n\n\n\n\nThe evolution of debt-to-asset ratios among these major technology companies reveals distinct capital structure strategies and their changes over time. While Apple and Microsoft reduced leverage over time, Amazon has maintained its leverage level.\nFigure 10 provides a cross-sectional view of debt-to-asset ratios across all stocks in 2023.\n\nselected_colors = [\"#F21A00\", \"#EBCC2A\", \"#78B7C5\", \"#3B9AB2\", \"lightgrey\"]\n\ndebt_to_asset_comparison = (balance_sheet_statements\n  .query(\"fiscal_year == 2023\")\n)\n\ndebt_to_asset_comparison[\"symbol\"] = pd.Categorical(\n  debt_to_asset_comparison[\"symbol\"],\n  categories=debt_to_asset_comparison.sort_values(\"debt_to_asset\")[\"symbol\"],\n  ordered=True\n)\n\ndebt_to_asset_comparison_figure = (\n  ggplot(\n    debt_to_asset_comparison,\n    aes(y=\"debt_to_asset\", x=\"symbol\", fill=\"label\")\n  )\n  + geom_col()\n  + coord_flip()\n  + scale_y_continuous(labels=percent_format())\n  + scale_fill_manual(selected_colors) \n  + labs(\n      x=\"\", y=\"\", fill=\"\",\n      title=\"Debt-to-asset ratios of selected stocks in 2023\"\n    ) \n  + theme(legend_position=\"none\")\n)\ndebt_to_asset_comparison_figure.show()\n\n\n\n\n\n\n\nFigure 10: Debt-to-asset ratios are based on financial statements provided through the FMP API.\n\n\n\n\n\nFigure 11 reveals the relationship between companies’ debt levels and their ability to service that debt.\n\ninterest_coverage = (income_statements\n  .query(\"fiscal_year == 2023\")\n  .get([\"symbol\", \"fiscal_year\", \"interest_coverage\"])\n  .merge(balance_sheet_statements, on=[\"symbol\", \"fiscal_year\"], how=\"left\")\n)\n\ninterest_coverage_figure = (\n  ggplot(\n    interest_coverage,\n    aes(x=\"debt_to_asset\", y=\"interest_coverage\", color=\"label\")\n  ) \n  + geom_point(size=2)\n  + geom_label(aes(label=\"label\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + scale_color_manual(values=selected_colors)\n  + labs(\n      x=\"Debt-to-Asset\", y=\"Interest Coverage\",\n      title=\"Debt-to-asset ratios and interest coverages for selected stocks\"\n    )\n  + theme(legend_position=\"none\")\n)\ninterest_coverage_figure.show()\n\n\n\n\n\n\n\nFigure 11: Debt-to-asset ratios and interest coverages are based on financial statements provided through the FMP API.\n\n\n\n\n\nThe scatter plot suggests that companies with higher debt-to-asset ratios tend to have lower interest coverage ratios, though there’s considerable variation in this relation. Quantification of this relation is left as an exercise.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#efficiency-ratios",
    "href": "python/financial-statement-analysis.html#efficiency-ratios",
    "title": "Financial Statement Analysis",
    "section": "Efficiency Ratios",
    "text": "Efficiency Ratios\nEfficiency ratios measure how a company utilizes its assets and manages its operations. These metrics help us to understand operational performance and management effectiveness, particularly in how well a company converts its various assets into revenue and profit.\nAsset Turnover measures how efficiently a company uses its total assets to generate revenue:\n\\[\\text{Asset Turnover} = \\frac{\\text{Revenue}}{\\text{Total Assets}}\\]\nA higher ratio indicates more efficient use of assets in generating sales. However, this ratio typically varies significantly across industries - retail companies often have higher turnover ratios due to lower asset requirements, while manufacturing companies might show lower ratios due to substantial fixed asset investments. Such industry-specifics show the importance of cross-sectional comparisons, when making decisions based on data.\nInventory turnover indicates how many times a company’s inventory is sold and replaced over a period:\n\\[\\text{Inventory Turnover} = \\frac{\\text{COGS}}{\\text{Inventory}}\\]\nHigher inventory turnover suggests more efficient inventory management and working capital utilization. However, extremely high ratios might indicate potential stockouts, while very low ratios could suggest obsolete inventory or overinvestment in working capital.\nReceivables turnover measures how effectively a company collects payments from customers:\n\\[\\text{Receivables Turnover} = \\frac{\\text{Revenue}}{\\text{Accounts Receivable}}\\] A higher ratio indicates more efficient credit and collection processes, though this must be balanced against the potential impact on sales from overly restrictive credit policies.\nHere is how we can calculate these efficiency metrics across our sample of selected stocks:\n\ncombined_statements = (balance_sheet_statements\n  .get(\n    [\"symbol\", \"fiscal_year\", \"label\", \"current_ratio\", \"quick_ratio\", \n     \"cash_ratio\", \"debt_to_equity\", \"debt_to_asset\", \"total_assets\", \n     \"total_equity\"]\n  )\n  .merge(\n    (income_statements\n      .get([\"symbol\", \"fiscal_year\", \"interest_coverage\", \"revenue\", \n            \"cost_of_revenue\", \"selling_general_and_administrative_expenses\", \n            \"interest_expense\",\"gross_profit\", \"net_income\"])\n    ),\n    on=[\"symbol\", \"fiscal_year\"],\n    how=\"left\"\n  )\n  .merge(\n    (cash_flow_statements\n      .assign(fiscal_year=lambda x: x[\"fiscal_year\"].astype(int))\n      .get([\"symbol\", \"fiscal_year\", \"inventory\", \"accounts_receivables\"])\n    ),\n    on=[\"symbol\", \"fiscal_year\"],\n    how=\"left\"\n  )\n)\n\ncombined_statements = (combined_statements\n  .assign(\n    asset_turnover=lambda x: x[\"revenue\"] / x[\"total_assets\"],\n    inventory_turnover=lambda x: x[\"cost_of_revenue\"] / x[\"inventory\"],\n    receivables_turnover=lambda x: x[\"revenue\"] / x[\"accounts_receivables\"]\n  )\n)\n\nWe leave the visualization and interpretation of these figures as an exercise and move on the the last category of financial ratios.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#profitability-ratios",
    "href": "python/financial-statement-analysis.html#profitability-ratios",
    "title": "Financial Statement Analysis",
    "section": "Profitability Ratios",
    "text": "Profitability Ratios\nProfitability ratios evaluate a company’s ability to generate earnings relative to its revenue, assets, and equity. These metrics are fundamental to investment analysis as they directly measure a company’s operational efficiency and financial success.\nThe gross margin measures what percentage of revenue remains after accounting for the direct costs of producing goods or services:\n\\[\\text{Gross Margin} = \\frac{\\text{Gross Profit}}{\\text{Revenue}}\\]\nA higher gross margin indicates stronger pricing power or more efficient production processes. This metric is particularly useful for comparing companies within the same industry, as it reveals their relative efficiency in core operations before accounting for operating expenses and other costs.\nThe profit margin reveals what percentage of revenue ultimately becomes net income:\n\\[\\text{Profit Margin} = \\frac{\\text{Net Income}}{\\text{Revenue}}\\] This comprehensive profitability measure accounts for all costs, expenses, interest, and taxes. A higher profit margin suggests more effective overall cost management and stronger competitive position, though optimal margins vary significantly across industries.\nReturn on Equity (ROE) measures how efficiently a company uses shareholders’ investments to generate profits:\n\\[\\text{After-Tax ROE} = \\frac{\\text{Net Income}}{\\text{Total Equity}}\\] This metric is particularly important for investors as it directly measures the return on their invested capital (at least in terms of book value of equity). A higher ROE indicates more effective use of shareholders’ equity, though it must be considered alongside leverage ratios since high debt levels can artificially inflate ROE.\nThe next code chunk calculates these profitability metrics for our sample of companies, allowing us to analyze how different firms convert their revenue into various levels of profit and return on investment.\n\ncombined_statements = combined_statements.assign(\n  gross_margin=lambda x: x[\"gross_profit\"] / x[\"revenue\"],\n  profit_margin=lambda x: x[\"net_income\"] / x[\"revenue\"],\n  after_tax_roe=lambda x: x[\"net_income\"] / x[\"total_equity\"]\n)\n\nFigure 12 shows the patterns in gross margin trends among Microsoft, Apple, and Amazon between 2019 and 2023.\n\ngross_margins = (combined_statements\n  .query(\"symbol in @selected_symbols\")\n)\n  \ngross_margins_figure = (\n  ggplot(\n    gross_margins,\n    aes(x=\"fiscal_year\", y=\"gross_margin\", color=\"symbol\")\n  )\n  + geom_line()\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", y=\"\", color=\"\",\n      title=\"Gross margins for selected stocks between 2019 and 2023\"\n  )\n)\ngross_margins_figure.show()\n\n\n\n\n\n\n\nFigure 12: Gross margins are based on financial statements provided through the FMP API.\n\n\n\n\n\nMicrosoft maintains the highest margins at 65-70%, reflecting its low-cost software business model, while Apple and Amazon show lower but improving margins from around 40% to 45-47%. This divergence highlights fundamental business model differences.\nFigure 13 illustrates the relationship between gross margins and profit margins across all stocks.\n\nprofit_margins = (combined_statements\n  .query(\"fiscal_year == 2023\")\n)\n\nprofit_margins_figure = (\n  ggplot(\n    profit_margins,\n    aes(x=\"gross_margin\", y=\"profit_margin\", color=\"label\")\n  )\n  + geom_point(size=2)\n  + geom_label(\n      aes(label=\"label\"), \n      adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format()) \n  + scale_color_manual(values=selected_colors) \n  + labs(\n      x=\"Gross margin\", y=\"Profit margin\",\n      title=\"Gross and profit margins for selected stocks for 2023\"\n    )\n  + theme(legend_position = \"none\")\n)\nprofit_margins_figure.show()\n\n\n\n\n\n\n\nFigure 13: Gross and profit margins are based on financial statements provided through the FMP API.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#combining-financial-ratios",
    "href": "python/financial-statement-analysis.html#combining-financial-ratios",
    "title": "Financial Statement Analysis",
    "section": "Combining Financial Ratios",
    "text": "Combining Financial Ratios\nWhile individual financial ratios provide specific insights, combining them offers a more comprehensive view of company performance. By examining how companies rank across different ratio categories, we can better understand their overall financial position and identify potential strengths and weaknesses in their operations.\nFigure 14 compares Microsoft, Apple, and Amazon’s rankings across four key financial ratio categories among all stocks. Rankings closer to 1 indicate better performance within each category.\n\nfinancial_ratios = (combined_statements\n  .query(\"fiscal_year == 2023\")\n  .filter(\n    items=[\"symbol\"] + [\n      col for col in combined_statements.columns \n      if any(x in col for x in [\"ratio\", \"margin\", \"roe\", \"_to_\", \"turnover\", \"interest_coverage\"])\n    ]\n  )\n  .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n  .assign(\n    type=lambda x: np.select(\n        [\n            x[\"name\"].isin([\"current_ratio\", \"quick_ratio\", \"cash_ratio\"]),\n            x[\"name\"].isin([\"debt_to_equity\", \"debt_to_asset\", \"interest_coverage\"]),\n            x[\"name\"].isin([\"asset_turnover\", \"inventory_turnover\", \"receivables_turnover\"]),\n            x[\"name\"].isin([\"gross_margin\", \"profit_margin\", \"after_tax_roe\"]),\n        ],\n        [\n            \"Liquidity Ratios\",\n            \"Leverage Ratios\",\n            \"Efficiency Ratios\",\n            \"Profitability Ratios\"\n        ],\n        default=\"Other\"\n    )\n  )\n)\n\nfinancial_ratios[\"rank\"] = (financial_ratios\n  .sort_values([\"type\", \"name\", \"value\"], ascending=[True, True, False])\n  .groupby([\"type\", \"name\"])\n  .cumcount() + 1\n)\n\nfinal_ranks = (financial_ratios\n  .groupby([\"symbol\", \"type\"], as_index=False)\n  .agg(rank=(\"rank\", \"mean\"))\n  .query(\"symbol in @selected_symbols\")\n)\n\nfinal_ranks_figure = (\n  ggplot(\n    final_ranks,\n    aes(x=\"rank\", y=\"type\", color=\"symbol\")\n  )\n  + geom_point(shape=\"^\", size=4)\n  + scale_color_manual(values=selected_colors) \n  + labs(\n      x=\"Average rank\", y=\"\", color=\"\",\n      title=\"Average rank among selected stocks\"\n  )\n  + coord_cartesian(xlim=[1, 30])\n)\nfinal_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 14: Ranks are based on financial statements provided through the FMP API.\n\n\n\n\n\nThese combined rankings highlight how different business models and strategies lead to varying financial profiles. This analysis underscores the importance of considering multiple financial metrics together rather than in isolation when evaluating company performance.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#financial-ratios-in-asset-pricing",
    "href": "python/financial-statement-analysis.html#financial-ratios-in-asset-pricing",
    "title": "Financial Statement Analysis",
    "section": "Financial Ratios in Asset Pricing",
    "text": "Financial Ratios in Asset Pricing\nThe Fama-French five-factor model aims to explain stock returns by incorporating specific financial metrics ratios. We provide more details in Replicating Fama-French Factors, but here is an intuitive overview:\n\nSize: Calculated as the logarithm of a company’s market capitalization, which is the total market value of its outstanding shares. This factor captures the tendency for smaller firms to outperform larger ones over time.\nBook-to-market ratio: Determined by dividing the company’s book equity by its market capitalization. A higher ratio indicates a ‘value’ stock, while a lower ratio suggests a ‘growth’’’ stock. This metric helps differentiate between undervalued and overvalued companies.\nProfitability: Measured as the ratio of operating profit to book equity, where operating profit is calculated as revenue minus cost of goods sold (COGS), selling, general, and administrative expenses (SG&A), and interest expense. This factor assesses a company’s efficiency in generating profits from its equity base.\nInvestment: Calculated as the percentage change in total assets from the previous period. This factor reflects the company’s growth strategy, indicating whether it is investing aggressively or conservatively.\n\nWe can calculate these factors using the FMP API as follows. Since the free tier only supports historical data for the last couple of months, we use the earliest available data that is returned by default:\n\nmarket_cap = pd.concat(\n  [fmp_get(\n      resource=\"historical-market-capitalization\", symbol=x, to_pandas=True\n    ) for x in sample],\n  ignore_index=True\n)\n\nmin_date = market_cap[\"date\"].min()\nmarket_cap = market_cap.loc[market_cap[\"date\"] == min_date]\n\ncombined_statements_ff = (combined_statements\n  .query(\"fiscal_year == 2023\")\n  .merge(market_cap, on=\"symbol\", how=\"left\")\n  .merge(\n    (balance_sheet_statements\n      .query(\"fiscal_year == 2022\")\n      .get([\"symbol\", \"total_assets\"])\n      .rename(columns={\"total_assets\": \"total_assets_lag\"})\n    ),\n    on=\"symbol\", how=\"left\"\n  )\n  .assign(\n    size=lambda x: np.log(x[\"market_cap\"]),\n    book_to_market=lambda x: x[\"total_equity\"] / x[\"market_cap\"],\n    operating_profitability=lambda x: (\n        (x[\"revenue\"] - x[\"cost_of_revenue\"] - x[\"selling_general_and_administrative_expenses\"] - x[\"interest_expense\"])\n        / x[\"total_equity\"]\n    ),\n    investment=lambda x: x[\"total_assets\"] / x[\"total_assets_lag\"]\n  )\n)\n\nFigure 15 shows the ranks of our selected stocks for ratios used in the Fama-French model. The ranks of Microsoft, Apple, and Amazon across Fama-French factors reveal interesting patterns in how these major technology companies align with established asset pricing factors.\n\nfactors_ranks = (combined_statements_ff\n  .get([\"symbol\", \"size\", \"book_to_market\", \"operating_profitability\", \"investment\"])\n  .rename(columns={\n    \"size\": \"Size\",\n    \"book_to_market\": \"Book-to-Market\",\n    \"operating_profitability\": \"Profitability\",\n    \"investment\": \"Investment\"\n  })\n  .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n  .assign(\n    rank=lambda x: (\n      x.sort_values([\"name\", \"value\"], ascending=[True, False])\n      .groupby(\"name\")\n      .cumcount() + 1\n      )\n  )\n  .query(\"symbol in @selected_symbols\")\n)\n\nfactors_ranks_figure = (\n  ggplot(\n    factors_ranks,\n    aes(x=\"rank\", y=\"name\", color=\"symbol\")\n  )\n  + geom_point(shape=\"^\", size=4)\n  + scale_color_manual(values=selected_colors) \n  + labs(\n      x=\"Rank\", y=\"\", color=\"\",\n      title=\"Rank in Fama-French variables for selected stocks\"\n  )\n  + coord_cartesian(xlim=[1, 30])\n)\nfactors_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 15: Ranks are based on financial statements and historical market capitalization provided through the FMP API.\n\n\n\n\n\nAs expected, all three tech giants rank among the largest firms by size. Apple shows the highest profitability among the three tech giants according to the new measure, while Microsoft ranks only in the middle. In terms of investment, however, Apple ranks in the lower third of the distribution. All three stocks exhibit relatively low book-to-market ratios—typical of growth stocks—but only when compared to other stocks in our sample.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#key-takeaways",
    "href": "python/financial-statement-analysis.html#key-takeaways",
    "title": "Financial Statement Analysis",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFinancial statements offer structured insights into a company’s financial health by summarizing its assets, liabilities, equity, revenues, expenses, and cash flows.\nLiquidity ratios, such as the current, quick, and cash ratios, help assess a company’s ability to meet short-term obligations using different levels of liquid assets.\nLeverage ratios, including debt-to-equity and debt-to-asset, measure how a company finances its operations and indicate long-term financial risk and capital structure.\nProfitability ratios, such as gross margin, profit margin, and return on equity, show how effectively a company turns revenues and investments into earnings.\nEfficiency ratios, including asset turnover and inventory turnover, highlight how well a company manages its assets and operations to generate sales.\nFinancial ratios also serve as key inputs in asset pricing models, such as the Fama-French five-factor model, linking corporate fundamentals to expected stock returns.",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/financial-statement-analysis.html#exercises",
    "href": "python/financial-statement-analysis.html#exercises",
    "title": "Financial Statement Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the financial statements for Netflix (NFLX) using the FMP API. Calculate its current ratio, quick ratio, and cash ratio for the past three years. Create a line plot showing how these liquidity ratios have evolved over time. How do Netflix’s liquidity ratios compare to those of the technology companies discussed in this chapter?\nSelect three companies from different industries. Calculate their debt-to-equity ratios, debt-to-asset ratios, and interest coverage ratios. Create a visualization comparing these leverage metrics across the companies. Write a brief analysis explaining how and why leverage patterns differ across industries.\nFor all stocks in the sample above, calculate asset turnover, inventory turnover, and receivables turnover. Create a scatter plot showing the relationship between asset turnover and profitability. Identify any outliers and explain potential reasons for their unusual performance. Which industries tend to show higher efficiency ratios? Why might this be the case?\nRevisit the scatter plot of debt-to-asset ratios and interest coverages by adding a regression line and quantifying the relationship between the two variables. How can you describe their relationship?",
    "crumbs": [
      "R",
      "Getting Started",
      "Financial Statement Analysis"
    ]
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "This website is the online version of Tidy Finance with Python, a book soon available via Chapman & Hall/CRC. The book is the result of a joint effort of Christoph Scheuch, Stefan Voigt, Patrick Weiss, and Christoph Frey.\nWe are grateful for any kind of feedback on every aspect of the book. So please get in touch with us via contact@tidy-finance.org if you spot typos, discover any issues that deserve more attention, or if you have suggestions for additional chapters and sections. Additionally, let us know if you found the text helpful. We look forward to hearing from you!\n\n\n\n\n\n\nSupport Tidy Finance\n\n\n\nBuy our book Tidy Finance with Python via your preferred vendor or support us with coffee here.\n\n\n\n\nOver our academic careers, we are continuously surprised by the lack of publicly available code for seminal papers or even textbooks in finance. This opaqueness and secrecy is particularly costly for young, aspiring financial economists. To tackle this issue, we started working on Tidy Finance to lift the curtain on reproducible finance. These efforts resulted in the book Tidy Finance with R (Scheuch, Voigt, and Weiss 2023), which provides a fully transparent code base in R for many common financial applications.\nSince the book’s publication, we received great feedback from students and teachers alike. However, one of the most common comments was that many interested coders are constrained and have to use Python in their institutions. We really love R for data analysis tasks, but we acknowledge the flexibility and popularity of Python. Hence, we decided to increase our team of authors with a Python expert and extend Tidy Finance to another programming language following the same tidy principles.\n\n\n\nWe write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from the undergraduate to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We choose a wide range of topics, from data handling and factor replication to portfolio allocation and option pricing, to offer something for every course and study focus. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nPractitioners like portfolio managers who like to validate and implement trading ideas or data analysts or statisticians who work with financial data and who need practical tools to succeed.\n\n\n\n\nThe book is divided into five parts:\n\nThe first part helps you to set up your Python development environment and introduces you to essential programming concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common datasets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access via R packages exists.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for portfolio optimization and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet, the data chapters provide an important background necessary for data management in all other chapters.\n\n\n\nThis book is about empirical work. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. Moreover, although we enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses, we refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nIn addition, we also do not explain all the functionalities and details about the Python functions we use. We only delve into the empirical research focus and data transformation logic and want to refer attentive readers to consult the package documentations for more information. In other words, this is not a book to learn Python from scratch. It is a book on how to use Python as a tool to produce consistent and replicable empirical results.\nThat being said, our book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nHilpisch (2018) is a great introduction to the power of Python for financial applications. It does a great job explaining the basics of the Python language, its programming structure, and packages like pandas, SciPy, and numpy and uses these methods for actual applications. The book and a series of follow-up books from the same author about financial data science, artificial intelligence, and algorithmic trading primarily target practitioners and have a hands-on focus. Our book, in contrast, emphasizes reproducibility and starts with the applications right away to utilize Python as the tool to perform data transformations and statistical analysis. Hence, we clearly focus on state-of-the-art applications for academic research in finance. Thus, we fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nFurthermore, Weiming (2019) and Kelliher (2022) are comprehensive introductions to quantitative finance with a greater focus on option pricing, quantitative modeling for various markets besides equity, and algorithmic trading. Again, these books are primarily written for finance professionals to introduce Python or enhance their Python knowledge.\nCoqueret and Guida (2023) constitutes a great compendium to our book concerning applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git.\nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Kibirige (2023) provides a highly customizable yet easy-to-use system for creating data visualizations based on the Grammar of Graphics (Wilkinson 2012). Second, in our daily work and to compile this book, we used Quarto, an open-source scientific and technical publishing system described in Allaire et al. (2023). Markdown documents are fully reproducible and support static and dynamic output formats. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).\n\n\n\n\nAs you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80 percent of data analysis is spent on preparing data. By tidying data, we want to structure datasets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as possible. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions.\nEmbrace functional programming.\nDesign for humans.\n\n\n\n\nPython (Python Software Foundation 2023) is an open-source, general-purpose, high-level programming language widely used across various industries. Python is prevalent for data science according to the Python Developers Survey (Foundation and JetBrains 2022), particularly for financial applications. Similar to R, the Python community values readable and straightforward code. Thus, Python is an excellent choice for first-time programmers. At the same time, experienced researchers in financial economics and analysts benefit from the wide range of possibilities to express complex ideas with concise and understandable code. Some of the highlights of Python include:\n\nOpen-source: Python uses a source license, making it usable and distributable for academic and commercial use.\nFlexibility: Python’s extensive ecosystem of standard libraries and community-contributed modules allows for all kinds of unique projects. It seamlessly integrates various data sources and APIs, facilitating efficient data retrieval and processing.\nVersatility: Python is a cross-platform, multipurpose language that can be used to write fast low-level executable code, large applications, and even graphical user interfaces (GUI).\nSpeed: Python is fast. In addition, parallelization is straightforward to implement in order to tackle big data problems without hassle.\nRobustness: Python provides robust tools for data manipulation, analysis, and visualization, which are crucial components in finance research.\nImportance: The language’s active community support and continuous development ensure access to cutting-edge technologies and methodologies. Learning Python enhances one’s ability to conduct sophisticated financial analysis, making it a valuable skill for professionals across diverse fields.\n\nThe so-called Zen of Python by Tim Peters summarizes its major syntax guidelines for structured, tidy, and human-readable code. It is easily accessible in every Python environment through:\n\nimport this\n\nPython comes in many flavors, and endless external packages extend the possibilities for conducting financial research. Any code we provide echoes some arguably subjective decisions we have taken to comply with our idea of what Tidy Finance comprises: Code should not simply yield the correct output but should be easy to read. Therefore, we advocate using chaining, which is the practice of calling multiple methods in a sequence, each operating on the result of the previous step.\nFurther, the entire book rests on tidy data, which we handle with a small set of powerful packages proven effective: pandas and numpy. Regarding visualization (which we deem highly relevant to provide a fundamentally human-centered experience), we follow the Grammars of Graphics’ philosophical framework (Wilkinson 2012), which has been carefully implemented using plotnine.\nArguably, neither chaining commands nor using the Grammar of Graphics can be considered mainstream within the Python ecosystem for financial research (yet). We believe in the value of the workflows we teach and practice on a daily basis. Therefore, we also believe that adopting such coding principles will dramatically increase in the near future. For more information on why Python is great, we refer to Hilpisch (2018).\n\n\n\n\nChristoph Scheuch is an independent data science and business intelligence expert, currently serving as an external lecturer at Humboldt University of Berlin and as a summer school instructor at the Barcelona School of Economics. Previously, he was the Head of AI, Director of Product, and Head of BI & Data Science at the social trading platform wikifolio.com. He also was an external lecturer at the Vienna University of Economics and Business (WU), where he obtained his PhD in finance as part of the Vienna Graduate School of Finance (VGSF).\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals and he received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on Tidy Finance.\n\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance, with his research appearing in leading journals in financial economics. Patrick is especially passionate about empirical asset pricing and strives to understand the impact of methodological uncertainty on research outcomes.\nChristoph Frey is a Quantitative Researcher and Portfolio Manager at a family office in Hamburg and Research Fellow at the Centre for Financial Econometrics, Asset Markets and Macroeconomic Policy at Lancaster University. Prior to this, he was the leading quantitative researcher for systematic multi-asset strategies at Berenberg Bank and worked as an Assistant Professor at the Erasmus Universiteit Rotterdam. Christoph published research on Bayesian Econometrics and specializes in financial econometrics and portfolio optimization problems.\n\n\n\n\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite the Python project as follows:\n\nScheuch, C., Voigt, S., Weiss, P., & Frey, C. (2024). Tidy Finance with Python (1st ed.). Chapman and Hall/CRC https://www.tidy-finance.org\n\nYou can also use the following BibTeX entry:\n@book{Scheuch2024,\n  title = {Tidy Finance with Python},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick and Frey, Christoph},\n  year = {2024},\n  publisher = {Chapman and Hall/CRC},\n  edition = {1st},\n  url = {https://tidy-finance.org/python},\n  doi = {https://doi.org/10.1201/9781032684307}\n}",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#why-does-this-book-exist",
    "href": "python/index.html#why-does-this-book-exist",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "Over our academic careers, we are continuously surprised by the lack of publicly available code for seminal papers or even textbooks in finance. This opaqueness and secrecy is particularly costly for young, aspiring financial economists. To tackle this issue, we started working on Tidy Finance to lift the curtain on reproducible finance. These efforts resulted in the book Tidy Finance with R (Scheuch, Voigt, and Weiss 2023), which provides a fully transparent code base in R for many common financial applications.\nSince the book’s publication, we received great feedback from students and teachers alike. However, one of the most common comments was that many interested coders are constrained and have to use Python in their institutions. We really love R for data analysis tasks, but we acknowledge the flexibility and popularity of Python. Hence, we decided to increase our team of authors with a Python expert and extend Tidy Finance to another programming language following the same tidy principles.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#who-should-read-this-book",
    "href": "python/index.html#who-should-read-this-book",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "We write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from the undergraduate to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We choose a wide range of topics, from data handling and factor replication to portfolio allocation and option pricing, to offer something for every course and study focus. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nPractitioners like portfolio managers who like to validate and implement trading ideas or data analysts or statisticians who work with financial data and who need practical tools to succeed.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#what-will-you-learn",
    "href": "python/index.html#what-will-you-learn",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "The book is divided into five parts:\n\nThe first part helps you to set up your Python development environment and introduces you to essential programming concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common datasets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access via R packages exists.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for portfolio optimization and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet, the data chapters provide an important background necessary for data management in all other chapters.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#what-wont-you-learn",
    "href": "python/index.html#what-wont-you-learn",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "This book is about empirical work. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. Moreover, although we enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses, we refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nIn addition, we also do not explain all the functionalities and details about the Python functions we use. We only delve into the empirical research focus and data transformation logic and want to refer attentive readers to consult the package documentations for more information. In other words, this is not a book to learn Python from scratch. It is a book on how to use Python as a tool to produce consistent and replicable empirical results.\nThat being said, our book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nHilpisch (2018) is a great introduction to the power of Python for financial applications. It does a great job explaining the basics of the Python language, its programming structure, and packages like pandas, SciPy, and numpy and uses these methods for actual applications. The book and a series of follow-up books from the same author about financial data science, artificial intelligence, and algorithmic trading primarily target practitioners and have a hands-on focus. Our book, in contrast, emphasizes reproducibility and starts with the applications right away to utilize Python as the tool to perform data transformations and statistical analysis. Hence, we clearly focus on state-of-the-art applications for academic research in finance. Thus, we fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nFurthermore, Weiming (2019) and Kelliher (2022) are comprehensive introductions to quantitative finance with a greater focus on option pricing, quantitative modeling for various markets besides equity, and algorithmic trading. Again, these books are primarily written for finance professionals to introduce Python or enhance their Python knowledge.\nCoqueret and Guida (2023) constitutes a great compendium to our book concerning applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git.\nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Kibirige (2023) provides a highly customizable yet easy-to-use system for creating data visualizations based on the Grammar of Graphics (Wilkinson 2012). Second, in our daily work and to compile this book, we used Quarto, an open-source scientific and technical publishing system described in Allaire et al. (2023). Markdown documents are fully reproducible and support static and dynamic output formats. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#why-tidy",
    "href": "python/index.html#why-tidy",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "As you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80 percent of data analysis is spent on preparing data. By tidying data, we want to structure datasets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as possible. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions.\nEmbrace functional programming.\nDesign for humans.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#why-python",
    "href": "python/index.html#why-python",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "Python (Python Software Foundation 2023) is an open-source, general-purpose, high-level programming language widely used across various industries. Python is prevalent for data science according to the Python Developers Survey (Foundation and JetBrains 2022), particularly for financial applications. Similar to R, the Python community values readable and straightforward code. Thus, Python is an excellent choice for first-time programmers. At the same time, experienced researchers in financial economics and analysts benefit from the wide range of possibilities to express complex ideas with concise and understandable code. Some of the highlights of Python include:\n\nOpen-source: Python uses a source license, making it usable and distributable for academic and commercial use.\nFlexibility: Python’s extensive ecosystem of standard libraries and community-contributed modules allows for all kinds of unique projects. It seamlessly integrates various data sources and APIs, facilitating efficient data retrieval and processing.\nVersatility: Python is a cross-platform, multipurpose language that can be used to write fast low-level executable code, large applications, and even graphical user interfaces (GUI).\nSpeed: Python is fast. In addition, parallelization is straightforward to implement in order to tackle big data problems without hassle.\nRobustness: Python provides robust tools for data manipulation, analysis, and visualization, which are crucial components in finance research.\nImportance: The language’s active community support and continuous development ensure access to cutting-edge technologies and methodologies. Learning Python enhances one’s ability to conduct sophisticated financial analysis, making it a valuable skill for professionals across diverse fields.\n\nThe so-called Zen of Python by Tim Peters summarizes its major syntax guidelines for structured, tidy, and human-readable code. It is easily accessible in every Python environment through:\n\nimport this\n\nPython comes in many flavors, and endless external packages extend the possibilities for conducting financial research. Any code we provide echoes some arguably subjective decisions we have taken to comply with our idea of what Tidy Finance comprises: Code should not simply yield the correct output but should be easy to read. Therefore, we advocate using chaining, which is the practice of calling multiple methods in a sequence, each operating on the result of the previous step.\nFurther, the entire book rests on tidy data, which we handle with a small set of powerful packages proven effective: pandas and numpy. Regarding visualization (which we deem highly relevant to provide a fundamentally human-centered experience), we follow the Grammars of Graphics’ philosophical framework (Wilkinson 2012), which has been carefully implemented using plotnine.\nArguably, neither chaining commands nor using the Grammar of Graphics can be considered mainstream within the Python ecosystem for financial research (yet). We believe in the value of the workflows we teach and practice on a daily basis. Therefore, we also believe that adopting such coding principles will dramatically increase in the near future. For more information on why Python is great, we refer to Hilpisch (2018).",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#about-the-authors",
    "href": "python/index.html#about-the-authors",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "Christoph Scheuch is an independent data science and business intelligence expert, currently serving as an external lecturer at Humboldt University of Berlin and as a summer school instructor at the Barcelona School of Economics. Previously, he was the Head of AI, Director of Product, and Head of BI & Data Science at the social trading platform wikifolio.com. He also was an external lecturer at the Vienna University of Economics and Business (WU), where he obtained his PhD in finance as part of the Vienna Graduate School of Finance (VGSF).\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals and he received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on Tidy Finance.\n\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance, with his research appearing in leading journals in financial economics. Patrick is especially passionate about empirical asset pricing and strives to understand the impact of methodological uncertainty on research outcomes.\nChristoph Frey is a Quantitative Researcher and Portfolio Manager at a family office in Hamburg and Research Fellow at the Centre for Financial Econometrics, Asset Markets and Macroeconomic Policy at Lancaster University. Prior to this, he was the leading quantitative researcher for systematic multi-asset strategies at Berenberg Bank and worked as an Assistant Professor at the Erasmus Universiteit Rotterdam. Christoph published research on Bayesian Econometrics and specializes in financial econometrics and portfolio optimization problems.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#license",
    "href": "python/index.html#license",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "This book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite the Python project as follows:\n\nScheuch, C., Voigt, S., Weiss, P., & Frey, C. (2024). Tidy Finance with Python (1st ed.). Chapman and Hall/CRC https://www.tidy-finance.org\n\nYou can also use the following BibTeX entry:\n@book{Scheuch2024,\n  title = {Tidy Finance with Python},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick and Frey, Christoph},\n  year = {2024},\n  publisher = {Chapman and Hall/CRC},\n  edition = {1st},\n  url = {https://tidy-finance.org/python},\n  doi = {https://doi.org/10.1201/9781032684307}\n}",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/clean-enhanced-trace-with-python.html",
    "href": "python/clean-enhanced-trace-with-python.html",
    "title": "Clean Enhanced TRACE with Python",
    "section": "",
    "text": "This appendix contains code to clean enhanced TRACE with Python. It is also available via the following GitHub gist. Hence, you could also source the file with the following chunk.\n\ngist_url = (\n  \"https://gist.githubusercontent.com/patrick-weiss/\"\n  \"86ddef6de978fbdfb22609a7840b5d8b/raw/\"\n  \"8fbcc6c6f40f537cd3cd37368be4487d73569c6b/\"\n)\n\nwith httpimport.remote_repo(gist_url):\n  from clean_enhanced_TRACE_python import clean_enhanced_trace\n\nWe need this function in TRACE and FISD to download and clean enhanced TRACE trade messages following Dick-Nielsen (2009) and Dick-Nielsen (2014) for enhanced TRACE specifically. This code is based on the resources provided by the project Open Source Bond Asset Pricing and their related publication Dickerson, Mueller, and Robotti (2023). We encourage that you acknowledge their effort. Relatedly, WRDS provides SAS code to clean enhanced TRACE data.\nThe function takes a vector of CUSIPs (in cusips), a connection to WRDS (connection) explained in Chapter 3, and a start and end date (start_date and end_date, respectively). Specifying too many CUSIPs will result in very slow downloads and a potential failure due to the size of the request to WRDS. The dates should be within the coverage of TRACE itself, i.e., starting after 2002, and the dates should be supplied as a string indicating MM/DD/YYYY. The output of the function contains all valid trade messages for the selected CUSIPs over the specified period.\n\ndef clean_enhanced_trace(cusips, \n                         connection, \n                         start_date=\"'01/01/2002'\", \n                         end_date=\"'12/31/2023'\"):\n  \"\"\"Clean enhanced TRACE data.\"\"\"\n  \n  import pandas as pd\n  import numpy as np\n  \n  # Load main file\n  trace_query = (\n    \"SELECT cusip_id, bond_sym_id, trd_exctn_dt, \"\n           \"trd_exctn_tm, days_to_sttl_ct, lckd_in_ind, \"\n           \"wis_fl, sale_cndtn_cd, msg_seq_nb, \"\n           \"trc_st, trd_rpt_dt, trd_rpt_tm, \"\n           \"entrd_vol_qt, rptd_pr, yld_pt, \" \n           \"asof_cd, orig_msg_seq_nb, rpt_side_cd, \"\n           \"cntra_mp_id, stlmnt_dt, spcl_trd_fl \" \n    \"FROM trace.trace_enhanced \" \n   f\"WHERE cusip_id IN {cusips} \" \n         f\"AND trd_exctn_dt BETWEEN {start_date} AND {end_date}‚\"\n  )\n\n  trace_all = pd.read_sql_query(\n    sql=trace_query,\n    con=connection,\n    parse_dates={\"trd_exctn_dt\",\"trd_rpt_dt\", \"stlmnt_dt\"}\n  )\n  \n  # Post 2012-06-02\n  ## Trades (trc_st = T) and correction (trc_st = R)\n  trace_post_TR = (trace_all\n    .query(\"trc_st in ['T', 'R']\")\n    .query(\"trd_rpt_dt &gt;= '2012-06-02'\")\n  )\n  \n  # Cancellations (trc_st = X) and correction cancellations (trc_st = C)\n  trace_post_XC = (trace_all\n    .query(\"trc_st in ['X', 'C']\")\n    .query(\"trd_rpt_dt &gt;= '2012-06-02'\")\n    .get([\"cusip_id\", \"msg_seq_nb\", \"entrd_vol_qt\",\n          \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\",\n          \"trd_exctn_dt\", \"trd_exctn_tm\"])\n    .assign(drop=True)\n  )\n  \n  ## Cleaning corrected and cancelled trades\n  trace_post_TR = (trace_post_TR\n    .merge(trace_post_XC, how=\"left\")\n    .query(\"drop != True\")\n    .drop(columns=\"drop\")\n  )\n  \n  # Reversals (trc_st = Y) \n  trace_post_Y = (trace_all\n    .query(\"trc_st == 'Y'\")\n    .query(\"trd_rpt_dt &gt;= '2012-06-02'\")\n    .get([\"cusip_id\", \"orig_msg_seq_nb\", \"entrd_vol_qt\",\n          \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\",\n          \"trd_exctn_dt\", \"trd_exctn_tm\"])\n    .assign(drop=True)\n    .rename(columns={\"orig_msg_seq_nb\": \"msg_seq_nb\"})\n  )\n  \n  # Clean reversals\n  ## Match the orig_msg_seq_nb of Y-message to msg_seq_nb of main message\n  trace_post = (trace_post_TR\n    .merge(trace_post_Y, how=\"left\")\n    .query(\"drop != True\")\n    .drop(columns=\"drop\")\n  )\n  \n  # Pre 06-02-12\n  ## Trades (trc_st = T)\n  trace_pre_T = (trace_all\n    .query(\"trd_rpt_dt &lt; '2012-06-02'\")\n  )\n    \n  # Cancellations (trc_st = C) \n  trace_pre_C = (trace_all\n    .query(\"trc_st == 'C'\")\n    .query(\"trd_rpt_dt &lt; '2012-06-02'\")\n    .get([\"cusip_id\", \"orig_msg_seq_nb\", \"entrd_vol_qt\",\n          \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\",\n          \"trd_exctn_dt\", \"trd_exctn_tm\"])\n    .assign(drop=True)\n    .rename(columns={\"orig_msg_seq_nb\": \"msg_seq_nb\"})\n  )\n  \n  # Remove cancellations from trades\n  ## Match orig_msg_seq_nb of C-message to msg_seq_nb of main message\n  trace_pre_T = (trace_pre_T\n    .merge(trace_pre_C, how=\"left\")\n    .query(\"drop != True\")\n    .drop(columns=\"drop\")\n  )\n  \n  # Corrections (trc_st = W)\n  trace_pre_W = (trace_all\n    .query(\"trc_st == 'W'\")\n    .query(\"trd_rpt_dt &lt; '2012-06-02'\")\n  )\n  \n  # Implement corrections in a loop\n  ## Correction control\n  correction_control = len(trace_pre_W)\n  correction_control_last = len(trace_pre_W)\n\n  ## Correction loop\n  while (correction_control &gt; 0):\n    # Create placeholder\n    ## Only identifying columns of trace_pre_T (for joins)\n    placeholder_trace_pre_T = (trace_pre_T\n      .get([\"cusip_id\", \"trd_exctn_dt\", \"msg_seq_nb\"])\n      .rename(columns={\"msg_seq_nb\": \"orig_msg_seq_nb\"})\n      .assign(matched_T=True)\n    )\n    \n    # Corrections that correct some msg\n    trace_pre_W_correcting = (trace_pre_W\n      .merge(placeholder_trace_pre_T, how=\"left\")\n      .query(\"matched_T == True\")\n      .drop(columns=\"matched_T\")\n    )\n\n    # Corrections that do not correct some msg\n    trace_pre_W = (trace_pre_W\n      .merge(placeholder_trace_pre_T, how=\"left\")\n      .query(\"matched_T != True\")\n      .drop(columns=\"matched_T\")\n    )\n    \n    # Create placeholder \n    ## Only identifying columns of trace_pre_W_correcting (for anti-joins)\n    placeholder_trace_pre_W_correcting = (trace_pre_W_correcting\n      .get([\"cusip_id\", \"trd_exctn_dt\", \"orig_msg_seq_nb\"])\n      .rename(columns={\"orig_msg_seq_nb\": \"msg_seq_nb\"})\n      .assign(corrected=True)\n    )\n    \n    # Delete msgs that are corrected \n    trace_pre_T = (trace_pre_T\n      .merge(placeholder_trace_pre_W_correcting, how=\"left\")\n      .query(\"corrected != True\")\n      .drop(columns=\"corrected\")\n    )\n    \n    # Add correction msgs\n    trace_pre_T = pd.concat([trace_pre_T, trace_pre_W_correcting])\n\n    # Escape if no corrections remain or they cannot be matched\n    correction_control = len(trace_pre_W)\n    \n    if correction_control == correction_control_last: \n      break\n    else:\n      correction_control_last = len(trace_pre_W)\n      continue\n  \n  # Reversals (asof_cd = R)\n  ## Record reversals\n  trace_pre_R = (trace_pre_T\n    .query(\"asof_cd == 'R'\")\n    .sort_values([\"cusip_id\", \"trd_exctn_dt\",\n                 \"trd_exctn_tm\", \"trd_rpt_dt\", \"trd_rpt_tm\"])\n  )\n  \n  ## Prepare final data\n  trace_pre = (trace_pre_T\n    .query(\n      \"asof_cd == None | asof_cd.isnull() | asof_cd not in ['R', 'X', 'D']\"\n    )\n    .sort_values([\"cusip_id\", \"trd_exctn_dt\",\n                 \"trd_exctn_tm\", \"trd_rpt_dt\", \"trd_rpt_tm\"])\n  )\n  \n  ## Add grouped row numbers\n  trace_pre_R[\"seq\"] = (trace_pre_R\n    .groupby([\"cusip_id\", \"trd_exctn_dt\", \"entrd_vol_qt\",\n              \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\"])\n    .cumcount()\n  )\n\n  trace_pre[\"seq\"] = (trace_pre\n    .groupby([\"cusip_id\", \"trd_exctn_dt\", \"entrd_vol_qt\",\n              \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\"])\n    .cumcount()\n  )\n  \n  ## Select columns for reversal cleaning\n  trace_pre_R = (trace_pre_R\n    .get([\"cusip_id\", \"trd_exctn_dt\", \"entrd_vol_qt\",\n         \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\", \"seq\"])\n    .assign(reversal=True)\n  )\n  \n  ## Remove reversals and the reversed trade\n  trace_pre = (trace_pre\n    .merge(trace_pre_R, how=\"left\")\n    .query(\"reversal != True\")\n    .drop(columns=[\"reversal\", \"seq\"])\n  )\n  \n  # Combine pre and post trades\n  trace_clean = pd.concat([trace_pre, trace_post])\n  \n  # Keep agency sells and unmatched agency buys\n  trace_agency_sells = (trace_clean \n    .query(\"cntra_mp_id == 'D' & rpt_side_cd == 'S'\")\n  )\n  \n  # Placeholder for trace_agency_sells with relevant columns\n  placeholder_trace_agency_sells = (trace_agency_sells\n    .get([\"cusip_id\", \"trd_exctn_dt\",\n          \"entrd_vol_qt\", \"rptd_pr\"])\n    .assign(matched=True)\n  )\n\n  # Agency buys that are unmatched\n  trace_agency_buys_filtered = (trace_clean  \n    .query(\"cntra_mp_id == 'D' & rpt_side_cd == 'B'\")\n    .merge(placeholder_trace_agency_sells, how=\"left\")\n    .query(\"matched != True\")\n    .drop(columns=\"matched\")\n  )\n  \n  # Non-agency\n  trace_nonagency = (trace_clean \n    .query(\"cntra_mp_id == 'C'\")\n  )\n  \n  # Agency cleaned\n  trace_clean = pd.concat([trace_nonagency, \n                           trace_agency_sells, \n                           trace_agency_buys_filtered])\n  \n  # Additional Filters\n  trace_add_filters = (trace_clean\n    .assign(\n      days_to_sttl_ct2 = lambda x: (\n        (x[\"stlmnt_dt\"]-x[\"trd_exctn_dt\"]).dt.days\n      )\n    )\n    .assign(\n      days_to_sttl_ct = lambda x: pd.to_numeric(\n        x[\"days_to_sttl_ct\"], errors='coerce'\n      )\n    )\n    .query(\"days_to_sttl_ct.isnull() | days_to_sttl_ct &lt;= 7\")\n    .query(\"days_to_sttl_ct2.isnull() | days_to_sttl_ct2 &lt;= 7\")\n    .query(\"wis_fl == 'N'\")\n    .query(\"spcl_trd_fl.isnull() | spcl_trd_fl == ''\")\n    .query(\"asof_cd.isnull() | asof_cd == ''\")\n  )\n  \n  # Only keep necessary columns\n  trace_final = (trace_add_filters\n    .sort_values([\"cusip_id\", \"trd_exctn_dt\", \"trd_exctn_tm\"])\n    .get([\"cusip_id\", \"trd_exctn_dt\", \"trd_exctn_tm\", \"rptd_pr\", \n          \"entrd_vol_qt\", \"yld_pt\", \"rpt_side_cd\", \"cntra_mp_id\"])\n  )\n  \n  return trace_final\n\n\n\n\n\nReferences\n\nDickerson, Alexander, Philippe Mueller, and Cesare Robotti. 2023. “Priced Risk in Corporate Bonds.” Journal of Financial Economics 150 (2): 103707. https://doi.org/10.1016/j.jfineco.2023.103707.\n\n\nDick-Nielsen, Jens. 2009. “Liquidity biases in TRACE.” The Journal of Fixed Income 19 (2): 43–55. https://doi.org/10.3905/jfi.2009.19.2.043.\n\n\n———. 2014. “How to clean enhanced TRACE data.” Working Paper. https://ssrn.com/abstract=2337908.",
    "crumbs": [
      "R",
      "Appendix",
      "Clean Enhanced TRACE with Python"
    ]
  },
  {
    "objectID": "python/cover-image.html",
    "href": "python/cover-image.html",
    "title": "Cover Image",
    "section": "",
    "text": "The cover of the book is inspired by the fast growing generative art community in R. Generative art refers to art that in whole or in part has been created with the use of an autonomous system. Instead of creating random dynamics, we rely on what is core to the book: The evolution of financial markets. Each circle corresponds to one of the twelve Fama-French industry portfolios, whereas each bar represents the average annual return between 1927 and 2022. The bar color is determined by the standard deviation of returns for each industry. The few lines of code below replicate the entire figure.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas_datareader as pdr\n\nfrom datetime import datetime\nfrom matplotlib.colors import LinearSegmentedColormap\n\nmain_colors = [\"#3B9AB2\", \"#78B7C5\", \"#EBCC2A\", \"#E1AF00\", \"#F21A00\"]\ncolormap = LinearSegmentedColormap.from_list(\"custom_colormap\", main_colors)\n\nindustries_ff_daily_raw = pdr.DataReader(\n  name=\"12_Industry_Portfolios_daily\",\n  data_source=\"famafrench\", \n  start=\"1927-01-01\", \n  end=\"2022-12-31\")[0]\n\nindustries_ff_daily = (industries_ff_daily_raw\n  .divide(100)\n  .reset_index(names=\"date\")\n  .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n  .rename(str.lower, axis=\"columns\")\n)\n\nindustries_long = (industries_ff_daily\n  .melt(id_vars=\"date\", var_name=\"name\", value_name=\"value\")\n)\n                          \nindustries_order = sorted(industries_long[\"name\"].unique())\n\ndata_figure = (industries_long\n  .assign(year=industries_long[\"date\"].dt.to_period(\"Y\"))\n  .groupby([\"year\", \"name\"])\n  .aggregate(total=(\"value\", \"mean\"),\n             vola=(\"value\", \"std\"))\n  .reset_index()\n  .assign(\n    vola_ntile=lambda x: pd.qcut(x[\"vola\"], 42, labels=False)\n  )\n)\n\ndpi = 300\nwidth = 2400/dpi\nheight = 1800/dpi\nnum_cols = 4\nnum_rows = int(len(industries_order)/num_cols)\nfig, axs = plt.subplots(\n  num_rows, num_cols,\n  constrained_layout=True,\n  subplot_kw={\"projection\": \"polar\"},\n  figsize=(width, height),\n  dpi=dpi\n)\naxs = axs.flatten()\n\nfor i in enumerate(industries_order):\n\n    df = data_figure.copy().query(f'name == \"{i[1]}\"')\n    min_value = df[\"total\"].min()\n    max_value = df[\"total\"].max()\n    std_value = df[\"total\"].std()\n    df[\"total\"] = 2*(df[\"total\"]-min_value)/(max_value-min_value)-1\n\n    angles = np.linspace(0, 2*np.pi, len(df), endpoint=False)\n    values = df[\"total\"].values\n    width = 2*np.pi/len(values)\n    offset = np.pi/2\n\n    ax = axs[i[0]]\n    ax.set_theta_offset(offset)\n    ax.set_ylim(-std_value*1400, 1)\n    ax.set_frame_on(False)\n    ax.xaxis.grid(False)\n    ax.yaxis.grid(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    color_values = df[\"vola_ntile\"].values\n    normalize = plt.Normalize(min(color_values), max(color_values))\n    colors = colormap(normalize(color_values))\n\n    ax.bar(\n      angles, values,\n      width=width, color=colors, edgecolor=\"white\", linewidth=0.2\n    )\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=-0.2, hspace=-0.1)\nplt.gcf().savefig(\n  \"images/cover-image.png\", dpi = 300, pad_inches=0, transparent=False\n)",
    "crumbs": [
      "R",
      "Appendix",
      "Cover Image"
    ]
  },
  {
    "objectID": "python/the-tidyfinance-python-package.html",
    "href": "python/the-tidyfinance-python-package.html",
    "title": "The tidyfinance Python package",
    "section": "",
    "text": "tidyfinance is an Python package that contains a set of helper functions for empirical research in financial economics, addressing a variety of topics covered in this book. We designed the package to provide easy shortcuts for the applications that we discuss in the book. If you want to inspect the details of the package or propose new features, feel free to visit the package repository on Github.",
    "crumbs": [
      "R",
      "Prerequisits",
      "The `tidyfinance` Python package"
    ]
  },
  {
    "objectID": "python/the-tidyfinance-python-package.html#installation",
    "href": "python/the-tidyfinance-python-package.html#installation",
    "title": "The tidyfinance Python package",
    "section": "Installation",
    "text": "Installation\nYou can install the released version of tidyfinance from PyPI via:\npip install tidyfinance\nYou can install the development version of tidyfinance from GitHub (which might not be fully tested) via:\npip install \"git+https://github.com/tidy-finance/py-fmpapi\"",
    "crumbs": [
      "R",
      "Prerequisits",
      "The `tidyfinance` Python package"
    ]
  },
  {
    "objectID": "python/the-tidyfinance-python-package.html#usage",
    "href": "python/the-tidyfinance-python-package.html#usage",
    "title": "The tidyfinance Python package",
    "section": "Usage",
    "text": "Usage\nThroughout the book, we refer to the corresponding features of the tidyfinance package. If you want to get an overview of the existing functionality, we suggest the PyPI page that discuss specific releases and new features.",
    "crumbs": [
      "R",
      "Prerequisits",
      "The `tidyfinance` Python package"
    ]
  },
  {
    "objectID": "python/the-tidyfinance-python-package.html#feature-requests",
    "href": "python/the-tidyfinance-python-package.html#feature-requests",
    "title": "The tidyfinance Python package",
    "section": "Feature requests",
    "text": "Feature requests\nWe are curious to learn in which direction we should extend the package, so please consider opening an issue in the package repository. For instance, we could support more data sources, add more parameters to the family of functions for data downloads, or we could put more emphasis on the generality of portfolio assignment or other modeling functions. Moreover, if you discover a bug, we are very grateful if you report the issue in our repository.",
    "crumbs": [
      "R",
      "Prerequisits",
      "The `tidyfinance` Python package"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html",
    "href": "python/fama-macbeth-regressions.html",
    "title": "Fama-MacBeth Regressions",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we present a simple implementation of Fama and MacBeth (1973), a regression approach commonly called Fama-MacBeth regressions. Fama-MacBeth regressions are widely used in empirical asset pricing studies. We use individual stocks as test assets to estimate the risk premium associated with the three factors included in Fama and French (1993).\nResearchers use the two-stage regression approach to estimate risk premiums in various markets, but predominately in the stock market. Essentially, the two-step Fama-MacBeth regressions exploit a linear relationship between expected returns and exposure to (priced) risk factors. The basic idea of the regression approach is to project asset returns on factor exposures or characteristics that resemble exposure to a risk factor in the cross-section in each time period. Then, in the second step, the estimates are aggregated across time to test if a risk factor is priced. In principle, Fama-MacBeth regressions can be used in the same way as portfolio sorts introduced in previous chapters.\nThe Fama-MacBeth procedure is a simple two-step approach: The first step uses the exposures (characteristics) as explanatory variables in \\(T\\) cross-sectional regressions. For example, if \\(r_{i,t+1}\\) denote the excess returns of asset \\(i\\) in month \\(t+1\\), then the famous Fama-French three-factor model implies the following return generating process (see also Campbell et al. 1998): \\[\\begin{aligned}r_{i,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{i,t} + \\lambda^{SMB}_t \\beta^{SMB}_{i,t} + \\lambda^{HML}_t \\beta^{HML}_{i,t} + \\epsilon_{i,t}.\\end{aligned} \\tag{1}\\] Here, we are interested in the compensation \\(\\lambda^{f}_t\\) for the exposure to each risk factor \\(\\beta^{f}_{i,t}\\) at each time point, i.e., the risk premium. Note the terminology: \\(\\beta^{f}_{i,t}\\) is an asset-specific characteristic, e.g., a factor exposure or an accounting variable. If there is a linear relationship between expected returns and the characteristic in a given month, we expect the regression coefficient to reflect the relationship, i.e., \\(\\lambda_t^{f}\\neq0\\).\nIn the second step, the time-series average \\(\\frac{1}{T}\\sum_{t=1}^T \\hat\\lambda^{f}_t\\) of the estimates \\(\\hat\\lambda^{f}_t\\) can then be interpreted as the risk premium for the specific risk factor \\(f\\). We follow Zaffaroni and Zhou (2022) and consider the standard cross-sectional regression to predict future returns. If the characteristics are replaced with time \\(t+1\\) variables, then the regression approach captures risk attributes rather than risk premiums.\nBefore we move to the implementation, we want to highlight that the characteristics, e.g., \\(\\hat\\beta^{f}_{i}\\), are often estimated in a separate step before applying the actual Fama-MacBeth methodology. You can think of this as a step 0. You might thus worry that the errors of \\(\\hat\\beta^{f}_{i}\\) impact the risk premiums’ standard errors. Measurement error in \\(\\hat\\beta^{f}_{i}\\) indeed affects the risk premium estimates, i.e., they lead to biased estimates. The literature provides adjustments for this bias (see, e.g., Shanken 1992; Kim 1995; Chen, Lee, and Lee 2015, among others) but also shows that the bias goes to zero as \\(T \\to \\infty\\). We refer to Gagliardini, Ossola, and Scaillet (2016) for an in-depth discussion also covering the case of time-varying betas. Moreover, if you plan to use Fama-MacBeth regressions with individual stocks, Hou, Xue, and Zhang (2020) advocates using weighted-least squares to estimate the coefficients such that they are not biased toward small firms. Without this adjustment, the high number of small firms would drive the coefficient estimates.\nThe current chapter relies on this set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#data-preparation",
    "href": "python/fama-macbeth-regressions.html#data-preparation",
    "title": "Fama-MacBeth Regressions",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe illustrate Fama and MacBeth (1973) with the monthly CRSP sample and use three characteristics to explain the cross-section of returns: Market capitalization, the book-to-market ratio, and the CAPM beta (i.e., the covariance of the excess stock returns with the market excess returns). We collect the data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT permno, gvkey, date, ret_excess, mktcap FROM crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\ncompustat = pd.read_sql_query(\n  sql=\"SELECT datadate, gvkey, be FROM compustat\",\n  con=tidy_finance,\n  parse_dates={\"datadate\"}\n)\n\nbeta = pd.read_sql_query(\n  sql=\"SELECT permno, date, beta FROM beta WHERE return_type = 'monthly'\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nWe use the Compustat and CRSP data to compute the book-to-market ratio and the (log) market capitalization. Furthermore, we also use the CAPM betas based on monthly returns we computed in the previous chapters.\n\ncharacteristics = (compustat\n  .assign(date=lambda x: x[\"datadate\"].dt.to_period(\"M\").dt.to_timestamp())\n  .merge(crsp_monthly, how=\"left\", on=[\"gvkey\", \"date\"], )\n  .merge(beta, how=\"left\", on=[\"permno\", \"date\"])\n  .assign(\n    bm=lambda x: x[\"be\"]/x[\"mktcap\"],\n    log_mktcap=lambda x: np.log(x[\"mktcap\"]),\n    sorting_date=lambda x: x[\"date\"]+pd.DateOffset(months=6)\n  )\n  .get([\"gvkey\", \"bm\", \"log_mktcap\", \"beta\", \"sorting_date\"])\n)\n\ndata_fama_macbeth = (crsp_monthly\n  .merge(characteristics, \n         how=\"left\",\n         left_on=[\"gvkey\", \"date\"], right_on=[\"gvkey\", \"sorting_date\"])\n  .sort_values([\"date\", \"permno\"])\n  .groupby(\"permno\")\n  .apply(lambda x: x.assign(\n      beta=x[\"beta\"].fillna(method=\"ffill\"),\n      bm=x[\"bm\"].fillna(method=\"ffill\"),\n      log_mktcap=x[\"log_mktcap\"].fillna(method=\"ffill\")\n    )\n  )\n  .reset_index(drop=True)  \n)\n\ndata_fama_macbeth_lagged = (data_fama_macbeth\n  .assign(date=lambda x: x[\"date\"]-pd.DateOffset(months=1))\n  .get([\"permno\", \"date\", \"ret_excess\"])\n  .rename(columns={\"ret_excess\": \"ret_excess_lead\"})\n)\n\ndata_fama_macbeth = (data_fama_macbeth\n  .merge(data_fama_macbeth_lagged, how=\"left\", on=[\"permno\", \"date\"])\n  .get([\"permno\", \"date\", \"ret_excess_lead\", \"beta\", \"log_mktcap\", \"bm\"])\n  .dropna()\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#cross-sectional-regression",
    "href": "python/fama-macbeth-regressions.html#cross-sectional-regression",
    "title": "Fama-MacBeth Regressions",
    "section": "Cross-Sectional Regression",
    "text": "Cross-Sectional Regression\nNext, we run the cross-sectional regressions with the characteristics as explanatory variables for each month. We regress the returns of the test assets at a particular time point on the characteristics of each asset. By doing so, we get an estimate of the risk premiums \\(\\hat\\lambda^{f}_t\\) for each point in time. \n\nrisk_premiums = (data_fama_macbeth\n  .groupby(\"date\")\n  .apply(lambda x: smf.ols(\n      formula=\"ret_excess_lead ~ beta + log_mktcap + bm\", \n      data=x\n    ).fit()\n    .params\n  )\n  .reset_index()\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#time-series-aggregation",
    "href": "python/fama-macbeth-regressions.html#time-series-aggregation",
    "title": "Fama-MacBeth Regressions",
    "section": "Time-Series Aggregation",
    "text": "Time-Series Aggregation\nNow that we have the risk premiums’ estimates for each period, we can average across the time-series dimension to get the expected risk premium for each characteristic. Similarly, we manually create the \\(t\\)-test statistics for each regressor, which we can then compare to usual critical values of 1.96 or 2.576 for two-tailed significance tests at a five percent and a one percent significance level.\n\nprice_of_risk = (risk_premiums\n  .melt(id_vars=\"date\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")[\"estimate\"]\n  .apply(lambda x: pd.Series({\n      \"risk_premium\": x.mean(),\n      \"t_statistic\": x.mean()/x.std()*np.sqrt(len(x))\n    })\n  )\n  .reset_index()\n  .pivot(index=\"factor\", columns=\"level_1\", values=\"estimate\")\n  .reset_index()\n)\n\nIt is common to adjust for autocorrelation when reporting standard errors of risk premiums. As in Univariate Portfolio Sorts, the typical procedure for this is computing Newey and West (1987) standard errors.\n\nprice_of_risk_newey_west = (risk_premiums\n  .melt(id_vars=\"date\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")\n  .apply(lambda x: (\n      x[\"estimate\"].mean()/ \n        smf.ols(\"estimate ~ 1\", x)\n        .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6}).bse\n    )\n  )\n  .reset_index()\n  .rename(columns={\"Intercept\": \"t_statistic_newey_west\"})\n)\n\n(price_of_risk\n  .merge(price_of_risk_newey_west, on=\"factor\")\n  .round(3)\n)\n\n\n\n\n\n\n\n\nfactor\nrisk_premium\nt_statistic\nt_statistic_newey_west\n\n\n\n\n0\nIntercept\n0.012\n4.675\n4.067\n\n\n1\nbeta\n0.000\n0.001\n0.001\n\n\n2\nbm\n0.002\n2.946\n2.752\n\n\n3\nlog_mktcap\n-0.001\n-2.837\n-2.655\n\n\n\n\n\n\n\nFinally, let us interpret the results. Stocks with higher book-to-market ratios earn higher expected future returns, which is in line with the value premium. The negative value for log market capitalization reflects the size premium for smaller stocks. Consistent with results from earlier chapters, we detect no relation between beta and future stock returns.\n\nimport tidyfinance as tf\n\ntf.estimate_fama_macbeth(\n  data=data_fama_macbeth,\n  model=\"ret_excess_lead ~ beta + bm + log_mktcap\",\n  vcov=\"newey-west\"\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#key-takeaways",
    "href": "python/fama-macbeth-regressions.html#key-takeaways",
    "title": "Fama-MacBeth Regressions",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFama-MacBeth regressions provide a two-step approach to estimate risk premiums by running time-series averages of cross-sectional regressions on asset characteristics.\nFama-MacBeth regressions are commonly used in empirical asset pricing to test whether factors like size, value, or market beta are priced in the cross-section of stock returns.\nMeasurement error in factor exposures, especially when estimated beforehand, can bias results, but corrections such as Newey-West standard errors and weighted regressions can improve accuracy.\nThe tidyfinance Python package provides a user-friendly estimate_fama_macbeth() function that simplifies the Fama-MacBeth estimation pipeline.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#exercises",
    "href": "python/fama-macbeth-regressions.html#exercises",
    "title": "Fama-MacBeth Regressions",
    "section": "Exercises",
    "text": "Exercises\n\nEstimate stock-specific value and size risk factors to similar to the CAPM-beta using rolling estimation based on Fama-French 3 Factors. Use these estimates instead of the stock characteristics in the Fama-MacBeth regression from above. How do the coefficient estimates differ?\nDownload the 49 Industry Portfolios from Ken French data library. Use these industry portfolios instead of the stocks to estimate the three rolling risk-factors (beta, value, size). Replicate the Fama-MacBeth regression from above. Are the coefficient estimates similar?\nUse individual stocks with weighted-least squares based on a firm’s size as suggested by Hou, Xue, and Zhang (2020). Then, repeat the Fama-MacBeth regressions without the weighting-scheme adjustment but drop the smallest 20 percent of firms each month. Compare the results of the three approaches.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/other-data-providers.html",
    "href": "python/other-data-providers.html",
    "title": "Other Data Providers",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn the previous chapters, we introduced many ways to get financial data that researchers regularly use. We showed how to load data into Python from Yahoo Finance (using the yfinance package) and from Kenneth French’s data library (using the pandas-datareader package). We also presented commonly used file types, such as comma-separated or Excel files. Then, we introduced remotely connecting to WRDS and downloading data from there. However, this is only a subset of the vast amounts of data available online these days.\nIn this chapter, we provide an overview of common alternative data providers for which direct access via Python packages exists. Such a list requires constant adjustments because both data providers and access methods change. However, we want to emphasize two main insights. First, the number of Python packages that provide access to (financial) data is large. Too large actually to survey here exhaustively. Instead, we can only cover the tip of the iceberg. Second, Python provides the functionalities to access any form of files or data available online. Thus, even if a desired data source does not come with a well-established Python package, chances are high that data can be retrieved by establishing your own API connection (using the Python requests package) or by scrapping the content.\nIn our non-exhaustive list below, we restrict ourselves to listing data sources accessed through easy-to-use Python packages. For further inspiration on potential data sources, we recommend reading the Awesome Quant curated list of insanely awesome libraries, packages, and resources for Quants (Quantitative Finance). In fact, the pandas-datareader package provides comprehensive access to a lot of databases, including some of those listed below.\nAlso, the requests library in Python provides a versatile and direct way to interact with APIs (Application Programming Interfaces) offered by various financial data providers. The package simplifies the process of making HTTP requests, handling authentication, and parsing the received data.\nApart from the list below, we want to advertise some amazing data compiled by others. First, there is Open Source Asset Pricing related to Chen and Zimmermann (2022). They provide return data for over 200 trading strategies with different time periods and specifications. The authors also provide signals and explanations of the factor construction. Moreover, in the same spirit, Global factor data provides the data related to Jensen2022b. They provide return data for characteristic-managed portfolios from around the world. The database includes factors for 153 characteristics in 13 themes, using data from 93 countries. Finally, we want to mention the TAQ data providing trades and quotes data for NYSE, Nasdaq, and regional exchanges, which is available via WRDS.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "python/other-data-providers.html#key-takeaways",
    "href": "python/other-data-providers.html#key-takeaways",
    "title": "Other Data Providers",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nAccess a wide range of financial and macroeconomic data through Python packages tailored to specific providers.\nEven if a specific R package doesn’t exist, Python can often retrieve data through APIs or web scraping methods.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "python/other-data-providers.html#exercises",
    "href": "python/other-data-providers.html#exercises",
    "title": "Other Data Providers",
    "section": "Exercises",
    "text": "Exercises\n\nSelect one of the data sources in the table above and retrieve some data. Browse the homepage of the data provider or the package documentation to find inspiration on which type of data is available to you and how to download the data into your Python session.\nGenerate summary statistics of the data you retrieved and provide some useful visualization. The possibilities are endless: Maybe there is some interesting economic event you want to analyze, such as stock market responses to Twitter activity.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Tidy Finance",
    "section": "",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-NC-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution, NonCommercial, and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-NC-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-noncommercial-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-noncommercial-sharealike-4.0-international-public-license",
    "title": "Tidy Finance",
    "section": "",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-NC-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution, NonCommercial, and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-NC-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "workshops/reproducible-research-workflows.html",
    "href": "workshops/reproducible-research-workflows.html",
    "title": "Reproducible Research Workflows",
    "section": "",
    "text": "As top economics and finance journals now mandate code and data sharing, ensuring credibility and longevity of scholarly work is essential. The maintainers of Tidy Finance hence developed a workshop that equips participants with tools and techniques for reproducibility, effective communication, and efficient collaboration. Participants can bring existing research projects to reflect upon and improve their workflows with guidance from instructors and peers. Prior programming experience is beneficial but not required, as the workshop covers foundational knowledge alongside advanced reproducibility techniques applicable to any programming language.\nKey Learnings\n\n  \n    Structuring research projects\n    How to incorporate project-oriented workflows, safe paths, and best practices for file naming\n  \n  \n    Tidy data principles\n    How to deal with any kind of messy data and work with various data storage technologies\n  \n  \n    Tidy coding principles\n    How to write reproducible, readable, and maintainable code\n  \n  \n    Version control\n    How to track collaborative projects and enhance accountability of published results\n  \n  \n    Literate programming\n    How to create reproducible and effective documents\n  \n  \n    Development environments\n    How to consistently and efficiently maintain projects\n  \n\nAbout the Instructors\nThe workshop is conducted by members of the Tidy Finance project team, which promotes a transparent, open-source approach to financial economics research. Alongside authoring the books Tidy Finance with R and Tidy Finance with Python (both Chapman & Hall/CRC), they have published in leading academic journals, including the Journal of Finance, Journal of Financial Economics, Review of Finance, and Journal of Econometrics.\n\n    \n      \n      Christoph Scheuch\n      Independent Expert in Finance & Data\n    \n    \n      \n      Stefan Voigt\n      Assistant Professor of Finance at University of Copenhagen\n    \n    \n      \n      Patrick Weiss\n      Assistant Professor of Finance at Reykjavik University\n    \n    \n      \n      Christoph Frey\n      Quantitative Researcher & Portfolio Manager\n    \n  \n  \n    \n      ×\n      \n      \n    \n  \n  \nAdministrative Details\nOur workshop can be booked either onsite or online, offering flexibility to accommodate your schedule. The online option allows sessions to be spread across several weeks or concentrated within a single week, depending on participant needs.\nThe standard workshop spans three days, featuring six teaching hours per day, and includes a mix of talks, discussions, and hands-on coding labs. For a more condensed experience, the workshop can be tailored to two days, focusing on talks and discussions, or a single day, covering talks only. For detailed pricing, please contact us.\nAdditionally, talks and discussions can be booked as standalone online sessions, with pricing determined by the session’s scope and timing. If desired, the workshop can include assignments and offer course credits. Reach out to us for a customized proposal.\nGet in Touch\nIf you’re interested in participating in a workshop or hosting one at your institution, please reach out to us using the form below.\n\n  \n    Message goes here\n  \n  \n    \n    Your name\n    \n    Your email\n    \n    Your institution or company\n    \n    Your request"
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html",
    "href": "python/size-sorts-and-p-hacking.html",
    "title": "Size Sorts and p-Hacking",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we continue with portfolio sorts in a univariate setting. Yet, we consider firm size as a sorting variable, which gives rise to a well-known return factor: the size premium. The size premium arises from buying small stocks and selling large stocks. Prominently, Fama and French (1993) include it as a factor in their three-factor model. Apart from that, asset managers commonly include size as a key firm characteristic when making investment decisions.\nWe also introduce new choices in the formation of portfolios. In particular, we discuss listing exchanges, industries, weighting regimes, and periods. These choices matter for the portfolio returns and result in different size premiums (see Hasler (2021), Soebhag, Van Vliet, and Verwijmeren (2022), and Walter, Weber, and Weiss (2022) for more insights into decision nodes and their effect on premiums). Exploiting these ideas to generate favorable results is called p-hacking. There is arguably a thin line between p-hacking and conducting robustness tests. Our purpose here is to illustrate the substantial variation that can arise along the evidence-generating process.\nThe chapter relies on the following set of Python packages:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom itertools import product\nfrom joblib import Parallel, delayed, cpu_count\nCompared to previous chapters, we introduce itertools, which is a component of the Python standard library and provides fast, memory-efficient tools for working with iterators.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#data-preparation",
    "href": "python/size-sorts-and-p-hacking.html#data-preparation",
    "title": "Size Sorts and p-Hacking",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we retrieve the relevant data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. Firm size is defined as market equity in most asset pricing applications that we retrieve from CRSP. We further use the Fama-French factor returns for performance evaluation.\n\ntidy_finance = sqlite3.connect(\n  database=\"data/tidy_finance_python.sqlite\"\n)\n\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp_monthly\", \n  con=tidy_finance, \n  parse_dates={\"date\"}\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM factors_ff3_monthly\", \n  con=tidy_finance, \n  parse_dates={\"date\"}\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#size-distribution",
    "href": "python/size-sorts-and-p-hacking.html#size-distribution",
    "title": "Size Sorts and p-Hacking",
    "section": "Size Distribution",
    "text": "Size Distribution\nBefore we build our size portfolios, we investigate the distribution of the variable firm size. Visualizing the data is a valuable starting point to understand the input to the analysis. Figure 1 shows the fraction of total market capitalization concentrated in the largest firm. To produce this graph, we create monthly indicators that track whether a stock belongs to the largest x percent of the firms. Then, we aggregate the firms within each bucket and compute the buckets’ share of total market capitalization.\nFigure 1 shows that the largest 1 percent of firms cover up to 50 percent of the total market capitalization, and holding just the 25 percent largest firms in the CRSP universe essentially replicates the market portfolio. The distribution of firm size thus implies that the largest firms of the market dominate many small firms whenever we use value-weighted benchmarks.\n\nmarket_cap_concentration = (crsp_monthly\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n    top01=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.99)),\n    top05=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.95)),\n    top10=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.90)),\n    top25=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.75)))\n  )\n  .reset_index(drop=True)\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"Largest 1%\": x[\"mktcap\"][x[\"top01\"]].sum()/x[\"mktcap\"].sum(),\n    \"Largest 5%\": x[\"mktcap\"][x[\"top05\"]].sum()/x[\"mktcap\"].sum(),\n    \"Largest 10%\": x[\"mktcap\"][x[\"top10\"]].sum()/x[\"mktcap\"].sum(),\n    \"Largest 25%\": x[\"mktcap\"][x[\"top25\"]].sum()/x[\"mktcap\"].sum()\n    })\n  )\n  .reset_index()\n  .melt(id_vars=\"date\", var_name=\"name\", value_name=\"value\")\n)\n\nmarket_cap_concentration_figure = (\n  ggplot(\n    market_cap_concentration, \n    aes(x=\"date\", y=\"value\", color=\"name\", linetype=\"name\")\n  )\n  + geom_line()\n  + scale_y_continuous(labels=percent_format())\n  + scale_x_date(name=\"\", date_labels=\"%Y\")\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\", \n      title=\"Percentage of total market capitalization in largest stocks\"\n    )\n  + theme(legend_title=element_blank())\n)\nmarket_cap_concentration_figure.show()\n\n\n\n\n\n\n\nFigure 1: The figure shows the percentage of total market capitalization in largest stocks. We report the aggregate market capitalization of all stocks that belong to the 1, 5, 10, and 25 percent quantile of the largest firms in the monthly cross-section relative to the market capitalization of all stocks during the month.\n\n\n\n\n\nNext, firm sizes also differ across listing exchanges. The primary listings of stocks were important in the past and are potentially still relevant today. Figure 2 shows that the New York Stock Exchange (NYSE) was and still is the largest listing exchange in terms of market capitalization. More recently, NASDAQ has gained relevance as a listing exchange. Do you know what the small peak in NASDAQ’s market cap around the year 2000 was?\n\nmarket_cap_share = (crsp_monthly\n  .groupby([\"date\", \"exchange\"])\n  .aggregate({\"mktcap\": \"sum\"})\n  .reset_index(drop=False)\n  .groupby(\"date\")\n  .apply(lambda x:\n    x.assign(total_market_cap=lambda x: x[\"mktcap\"].sum(),\n             share=lambda x: x[\"mktcap\"]/x[\"total_market_cap\"]\n             )\n    )\n  .reset_index(drop=True)\n)\n\nplot_market_cap_share = (\n  ggplot(market_cap_share, \n         aes(x=\"date\", y=\"share\", \n             fill=\"exchange\", color=\"exchange\")) +\n  geom_area(position=\"stack\", stat=\"identity\", alpha=0.5) +\n  geom_line(position=\"stack\") +\n  scale_y_continuous(labels=percent_format()) +\n  scale_x_date(name=\"\", date_labels=\"%Y\") +\n  labs(x=\"\", y=\"\", fill=\"\", color=\"\",\n       title=\"Share of total market capitalization per listing exchange\") +\n  theme(legend_title=element_blank())\n)\nplot_market_cap_share.draw()\n\n\n\n\n\n\n\nFigure 2: The figure shows the share of total market capitalization per listing exchange. Years are on the horizontal axis and the corresponding share of total market capitalization per listing exchange on the vertical axis.\n\n\n\n\n\nFinally, we consider the distribution of firm size across listing exchanges and create summary statistics. The function describe() does not include all statistics we are interested in, which is why we create the function compute_summary() that adds the standard deviation and the number of observations. Then, we apply it to the most current month of our CRSP data on each listing exchange. We also add a row with the overall summary statistics.\nThe resulting table shows that firms listed on NYSE in December 2023 are significantly larger on average than firms listed on the other exchanges. Moreover, NASDAQ lists the largest number of firms. This discrepancy between firm sizes across listing exchanges motivated researchers to form breakpoints exclusively on the NYSE sample and apply those breakpoints to all stocks. In the following, we use this distinction to update our portfolio sort procedure.\n\ndef compute_summary(data, variable, filter_variable, percentiles):\n    \"\"\"Compute summary statistics for a given variable and percentiles.\"\"\"\n    \n    summary = (data\n      .get([filter_variable, variable])\n      .groupby(filter_variable)\n      .describe(percentiles=percentiles)\n    ) \n    \n    summary.columns = summary.columns.droplevel(0)\n    \n    summary_overall = (data\n      .get(variable)\n      .describe(percentiles=percentiles)\n    )\n    \n    summary.loc[\"Overall\", :] = summary_overall\n    \n    return summary.round(0)\n\ncompute_summary(\n  crsp_monthly[crsp_monthly[\"date\"] == crsp_monthly[\"date\"].max()],\n  variable=\"mktcap\",\n  filter_variable=\"exchange\",\n  percentiles=[0.05, 0.5, 0.95]\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nexchange\n\n\n\n\n\n\n\n\n\n\n\n\nAMEX\n166.0\n422.0\n3182.0\n1.0\n4.0\n47.0\n850.0\n40673.0\n\n\nNASDAQ\n2556.0\n8920.0\n97176.0\n0.0\n6.0\n263.0\n16817.0\n2976557.0\n\n\nNYSE\n1289.0\n18432.0\n51240.0\n13.0\n120.0\n3515.0\n77878.0\n553370.0\n\n\nOverall\n4011.0\n11625.0\n82977.0\n0.0\n8.0\n601.0\n40704.0\n2976557.0",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "href": "python/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "title": "Size Sorts and p-Hacking",
    "section": "Univariate Size Portfolios with Flexible Breakpoints",
    "text": "Univariate Size Portfolios with Flexible Breakpoints\nIn Univariate Portfolio Sorts, we construct portfolios with a varying number of breakpoints and different sorting variables. Here, we extend the framework such that we compute breakpoints on a subset of the data, for instance, based on selected listing exchanges. In published asset pricing articles, many scholars compute sorting breakpoints only on NYSE-listed stocks. These NYSE-specific breakpoints are then applied to the entire universe of stocks.\nTo replicate the NYSE-centered sorting procedure, we introduce exchanges as an argument in our assign_portfolio() function from Univariate Portfolio Sorts. The exchange-specific argument then enters in the filter data[\"exchanges\"].isin(exchanges). For example, if exchanges='NYSE' is specified, only stocks listed on NYSE are used to compute the breakpoints. Alternatively, you could specify exchanges=[\"NYSE\", \"NASDAQ\", \"AMEX\"], which keeps all stocks listed on either of these exchanges.\n\ndef assign_portfolio(data, exchanges, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolio for a given sorting variable.\"\"\"\n    \n    breakpoints = (data\n      .query(f\"exchange in {exchanges}\")\n      .get(sorting_variable)\n      .quantile(np.linspace(0, 1, num=n_portfolios+1), \n                interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    breakpoints.iloc[[0, -1]] = [-np.Inf, np.Inf]\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nNote that the tidyfinance package also provides a assing_portfolio() function, albeit with more flexibility. For the sake of simplicity, we continue to use the function that we just defined.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "href": "python/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "title": "Size Sorts and p-Hacking",
    "section": "Weighting Schemes for Portfolios",
    "text": "Weighting Schemes for Portfolios\nApart from computing breakpoints on different samples, researchers often use different portfolio weighting schemes. So far, we weighted each portfolio constituent by its relative market equity of the previous period. This protocol is called value-weighting. The alternative protocol is equal-weighting, which assigns each stock’s return the same weight, i.e., a simple average of the constituents’ returns. Notice that equal-weighting is difficult in practice as the portfolio manager needs to rebalance the portfolio monthly, while value-weighting is a truly passive investment.\nWe implement the two weighting schemes in the function compute_portfolio_returns() that takes a logical argument to weight the returns by firm value. Additionally, the long-short portfolio is long in the smallest firms and short in the largest firms, consistent with research showing that small firms outperform their larger counterparts. Apart from these two changes, the function is similar to the procedure in Univariate Portfolio Sorts.\n\ndef calculate_returns(data, value_weighted):\n    \"\"\"Calculate (value-weighted) returns.\"\"\"\n    \n    if value_weighted:\n      return np.average(data[\"ret_excess\"], weights=data[\"mktcap_lag\"])\n    else:\n      return data[\"ret_excess\"].mean()\n          \ndef compute_portfolio_returns(n_portfolios=10, \n                              exchanges=[\"NYSE\", \"NASDAQ\", \"AMEX\"], \n                              value_weighted=True, \n                              data=crsp_monthly):\n    \"\"\"Compute (value-weighted) portfolio returns.\"\"\"\n    \n    returns = (data\n      .groupby(\"date\")\n      .apply(lambda x: x.assign(\n        portfolio=assign_portfolio(x, exchanges, \n                                   \"mktcap_lag\", n_portfolios))\n      )\n      .reset_index(drop=True)\n      .groupby([\"portfolio\", \"date\"])\n      .apply(lambda x: x.assign(\n        ret=calculate_returns(x, value_weighted))\n      )\n      .reset_index(drop=True)\n      .groupby(\"date\")\n      .apply(lambda x: \n        pd.Series({\"size_premium\": x.loc[x[\"portfolio\"].idxmin(), \"ret\"]-\n          x.loc[x[\"portfolio\"].idxmax(), \"ret\"]}))\n      .reset_index(drop=True)\n      .aggregate({\"size_premium\": \"mean\"})\n    )\n    \n    return returns\n\nTo see how the function compute_portfolio_returns() works, we consider a simple median breakpoint example with value-weighted returns. We are interested in the effect of restricting listing exchanges on the estimation of the size premium. In the first function call, we compute returns based on breakpoints from all listing exchanges. Then, we computed returns based on breakpoints from NYSE-listed stocks.\n\nret_all = compute_portfolio_returns(\n  n_portfolios=2,\n  exchanges=[\"NYSE\", \"NASDAQ\", \"AMEX\"],\n  value_weighted=True,\n  data=crsp_monthly\n)\n\nret_nyse = compute_portfolio_returns(\n  n_portfolios=2,\n  exchanges=[\"NYSE\"],\n  value_weighted=True,\n  data=crsp_monthly\n)\n\ndata = pd.DataFrame([ret_all*100, ret_nyse*100], \n                    index=[\"NYSE, NASDAQ & AMEX\", \"NYSE\"])\ndata.columns = [\"Premium\"]\ndata.round(2)\n\n\n\n\n\n\n\n\nPremium\n\n\n\n\nNYSE, NASDAQ & AMEX\n0.06\n\n\nNYSE\n0.15\n\n\n\n\n\n\n\nThe table shows that the size premium is more than 60 percent larger if we consider only stocks from NYSE to form the breakpoint each month. The NYSE-specific breakpoints are larger, and there are more than 50 percent of the stocks in the entire universe in the resulting small portfolio because NYSE firms are larger on average. The impact of this choice is not negligible.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "href": "python/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "title": "Size Sorts and p-Hacking",
    "section": "P-Hacking and Non-Standard Errors",
    "text": "P-Hacking and Non-Standard Errors\nSince the choice of the listing exchange has a significant impact, the next step is to investigate the effect of other data processing decisions researchers have to make along the way. In particular, any portfolio sort analysis has to decide at least on the number of portfolios, the listing exchanges to form breakpoints, and equal- or value-weighting. Further, one may exclude firms that are active in the finance industry or restrict the analysis to some parts of the time series. All of the variations of these choices that we discuss here are part of scholarly articles published in the top finance journals. We refer to Walter, Weber, and Weiss (2022) for an extensive set of other decision nodes at the discretion of researchers.\nThe intention of this application is to show that the different ways to form portfolios result in different estimated size premiums. Despite the effects of this multitude of choices, there is no correct way. It should also be noted that none of the procedures is wrong. The aim is simply to illustrate the changes that can arise due to the variation in the evidence-generating process (Menkveld et al., n.d.). The term non-standard errors refers to the variation due to (suitable) choices made by researchers. Interestingly, in a large-scale study, Menkveld et al. (n.d.) find that the magnitude of non-standard errors are similar to the estimation uncertainty based on a chosen model. This shows how important it is to adjust for the seemingly innocent choices in the data preparation and evaluation workflow. Moreover, it seems that this methodology-related uncertainty should be embraced rather than hidden away.\nFrom a malicious perspective, these modeling choices give the researcher multiple chances to find statistically significant results. Yet this is considered p-hacking, which renders the statistical inference invalid due to multiple testing (Harvey, Liu, and Zhu 2016).\nNevertheless, the multitude of options creates a problem since there is no single correct way of sorting portfolios. How should a researcher convince a reader that their results do not come from a p-hacking exercise? To circumvent this dilemma, academics are encouraged to present evidence from different sorting schemes as robustness tests and report multiple approaches to show that a result does not depend on a single choice. Thus, the robustness of premiums is a key feature.\nBelow, we conduct a series of robustness tests, which could also be interpreted as a p-hacking exercise. To do so, we examine the size premium in different specifications presented in the table p_hacking_setup. The function itertools.product() produces all possible permutations of its arguments. Note that we use the argument data to exclude financial firms and truncate the time series.\n\nn_portfolios = [2, 5, 10]\nexchanges = [[\"NYSE\"], [\"NYSE\", \"NASDAQ\", \"AMEX\"]]\nvalue_weighted = [True, False]\ndata = [\n  crsp_monthly,\n  crsp_monthly[crsp_monthly[\"industry\"] != \"Finance\"],\n  crsp_monthly[crsp_monthly[\"date\"] &lt; \"1990-06-01\"],\n  crsp_monthly[crsp_monthly[\"date\"] &gt;= \"1990-06-01\"],\n]\np_hacking_setup = list(\n  product(n_portfolios, exchanges, value_weighted, data)\n)\n\nTo speed the computation up, we parallelize the (many) different sorting procedures, as in Beta Estimation using the joblib package. Finally, we report the resulting size premiums in descending order. There are indeed substantial size premiums possible in our data, in particular when we use equal-weighted portfolios.\n\nn_cores = cpu_count()-1\np_hacking_results = pd.concat(\n  Parallel(n_jobs=n_cores)\n  (delayed(compute_portfolio_returns)(x, y, z, w) \n   for x, y, z, w in p_hacking_setup)\n)\np_hacking_results = p_hacking_results.reset_index(name=\"size_premium\")",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#size-premium-variation",
    "href": "python/size-sorts-and-p-hacking.html#size-premium-variation",
    "title": "Size Sorts and p-Hacking",
    "section": "Size-Premium Variation",
    "text": "Size-Premium Variation\nWe provide a graph in Figure 3 that shows the different premiums. The figure also shows the relation to the average Fama-French SMB (small minus big) premium used in the literature, which we include as a dotted vertical line.\n\np_hacking_results_figure = (\n  ggplot(\n    p_hacking_results, \n    aes(x=\"size_premium\")\n  )\n  + geom_histogram(bins=len(p_hacking_results))\n  + scale_x_continuous(labels=percent_format())\n  + labs(\n      x=\"\", y=\"\",\n      title=\"Distribution of size premiums for various sorting choices\"\n    )\n  + geom_vline(\n      aes(xintercept=factors_ff3_monthly[\"smb\"].mean()), linetype=\"dashed\"\n    )\n)\np_hacking_results_figure.show()\n\n\n\n\n\n\n\nFigure 3: The figure shows the distribution of size premiums for various sorting choices. The dashed vertical line indicates the average Fama-French SMB premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#key-takeaways",
    "href": "python/size-sorts-and-p-hacking.html#key-takeaways",
    "title": "Size Sorts and p-Hacking",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFirm size is a crucial factor in asset pricing, and sorting stocks by size reveals the size premium, where small-cap stocks tend to outperform large-cap stocks.\nPortfolio returns are sensitive to research design choices like exchange filters, weighting schemes, and the number of portfolios—decisions that can meaningfully shift results.\nMethodological flexibility can lead to non-standard errors and potential p-hacking.\nValidate results by varying assumptions and show that findings hold across multiple specifications.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#exercises",
    "href": "python/size-sorts-and-p-hacking.html#exercises",
    "title": "Size Sorts and p-Hacking",
    "section": "Exercises",
    "text": "Exercises\n\nWe gained several insights on the size distribution above. However, we did not analyze the average size across listing exchanges and industries. Which listing exchanges/industries have the largest firms? Plot the average firm size for the three listing exchanges over time. What do you conclude?\nWe compute breakpoints but do not take a look at them in the exposition above. This might cover potential data errors. Plot the breakpoints for ten size portfolios over time. Then, take the difference between the two extreme portfolios and plot it. Describe your results.\nThe returns that we analyze above do not account for differences in the exposure to market risk, i.e., the CAPM beta. Change the function compute_portfolio_returns() to output the CAPM alpha or beta instead of the average excess return.\nWhile you saw the spread in returns from the p-hacking exercise, we did not show which choices led to the largest effects. Find a way to investigate which choice variable has the largest impact on the estimated size premium.\nWe computed several size premiums, but they do not follow the definition of Fama and French (1993). Which of our approaches comes closest to their SMB premium?",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html",
    "href": "python/replicating-fama-and-french-factors.html",
    "title": "Replicating Fama-French Factors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we provide a replication of the famous Fama-French factor portfolios. The Fama-French factor models are a cornerstone of empirical asset pricing Fama and French (2015). On top of the market factor represented by the traditional CAPM beta, the three-factor model includes the size and value factors to explain the cross section of returns. Its successor, the five-factor model, additionally includes profitability and investment as explanatory factors.\nWe start with the three-factor model. We already introduced the size and value factors in Value and Bivariate Sorts, and their definition remains the same: size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nAfter the replication of the three-factor model, we move to the five-factors by constructing the profitability factor RMW (robust-minus-weak) as the difference between the returns of firms with high and low operating profitability and the investment factor CMA (conservative-minus-aggressive) as the difference between firms with high versus low investment rates.\nThe current chapter relies on this set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n\nfrom regtabletotext import prettify_result",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#data-preparation",
    "href": "python/replicating-fama-and-french-factors.html#data-preparation",
    "title": "Replicating Fama-French Factors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and Compustat as data sources, as we need exactly the same variables to compute the size and value factors in the way Fama and French do it. Hence, there is nothing new below, and we only load data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.1 \n\ntidy_finance = sqlite3.connect(\n  database=\"data/tidy_finance_python.sqlite\"\n)\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=(\"SELECT permno, gvkey, date, ret_excess, mktcap, \"\n         \"mktcap_lag, exchange FROM crsp_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .dropna()\n)\n\ncompustat = (pd.read_sql_query(\n    sql=\"SELECT gvkey, datadate, be, op, inv FROM compustat\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"})\n  .dropna()\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT date, smb, hml FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nfactors_ff5_monthly = pd.read_sql_query(\n  sql=\"SELECT date, smb, hml, rmw, cma FROM factors_ff5_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nYet when we start merging our dataset for computing the premiums, there are a few differences to Value and Bivariate Sorts. First, Fama and French form their portfolios in June of year \\(t\\), whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year \\(t+1\\).\nSecond, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year \\(t - 1\\) and the book equity reported in year \\(t-1\\), i.e., the datadate is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity.\nTo implement all these time lags, we again employ the temporary sorting_date-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year. We ensure this by a call of drop_duplicates() at the end of the chunk below.\n\nsize = (crsp_monthly\n  .query(\"date.dt.month == 6\")\n  .assign(sorting_date=lambda x: (x[\"date\"]+pd.DateOffset(months=1)))\n  .get([\"permno\", \"exchange\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"size\"})\n)\n\nmarket_equity = (crsp_monthly\n  .query(\"date.dt.month == 12\")\n  .assign(sorting_date=lambda x: (x[\"date\"]+pd.DateOffset(months=7)))\n  .get([\"permno\", \"gvkey\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"me\"})\n)\n\nbook_to_market = (compustat\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      (x[\"datadate\"].dt.year+1).astype(str)+\"0701\", format=\"%Y%m%d\")\n    )\n  )\n  .merge(market_equity, how=\"inner\", on=[\"gvkey\", \"sorting_date\"])\n  .assign(bm=lambda x: x[\"be\"]/x[\"me\"])\n  .get([\"permno\", \"sorting_date\", \"me\", \"bm\"])\n)\n\nsorting_variables = (size\n  .merge(book_to_market, how=\"inner\", on=[\"permno\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"permno\", \"sorting_date\"])\n )",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#portfolio-sorts",
    "href": "python/replicating-fama-and-french-factors.html#portfolio-sorts",
    "title": "Replicating Fama-French Factors",
    "section": "Portfolio Sorts",
    "text": "Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on NYSE-specific breakpoints to independently form two portfolios in the size dimension at the median and three portfolios in the dimension of book-to-market at the 30- and 70-percentiles. The sorts for book-to-market require an adjustment to the function in Value and Bivariate Sorts because it would not produce the right breakpoints. Instead of n_portfolios, we now specify percentiles, which takes the sequence of breakpoints as an object specified in the function’s call. Specifically, we give percentiles = [0.3, 0.7] to the function. Additionally, we perform a join with our return data to ensure that we only use traded stocks when computing the breakpoints as a first step.\n\ndef assign_portfolio(data, sorting_variable, percentiles):\n    \"\"\"Assign portfolios to a bin according to a sorting variable.\"\"\"\n    \n    breakpoints = (data\n      .query(\"exchange == 'NYSE'\")\n      .get(sorting_variable)\n      .quantile([0] + percentiles + [1], interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    breakpoints.iloc[0] = -np.Inf\n    breakpoints.iloc[breakpoints.size-1] = np.Inf\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=pd.Series(range(1, breakpoints.size)),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nportfolios = (sorting_variables\n  .groupby(\"sorting_date\")\n  .apply(lambda x: x\n    .assign(\n      portfolio_size=assign_portfolio(x, \"size\", [0, 0.5, 1]),\n      portfolio_bm=assign_portfolio(x, \"bm\", [0, 0.3, 0.7, 1])\n    )\n  )\n  .reset_index(drop=True)\n  .get([\"permno\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"])\n)\n\nAlternatively, you can implement the same portfolio sorts using the assign_portfolio() function from the tidyfinance package, which we omit to avoid repeating almost the same code chunk as above.\nNext, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column sorting_date in our return data by setting the date to sort on to July of \\(t-1\\) if the month is June (of year \\(t\\)) or earlier or to July of year \\(t\\) if the month is July or later.\n\nportfolios = (crsp_monthly\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      x[\"date\"].apply(lambda x: str(x.year-1)+\n        \"0701\" if x.month &lt;= 6 else str(x.year)+\"0701\")))\n  )\n  .merge(portfolios, how=\"inner\", on=[\"permno\", \"sorting_date\"])\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#fama-french-three-factor-model",
    "href": "python/replicating-fama-and-french-factors.html#fama-french-three-factor-model",
    "title": "Replicating Fama-French Factors",
    "section": "Fama-French Three-Factor Model",
    "text": "Fama-French Three-Factor Model\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama-French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally (using the mean() function).\n\nfactors_replicated = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"])\n  .apply(lambda x: pd.Series({\n    \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n   )\n  .reset_index()\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"smb_replicated\": (\n      x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_size\"] == 2].mean()),\n    \"hml_replicated\": (\n      x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() -\n        x[\"ret\"][x[\"portfolio_bm\"] == 1].mean())\n    }))\n  .reset_index()\n)\n\nfactors_replicated = (factors_replicated\n  .merge(factors_ff3_monthly, how=\"inner\", on=\"date\")\n  .round(4)\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#replication-evaluation",
    "href": "python/replicating-fama-and-french-factors.html#replication-evaluation",
    "title": "Replicating Fama-French Factors",
    "section": "Replication Evaluation",
    "text": "Replication Evaluation\nIn the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using smf.ols(). If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to one (indicating a high correlation), and an adjusted R-squared close to one (indicating a high proportion of explained variance).\nTo test the success of the SMB factor, we hence run the following regression:\n\nmodel_smb = (smf.ols(\n    formula=\"smb ~ smb_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_smb)\n\nOLS Model:\nsmb ~ smb_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          -0.00       0.000       -1.855    0.064\nsmb_replicated      0.98       0.004      235.865    0.000\n\nSummary statistics:\n- Number of observations: 750\n- R-squared: 0.987, Adjusted R-squared: 0.987\n- F-statistic: 55,632.234 on 1 and 748 DF, p-value: 0.000\n\n\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient is 0.98 and R-squared are at 99 percent.\n\nmodel_hml = (smf.ols(\n    formula=\"hml ~ hml_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_hml)\n\nOLS Model:\nhml ~ hml_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          0.000       0.000        2.201    0.028\nhml_replicated     0.956       0.007      138.271    0.000\n\nSummary statistics:\n- Number of observations: 750\n- R-squared: 0.962, Adjusted R-squared: 0.962\n- F-statistic: 19,118.756 on 1 and 748 DF, p-value: 0.000\n\n\n\nThe replication of the HML factor is also a success, although at a slightly lower level with coefficient of 0.96 and R-squared around 96 percent.\nThe evidence allows us to conclude that we did a relatively good job in replicating the original Fama-French size and value premiums, although we do not know their underlying code. From our perspective, a perfect match is only possible with additional information from the maintainers of the original data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#fama-french-five-factor-model",
    "href": "python/replicating-fama-and-french-factors.html#fama-french-five-factor-model",
    "title": "Replicating Fama-French Factors",
    "section": "Fama-French Five-Factor Model",
    "text": "Fama-French Five-Factor Model\nNow, let us move to the replication of the five-factor model. We extend the other_sorting_variables table from above with the additional characteristics operating profitability op and investment inv. Note that the dropna() statement yields different sample sizes, as some firms with be values might not have op or inv values.\n\nother_sorting_variables = (compustat\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      (x[\"datadate\"].dt.year+1).astype(str)+\"0701\", format=\"%Y%m%d\")\n    )\n  )\n  .merge(market_equity, how=\"inner\", on=[\"gvkey\", \"sorting_date\"])\n  .assign(bm=lambda x: x[\"be\"]/x[\"me\"])\n  .get([\"permno\", \"sorting_date\", \"me\", \"bm\", \"op\", \"inv\"])\n)\n\nsorting_variables = (size\n  .merge(other_sorting_variables, how=\"inner\", on=[\"permno\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"permno\", \"sorting_date\"])\n )\n\nIn each month, we independently sort all stocks into the two size portfolios. The value, profitability, and investment portfolios, on the other hand, are the results of dependent sorts based on the size portfolios. We then merge the portfolios to the return data for the rest of the year just as above.\n\nportfolios = (sorting_variables\n  .groupby(\"sorting_date\")\n  .apply(lambda x: x\n    .assign(\n      portfolio_size=assign_portfolio(x, \"size\", [0, 0.5, 1])\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"sorting_date\", \"portfolio_size\"])\n  .apply(lambda x: x\n    .assign(\n      portfolio_bm=assign_portfolio(x, \"bm\", [0, 0.3, 0.7, 1]),\n      portfolio_op=assign_portfolio(x, \"op\", [0, 0.3, 0.7, 1]),\n      portfolio_inv=assign_portfolio(x, \"inv\", [0, 0.3, 0.7, 1])\n    )\n  )\n  .reset_index(drop=True)\n  .get([\"permno\", \"sorting_date\", \n        \"portfolio_size\", \"portfolio_bm\",\n        \"portfolio_op\", \"portfolio_inv\"])\n)\n\nportfolios = (crsp_monthly\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      x[\"date\"].apply(lambda x: str(x.year-1)+\n        \"0701\" if x.month &lt;= 6 else str(x.year)+\"0701\")))\n  )\n  .merge(portfolios, how=\"inner\", on=[\"permno\", \"sorting_date\"])\n)\n\nNow, we want to construct each of the factors, but this time, the size factor actually comes last because it is the result of averaging across all other factor portfolios. This dependency is the reason why we keep the table with value-weighted portfolio returns as a separate object that we reuse later. We construct the value factor, HML, as above by going long the two portfolios with high book-to-market ratios and shorting the two portfolios with low book-to-market.\n\nportfolios_value = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_value = (portfolios_value\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"hml_replicated\": (\n      x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_bm\"] == 1].mean())})\n  )\n  .reset_index()\n)\n\nFor the profitability factor, RMW (robust-minus-weak), we take a long position in the two high profitability portfolios and a short position in the two low profitability portfolios.\n\nportfolios_profitability = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_profitability = (portfolios_profitability\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"rmw_replicated\": (\n      x[\"ret\"][x[\"portfolio_op\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_op\"] == 1].mean())})\n  )\n  .reset_index()\n)\n\nFor the investment factor, CMA (conservative-minus-aggressive), we go long the two low investment portfolios and short the two high investment portfolios.\n\nportfolios_investment = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_investment = (portfolios_investment\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"cma_replicated\": (\n      x[\"ret\"][x[\"portfolio_inv\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_inv\"] == 3].mean())})\n  )\n  .reset_index()\n)\n\nFinally, the size factor, SMB, is constructed by going long the nine small portfolios and short the nine large portfolios.\n\nfactors_size = (\n  pd.concat(\n    [portfolios_value, portfolios_profitability, portfolios_investment], \n    ignore_index=True\n  )\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"smb_replicated\": (\n      x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_size\"] == 2].mean())})\n  )\n  .reset_index()\n)\n\nWe then join all factors together into one dataframe and construct again a suitable table to run tests for evaluating our replication.\n\nfactors_replicated = (factors_size\n  .merge(factors_value, how=\"outer\", on=\"date\")\n  .merge(factors_profitability, how=\"outer\", on=\"date\")\n  .merge(factors_investment, how=\"outer\", on=\"date\")\n)\n\nfactors_replicated = (factors_replicated\n  .merge(factors_ff5_monthly, how=\"inner\", on=\"date\")\n  .round(4)\n)\n\nLet us start the replication evaluation again with the size factor:\n\nmodel_smb = (smf.ols(\n    formula=\"smb ~ smb_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_smb)\n\nOLS Model:\nsmb ~ smb_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          -0.00       0.000       -1.715    0.087\nsmb_replicated      0.96       0.004      234.692    0.000\n\nSummary statistics:\n- Number of observations: 738\n- R-squared: 0.987, Adjusted R-squared: 0.987\n- F-statistic: 55,080.127 on 1 and 736 DF, p-value: 0.000\n\n\n\nThe results for the SMB factor are quite convincing, as all three criteria outlined above are met and the coefficient is 0.96 and the R-squared is at 99 percent.\n\nmodel_hml = (smf.ols(\n    formula=\"hml ~ hml_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_hml)\n\nOLS Model:\nhml ~ hml_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          0.001        0.00        2.073    0.038\nhml_replicated     0.980        0.01      100.031    0.000\n\nSummary statistics:\n- Number of observations: 738\n- R-squared: 0.931, Adjusted R-squared: 0.931\n- F-statistic: 10,006.277 on 1 and 736 DF, p-value: 0.000\n\n\n\nThe replication of the HML factor is also a success, although at a slightly higher coefficient of 0.98 and an R-squared around 93 percent.\n\nmodel_rmw = (smf.ols(\n    formula=\"rmw ~ rmw_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_rmw)\n\nOLS Model:\nrmw ~ rmw_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          0.000       0.000        0.217    0.828\nrmw_replicated     0.949       0.009      108.608    0.000\n\nSummary statistics:\n- Number of observations: 738\n- R-squared: 0.941, Adjusted R-squared: 0.941\n- F-statistic: 11,795.636 on 1 and 736 DF, p-value: 0.000\n\n\n\nWe are also able to replicate the RMW factor quite well with a coefficient of 0.95 and an R-squared around 94 percent.\n\nmodel_cma = (smf.ols(\n    formula=\"cma ~ cma_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_cma)\n\nOLS Model:\ncma ~ cma_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          0.000       0.000        3.096    0.002\ncma_replicated     0.955       0.008      117.013    0.000\n\nSummary statistics:\n- Number of observations: 738\n- R-squared: 0.949, Adjusted R-squared: 0.949\n- F-statistic: 13,692.051 on 1 and 736 DF, p-value: 0.000\n\n\n\nFinally, the CMA factor also replicates well with a coefficient of 0.96 and an R-squared around 95 percent.\nOverall, our approach seems to replicate the Fama-French three and five-factor models just as well as the three-factors.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#key-takeaways",
    "href": "python/replicating-fama-and-french-factors.html#key-takeaways",
    "title": "Replicating Fama-French Factors",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe three-factor model adds size (SMB) and value (HML) to the traditional CAPM, while the five-factor model extends this with profitability (RMW) and investment (CMA) factors.\nThe portfolio construction follows the original Fama-French methodology, including NYSE breakpoints, specific time lags, and sorting rules based on firm characteristics.\nThe quality of replication can be evaluated using regression analysis and confirms strong alignment with the original Fama-French data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#exercises",
    "href": "python/replicating-fama-and-french-factors.html#exercises",
    "title": "Replicating Fama-French Factors",
    "section": "Exercises",
    "text": "Exercises\n\nReplicate the market factor mkt_excess from the factors_ff3_monthly data as the value-weight return of all CRSP firms incorporated in the US and listed on the NYSE, AMEX, or NASDAQ that have a CRSP share code of 10 or 11. Assess your replication effort using linear regressions as above.\nFama and French (1993) claim that their sample excludes firms until they have appeared in Compustat for two years. Implement this additional filter and compare the improvements of your replication effort.\nOn his homepage, Kenneth French provides instructions on how to construct the most common variables used for portfolio sorts. Try to replicate the univariate portfolio sort return time series for E/P (earnings/price) provided on his homepage and evaluate your replication effort using regressions.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#footnotes",
    "href": "python/replicating-fama-and-french-factors.html#footnotes",
    "title": "Replicating Fama-French Factors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that Fama and French (1992) claim to exclude financial firms. To a large extent this happens through using industry format “INDL”, as we do in WRDS, CRSP, and Compustat. Neither the original paper, nor Ken French’s website, or the WRDS replication contains any indication that financial companies are excluded using additional filters such as industry codes.↩︎",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html",
    "href": "python/trace-and-fisd.html",
    "title": "TRACE and FISD",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we dive into the US corporate bond market. Bond markets are far more diverse than stock markets, as most issuers have multiple bonds outstanding simultaneously with potentially very different indentures. This market segment is exciting due to its size (roughly ten trillion USD outstanding), heterogeneity of issuers (as opposed to government bonds), market structure (mostly over-the-counter trades), and data availability. We introduce how to use bond characteristics from FISD and trade reports from TRACE and provide code to download and clean TRACE in Python.\nMany researchers study liquidity in the US corporate bond market, with notable contributions from Bessembinder, Maxwell, and Venkataraman (2006), Edwards, Harris, and Piwowar (2007), and O’Hara and Zhou (2021), among many others. We do not cover bond returns here, but you can compute them from TRACE data. Instead, we refer to studies on the topic such as Bessembinder et al. (2008), Bai, Bali, and Wen (2019), and Kelly, Palhares, and Pruitt (2021) and a survey by Huang and Shi (2021).\nThis chapter also draws on the resources provided by the project Open Source Bond Asset Pricing and their related publication, i.e., Dickerson, Mueller, and Robotti (2023). We encourage you to visit their website to check out the additional resources they provide. Moreover, WRDS provides bond returns computed from TRACE data at a monthly frequency.\nThe current chapter relies on the following set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\nimport sqlite3\nimport httpimport\n\nfrom plotnine import *\nfrom sqlalchemy import create_engine\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format, comma_format\nCompared to previous chapters, we load httpimport (Torakis 2023) to source code provided in the public Gist. Note that you should be careful with loading anything from the web via this method, and it is highly discouraged to use any unsecured “HTTP” links. Also, you might encounter a problem when using this from a corporate computer that prevents downloading data through a firewall.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#bond-data-from-wrds",
    "href": "python/trace-and-fisd.html#bond-data-from-wrds",
    "title": "TRACE and FISD",
    "section": "Bond Data from WRDS",
    "text": "Bond Data from WRDS\nBoth bond databases we need are available on WRDS to which we establish the PostgreSQL connection described in WRDS, CRSP, and Compustat. Additionally, we connect to our local SQLite-database to store the data we download.\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nconnection_string = (\n  \"postgresql+psycopg2://\"\n  f\"{os.getenv('WRDS_USER')}:{os.getenv('WRDS_PASSWORD')}\"\n  \"@wrds-pgdata.wharton.upenn.edu:9737/wrds\"\n)\n\nwrds = create_engine(connection_string, pool_pre_ping=True)\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#mergent-fisd",
    "href": "python/trace-and-fisd.html#mergent-fisd",
    "title": "TRACE and FISD",
    "section": "Mergent FISD",
    "text": "Mergent FISD\nFor research on US corporate bonds, the Mergent Fixed Income Securities Database (FISD) is the primary resource for bond characteristics. There is a detailed manual on WRDS, so we only cover the necessary subjects here. FISD data comes in two main variants, namely, centered on issuers or issues. In either case, the most useful identifiers are CUSIPs. 9-digit CUSIPs identify securities issued by issuers. The issuers can be identified from the first six digits of a security CUSIP, which is also called a 6-digit CUSIP. Both stocks and bonds have CUSIPs. This connection would, in principle, allow matching them easily, but due to changing issuer details, this approach only yields small coverage.\nWe use the issue-centered version of FISD to identify the subset of US corporate bonds that meet the standard criteria (Bessembinder, Maxwell, and Venkataraman 2006). The WRDS table fisd_mergedissue contains most of the information we need on a 9-digit CUSIP level. Due to the diversity of corporate bonds, details in the indenture vary significantly. We focus on common bonds that make up the majority of trading volume in this market without diverging too much in indentures.\nThe following chunk connects to the data and selects the bond sample to remove certain bond types that are less commonly used (see, e.g., Dick-Nielsen, Feldhütter, and Lando 2012; O’Hara and Zhou 2021, among many others). In particular, we use the filters listed below. Note that we also treat missing values in these flags.\n\nKeep only senior bonds (security_level = 'SEN').\nExclude bonds which are secured lease obligations (slob = 'N' OR slob IS NULL).\nExclude secured bonds (security_pledge IS NULL).\nExclude asset-backed bonds (asset_backed = 'N' OR asset_backed IS NULL).\nExclude defeased bonds ((defeased = 'N' OR defeased IS NULL) AND defeased_date IS NULL).\nKeep only the bond types US Corporate Debentures ('CDEB'), US Corporate Medium Term Notes ('CMTN'), US Corporate Zero Coupon Notes and Bonds ('CMTZ', 'CZ'), and US Corporate Bank Note ('USBN').\nExclude bonds that are payable in kind ((pay_in_kind != 'Y' OR pay_in_kind IS NULL) AND pay_in_kind_exp_date IS NULL).\nExclude foreign (yankee == \"N\" OR is.na(yankee)) and Canadian issuers (canadian = 'N' OR canadian IS NULL).\nExclude bonds denominated in foreign currency (foreign_currency = 'N').\nKeep only fixed (F) and zero (Z) coupon bonds with additional requirements of fix_frequency IS NULL, coupon_change_indicator = 'N' and annual, semi-annual, quarterly, or monthly interest frequencies.\nExclude bonds that were issued under SEC Rule 144A (rule_144a = 'N').\nExlcude privately placed bonds (private_placement = 'N' OR private_placement IS NULL).\nExclude defaulted bonds (defaulted = 'N' AND filing_date IS NULL AND settlement IS NULL).\nExclude convertible (convertible = 'N'), putable (putable = 'N' OR putable IS NULL), exchangeable (exchangeable = 'N' OR exchangeable IS NULL), perpetual (perpetual = 'N'), or preferred bonds (preferred_security = 'N' OR preferred_security IS NULL).\nExclude unit deal bonds ((unit_deal = 'N' OR unit_deal IS NULL)).\n\n\nfisd_query = (\n  \"SELECT complete_cusip, maturity, offering_amt, offering_date, \"\n         \"dated_date, interest_frequency, coupon, last_interest_date, \"\n         \"issue_id, issuer_id \"\n    \"FROM fisd.fisd_mergedissue \"\n    \"WHERE security_level = 'SEN' \"\n          \"AND (slob = 'N' OR slob IS NULL) \"\n          \"AND security_pledge IS NULL \"              \n          \"AND (asset_backed = 'N' OR asset_backed IS NULL) \"\n          \"AND (defeased = 'N' OR defeased IS NULL) \"\n          \"AND defeased_date IS NULL \"\n          \"AND bond_type IN ('CDEB', 'CMTN', 'CMTZ', 'CZ', 'USBN') \"\n          \"AND (pay_in_kind != 'Y' OR pay_in_kind IS NULL) \"\n          \"AND pay_in_kind_exp_date IS NULL \"\n          \"AND (yankee = 'N' OR yankee IS NULL) \"\n          \"AND (canadian = 'N' OR canadian IS NULL) \"\n          \"AND foreign_currency = 'N' \"\n          \"AND coupon_type IN ('F', 'Z') \"\n          \"AND fix_frequency IS NULL \"\n          \"AND coupon_change_indicator = 'N' \"\n          \"AND interest_frequency IN ('0', '1', '2', '4', '12') \"\n          \"AND rule_144a = 'N' \"\n          \"AND (private_placement = 'N' OR private_placement IS NULL) \"\n          \"AND defaulted = 'N' \"\n          \"AND filing_date IS NULL \"\n          \"AND settlement IS NULL \"\n          \"AND convertible = 'N' \"\n          \"AND exchange IS NULL \"\n          \"AND (putable = 'N' OR putable IS NULL) \"\n          \"AND (unit_deal = 'N' OR unit_deal IS NULL) \"\n          \"AND (exchangeable = 'N' OR exchangeable IS NULL) \"\n          \"AND perpetual = 'N' \"\n          \"AND (preferred_security = 'N' OR preferred_security IS NULL)\"\n)\n\nfisd = pd.read_sql_query(\n  sql=fisd_query,\n  con=wrds,\n  dtype={\"complete_cusip\": str, \"interest_frequency\": str, \n         \"issue_id\": int, \"issuer_id\": int},\n  parse_dates={\"maturity\", \"offering_date\", \n               \"dated_date\", \"last_interest_date\"}\n)\n\nWe also pull issuer information from fisd_mergedissuer regarding the industry and country of the firm that issued a particular bond. Then, we filter to include only US-domiciled firms’ bonds. We match the data by issuer_id.\n\nfisd_issuer_query = (\n  \"SELECT issuer_id, sic_code, country_domicile \"\n    \"FROM fisd.fisd_mergedissuer\"\n)\n\nfisd_issuer = pd.read_sql_query(\n  sql=fisd_issuer_query,\n  con=wrds,\n  dtype={\"issuer_id\": int, \"sic_code\": str, \"country_domicile\": str}\n)\n\nfisd = (fisd\n  .merge(fisd_issuer, how=\"inner\", on=\"issuer_id\")\n  .query(\"country_domicile == 'USA'\")\n  .drop(columns=\"country_domicile\")\n)\n\nTo download the FISD data with the above filters and processing steps, you can use the tidyfinance package. Note that you might have to set the login credentials for WRDS first using tf.set_wrds_credentials().\n\nfisd = tf.download_data(domain=\"wrds\", dataset=\"fisd\")\n\nFinally, we save the bond characteristics to our local database. This selection of bonds also constitutes the sample for which we will collect trade reports from TRACE below.\n\n(fisd\n  .to_sql(name=\"fisd\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\nThe FISD database also contains other data. The issue-based file contains information on covenants, i.e., restrictions included in bond indentures to limit specific actions by firms (e.g., Handler, Jankowitsch, and Weiss 2021). The FISD redemption database also contains information on callable bonds. Moreover, FISD also provides information on bond ratings. We do not need either here.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#trace",
    "href": "python/trace-and-fisd.html#trace",
    "title": "TRACE and FISD",
    "section": "TRACE",
    "text": "TRACE\nThe Financial Industry Regulatory Authority (FINRA) provides the Trade Reporting and Compliance Engine (TRACE). In TRACE, dealers that trade corporate bonds must report such trades individually. Hence, we observe trade messages in TRACE that contain information on the bond traded, the trade time, price, and volume. TRACE comes in two variants: standard and enhanced TRACE. We show how to download and clean enhanced TRACE as it contains uncapped volume, a crucial quantity missing in the standard distribution. Moreover, enhanced TRACE also provides information on the respective parties’ roles and the direction of the trade report. These items become essential in cleaning the messages.\nWhy do we repeatedly talk about cleaning TRACE? Trade messages are submitted within a short time window after a trade is executed (less than 15 minutes). These messages can contain errors, and the reporters subsequently correct them or they cancel a trade altogether. The cleaning needs are described by Dick-Nielsen (2009) in detail, and Dick-Nielsen (2014) shows how to clean the enhanced TRACE data using SAS. We do not go into the cleaning steps here, since the code is lengthy and is not our focus here. However, downloading and cleaning enhanced TRACE data is straightforward with our setup.\nWe store code for cleaning enhanced TRACE with Python on the following GitHub Gist. Clean Enhanced TRACE with Python also contains the code for reference. We only need to source the code from the Gist, which we can do with the code below using httpimport. In the chunk, we explicitly load the necessary function interpreting the Gist as a module (i.e., you could also use it as a module and precede the function calls with the module’s name). Alternatively, you can also go to the Gist, download it, and manually execute it. The clean_enhanced_trace() function takes a vector of CUSIPs, a connection to WRDS explained in WRDS, CRSP, and Compustat, and a start and end date, respectively.\n\ngist_url = (\n  \"https://gist.githubusercontent.com/patrick-weiss/\"\n  \"86ddef6de978fbdfb22609a7840b5d8b/raw/\"\n  \"8fbcc6c6f40f537cd3cd37368be4487d73569c6b/\"\n)\n\nwith httpimport.remote_repo(gist_url):\n  from clean_enhanced_TRACE_python import clean_enhanced_trace\n\nThe TRACE database is considerably large. Therefore, we only download subsets of data at once. Specifying too many CUSIPs over a long time horizon will result in very long download times and a potential failure due to the size of the request to WRDS. The size limit depends on many parameters, and we cannot give you a guideline here. For the applications in this book, we need data around the Paris Agreement in December 2015 and download the data in sets of 1000 bonds, which we define below.\n\ncusips = list(fisd[\"complete_cusip\"].unique())\nbatch_size = 1000\nbatches = np.ceil(len(cusips)/batch_size).astype(int)\n\nFinally, we run a loop in the same style as in WRDS, CRSP, and Compustat where we download daily returns from CRSP. For each of the CUSIP sets defined above, we call the cleaning function and save the resulting output. We add new data to the existing table for batch two and all following batches.\n\nfor j in range(1, batches + 1):\n  \n  cusip_batch = cusips[\n    ((j-1)*batch_size):(min(j*batch_size, len(cusips)))\n  ]\n  \n  cusip_batch_formatted = \", \".join(f\"'{cusip}'\" for cusip in cusip_batch)\n  cusip_string = f\"({cusip_batch_formatted})\"\n\n  trace_enhanced_sub = clean_enhanced_trace(\n    cusips=cusip_string,\n    connection=wrds, \n    start_date=\"'01/01/2014'\", \n    end_date=\"'11/30/2016'\"\n  )\n  \n  if not trace_enhanced_sub.empty:\n      if j == 1:\n        if_exists_string = \"replace\"\n      else:\n        if_exists_string = \"append\"\n\n      trace_enhanced_sub.to_sql(\n        name=\"trace_enhanced\", \n        con=tidy_finance, \n        if_exists=if_exists_string, \n        index=False\n      )\n    \n  print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")\n\nIf you want to download the prepared enhanced TRACE data for selected bonds via the tidyfinance package, you can call, e.g.:\n\ntf.download_data(\n  domain=\"wrds\",\n  dataset=\"trace_enhanced\",\n  cusips=[\"00101JAH9\"],\n  start_date=\"2019-01-01\", \n  end_date=\"2021-12-31\"\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#insights-into-corporate-bonds",
    "href": "python/trace-and-fisd.html#insights-into-corporate-bonds",
    "title": "TRACE and FISD",
    "section": "Insights into Corporate Bonds",
    "text": "Insights into Corporate Bonds\nWhile many news outlets readily provide information on stocks and the underlying firms, corporate bonds are not covered frequently. Additionally, the TRACE database contains trade-level information, potentially new to students. Therefore, we provide you with some insights by showing some summary statistics.\nWe start by looking into the number of bonds outstanding over time and compare it to the number of bonds traded in our sample. First, we compute the number of bonds outstanding for each quarter around the Paris Agreement from 2014 to 2016.\n\ndates = pd.date_range(start=\"2014-01-01\", end=\"2016-11-30\", freq=\"Q\")\n\nbonds_outstanding = (pd.DataFrame({\"date\": dates})\n  .merge(fisd[[\"complete_cusip\"]], how=\"cross\")\n  .merge(fisd[[\"complete_cusip\", \"offering_date\", \"maturity\"]],\n         on=\"complete_cusip\", how=\"left\")\n  .assign(offering_date=lambda x: x[\"offering_date\"].dt.floor(\"D\"),\n          maturity=lambda x: x[\"maturity\"].dt.floor(\"D\"))\n  .query(\"date &gt;= offering_date & date &lt;= maturity\")\n  .groupby(\"date\")\n  .size()\n  .reset_index(name=\"count\")\n  .assign(type=\"Outstanding\")\n)\n\nNext, we look at the bonds traded each quarter in the same period. Notice that we load the complete trace table from our database, as we only have a single part of it in the environment from the download loop above.\n\ntrace_enhanced = pd.read_sql_query(\n  sql=(\"SELECT cusip_id, trd_exctn_dt, rptd_pr, entrd_vol_qt, yld_pt \" \n        \"FROM trace_enhanced\"),\n  con=tidy_finance,\n  parse_dates={\"trd_exctn_dt\"}\n)\n\nbonds_traded = (trace_enhanced\n  .assign(\n    date=lambda x: (\n      (x[\"trd_exctn_dt\"]-pd.offsets.MonthBegin(1))\n        .dt.to_period(\"Q\").dt.start_time\n    )\n  )\n  .groupby(\"date\")\n  .aggregate(count=(\"cusip_id\", \"nunique\"))\n  .reset_index()\n  .assign(type=\"Traded\")\n)\n\nFinally, we plot the two time series in Figure 1.\n\nbonds_combined = pd.concat(\n  [bonds_outstanding, bonds_traded], ignore_index=True\n)\n\nbonds_figure = (\n  ggplot(\n    bonds_combined, \n    aes(x=\"date\", y=\"count\", color=\"type\", linetype=\"type\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"Number of bonds outstanding and traded each quarter\"\n    )\n  + scale_x_datetime(breaks=date_breaks(\"1 year\"), labels=date_format(\"%Y\"))\n  + scale_y_continuous(labels=comma_format())\n)\nbonds_figure.draw()\n\n\n\n\n\n\n\nFigure 1: Number of corporate bonds outstanding each quarter as reported by Mergent FISD and the number of traded bonds from enhanced TRACE between 2014 and end of 2016.\n\n\n\n\n\nWe see that the number of bonds outstanding increases steadily between 2014 and 2016. During our sample period of trade data, we see that the fraction of bonds trading each quarter is roughly 60 percent. The relatively small number of traded bonds means that many bonds do not trade through an entire quarter. This lack of trading activity illustrates the generally low level of liquidity in the corporate bond market, where it can be hard to trade specific bonds. Does this lack of liquidity mean that corporate bond markets are irrelevant in terms of their size? With over 7,500 traded bonds each quarter, it is hard to say that the market is small. However, let us also investigate the characteristics of issued corporate bonds. In particular, we consider maturity (in years), coupon, and offering amount (in million USD).\n\naverage_characteristics = (fisd\n  .assign(\n    maturity=lambda x: (x[\"maturity\"] - x[\"offering_date\"]).dt.days/365,\n    offering_amt=lambda x: x[\"offering_amt\"]/10**3\n  )\n  .melt(var_name=\"measure\",\n        value_vars=[\"maturity\", \"coupon\", \"offering_amt\"], \n        value_name=\"value\")\n  .dropna()\n  .groupby(\"measure\")[\"value\"]\n  .describe(percentiles=[.05, .50, .95])\n  .drop(columns=\"count\")\n)\naverage_characteristics.round(2)\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nmeasure\n\n\n\n\n\n\n\n\n\n\n\ncoupon\n2.55\n3.52\n0.00\n0.00\n0.0\n8.88\n39.00\n\n\nmaturity\n5.66\n6.85\n-6.24\n1.04\n4.0\n20.03\n100.74\n\n\noffering_amt\n133.01\n369.76\n0.00\n0.23\n3.0\n750.00\n15000.00\n\n\n\n\n\n\n\nWe see that the average bond in our sample period has an offering amount of over 357 million USD with a median of 200 million USD, which both cannot be considered small. The average bond has a maturity of ten years and pays around 6 percent in coupons.\nFinally, let us compute some summary statistics for the trades in this market. To this end, we show a summary based on aggregate information daily. In particular, we consider the trade size (in million USD) and the number of trades.\n\naverage_trade_size = (trace_enhanced\n  .groupby(\"trd_exctn_dt\")\n  .aggregate(\n    trade_size=(\"entrd_vol_qt\", lambda x: (\n      sum(x*trace_enhanced.loc[x.index, \"rptd_pr\"]/100)/10**6)\n    ),\n    trade_number=(\"trd_exctn_dt\", \"size\")\n  )\n  .reset_index()\n  .melt(id_vars=[\"trd_exctn_dt\"], var_name=\"measure\",\n        value_vars=[\"trade_size\", \"trade_number\"], value_name=\"value\")\n  .groupby(\"measure\")[\"value\"]\n  .describe(percentiles=[.05, .50, .95])\n  .drop(columns=\"count\")\n)\naverage_trade_size.round(0)\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nmeasure\n\n\n\n\n\n\n\n\n\n\n\ntrade_number\n25914.0\n5444.0\n439.0\n17844.0\n26051.0\n34383.0\n40839.0\n\n\ntrade_size\n12968.0\n3577.0\n17.0\n6128.0\n13421.0\n17850.0\n21312.0\n\n\n\n\n\n\n\nOn average, nearly 26 billion USD of corporate bonds are traded daily in nearly 13,000 transactions. We can, hence, conclude that the corporate bond market is indeed significant in terms of trading volume and activity.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#key-takeaways",
    "href": "python/trace-and-fisd.html#key-takeaways",
    "title": "TRACE and FISD",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe US corporate bond market is large, diverse, and primarily trades over-the-counter, making it an important yet complex subject of financial research.\nThe Mergent FISD database on WRDS provides detailed bond characteristics, which are essential for selecting a representative sample of US corporate bonds.\nEnhanced TRACE data includes uncapped trade volumes and dealer roles, offering valuable insights into bond market liquidity and trade execution.\nCleaning TRACE data is crucial, as trades may be corrected or canceled shortly after reporting, but automated functions in the tidyfinance Python package simplify this task.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#exercises",
    "href": "python/trace-and-fisd.html#exercises",
    "title": "TRACE and FISD",
    "section": "Exercises",
    "text": "Exercises\n\nCompute the amount outstanding across all bonds over time. Make sure to subtract all matured bonds. How would you describe the resulting plot?\nCompute the number of days each bond is traded (accounting for the bonds’ maturities and issuances). Start by looking at the number of bonds traded each day in a graph similar to the one above. How many bonds trade on more than 75 percent of trading days?\nWRDS provides more information from Mergent FISD such as ratings in the table fisd_ratings. Download the ratings table and plot the distribution of ratings for the different rating providers. How would you map the different providers to a common numeric rating scale?",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html",
    "href": "python/option-pricing-via-machine-learning.html",
    "title": "Option Pricing via Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nThis chapter covers machine learning methods in option pricing. First, we briefly introduce regression trees, random forests, and neural networks; these methods are advocated as highly flexible universal approximators, capable of recovering highly non-linear structures in the data. As the focus is on implementation, we leave a thorough treatment of the statistical underpinnings to other textbooks from authors with a real comparative advantage on these issues. We show how to implement random forests and deep neural networks with tidy principles using scikit-learn.\nMachine learning (ML) is seen as a part of artificial intelligence. ML algorithms build a model based on training data in order to make predictions or decisions without being explicitly programmed to do so. While ML can be specified along a vast array of different branches, this chapter focuses on so-called supervised learning for regressions. The basic idea of supervised learning algorithms is to build a mathematical model for data that contains both the inputs and the desired outputs. In this chapter, we apply well-known methods such as random forests and neural networks to a simple application in option pricing. More specifically, we create an artificial dataset of option prices for different values based on the Black-Scholes pricing equation for call options. Then, we train different models to learn how to price call options without prior knowledge of the theoretical underpinnings of the famous option pricing equation by Black and Scholes (1973).\nThroughout this chapter, we need the following Python packages.\nimport pandas as pd \nimport numpy as np\n\nfrom plotnine import *\nfrom itertools import product\nfrom scipy.stats import norm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Lasso",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "href": "python/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "title": "Option Pricing via Machine Learning",
    "section": "Regression Trees and Random Forests",
    "text": "Regression Trees and Random Forests\nRegression trees are a popular ML approach for incorporating multiway predictor interactions. In Finance, regression trees are gaining popularity, also in the context of asset pricing (see, e.g., Bryzgalova, Pelger, and Zhu 2022). Trees possess a logic that departs markedly from traditional regressions. Trees are designed to find groups of observations that behave similarly to each other. A tree grows in a sequence of steps. At each step, a new branch sorts the data leftover from the preceding step into bins based on one of the predictor variables. This sequential branching slices the space of predictors into partitions and approximates the unknown function \\(f(x)\\) which yields the relation between the predictors \\(x\\) and the outcome variable \\(y\\) with the average value of the outcome variable within each partition. For a more thorough treatment of regression trees, we refer to Coqueret and Guida (2020).\nFormally, we partition the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). For any predictor \\(x\\) that falls within region \\(R_j\\), we estimate \\(f(x)\\) with the average of the training observations, \\(\\hat y_i\\), for which the associated predictor \\(x_i\\) is also in \\(R_j\\). Once we select a partition \\(x\\) to split in order to create the new partitions, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, called \\(R_1(j,s)\\) and \\(R_2(j,s)\\), which split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\): \\[R_1(j,s) = \\{x \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{x \\mid x_j \\geq s\\}. \\tag{1}\\] To pick \\(j\\) and \\(s\\), we find the pair that minimizes the residual sum of square (RSS): \\[\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2 \\tag{2}\\] As in Factor Selection via Machine Learning in the context of penalized regressions, the first relevant question is: What are the hyperparameter decisions? Instead of a regularization parameter, trees are fully determined by the number of branches used to generate a partition (sometimes one specifies the minimum number of observations in each final branch instead of the maximum number of branches).\nModels with a single tree may suffer from high predictive variance. Random forests address these shortcomings of decision trees. The goal is to improve the predictive performance and reduce instability by averaging multiple regression trees. A forest basically implies creating many regression trees and averaging their predictions. To assure that the individual trees are not the same, we use a bootstrap to induce randomness. More specifically, we build \\(B\\) decision trees \\(T_1, \\ldots, T_B\\) using the training sample. For that purpose, we randomly select features to be included in the building of each tree. For each observation in the test set, we then form a prediction \\(\\hat{y} = \\frac{1}{B}\\sum\\limits_{i=1}^B\\hat{y}_{T_i}\\).",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#neural-networks",
    "href": "python/option-pricing-via-machine-learning.html#neural-networks",
    "title": "Option Pricing via Machine Learning",
    "section": "Neural Networks",
    "text": "Neural Networks\nRoughly speaking, neural networks propagate information from an input layer, through one or multiple hidden layers, to an output layer. While the number of units (neurons) in the input layer is equal to the dimension of the predictors, the output layer usually consists of one neuron (for regression) or multiple neurons for classification. The output layer predicts the future data, similar to the fitted value in a regression analysis. Neural networks have theoretical underpinnings as universal approximators for any smooth predictive association (Hornik 1991). Their complexity, however, ranks neural networks among the least transparent, least interpretable, and most highly parameterized ML tools. In finance, applications of neural networks can be found in many different contexts, e.g., Avramov, Cheng, and Metzker (2022), Chen, Pelger, and Zhu (2023), and Gu, Kelly, and Xiu (2020).\nEach neuron applies a non-linear activation function \\(f\\) to its aggregated signal before sending its output to the next layer \\[x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right) \\tag{3}\\] Here, \\(\\theta\\) are the parameters to fit, \\(N^l\\) denotes the number of units (a hyperparameter to tune), and \\(z_j\\) are the input variables which can be either the raw data or, in the case of multiple chained layers, the outcome from a previous layer \\(z_j = x_k-1\\). While the easiest case with \\(f(x) = \\alpha + \\beta x\\) resembles linear regression, typical activation functions are sigmoid (i.e., \\(f(x) = (1+e^{-x})^{-1}\\)) or ReLu (i.e., \\(f(x) = max(x, 0)\\)).\nNeural networks gain their flexibility from chaining multiple layers together. Naturally, this imposes many degrees of freedom on the network architecture for which no clear theoretical guidance exists. The specification of a neural network requires, at a minimum, a stance on depth (number of hidden layers), the activation function, the number of neurons, the connection structure of the units (dense or sparse), and the application of regularization techniques to avoid overfitting. Finally, learning means to choose optimal parameters relying on numerical optimization, which often requires specifying an appropriate learning rate. Despite these computational challenges, implementation in Python is not tedious at all because we can use the API to TensorFlow.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#option-pricing",
    "href": "python/option-pricing-via-machine-learning.html#option-pricing",
    "title": "Option Pricing via Machine Learning",
    "section": "Option Pricing",
    "text": "Option Pricing\nTo apply ML methods in a relevant field of finance, we focus on option pricing. The application in its core is taken from Hull (2020). In its most basic form, call options give the owner the right but not the obligation to buy a specific stock (the underlying) at a specific price (the strike price \\(K\\)) at a specific date (the exercise date \\(T\\)). The Black-Scholes price (Black and Scholes 1973) of a call option for a non-dividend-paying underlying stock is given by \\[\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_2)Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left(\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right) \\\\\n     d_2 &= d_1 - \\sigma\\sqrt{T}\n\\end{aligned}\n\\tag{4}\\] where \\(C(S, T)\\) is the price of the option as a function of today’s stock price of the underlying, \\(S\\), with time to maturity \\(T\\), \\(r_f\\) is the risk-free interest rate, and \\(\\sigma\\) is the volatility of the underlying stock return. \\(\\Phi\\) is the cumulative distribution function of a standard normal random variable.\nThe Black-Scholes equation provides a way to compute the arbitrage-free price of a call option once the parameters \\(S, K, r_f, T\\), and \\(\\sigma\\) are specified (arguably, in a realistic context, all parameters are easy to specify except for \\(\\sigma\\) which has to be estimated). A simple R function allows computing the price as we do below.\n\ndef black_scholes_price(S, K, r, T, sigma):\n    \"\"\"Calculate Black Scholes option price.\"\"\"\n  \n    d1 = (np.log(S/K)+(r+sigma**2/2)*T)/(sigma*np.sqrt(T))\n    d2 = d1-sigma*np.sqrt(T)\n    price = S*norm.cdf(d1)-K*np.exp(-r*T)*norm.cdf(d2)\n    \n    return price",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#learning-black-scholes",
    "href": "python/option-pricing-via-machine-learning.html#learning-black-scholes",
    "title": "Option Pricing via Machine Learning",
    "section": "Learning Black-Scholes",
    "text": "Learning Black-Scholes\nWe illustrate the concept of ML by showing how ML methods learn the Black-Scholes equation after observing some different specifications and corresponding prices without us revealing the exact pricing equation.\n\nData simulation\nTo that end, we start with simulated data. We compute option prices for call options for a grid of different combinations of times to maturity (T), risk-free rates (r), volatilities (sigma), strike prices (K), and current stock prices (S). In the code below, we add an idiosyncratic error term to each observation such that the prices considered do not exactly reflect the values implied by the Black-Scholes equation.\nIn order to keep the analysis reproducible, we use np.random.seed(). A random seed specifies the start point when a computer generates a random number sequence and ensures that our simulated data is the same across different machines.\n\nrandom_state = 42\nnp.random.seed(random_state)\n\nS = np.arange(40, 61)\nK = np.arange(20, 91)\nr = np.arange(0, 0.051, 0.01)\nT = np.arange(3/12, 2.01, 1/12)\nsigma = np.arange(0.1, 0.81, 0.1)\n\noption_prices = pd.DataFrame(\n  product(S, K, r, T, sigma),\n  columns=[\"S\", \"K\", \"r\", \"T\", \"sigma\"]\n)\n\noption_prices[\"black_scholes\"] = black_scholes_price(\n  option_prices[\"S\"].values, \n  option_prices[\"K\"].values, \n  option_prices[\"r\"].values, \n  option_prices[\"T\"].values, \n  option_prices[\"sigma\"].values\n)\n\noption_prices = (option_prices\n  .assign(\n    observed_price=lambda x: (\n      x[\"black_scholes\"] + np.random.normal(scale=0.15)\n    )\n  )\n)\n\nThe code above generates more than 1.5 million random parameter constellations (in the definition of the option_prices dataframe). For each of these values, the true prices reflecting the Black-Scholes model are given and a random innovation term pollutes the observed prices. The intuition of this application is simple: the simulated data provides many observations of option prices, by using the Black-Scholes equation we can evaluate the actual predictive performance of a ML method, which would be hard in a realistic context where the actual arbitrage-free price would be unknown.\nNext, we split the data into a training set (which contains 1 percent of all the observed option prices) and a test set that will only be used for the final evaluation. Note that the entire grid of possible combinations contains python len(option_prices.columns) different specifications. Thus, the sample to learn the Black-Scholes price contains only 31,489 observations and is therefore relatively small.\n\ntrain_data, test_data = train_test_split(\n  option_prices, \n  test_size=0.01, random_state=random_state\n)\n\nWe process the training dataset further before we fit the different ML models. We define a ColumnTransformer() that defines all processing steps for that purpose. For our specific case, we want to explain the observed price by the five variables that enter the Black-Scholes equation. The true price (stored in column black_scholes) should obviously not be used to fit the model. The recipe also reflects that we standardize all predictors via StandardScaler() to ensure that each variable exhibits a sample average of zero and a sample standard deviation of one.\n\npreprocessor = ColumnTransformer(\n  transformers=[(\n    \"normalize_predictors\", \n     StandardScaler(),\n     [\"S\", \"K\", \"r\", \"T\", \"sigma\"]\n  )],\n  remainder=\"drop\"\n)\n\n\n\nSingle layer networks and random forests\nNext, we show how to fit a neural network to the data. The function MLPRegressor() from the package scikit-learn provides the functionality to initialize a single layer, feed-forward neural network. The specification below defines a single layer feed-forward neural network with ten hidden units. We set the number of training iterations to max_iter=1000.\n\nmax_iter = 1000\n\nnnet_model = MLPRegressor(\n  hidden_layer_sizes=10, \n  max_iter=max_iter, \n  random_state=random_state\n)\n\nWe can follow the straightforward workflow as in the chapter before: define a workflow, equip it with the recipe, and specify the associated model. Finally, fit the model with the training data.\n\nnnet_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", nnet_model)\n])\n\nnnet_fit = nnet_pipeline.fit(\n  train_data.drop(columns=[\"observed_price\"]), \n  train_data.get(\"observed_price\")\n)\n\nOne word of caution regarding the training of Neural networks: For illustrative purposes we sequentially update the parameters by reiterating through the data 1,000 times (max_iter=1000). Typically, however, early stopping rules are advised that aim to interrupt the process of updating parameters as soon as the predictive performance on the validation test seems to deteriorate. A detailed discussion of these details in the implementation would go beyond the scope of this book.\nOnce you are familiar with the scikit-learn workflow, it is a piece of cake to fit other models. For instance, the model below initializes a random forest with 50 trees contained in the ensemble, where we require at least 2000 observations in a node. The random forests are trained using the function RandomForestRegressor().\n\nrf_model = RandomForestRegressor(\n  n_estimators=50, \n  min_samples_leaf=2000, \n  random_state=random_state\n)\n\nFitting the model follows exactly the same convention as for the neural network before.\n\nrf_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", rf_model)\n])\n\nrf_fit = rf_pipeline.fit(\n  train_data.drop(columns=[\"observed_price\"]), \n  train_data.get(\"observed_price\")\n)\n\n\n\nDeep neural networks\nA deep neural network is a neural network with multiple layers between the input and output layers. By chaining multiple layers together, more complex structures can be represented with fewer parameters than simple shallow (one-layer) networks as the one implemented above. For instance, image or text recognition are typical tasks where deep neural networks are used (for applications of deep neural networks in finance, see, for instance, Jiang, Kelly, and Xiu 2023; Jensen et al. 2022). The following code chunk implements a deep neural network with three hidden layers of size ten each and logistic activation functions.\n\ndeepnnet_model = MLPRegressor(\n  hidden_layer_sizes=(10, 10, 10),\n  activation=\"logistic\", \n  solver=\"lbfgs\",\n  max_iter=max_iter, \n  random_state=random_state\n)\n                              \ndeepnnet_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", deepnnet_model)\n])\n\ndeepnnet_fit = deepnnet_pipeline.fit(\n  train_data.drop(columns=[\"observed_price\"]),\n  train_data.get(\"observed_price\")\n)\n\n\n\nUniversal approximation\nBefore we evaluate the results, we implement one more model. In principle, any non-linear function can also be approximated by a linear model containing the input variables’ polynomial expansions. To illustrate this, we include polynomials up to the fifth degree of each predictor and then add all possible pairwise interaction terms. We fit a Lasso regression model with a pre-specified penalty term (consult Factor Selection via Machine Learning on how to tune the model hyperparameters).\n\nlm_pipeline = Pipeline([\n  (\"polynomial\", PolynomialFeatures(degree=5, \n                                    interaction_only=False, \n                                    include_bias=True)),\n  (\"scaler\", StandardScaler()),\n  (\"regressor\", Lasso(alpha=0.01))\n])\n\nlm_fit = lm_pipeline.fit(\n  train_data.get([\"S\", \"K\", \"r\", \"T\", \"sigma\"]),\n  train_data.get(\"observed_price\")\n)",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#prediction-evaluation",
    "href": "python/option-pricing-via-machine-learning.html#prediction-evaluation",
    "title": "Option Pricing via Machine Learning",
    "section": "Prediction Evaluation",
    "text": "Prediction Evaluation\nFinally, we collect all predictions to compare the out-of-sample prediction error evaluated on 10,000 new data points.\n\ntest_X = test_data.get([\"S\", \"K\", \"r\", \"T\", \"sigma\"])\ntest_y = test_data.get(\"observed_price\")\n\npredictive_performance = (pd.concat(\n    [test_data.reset_index(drop=True), \n     pd.DataFrame({\"Random forest\": rf_fit.predict(test_X),\n                   \"Single layer\": nnet_fit.predict(test_X),\n                   \"Deep NN\": deepnnet_fit.predict(test_X),\n                   \"Lasso\": lm_fit.predict(test_X)})\n    ], axis=1)\n  .melt(\n    id_vars=[\"S\", \"K\", \"r\", \"T\", \"sigma\",\n             \"black_scholes\", \"observed_price\"],\n    var_name=\"Model\",\n    value_name=\"Predicted\"\n  )\n  .assign(\n    moneyness=lambda x: x[\"S\"]-x[\"K\"],\n    pricing_error=lambda x: np.abs(x[\"Predicted\"]-x[\"black_scholes\"])\n  )\n)\n\nIn the lines above, we use each of the fitted models to generate predictions for the entire test dataset of option prices. We evaluate the absolute pricing error as one possible measure of pricing accuracy, defined as the absolute value of the difference between predicted option price and the theoretical correct option price from the Black-Scholes model. We show the results graphically in Figure 1.\n\npredictive_performance_figure = (\n  ggplot(\n    predictive_performance, \n    aes(x=\"moneyness\", y=\"pricing_error\")\n  )\n  + geom_point(alpha=0.05)\n  + facet_wrap(\"Model\")\n  + labs(\n      x=\"Moneyness (S - K)\", y=\"Absolut prediction error (USD)\",\n      title=\"Prediction errors of call options for different models\"\n    )\n  + theme(legend_position=\"\")\n)\npredictive_performance_figure.show()\n\n\n\n\n\n\n\nFigure 1: The figure shows absolut prediction error in USD for the different fitted methods. The prediction error is evaluated on a sample of call options that were not used for training.\n\n\n\n\n\nThe results can be summarized as follows:\n\nAll ML methods seem to be able to price call options after observing the training test set.\nRandom forest and the Lasso seem to perform consistently worse in predicting option prices than the neural networks.\nFor random forest and Lasso, the average prediction errors increase for far in-the-money options.\nThe increased complexity of the deep neural network relative to the single-layer neural network results in lower prediction errors.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#key-takeaways",
    "href": "python/option-pricing-via-machine-learning.html#key-takeaways",
    "title": "Option Pricing via Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nMachine learning methods like random forests and neural networks can be used to estimate call option prices in Python without relying on the Black-Scholes formula.\nSimulating noisy option price data and applying supervised learning models via the scikit-learn framework provides a clean, reproducible analysis.\nDeep neural networks do not consistently outperform single-layer networks, underscoring the trade-off between model complexity and prediction performance.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#exercises",
    "href": "python/option-pricing-via-machine-learning.html#exercises",
    "title": "Option Pricing via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that takes y and a matrix of predictors X as inputs and returns a characterization of the relevant parameters of a regression tree with one branch.\nCreate a function that creates predictions for a new matrix of predictors newX based on the estimated regression tree.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html",
    "href": "python/discounted-cash-flow-analysis.html",
    "title": "Discounted Cash Flow Analysis",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we address a fundamental question: What is the value of a company? Company valuation is a critical tool that helps us determine the economic value of a business. Understanding a company’s value is essential, whether it is for investment decisions, mergers and acquisitions, or financial reporting. Valuation is not just about assigning a number, as it is also a framework for making informed decisions. For example, investors use valuation to identify whether a stock is under- or overvalued, and companies rely on valuation for strategic decisions, like pricing an acquisition or preparing for an IPO.\nCompany valuation methods broadly fall into three categories:\nWe focus on Discounted Cash Flow (DCF) analysis, an income-based approach, because it captures three crucial aspects of valuation: First, DCF explicitly accounts for the time value of money - the principle that a dollar today is worth more than a dollar in the future. By discounting future cash flows to present value with the appropriate discount rate, we incorporate both time preferences and risk. Second, DCF is inherently forward-looking, making it particularly suitable for companies where historical performance may not reflect future potential. This characteristic is especially relevant when valuing growth companies or analyzing new business opportunities. Third, DCF analysis is flexible enough to accommodate various business models and capital structures, making it applicable across different industries and company sizes.\nIn our exposition of the DCF valuation framework, we focus on its three key components:\nWe make a few simplifying assumptions due to the diverse nature of businesses, which a simple model cannot cover. After all, there are entire textbooks written just on valuation. In particular, we assume that firms only conduct operating activities (i.e., financial statements do not include non-operating items), implying that firms do not own any assets that do not produce operating cash flows. Otherwise, you must value these non-operating activities separately for many real-world examples.\nIn this chapter, we rely on the following packages to build a simple DCF analysis:\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\n\nfrom fmpapi import fmp_get\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\nfrom itertools import product",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#prepare-data",
    "href": "python/discounted-cash-flow-analysis.html#prepare-data",
    "title": "Discounted Cash Flow Analysis",
    "section": "Prepare Data",
    "text": "Prepare Data\nBefore we can perform a DCF analysis, we need historical financial data to inform our forecasts. We use the Financial Modeling Prep (FMP) API to download financial statements. The fmpapi package provides a convenient interface for accessing this data in a tidy format.\n\nsymbol = \"MSFT\"\n\nincome_statements = fmp_get(\n  \"income-statement\", symbol, {\"period\": \"annual\", \"limit\": 5}, to_pandas = True\n)\n\ncash_flow_statements = fmp_get(\n  \"cash-flow-statement\", symbol, {\"period\": \"annual\", \"limit\": 5}, to_pandas = True\n)\n\nOur analysis centers on Free Cash Flow (FCF), which represents the cash available to all investors after the company has covered its operational needs and capital investments. We calculate FCF using the following formula:\n\\[\\text{FCF} = \\text{EBIT} + \\text{Depreciation \\& Amortization} - \\text{Taxes} + \\Delta \\text{Working Capital} - \\text{CAPEX}\\]\nEach component of this formula serves a specific purpose in capturing the company’s cash-generating ability:\n\nEBIT (Earnings Before Interest and Taxes) measures core operating profit\nDepreciation & Amortization accounts for non-cash expenses\nTaxes reflect actual cash payments to tax authorities\nChanges in Working Capital capture cash tied up in operations\nCapital Expenditures (CAPEX) represent investments in long-term assets\n\nWe can implement this calculation by combining and transforming our financial statement data. Note that we also arrange by year, which is important for some of the following code chunks.\n\ndcf_data = (income_statements\n  .assign(\n    fiscal_year=lambda x: x[\"fiscal_year\"].astype(int),\n    ebit=lambda x: x[\"net_income\"] + x[\"income_tax_expense\"] + x[\"interest_expense\"] - x[\"interest_income\"]\n  )\n  .get([\"fiscal_year\", \"ebit\", \"revenue\", \"depreciation_and_amortization\", \"income_tax_expense\"])\n  .rename(columns={\n      \"fiscal_year\": \"year\",\n      \"income_tax_expense\": \"taxes\"\n  })\n  .merge(\n    (cash_flow_statements\n      .get([\"fiscal_year\", \"change_in_working_capital\", \"capital_expenditure\"])\n      .assign(fiscal_year=lambda x: x[\"fiscal_year\"].astype(int))\n      .rename(columns={\n        \"fiscal_year\": \"year\",\n        \"change_in_working_capital\": \"delta_working_capital\",\n        \"capital_expenditure\": \"capex\"\n      })\n    ), on=\"year\", how=\"left\"\n  )\n  .assign(\n    fcf=lambda x: x[\"ebit\"] + x[\"depreciation_and_amortization\"] - x[\"taxes\"] + x[\"delta_working_capital\"] - x[\"capex\"]\n  )\n  .sort_values(\"year\")\n)",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#forecasting-free-cash-flow",
    "href": "python/discounted-cash-flow-analysis.html#forecasting-free-cash-flow",
    "title": "Discounted Cash Flow Analysis",
    "section": "Forecasting Free Cash Flow",
    "text": "Forecasting Free Cash Flow\nAfter calculating historical FCF, we need to project it into the future. While historical data provides a foundation, forecasting requires both quantitative analysis and qualitative judgment. We use a ratio-based approach that links all FCF components to revenue growth, making our forecasts more tractable. This is another crucial assumption for our exposition that will not hold in reality. Thus, you must put more thought into these forecast ratios and their dynamics over time.\nFirst, we express each FCF component as a ratio relative to revenue. This standardization helps identify trends and makes forecasting more systematic. Figure 1 shows the historical evolution of these key financial ratios.\n\ndcf_data = (dcf_data\n  .assign(\n    revenue_growth=lambda x: x[\"revenue\"] / x[\"revenue\"].shift(1) - 1,\n    operating_margin=lambda x: x[\"ebit\"] / x[\"revenue\"],\n    da_margin=lambda x: x[\"depreciation_and_amortization\"] / x[\"revenue\"],\n    taxes_to_revenue=lambda x: x[\"taxes\"] / x[\"revenue\"],\n    delta_working_capital_to_revenue=lambda x: x[\"delta_working_capital\"] / x[\"revenue\"],\n    capex_to_revenue=lambda x: x[\"capex\"] / x[\"revenue\"]\n  )\n)\n\ndcf_data_long = (dcf_data\n  .melt(\n    id_vars=\"year\",\n    value_vars=[\n      \"operating_margin\", \"da_margin\", \"taxes_to_revenue\",\n      \"delta_working_capital_to_revenue\", \"capex_to_revenue\"\n    ],\n    var_name=\"name\",\n    value_name=\"value\"\n  )\n)\n\nratios_figure = (\n  ggplot(\n    dcf_data_long, \n    aes(x=\"year\", y=\"value\", color=\"name\")\n  )\n  + geom_line()\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", y=\"\", color=\"\",\n      title=\"Key financial ratios of Microsoft between 2021 and 2025\"\n    )\n)\nratios_figure.show()\n\n\n\n\n\n\n\nFigure 1: Ratios are based on financial statements provided through the FMP API.\n\n\n\n\n\nThe operating margin, for instance, represents how much of each revenue dollar translates into operating profit (EBIT), while the CAPEX-to-revenue ratio indicates the company’s investment intensity.\nFor our DCF analysis, we need to project these ratios into the future. These projections should reflect both historical patterns and forward-looking considerations such as: Industry trends and competitive dynamics, company-specific growth initiatives, expected operational efficiency improvements, planned capital investments, or working capital management strategies. Overall, there is a lot to consider in practice. However, forecast ratios make this process tractable.\nAnother crucial question is the length of the forecast period. We use five years below ad-hoc, but you want to make detailed projections for each year when the company undergoes significant changes. As long as forecast ratios change, you need to make explicit forecasts. Once the company reaches a steady state, you can switch to computing the continuation value discussed later.\nWe demonstrate this forecasting approach in Figure 2. Note that our forecast ratio dynamics just serve as an example and are not grounded in deeper economic analysis.\n\nforecast_ratios = pd.DataFrame({\n    \"year\": [2026, 2027, 2028, 2029, 2030],\n    \"operating_margin\": [0.41, 0.42, 0.43, 0.44, 0.45],\n    \"da_margin\": [0.09, 0.09, 0.09, 0.09, 0.09],\n    \"taxes_to_revenue\": [0.08, 0.07, 0.06, 0.06, 0.06],\n    \"delta_working_capital_to_revenue\": [0.001, 0.001, 0.001, 0.001, 0.001],\n    \"capex_to_revenue\": [-0.2, -0.22, -0.2, -0.18, -0.16]\n})\nforecast_ratios[\"type\"] = \"Forecast\"\n\ndcf_data[\"type\"] = \"Realized\"\ndcf_data = pd.concat([dcf_data, forecast_ratios], ignore_index=True)\n\ndcf_data_long = (dcf_data\n  .melt(\n    id_vars=[\"year\", \"type\"],\n    value_vars=[\n      \"operating_margin\", \"da_margin\", \"taxes_to_revenue\",\n      \"delta_working_capital_to_revenue\", \"capex_to_revenue\"\n    ],\n    var_name=\"name\",\n    value_name=\"value\"\n  )\n)\n\ndcf_data_long[\"type\"] = pd.Categorical(\n  dcf_data_long[\"type\"], categories=[\"Realized\", \"Forecast\"]\n)\n\nratios_forecast_figure = (\n  ggplot(\n    dcf_data_long,\n    aes(x=\"year\", y=\"value\", color=\"name\", linetype=\"type\")\n  )\n  + geom_line()\n  + scale_x_continuous(breaks=range(2021, 2030))\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Key financial ratios and ad-hoc forecasts of Microsoft between 2021 and 2030\"\n    )\n  + theme(legend_position=\"right\")\n)\nratios_forecast_figure.show()\n\n\n\n\n\n\n\nFigure 2: Historical ratios are based on financial statements provided through the FMP API, while forecasts are manually defined.\n\n\n\n\n\nThe final step in our FCF forecast is projecting revenue growth. While there are multiple approaches to this task, we demonstrate a GDP-based method that links company growth to macroeconomic forecasts.\nWe use GDP growth forecasts from the IMF World Economic Outlook (WEO) database as our baseline. The WEO provides comprehensive economic projections, though it is important to note that these forecasts are periodically revised as new data becomes available. We manually collect these forecasts and store them in a tibble.\n\ngdp_growth = pd.DataFrame({\n  \"year\": list(range(2021, 2031)),\n  \"gdp_growth\": [\n    0.06055, 0.02512, 0.02887, 0.02765, 0.02153,\n    0.02028, 0.02120, 0.02122, 0.02122, 0.02122\n  ]\n})\n\ndcf_data = (dcf_data\n  .merge(gdp_growth, on=\"year\", how=\"left\")\n)\n\nOur approach models revenue growth as a linear function of GDP growth. This relation captures the intuition that company revenues often move in tandem with broader economic activity, though usually with different sensitivity. Let us estimate the model with historical data.\n\nrevenue_growth_model = smf.ols(\"revenue_growth ~ gdp_growth\", data=dcf_data).fit().params\n\ndcf_data = (dcf_data\n  .assign(\n    revenue_growth_modeled=lambda x: (\n      revenue_growth_model[\"Intercept\"] + revenue_growth_model[\"gdp_growth\"] * x[\"gdp_growth\"]\n    ),\n    revenue_growth=lambda x: (\n      np.where(x[\"type\"] == \"Forecast\", x[\"revenue_growth_modeled\"], x[\"revenue_growth\"])\n    )\n  )\n)\n\nThe model estimates two parameters: (i) an intercept that captures the company’s baseline growth, and (ii) a slope coefficient that measures the company’s sensitivity to GDP changes. Then, we can use the model and the growth forecasts to make revenue growth projections. We visualize the historical and projected growth rates using this approach in Figure 3:\n\nrevenue_growth = (dcf_data\n  .query(\"year &gt;= 2021\")\n  .melt(\n    id_vars=[\"year\", \"type\"],\n    value_vars=[\"revenue_growth\", \"gdp_growth\"], \n    var_name=\"name\",\n    value_name=\"value\"\n  )\n)\n\nrevenue_growth[\"type\"] = pd.Categorical(\n  revenue_growth[\"type\"], categories=[\"Realized\", \"Forecast\"]\n)\n\nrevenue_growth_figure = (\n  ggplot(\n    revenue_growth,\n    aes(x=\"year\", y=\"value\", color=\"name\", linetype=\"type\")\n  )\n  + geom_line()\n  + scale_x_continuous(breaks=range(2021, 2030))\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"GDP growth and Microsoft's revenue growth and forecasts between 2021 and 2030\"\n    )\n  + theme(legend_position=\"right\")\n)\nrevenue_growth_figure.show()\n\n\n\n\n\n\n\nFigure 3: Realized revenue growth rates are based on financial statements provided through the FMP API, while forecasts are modeled unsing IMF WEO forecasts.\n\n\n\n\n\nWhile more sophisticated approaches exist (e.g., proprietary analyst forecasts or bottom-up market analyses), this method provides a transparent and data-driven starting point for revenue projections.\nWith all components in place - revenue growth projections and forecast ratios - we can now calculate our FCF forecasts. We must first convert our growth rates into revenue projections and then apply our forecast ratios to compute each FCF component.\n\ndcf_data.loc[0, \"revenue_growth\"] = 0\ndcf_data[\"revenue\"] = dcf_data[\"revenue\"].iloc[0] * (1 + dcf_data[\"revenue_growth\"]).cumprod()\n\ndcf_data = (dcf_data\n  .assign(\n    ebit=lambda x: x[\"operating_margin\"] * x[\"revenue\"],\n    depreciation_and_amortization=lambda x: x[\"da_margin\"] * x[\"revenue\"],\n    taxes=lambda x: x[\"taxes_to_revenue\"] * x[\"revenue\"],\n    delta_working_capital=lambda x: x[\"delta_working_capital_to_revenue\"] * x[\"revenue\"],\n    capex=lambda x: x[\"capex_to_revenue\"] * x[\"revenue\"],\n    fcf=lambda x: x[\"ebit\"] + x[\"depreciation_and_amortization\"] - x[\"taxes\"] + x[\"delta_working_capital\"] - x[\"capex\"]\n  )\n)\n\nWe visualize the resulting FCF projections in Figure 4.\n\ncash_flows_figure = (\n  ggplot(\n    dcf_data,\n    aes(x=\"year\", y=\"fcf / 1e9\", fill=\"type\")\n  )\n  + geom_col()\n  + scale_x_continuous(breaks=range(2021, 2030))\n  + labs(\n      x=\"\", y=\"Free Cash Flow (in B USD)\", fill=\"\",\n      title=\"Actual and predicted free cash flow for Microsoft from 2021 to 2030\"\n    )\n)\ncash_flows_figure.show()\n\n\n\n\n\n\n\nFigure 4: Realized growth rates are based on financial statements provided through the FMP API, while forecasts are manually defined.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#continuation-value",
    "href": "python/discounted-cash-flow-analysis.html#continuation-value",
    "title": "Discounted Cash Flow Analysis",
    "section": "Continuation Value",
    "text": "Continuation Value\nA key component of DCF analysis is the continuation value (or terminal value), representing the company’s value beyond the explicit forecast period. This value often constitutes the majority of the total valuation, making its estimation particularly important. You can compute the continuation value once the company has reached its steady state.\nThe most common approach is the Perpetuity Growth Model (or Gordon Growth Model), which assumes FCF grows at a constant rate indefinitely. The formula for this model is:\n\\[TV_{T} = \\frac{FCF_{T+1}}{r - g},\\]\nwhere \\(TV_{T}\\) is the terminal value at time \\(T\\), \\(FCF_{T+1}\\) is the free cash flow in the first year after our forecast period, \\(r\\) is the discount rate (typically WACC, see below), and \\(g\\) is the perpetual growth rate. A common mistake is to ignore the time shift in the model. You must compute \\(FCF_{T+1}\\), which is equal to \\(FCF_{T}\\cdot(1+g)\\)\nThe perpetual growth rate \\(g\\) should reflect the long-term economic growth potential. A common benchmark is the long-term GDP growth rate, as few companies can sustainably grow faster than the overall economy indefinitely. Exceeding GDP growth indefinitely also implies that the whole economy eventually consists of one company. For example, the last 20 years of GDP growth is a sensible assumption (the nominal growth rate is 4% for the US).\nLet us implement the Perpetuity Growth Model:\n\ndef compute_terminal_value(last_fcf, growth_rate, discount_rate):\n    return last_fcf * (1 + growth_rate) / (discount_rate - growth_rate)\n\nlast_fcf = dcf_data['fcf'].iloc[-1]\nterminal_value = compute_terminal_value(\n  last_fcf, growth_rate=0.04, discount_rate=0.08\n)\n\nprint(np.round(terminal_value / 1e9))\n\n10607.0\n\n\nNote that while we use the Perpetuity Growth Model here, practitioners often cross-check their estimates with alternative methods like the exit multiple approach, which bases the terminal value on comparable company valuations.1",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#discount-rate",
    "href": "python/discounted-cash-flow-analysis.html#discount-rate",
    "title": "Discounted Cash Flow Analysis",
    "section": "Discount Rate",
    "text": "Discount Rate\nThe final component of our DCF analysis involves discounting future cash flows to present value. We typically use the Weighted Average Cost of Capital (WACC) as the discount rate, representing the blended cost of financing for all company stakeholders. The WACC is the correct rate to discount cash flows distributed to both debt and equity holders, which is the case in our model as we use free cash flows.\nThe WACC formula combines the costs of equity and debt financing:\n\\[WACC = \\frac{E}{D+E} \\cdot r^E + \\frac{D}{D+E} \\cdot r^D \\cdot (1 - \\tau),\\]\nwhere \\(E\\) is the market value of the company’s equity with required return \\(r^E\\), \\(D\\) is the market value of the company’s debt with pre-tax return \\(r^D\\), and \\(\\tau\\) is the tax rate.\nWhile you can often find estimates of WACC from financial databases or analysts’ reports, you may need to calculate it yourself. Let us walk through the practical steps to estimate WACC using real-world data:\n\n\\(E\\) is typically measured as the market value of the company’s equity. One common approach is to use market capitalization from the stock exchange.\n\\(D\\) is often measured using the book value of the company’s debt. While this might not perfectly reflect market conditions, it is a practical starting point when market data is unavailable. Moreover, it is close to correct without default.\nThe Capital Asset Pricing Model (CAPM) is a popular method to estimate the cost of equity \\(r^E\\). It considers the risk-free rate, the equity risk premium, and the company’s risk exposure (i.e., beta). For a detailed guide estimating the CAPM, we refer to Chapter Capital Asset Pricing Model.\nThe return on debt \\(r^D\\) can also be estimated in different ways. For instance, effective interest rates can be calculated as the ratio of interest expense to total debt from financial statements. This gives you a real-world measure of what the company is currently paying. Alternatively, you can look up corporate bond spreads for companies in the same rating group.\n\nIf you would rather not estimate WACC manually, resources are available to help you find industry-specific discount rates. One of the most widely used sources is Aswath Damodaran’s database. He maintains an extensive database that provides a wealth of financial data, including estimated discount rates, cash flows, growth rates, multiples, and more. For example, if you are analyzing a company in the Computer Services sector, as we do here, you can look up the industry’s average WACC and use it as a benchmark for your analysis. The following code chunk downloads the WACC data and extracts the value for this industry:\n\nimport requests\n\nurl = \"https://pages.stern.nyu.edu/~adamodar/pc/datasets/wacc.xls\"\nresponse = requests.get(url)\n\nwith open(\"wacc.xls\", \"wb\") as file:\n    file.write(response.content)\n    \nwacc_raw = pd.read_excel(\"wacc.xls\", sheet_name=1, skiprows=18)\n\nimport os\nos.remove(\"wacc.xls\")\n\nwacc = wacc_raw.loc[\n  wacc_raw[\"Industry Name\"] == \"Computer Services\", \"Cost of Capital\"\n].values[0]\n\n\n\n\n\n\n\nTip\n\n\n\nYou need the xlrd package to use pd.read_excel(), so make sure to install it via, e.g., pip install xlrd",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#computing-enterprise-value",
    "href": "python/discounted-cash-flow-analysis.html#computing-enterprise-value",
    "title": "Discounted Cash Flow Analysis",
    "section": "Computing Enterprise Value",
    "text": "Computing Enterprise Value\nHaving established all components, we can now compute the total company value (given that there are no non-operating activities). The enterprise value combines two elements:\n\nThe present value of cash flows during the explicit forecast period and\nThe present value of the continuation (or terminal) value.\n\nThis is expressed mathematically as:\n\\[\n\\text{Total DCF Value} = \\sum_{t=1}^{T} \\frac{\\text{FCF}t}{(1 + \\text{WACC})^t} + \\frac{\\text{TV}_{T}}{(1 + \\text{WACC})^T},\n\\]\nwhere \\(T\\) is the length of our forecast period. Let us implement this calculation in a simple function that takes the WACC and growth rate as input. Then, we present an example at the values discussed above.\n\ndef compute_dcf(wacc, growth_rate):\n    free_cash_flow = dcf_data[\"fcf\"].values\n    last_fcf = free_cash_flow[-1]\n    terminal_value = compute_terminal_value(last_fcf, growth_rate, wacc)\n    \n    years = len(free_cash_flow)\n    present_value_fcf = free_cash_flow / ((1 + wacc) ** np.arange(1, years + 1))\n    present_value_tv = terminal_value / ((1 + wacc) ** years)\n    \n    return present_value_fcf.sum() + present_value_tv\n\ndcf_value = compute_dcf(wacc, 0.04)\n\nprint(np.round(dcf_value / 1e9))\n\n5169.0\n\n\nNote that this valuation represents an enterprise value - the total value of the company’s operations. To arrive at the equity value, we need to subtract net debt (total debt minus cash and equivalents).",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#sensitivity-analysis",
    "href": "python/discounted-cash-flow-analysis.html#sensitivity-analysis",
    "title": "Discounted Cash Flow Analysis",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\nDCF valuation is only as robust as its underlying assumptions. Given the inherent uncertainty in forecasting, it is crucial to understand how changes in key inputs affect our valuation.\nWhile we could examine sensitivity to various inputs like operating margins or capital expenditure ratios, we focus on two critical drivers for our exposition:\n\nThe perpetual growth rate, which determines long-term value creation and\nThe WACC, which affects how we value future cash flows.\n\nLet us implement a sensitivity analysis that varies these two parameters:\n\nwacc_range = np.arange(0.06, 0.08 + 0.01, 0.01)\ngrowth_rate_range = np.arange(0.02, 0.04 + 0.01, 0.01)\n\nsensitivity = pd.DataFrame(\n  product(wacc_range, growth_rate_range),\n  columns=[\"wacc\", \"growth_rate\"]\n)\n\nsensitivity[\"value\"] = (sensitivity\n  .apply(\n    lambda x: np.round(compute_dcf(x[\"wacc\"], x[\"growth_rate\"]) / 1e9, 0), axis=1\n  )\n)\n\nsensitivity_figure = (\n  ggplot(\n    sensitivity,\n    aes(x=\"wacc\", y=\"growth_rate\", fill=\"value\", label=\"value\")\n  )\n  + geom_tile()\n  + geom_text(color=\"white\")\n  + scale_x_continuous(labels=percent_format()) \n  + scale_y_continuous(labels=percent_format())\n  + scale_fill_continuous(labels=comma_format())\n  + labs(\n      x=\"WACC\", y=\"Perpetual growth rate\", fill=\"Enterprise value\",\n      title=\"Enterprise value of Microsoft for different WACC and growth rate scenarios\",\n    )\n)\nsensitivity_figure.show()\n\n\n\n\n\n\n\nFigure 5: The enterprise values combine data from FMP API, ad-hoc forecasts of financial ratios, and IMF WEO growth forecasts.\n\n\n\n\n\nFigure 5 reveals several key insights about our valuation: The valuation is highly sensitive to both WACC and growth assumptions, as small changes in either parameter can lead to substantial changes in enterprise value. Moreover, the relation between these parameters and company value is non-linear as the impact of growth rate changes becomes more pronounced at lower WACCs.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#from-enterprise-to-equity-value",
    "href": "python/discounted-cash-flow-analysis.html#from-enterprise-to-equity-value",
    "title": "Discounted Cash Flow Analysis",
    "section": "From Enterprise to Equity Value",
    "text": "From Enterprise to Equity Value\nAs we have discussed, our DCF analysis yields the value of the company’s operations. We have assumed that there are no non-operating assets. Now, we explicitly consider their existence and show you how to compute the equity value belonging to shareholders.\nNon-operating assets are not essential to operations but could, in some cases, generate income (e.g., marketable securities, vacant land, idle equipment). If they exist, you must restate the financial statements to exclude the impact of these non-operating assets. With their effects removed, you can conduct the DCF analysis presented above. Afterwards, you value these non-operating assets and add them to the DCF value to arrive at the company’s enterprise value.\nSecond, we want to discuss equity value. As you saw in the computation of the WACC, free cash flows go to both debt and equity holders. Hence, we must consider the share of the enterprise value that goes to debt. In theory, the value of debt is the market value of total debt, but in practice, typically book debt. This value has to be subtracted from enterprise value.\nCombining these adjustments, we can compute the equity value that belongs to shareholders as:\n\\[\\text{Equity Value} = \\text{DCF Value} + \\text{Non-Operating Assets} - \\text{Value of Debt}\\]\nWe leave it as an exercise to calculate the equity value using the DCF value from above.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#key-takeaways",
    "href": "python/discounted-cash-flow-analysis.html#key-takeaways",
    "title": "Discounted Cash Flow Analysis",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nFree Cash Flow can be calculated by transforming financial statement data into standardized ratios linked to company revenue.\nForecasting future cash flows requires both historical financial data and macroeconomic projections such as GDP growth rates.\nThe terminal value represents long-term company value and can be estimated using the perpetuity growth model.\nThe WACC is used as the discount rate and reflects the cost of financing from both equity and debt.\nA DCF model combines present values of projected free cash flows and terminal value to estimate enterprise value.\nSensitivity analysis reveals how small changes in WACC or growth assumptions can significantly impact company valuation.\nTo determine equity value, subtract net debt from enterprise value and adjust for any non-operating assets or liabilities.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#exercises",
    "href": "python/discounted-cash-flow-analysis.html#exercises",
    "title": "Discounted Cash Flow Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nDownload financial statements for another company of your choice and compute its historical Free Cash Flow. Compare the results with the Microsoft example from this chapter.\nCreate a function that automatically generates FCF forecasts using different sets of ratio assumptions. Use it to create alternative scenarios for Microsoft.\nImplement an exit multiple approach for terminal value calculation and compare the results with the perpetuity growth method.\nExtend the sensitivity analysis to include operating margin assumptions. Create a visualization showing how changes in margins affect the final valuation.\nCalculate Microsoft’s equity value by adjusting the DCF value as described above.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/discounted-cash-flow-analysis.html#footnotes",
    "href": "python/discounted-cash-flow-analysis.html#footnotes",
    "title": "Discounted Cash Flow Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee (corporatefinanceinstitute.com/)[https://corporatefinanceinstitute.com/resources/valuation/exit-multiple/)] for an intuitive explanation of the exit multiple approach.↩︎",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html",
    "href": "python/factor-selection-via-machine-learning.html",
    "title": "Factor Selection via Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nThe aim of this chapter is twofold. From a data science perspective, we introduce scikit-learn, a collection of packages for modeling and machine learning (ML). scikit-learn comes with a handy workflow for all sorts of typical prediction tasks. From a finance perspective, we address the notion of factor zoo (Cochrane 2011) using ML methods. We introduce Lasso, Ridge, and Elastic Net regression as a special case of penalized regression models. Then, we explain the concept of cross-validation for model tuning with Elastic Net regularization as a popular example. We implement and showcase the entire cycle from model specification, training, and forecast evaluation within the scikit-learn universe. While the tools can generally be applied to an abundance of interesting asset pricing problems, we apply penalized regressions for identifying macroeconomic variables and asset pricing factors that help explain a cross-section of industry portfolios.\nIn previous chapters, we illustrate that stock characteristics such as size provide valuable pricing information in addition to the market beta. Such findings question the usefulness of the Capital Asset Pricing Model. In fact, during the last decades, financial economists discovered a plethora of additional factors which may be correlated with the marginal utility of consumption (and would thus deserve a prominent role in pricing applications). The search for factors that explain the cross-section of expected stock returns has produced hundreds of potential candidates, as noted more recently by Harvey, Liu, and Zhu (2016), Harvey (2017), Mclean and Pontiff (2016), and Hou, Xue, and Zhang (2020). Therefore, given the multitude of proposed risk factors, the challenge these days rather is: do we believe in the relevance of hundreds of risk factors? During recent years, promising methods from the field of ML got applied to common finance applications. We refer to Mullainathan and Spiess (2017) for a treatment of ML from the perspective of an econometrician, Nagel (2021) for an excellent review of ML practices in asset pricing, Easley et al. (2020) for ML applications in (high-frequency) market microstructure, and Dixon, Halperin, and Bilokon (2020) for a detailed treatment of all methodological aspects.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "href": "python/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "title": "Factor Selection via Machine Learning",
    "section": "Brief Theoretical Background",
    "text": "Brief Theoretical Background\nThis is a book about doing empirical work in a tidy manner, and we refer to any of the many excellent textbook treatments of ML methods and especially penalized regressions for some deeper discussion. Excellent material is provided, for instance, by Hastie, Tibshirani, and Friedman (2009), Gareth et al. (2013), and De Prado (2018). Instead, we briefly summarize the idea of Lasso and Ridge regressions as well as the more general Elastic Net. Then, we turn to the fascinating question on how to implement, tune, and use such models with the scikit-learn package.\nTo set the stage, we start with the definition of a linear model: Suppose we have data \\((y_t, x_t), t = 1,\\ldots, T\\), where \\(x_t\\) is a \\((K \\times 1)\\) vector of regressors and \\(y_t\\) is the response for observation \\(t\\). The linear model takes the form \\(y_t = \\beta' x_t + \\varepsilon_t\\) with some error term \\(\\varepsilon_t\\) and has been studied in abundance. For \\(K\\leq T\\), the well-known ordinary-least square (OLS) estimator for the \\((K \\times 1)\\) vector \\(\\beta\\) minimizes the sum of squared residuals and is then \\[\\hat{\\beta}^\\text{ols} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t. \\tag{1}\\] \nWhile we are often interested in the estimated coefficient vector \\(\\hat\\beta^\\text{ols}\\), ML is about the predictive performance most of the time. For a new observation \\(\\tilde{x}_t\\), the linear model generates predictions such that \\[\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta^\\text{ols}{}' \\tilde x_t. \\tag{2}\\] Is this the best we can do? Not necessarily: instead of minimizing the sum of squared residuals, penalized linear models can improve predictive performance by choosing other estimators \\(\\hat{\\beta}\\) with lower variance than the estimator \\(\\hat\\beta^\\text{ols}\\). At the same time, it seems appealing to restrict the set of regressors to a few meaningful ones, if possible. In other words, if \\(K\\) is large (such as for the number of proposed factors in the asset pricing literature), it may be a desirable feature to select reasonable factors and set \\(\\hat\\beta^{\\text{ols}}_k = 0\\) for some redundant factors.\nIt should be clear that the promised benefits of penalized regressions, i.e., reducing the mean squared error (MSE), come at a cost. In most cases, reducing the variance of the estimator introduces a bias such that \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). What is the effect of such a bias-variance trade-off? To understand the implications, assume the following data-generating process for \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2) \\tag{3}\\] We want to recover \\(f(x)\\), which denotes some unknown functional which maps the relationship between \\(x\\) and \\(y\\). While the properties of \\(\\hat\\beta^\\text{ols}\\) as an unbiased estimator may be desirable under some circumstances, they are certainly not if we consider predictive accuracy. Alternative predictors \\(\\hat{f}(x)\\) could be more desirable: For instance, the MSE depends on our model choice as follows: \\[\\begin{aligned}\nMSE &=E\\left(\\left(y-\\hat{f}(x)\\right)^2\\right)=E\\left(\\left(f(x)+\\epsilon-\\hat{f}(x)\\right)^2\\right)\\\\\n&= \\underbrace{E\\left(\\left(f(x)-\\hat{f}(x)\\right)^2\\right)}_{\\text{total quadratic error}}+\\underbrace{E\\left(\\epsilon^2\\right)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(x)^2\\right)+E\\left(f(x)^2\\right)-2E\\left(f(x)\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(x)^2\\right)+f(x)^2-2f(x)E\\left(\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(x)\\right)}_{\\text{variance of model}}+ \\underbrace{\\left(E(f(x)-\\hat{f}(x))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2.\n\\end{aligned} \\tag{4}\\] While no model can reduce \\(\\sigma_\\varepsilon^2\\), a biased estimator with small variance may have a lower MSE than an unbiased estimator.\n\nRidge regression\n\nOne biased estimator is known as Ridge regression. Hoerl and Kennard (1970) propose to minimize the sum of squared errors while simultaneously imposing a penalty on the \\(L_2\\) norm of the parameters \\(\\hat\\beta\\). Formally, this means that for a penalty factor \\(\\lambda\\geq 0\\), the minimization problem takes the form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq c\\). Here \\(c\\geq 0\\) is a constant that depends on the choice of \\(\\lambda\\). The larger \\(\\lambda\\), the smaller \\(c\\) (technically speaking, there is a one-to-one relationship between \\(\\lambda\\), which corresponds to the Lagrangian of the minimization problem above and \\(c\\)). Here, \\(X = \\left(x_1 \\ldots x_T\\right)'\\) and \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). A closed-form solution for the resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda I\\right)^{-1}X'y,\n\\tag{5}\\] where \\(I\\) is the identity matrix of dimension \\(K\\). A couple of observations are worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta^\\text{ols}\\) for \\(\\lambda = 0\\) and \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) for \\(\\lambda\\rightarrow \\infty\\). Also for \\(\\lambda &gt; 0\\), \\(\\left(X'X + \\lambda I\\right)\\) is non-singular even if \\(X'X\\) is which means that \\(\\hat\\beta^\\text{ridge}\\) exists even if \\(\\hat\\beta\\) is not defined. However, note also that the Ridge estimator requires careful choice of the hyperparameter \\(\\lambda\\) which controls the amount of regularization: a larger value of \\(\\lambda\\) implies shrinkage of the regression coefficient toward 0; a smaller value of \\(\\lambda\\) reduces the bias of the resulting estimator.\n\nNote that \\(X\\) usually contains an intercept column with ones. As a general rule, the associated intercept coefficient is not penalized. In practice, this often implies that \\(y\\) is simply demeaned before computing \\(\\hat\\beta^\\text{ridge}\\).\n\nWhat about the statistical properties of the Ridge estimator? First, the bad news is that \\(\\hat\\beta^\\text{ridge}\\) is a biased estimator of \\(\\beta\\). However, the good news is that (under homoscedastic error terms) the variance of the Ridge estimator is guaranteed to be smaller than the variance of the OLS estimator. We encourage you to verify these two statements in the Exercises. As a result, we face a trade-off: The Ridge regression sacrifices some unbiasedness to achieve a smaller variance than the OLS estimator.\n\n\nLasso\n\nAn alternative to Ridge regression is the Lasso (least absolute shrinkage and selection operator). Similar to Ridge regression, the Lasso (Tibshirani 1996) is a penalized and biased estimator. The main difference to Ridge regression is that Lasso does not only shrink coefficients but effectively selects variables by setting coefficients for irrelevant variables to zero. Lasso implements a \\(L_1\\) penalization on the parameters such that: \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right)\\text{ s.t. } \\sum\\limits_{k=1}^K|\\beta_k| &lt; c(\\lambda). \\tag{6}\\] There is no closed-form solution for \\(\\hat\\beta^\\text{Lasso}\\) in the above maximization problem, but efficient algorithms exist (e.g., the glmnet package for R and Python). Like for Ridge regression, the hyperparameter \\(\\lambda\\) has to be specified beforehand.\nThe corresponding Lagrangian reads as follows \\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned} \\tag{7}\\]\n\n\nElastic Net\nThe Elastic Net (Zou and Hastie 2005) combines \\(L_1\\) with \\(L_2\\) penalization and encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. In terms of the Lagrangian, this more general framework considers the following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2 \\tag{8}\\] Now, we have to choose two hyperparameters: the shrinkage factor \\(\\lambda\\) and the weighting parameter \\(\\rho\\). The Elastic Net resembles Lasso for \\(\\rho = 0\\) and Ridge regression for \\(\\rho = 1\\). While the glmnet package provides efficient algorithms to compute the coefficients of penalized regressions, it is a good exercise to implement Ridge and Lasso estimation on your own before you use the scikit-learn back-end.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#python-packages",
    "href": "python/factor-selection-via-machine-learning.html#python-packages",
    "title": "Factor Selection via Machine Learning",
    "section": "Python Packages",
    "text": "Python Packages\nTo get started, we load the required packages and data. The main focus is on the workflow behind the scikit-learn (Pedregosa et al. 2011) package collection.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nfrom plotnine import * \nfrom mizani.formatters import percent_format, date_format\nfrom mizani.breaks import date_breaks\nfrom itertools import product\nfrom sklearn.model_selection import (\n  train_test_split, GridSearchCV, TimeSeriesSplit, cross_val_score\n)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#data-preparation",
    "href": "python/factor-selection-via-machine-learning.html#data-preparation",
    "title": "Factor Selection via Machine Learning",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn this analysis, we use four different data sources that we load from our SQLite database introduced in Accessing and Managing Financial Data. We start with two different sets of factor portfolio returns which have been suggested as representing practical risk factor exposure and thus should be relevant when it comes to asset pricing applications.\n\nThe standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big, and high-minus-low book-to-market valuation sorts) defined in Fama and French (1992) and Fama and French (1993).\nMonthly q-factor returns from Hou, Xue, and Zhang (2014). The factors contain the size factor, the investment factor, the return-on-equity factor, and the expected growth factor.\n\nNext, we include macroeconomic predictors which may predict the general stock market economy. Macroeconomic variables effectively serve as conditioning information such that their inclusion hints at the relevance of conditional models instead of unconditional asset pricing. We refer the interested reader to Cochrane (2009) on the role of conditioning information.\n\nOur set of macroeconomic predictors comes from Welch and Goyal (2008). The data has been updated by the authors until 2021 and contains monthly variables that have been suggested as good predictors for the equity premium. Some of the variables are the dividend price ratio, earnings price ratio, stock variance, net equity expansion, treasury bill rate, and inflation.\n\nFinally, we need a set of test assets. The aim is to understand which of the plenty factors and macroeconomic variable combinations prove helpful in explaining our test assets’ cross-section of returns. In line with many existing papers, we use monthly portfolio returns from ten different industries according to the definition from Kenneth French’s homepage as test assets.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_ff3_monthly = (pd.read_sql_query(\n     sql=\"SELECT * FROM factors_ff3_monthly\",\n     con=tidy_finance,\n     parse_dates={\"date\"})\n  .add_prefix(\"factor_ff_\")\n)\n\nfactors_q_monthly = (pd.read_sql_query(\n    sql=\"SELECT * FROM factors_q_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .add_prefix(\"factor_q_\")\n)\n\nmacro_predictors = (pd.read_sql_query(\n    sql=\"SELECT * FROM macro_predictors\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n .add_prefix(\"macro_\")\n)\n\nindustries_ff_monthly = (pd.read_sql_query(\n    sql=\"SELECT * FROM industries_ff_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .melt(id_vars=\"date\", var_name=\"industry\", value_name=\"ret\")\n)\n\nWe combine all the monthly observations into one dataframe.\n\ndata = (industries_ff_monthly\n  .merge(factors_ff3_monthly, \n         how=\"left\", left_on=\"date\", right_on=\"factor_ff_date\")\n  .merge(factors_q_monthly, \n         how=\"left\", left_on=\"date\", right_on=\"factor_q_date\")\n  .merge(macro_predictors, \n         how=\"left\", left_on=\"date\", right_on=\"macro_date\") \n  .assign(ret_excess=lambda x: x[\"ret\"] - x[\"factor_ff_rf\"]) \n  .drop(columns=[\"ret\", \"factor_ff_date\", \"factor_q_date\", \"macro_date\"])\n  .dropna()\n)\n\nOur data contains 23 columns of regressors with the 13 macro-variables and 9 factor returns for each month. Figure 1 provides summary statistics for the 10 monthly industry excess returns in percent. One can see that the dispersion in the excess returns varies widely across industries. \n\ndata_figure = (\n  ggplot(\n    data, \n    aes(x=\"industry\", y=\"ret_excess\")\n  )\n  + geom_boxplot() \n  + coord_flip()\n  + labs(x=\"\", y=\"\", title=\"Excess return distributions by industry in percent\")\n  + scale_y_continuous(labels=percent_format())\n)\ndata_figure.show()\n\n\n\n\n\n\n\nFigure 1: The box plots show the monthly dispersion of returns for 10 different industries.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#machine-learning-workflow",
    "href": "python/factor-selection-via-machine-learning.html#machine-learning-workflow",
    "title": "Factor Selection via Machine Learning",
    "section": "Machine Learning Workflow",
    "text": "Machine Learning Workflow\nTo illustrate penalized linear regressions, we employ the scikit-learn collection of packages for modeling and ML. Using the ideas of Ridge and Lasso regressions, the following example guides you through (i) pre-processing the data (data split and variable mutation), (ii) building models, (iii) fitting models, and (iv) tuning models to create the “best” possible predictions.\n\nPre-process data\nWe want to explain excess returns with all available predictors. The regression equation thus takes the form \\[r_{t} = \\alpha_0 + \\left(\\tilde f_t \\otimes \\tilde z_t\\right)B + \\varepsilon_t  \\tag{9}\\] where \\(r_t\\) is the vector of industry excess returns at time \\(t\\), \\(\\otimes\\) denotes the Kronecker product and \\(\\tilde f_t\\) and \\(\\tilde z_t\\) are the (standardized) vectors of factor portfolio returns and macroeconomic variables.\nWe hence perform the following pre-processing steps:\n\nWe exclude the column month from the analysis\nWe include all interaction terms between factors and macroeconomic predictors\nWe demean and scale each regressor such that the standard deviation is one\n\nScaling is often necessary in machine learning applications, especially when combining variables of different magnitudes or units, or when using algorithms sensitive to feature scales (e.g., gradient descent-based algorithms). We use ColumnTransformer() to scale all regressors using StandardScaler(). The remainder=\"drop\" ensures that only the specified columns are retained in the output, and others are dropped. The option verbose_feature_names_out=False ensures that the output feature names remain unchanged. Also note that we use the zip() function to pair each element from column_names with its corresponding list of values from new_column_values, creating tuples, and then convert these tuples into a dictionary using dict() from which we create a dataframe.\n\nmacro_variables = data.filter(like=\"macro\").columns\nfactor_variables = data.filter(like=\"factor\").columns\n\ncolumn_combinations = list(product(macro_variables, factor_variables))\n\nnew_column_values = []\nfor macro_column, factor_column in column_combinations:\n    new_column_values.append(data[macro_column] * data[factor_column])\n\ncolumn_names = [\" x \".join(t) for t in column_combinations]\nnew_columns = pd.DataFrame(dict(zip(column_names, new_column_values)))\n\ndata = pd.concat([data, new_columns], axis=1)\n\npreprocessor = ColumnTransformer(\n  transformers=[\n    (\"scale\", StandardScaler(), \n    [col for col in data.columns \n      if col not in [\"ret_excess\", \"date\", \"industry\"]])\n  ],\n  remainder=\"drop\",\n  verbose_feature_names_out=False\n)\n\n\n\nBuild a model\n Next, we can build an actual model based on our pre-processed data. In line with the definition above, we estimate regression coefficients of a Lasso regression such that we get\n\\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned} \\tag{10}\\] In the application at hand, \\(X\\) contains 117 columns with all possible interactions between factor returns and macroeconomic variables. We want to emphasize that the workflow for any model is very similar, irrespective of the specific model. As you will see further below, it is straightforward to fit Ridge regression coefficients and, later, Neural networks or Random forests with similar code. For now, we start with the linear regression model with an arbitrarily chosen value for the penalty factor \\(\\lambda\\) (denoted as alpha=0.007 in the code below). In the setup below, l1_ratio denotes the value of \\(1-\\rho\\), hence setting l1_ratio=1 implies the Lasso.\n\nlm_model = ElasticNet(\n  alpha=0.007,\n  l1_ratio=1, \n  max_iter=5000, \n  fit_intercept=False\n)  \n\nlm_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", lm_model)\n])\n\nThat’s it - we are done! The object lm_model_pipeline contains the definition of our model with all required information, in particular the pre-processing steps and the regression model.\n\n\nFit a model\nWith the pipeline from above, we are ready to fit it to the data. Typically, we use training data to fit the model. The training data is pre-processed according to our recipe steps, and the Lasso regression coefficients are computed. For illustrative purposes, we focus on the manufacturing industry for now.\n\ndata_manufacturing = data.query(\"industry == 'manuf'\")\ntraining_date = \"2011-12-01\"\n\ndata_manufacturing_training = (data_manufacturing\n  .query(f\"date&lt;'{training_date}'\")\n)\n\nlm_fit = lm_pipeline.fit(\n  data_manufacturing_training, \n  data_manufacturing_training.get(\"ret_excess\")\n)\n\nFirst, we focus on the in-sample predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) Figure 2 illustrates the projections for the entire time series of the manufacturing industry portfolio returns.\n\npredicted_values = (pd.DataFrame({\n    \"Fitted value\": lm_fit.predict(data_manufacturing),\n    \"Realization\": data_manufacturing.get(\"ret_excess\")\n  })\n  .assign(date = data_manufacturing[\"date\"])\n  .melt(id_vars=\"date\", var_name=\"Variable\", value_name=\"return\")\n)\n\npredicted_values_figure = (\n  ggplot(\n    predicted_values, \n    aes(x=\"date\", y=\"return\", color=\"Variable\", linetype=\"Variable\")\n  )\n  + annotate(\n    \"rect\",\n    xmin=data_manufacturing_training[\"date\"].max(),\n    xmax=data_manufacturing[\"date\"].max(),\n    ymin=-np.inf, ymax=np.inf,\n    alpha=0.25, fill=\"#808080\"\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly realized and fitted manufacturing risk premia\"\n    )\n  + scale_x_datetime(breaks=date_breaks(\"5 years\"), labels=date_format(\"%Y\"))\n  + scale_y_continuous(labels=percent_format())\n)\npredicted_values_figure.show()\n\n\n\n\n\n\n\nFigure 2: The figure shows monthly realized and fitted manufacturing industry risk premium. The grey area corresponds to the out of sample period.\n\n\n\n\n\nWhat do the estimated coefficients look like? To analyze these values, it is worth computing the coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. The code below estimates the coefficients for the Lasso and Ridge regression for the processed training data sample for a grid of different \\(\\lambda\\)’s.\n\nx = preprocessor.fit_transform(data_manufacturing)\ny = data_manufacturing[\"ret_excess\"]\n\nalphas = np.logspace(-5, 5, 100)\n\ncoefficients_lasso = []\nfor a in alphas:\n    lasso = Lasso(alpha=a, fit_intercept=False)\n    coefficients_lasso.append(lasso.fit(x, y).coef_)\n    \ncoefficients_lasso = (pd.DataFrame(coefficients_lasso)\n  .assign(alpha=alphas, model=\"Lasso\")\n  .melt(id_vars=[\"alpha\", \"model\"])\n)\n    \ncoefficients_ridge = []\nfor a in alphas:\n    ridge = Ridge(alpha=a, fit_intercept=False)\n    coefficients_ridge.append(ridge.fit(x, y).coef_)\n\ncoefficients_ridge = (pd.DataFrame(coefficients_ridge)\n  .assign(alpha=alphas, model=\"Ridge\")\n  .melt(id_vars=[\"alpha\", \"model\"])\n)\n\nThe dataframes lasso_coefficients and ridge_coefficients contain an entire sequence of estimated coefficients for multiple values of the penalty factor \\(\\lambda\\). Figure 3 illustrates the trajectories of the regression coefficients as a function of the penalty factor. Both Lasso and Ridge coefficients converge to zero as the penalty factor increases.\n\ncoefficients_figure = (\n  ggplot(\n    pd.concat([coefficients_lasso, coefficients_ridge]), \n    aes(x=\"alpha\", y=\"value\", color=\"variable\")\n  )\n  + geom_line()\n  + facet_wrap(\"model\")\n  + labs(\n      x=\"Penalty factor (lambda)\", y=\"\",\n      title=\"Estimated coefficient paths for different penalty factors\"\n    )\n  + scale_x_log10()\n  + theme(legend_position=\"none\"))\ncoefficients_figure.show()\n\n\n\n\n\n\n\nFigure 3: The figure shows estimated coefficient paths for different penalty factors. The penalty parameters are chosen iteratively to resemble the path from no penalization to a model that excludes all variables.\n\n\n\n\n\n\n\nTune a model\nTo compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , we simply imposed an arbitrary value for the penalty hyperparameter \\(\\lambda\\). Model tuning is the process of optimally selecting such hyperparameters through cross-validation.\nThe goal for choosing \\(\\lambda\\) (or any other hyperparameter, e.g., \\(\\rho\\) for the Elastic Net) is to find a way to produce predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, the MSPE is not directly observable. We can only compute an estimate because our data is random and because we do not observe the entire population.\nObviously, if we train an algorithm on the same data that we use to compute the error, our estimate \\(\\text{MSPE}\\) would indicate way better predictive accuracy than what we can expect in real out-of-sample data. The result is called overfitting.\nCross-validation is a technique that allows us to alleviate this problem. We approximate the true MSPE as the average of many MSPE obtained by creating predictions for \\(K\\) new random samples of the data, none of them used to train the algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). In practice, this is done by carving out a piece of our data and pretending it is an independent sample. We again divide the data into a training set and a test set. The MSPE on the test set is our measure for actual predictive ability, while we use the training set to fit models with the aim to find the optimal hyperparameter values. To do so, we further divide our training sample into (several) subsets, fit our model for a grid of potential hyperparameter values (e.g., \\(\\lambda\\)), and evaluate the predictive accuracy on an independent sample. This works as follows:\n\nSpecify a grid of hyperparameters\nObtain predictors \\(\\hat{y}_i(\\lambda)\\) to denote the predictors for the used parameters \\(\\lambda\\)\nCompute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2 .\n\\tag{11}\\] With K-fold cross-validation, we do this computation \\(K\\) times. Simply pick a validation set with \\(M=T/K\\) observations at random and think of these as random samples \\(y_1^k, \\dots, y_{\\tilde{T}}^k\\), with \\(k=1\\).\n\nHow should you pick \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original data. However, larger values of \\(K\\) will have much higher computation time. scikit-learn provides all required tools to conduct \\(K\\)-fold cross-validation. We just have to update our model specification. In our case, we specify the penalty factor \\(\\lambda\\) as well as the mixing factor \\(\\rho\\) as free parameters.\nFor our sample, we consider a time-series cross-validation sample. This means that we tune our models with 20 samples of length five years with a validation period of four years. For a grid of possible hyperparameters, we then fit the model for each fold and evaluate \\(\\hat{\\text{MSPE}}\\) in the corresponding validation set. Finally, we select the model specification with the lowest MSPE in the validation set. First, we define the cross-validation folds based on our training data only.\nThen, we evaluate the performance for a grid of different penalty values. scikit-learn provides functionalities to construct a suitable grid of hyperparameters with GridSearchCV(). The code chunk below creates a \\(10 \\times 3\\) hyperparameters grid. Then, the method fit() evaluates all the models for each fold.\n\ninitial_years = 5\nassessment_months = 48\nn_splits = int(len(data_manufacturing)/assessment_months) - 1\nlength_of_year = 12\nalphas = np.logspace(-6, 2, 100)\n\ndata_folds = TimeSeriesSplit(\n  n_splits=n_splits, \n  test_size=assessment_months, \n  max_train_size=initial_years * length_of_year\n)\n\nparams = {\n  \"regressor__alpha\": alphas,\n  \"regressor__l1_ratio\": (0.0, 0.5, 1)\n}\n\nfinder = GridSearchCV(\n  lm_pipeline,\n  param_grid=params,\n  scoring=\"neg_root_mean_squared_error\",\n  cv=data_folds\n)\n\nfinder = finder.fit(\n  data_manufacturing, data_manufacturing.get(\"ret_excess\")\n)\n\nAfter the tuning process, we collect the evaluation metrics (the root mean-squared error in our example) to identify the optimal model. Figure 4 illustrates the average validation set’s root mean-squared error for each value of \\(\\lambda\\) and \\(\\rho\\).\n\nvalidation = (pd.DataFrame(finder.cv_results_)\n  .assign(\n    mspe=lambda x: -x[\"mean_test_score\"],\n    param_regressor__alpha=lambda x: pd.to_numeric(\n      x[\"param_regressor__alpha\"], errors=\"coerce\"\n    )\n  )\n)\n\nvalidation_figure = (\n  ggplot(\n    validation, \n    aes(x=\"param_regressor__alpha\", y=\"mspe\", \n        color=\"param_regressor__l1_ratio\",\n        shape=\"param_regressor__l1_ratio\",\n        group=\"param_regressor__l1_ratio\")\n  )\n  + geom_point()\n  + geom_line()\n  + labs(\n      x =\"Penalty factor (lambda)\", y=\"Root MSPE\", \n      title=\"Root MSPE for different penalty factors\",\n       color=\"Proportion of Lasso Penalty\",\n       shape=\"Proportion of Lasso Penalty\"\n    )\n  + scale_x_log10()\n  + guides(linetype=\"none\")\n)\nvalidation_figure.show()\n\n\n\n\n\n\n\nFigure 4: The figure shows root MSPE for different penalty factors. Evaluation of manufacturing excess returns for different penalty factors (lambda) and proportions of Lasso penalty (rho). 1.0 indicates Lasso, 0.5 indicates Elastic Net, and 0.0 indicates Ridge.\n\n\n\n\n\nFigure 4 shows that the MSPE drops faster for Lasso and Elastic Net compared to Ridge regressions as penalty factor increases. However, for higher penalty factors, the MSPE for Ridge regressions dips below the others, which both slightly increase again above a certain threshold. Recall that the larger the regularization, the more restricted the model becomes. The best performing model yields a penalty parameter (alpha) of 0.0043 and a mixture factor (\\(\\rho\\)) of 0.5.\n\n\nFull workflow\nOur starting point was the question: Which factors determine industry returns? While Avramov et al. (2023) provide a Bayesian analysis related to the research question above, we choose a simplified approach: To illustrate the entire workflow, we now run the penalized regressions for all ten industries. We want to identify relevant variables by fitting Lasso models for each industry returns time series. More specifically, we perform cross-validation for each industry to identify the optimal penalty factor \\(\\lambda\\).\nFirst, we define the Lasso model with one tuning parameter.\n\nlm_model = Lasso(fit_intercept=False, max_iter=5000)\n\nparams = {\"regressor__alpha\": alphas}\n\nlm_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", lm_model)\n])\n\nThe following task can be easily parallelized to reduce computing time, but we use a simple loop for ease of exposition.\n\nall_industries = data[\"industry\"].drop_duplicates()\n\nresults = []\nfor industry in all_industries:\n  print(industry)\n  finder = GridSearchCV(\n    lm_pipeline,\n    param_grid=params,\n    scoring=\"neg_mean_squared_error\",\n    cv=data_folds\n  )\n\n  finder = finder.fit(\n    data.query(\"industry == @industry\"),\n    data.query(\"industry == @industry\").get(\"ret_excess\")\n  )\n  results.append(\n    pd.DataFrame(finder.best_estimator_.named_steps.regressor.coef_ != 0)\n  )\n\nselected_factors = (\n  pd.DataFrame(\n    lm_pipeline[:-1].get_feature_names_out(),\n    columns=[\"variable\"]\n  )\n  .assign(variable = lambda x: (\n    x[\"variable\"].str.replace(\"factor_|ff_|q_|macro_\",\"\"))\n  )\n  .assign(**dict(zip(all_industries, results)))\n  .melt(id_vars=\"variable\", var_name =\"industry\")\n  .query(\"value == True\")\n)\n\nWhat has just happened? In principle, exactly the same as before but instead of computing the Lasso coefficients for one industry, we did it for ten sequentially. Now, we just have to do some housekeeping and keep only variables that Lasso does not set to zero. We illustrate the results in a heat map in Figure 5.\n\nselected_factors_figure = (\n  ggplot(\n    selected_factors, \n    aes(x=\"variable\", y=\"industry\")\n  )\n  + geom_tile()\n  + labs(x=\"\", y=\"\", title=\"Selected variables for different industries\")\n  + coord_flip()\n  + scale_x_discrete(limits=reversed)\n  + theme(axis_text_x=element_text(rotation=70, hjust=1), figure_size=(6.4, 6.4))\n)\nselected_factors_figure.show()\n\n\n\n\n\n\n\nFigure 5: The figure shows selected variables for different industries. Dark areas indicate that the estimated Lasso regression coefficient is not set to zero. White fields show which variables get assigned a value of exactly zero.\n\n\n\n\n\nThe heat map in Figure 5 conveys two main insights: first, we see that many factors, macroeconomic variables, and interaction terms are not relevant for explaining the cross-section of returns across the industry portfolios. In fact, only factor_ff_mkt_excess and its interaction with macro_bm a role for several industries. Second, there seems to be quite some heterogeneity across different industries. While barely any variable is selected by Lasso for Utilities, many factors are selected for, e.g., Durable and Manufacturing, but the selected factors do not necessarily coincide. In other words, there seems to be a clear picture that we do not need many factors, but Lasso does not provide a factor that consistently provides pricing abilities across industries.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#key-takeaways",
    "href": "python/factor-selection-via-machine-learning.html#key-takeaways",
    "title": "Factor Selection via Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe scikit-learn framework in Python enables a clean, modular workflow for pre-processing financial data, fitting models, and tuning hyperparameters through cross-validation.\nLasso regression is especially useful in high-dimensional settings, as it performs automatic variable selection by setting irrelevant coefficients to zero, offering insights into which factors truly matter.\nApplying these methods to real-world data shows that only a few factors consistently explain industry portfolio returns, and the relevant predictors vary across industries.\nThe analysis demonstrates practical tools to handle overfitting, model complexity, and interpretability in empirical asset pricing.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#exercises",
    "href": "python/factor-selection-via-machine-learning.html#exercises",
    "title": "Factor Selection via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and lambda and then returns the Ridge estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_2\\) penalty.\nCompute the \\(L_2\\) norm (\\(\\beta'\\beta\\)) for the regression coefficients based on the predictive regression from the previous exercise for a range of \\(\\lambda\\)’s and illustrate the effect of penalization in a suitable figure.\nNow, write a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and \\(\\lambda\\) and then returns the Lasso estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_1\\) penalty.\nAfter you understand what Ridge and Lasso regressions are doing, familiarize yourself with the glmnet package’s documentation. It is a thoroughly tested and well-established package that provides efficient code to compute the penalized regression coefficients for Ridge and Lasso and for combinations, commonly called Elastic Nets.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html",
    "href": "python/value-and-bivariate-sorts.html",
    "title": "Value and Bivariate Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we extend the univariate portfolio analysis of Univariate Portfolio Sorts to bivariate sorts, which means we assign stocks to portfolios based on two characteristics. Bivariate sorts are regularly used in the academic asset pricing literature and are the basis for factors in the Fama-French three-factor model. However, some scholars also use sorts with three grouping variables. Conceptually, portfolio sorts are easily applicable in higher dimensions.\nWe form portfolios on firm size and the book-to-market ratio. To calculate book-to-market ratios, accounting data is required, which necessitates additional steps during portfolio formation. In the end, we demonstrate how to form portfolios on two sorting variables using so-called independent and dependent portfolio sorts.\nThe current chapter relies on this set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport sqlite3\nCompared to previous chapters, we introduce the datetime module that is part of the Python standard library for manipulating dates.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#data-preparation",
    "href": "python/value-and-bivariate-sorts.html#data-preparation",
    "title": "Value and Bivariate Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we load the necessary data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. We conduct portfolio sorts based on the CRSP sample but keep only the necessary columns in our memory. We use the same data sources for firm size as in Size Sorts and P-Hacking.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=(\"SELECT permno, gvkey, date, ret_excess, mktcap, \" \n         \"mktcap_lag, exchange FROM crsp_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .dropna()\n)\n\nFurther, we utilize accounting data. The most common source of accounting data is Compustat. We only need book equity data in this application, which we select from our database. Additionally, we convert the variable datadate to its monthly value, as we only consider monthly returns here and do not need to account for the exact date.\n\nbook_equity = (pd.read_sql_query(\n    sql=\"SELECT gvkey, datadate, be FROM compustat\",\n    con=tidy_finance, \n    parse_dates={\"datadate\"})\n  .dropna()\n  .assign(\n    date=lambda x: (\n      pd.to_datetime(x[\"datadate\"]).dt.to_period(\"M\").dt.to_timestamp()\n    )\n  )\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#book-to-market-ratio",
    "href": "python/value-and-bivariate-sorts.html#book-to-market-ratio",
    "title": "Value and Bivariate Sorts",
    "section": "Book-to-Market Ratio",
    "text": "Book-to-Market Ratio\nA fundamental problem in handling accounting data is the look-ahead bias; we must not include data in forming a portfolio that was not available knowledge at the time. Of course, researchers have more information when looking into the past than agents actually had at that moment. However, abnormal excess returns from a trading strategy should not rely on an information advantage because the differential cannot be the result of informed agents’ trades. Hence, we have to lag accounting information.\nAs in the previous chapter, we continue to lag firm size by one month. Then, we compute the book-to-market ratio, which relates a firm’s book equity to its market equity. Firms with high (low) book-to-market ratio are called value (growth) firms. After matching the accounting and market equity information from the same month, we lag book-to-market by six months. This is a sufficiently conservative approach because accounting information is usually released well before six months pass. However, in the asset pricing literature, even longer lags are used as well.1\nHaving both variables, i.e., firm size lagged by one month and book-to-market lagged by six months, we merge these sorting variables to our returns using the sorting_date-column created for this purpose. The final step in our data preparation deals with differences in the frequency of our variables. Returns and firm size are recorded monthly. Yet, the accounting information is only released on an annual basis. Hence, we only match book-to-market to one month per year and have eleven empty observations. To solve this frequency issue, we carry the latest book-to-market ratio of each firm to the subsequent months, i.e., we fill the missing observations with the most current report. This is done via the fillna(method=\"ffill\")-function after sorting by date and firm (which we identify by permno and gvkey) and on a firm basis (which we do by .groupby() as usual). We filter out all observations with accounting data that is older than a year. As the last step, we remove all rows with missing entries because the returns cannot be matched to any annual report.\n\nsize = (crsp_monthly\n  .assign(sorting_date=lambda x: x[\"date\"]+pd.DateOffset(months=1))\n  .rename(columns={\"mktcap\": \"size\"})\n  .get([\"permno\", \"sorting_date\", \"size\"])\n)\n\nbm = (book_equity\n  .merge(crsp_monthly, how=\"inner\", on=[\"gvkey\", \"date\"])\n  .assign(bm=lambda x: x[\"be\"]/x[\"mktcap\"],\n          sorting_date=lambda x: x[\"date\"]+pd.DateOffset(months=6))\n  .assign(accounting_date=lambda x: x[\"sorting_date\"])\n  .get([\"permno\", \"gvkey\", \"sorting_date\", \"accounting_date\", \"bm\"])\n)\n\ndata_for_sorts = (crsp_monthly\n  .merge(bm, \n         how=\"left\", \n         left_on=[\"permno\", \"gvkey\", \"date\"], \n         right_on=[\"permno\", \"gvkey\", \"sorting_date\"])\n  .merge(size, \n         how=\"left\", \n         left_on=[\"permno\", \"date\"], \n         right_on=[\"permno\", \"sorting_date\"])\n  .get([\"permno\", \"gvkey\", \"date\", \"ret_excess\", \n        \"mktcap_lag\", \"size\", \"bm\", \"exchange\", \"accounting_date\"])\n)\n\ndata_for_sorts = (data_for_sorts\n  .sort_values(by=[\"permno\", \"gvkey\", \"date\"])\n  .groupby([\"permno\", \"gvkey\"])\n  .apply(lambda x: x.assign(\n      bm=x[\"bm\"].fillna(method=\"ffill\"), \n      accounting_date=x[\"accounting_date\"].fillna(method=\"ffill\")\n    )\n  )\n  .reset_index(drop=True)\n  .assign(threshold_date = lambda x: (x[\"date\"]-pd.DateOffset(months=12)))\n  .query(\"accounting_date &gt; threshold_date\")\n  .drop(columns=[\"accounting_date\", \"threshold_date\"])\n  .dropna()\n)\n\nThe last step of preparation for the portfolio sorts is the computation of breakpoints. We continue to use the same function, allowing for the specification of exchanges to be used for the breakpoints. Additionally, we reintroduce the argument sorting_variable into the function for defining different sorting variables.\n\ndef assign_portfolio(data, exchanges, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolio for a given sorting variable.\"\"\"\n    \n    breakpoints = (data\n      .query(f\"exchange in {exchanges}\")\n      .get(sorting_variable)\n      .quantile(np.linspace(0, 1, num=n_portfolios+1), \n                interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    breakpoints.iloc[0] = -np.Inf\n    breakpoints.iloc[breakpoints.size-1] = np.Inf\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nNote that the tidyfinance package also provides an assing_portfolio() function, albeit with more flexibility. For ease of exposition, we continue to use the function that we just defined.\nAfter these data preparation steps, we present bivariate portfolio sorts on an independent and dependent basis.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#independent-sorts",
    "href": "python/value-and-bivariate-sorts.html#independent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Independent Sorts",
    "text": "Independent Sorts\nBivariate sorts create portfolios within a two-dimensional space spanned by two sorting variables. It is then possible to assess the return impact of either sorting variable by the return differential from a trading strategy that invests in the portfolios at either end of the respective variables spectrum. We create a five-by-five matrix using book-to-market and firm size as sorting variables in our example below. We end up with 25 portfolios. Since we are interested in the value premium (i.e., the return differential between high and low book-to-market firms), we go long the five portfolios of the highest book-to-market firms and short the five portfolios of the lowest book-to-market firms. The five portfolios at each end are due to the size splits we employed alongside the book-to-market splits.\nTo implement the independent bivariate portfolio sort, we assign monthly portfolios for each of our sorting variables separately to create the variables portfolio_bm and portfolio_size, respectively. Then, these separate portfolios are combined to the final sort stored in portfolio_combined. After assigning the portfolios, we compute the average return within each portfolio for each month. Additionally, we keep the book-to-market portfolio as it makes the computation of the value premium easier. The alternative would be to disaggregate the combined portfolio in a separate step. Notice that we weigh the stocks within each portfolio by their market capitalization, i.e., we decide to value-weight our returns.\n\nvalue_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio_bm=assign_portfolio(\n        data=x, sorting_variable=\"bm\", n_portfolios=5, exchanges=[\"NYSE\"]\n      ),\n      portfolio_size=assign_portfolio(\n        data=x, sorting_variable=\"size\", n_portfolios=5, exchanges=[\"NYSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"date\", \"portfolio_bm\", \"portfolio_size\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nEquipped with our monthly portfolio returns, we are ready to compute the value premium. However, we still have to decide how to invest in the five high and the five low book-to-market portfolios. The most common approach is to weigh these portfolios equally, but this is yet another researcher’s choice. Then, we compute the return differential between the high and low book-to-market portfolios and show the average value premium.\n\nvalue_premium = (value_portfolios\n  .groupby([\"date\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"value_premium\": (\n        x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() - \n          x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean()\n      )\n    })\n  )\n  .aggregate({\"value_premium\": \"mean\"})\n)\n\nThe resulting monthly value premium is 0.41 percent with an annualized return of 5.0 percent.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#dependent-sorts",
    "href": "python/value-and-bivariate-sorts.html#dependent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Dependent Sorts",
    "text": "Dependent Sorts\nIn the previous exercise, we assigned the portfolios without considering the second variable in the assignment. This protocol is called independent portfolio sorts. The alternative, i.e., dependent sorts, creates portfolios for the second sorting variable within each bucket of the first sorting variable. In our example below, we sort firms into five size buckets, and within each of those buckets, we assign firms to five book-to-market portfolios. Hence, we have monthly breakpoints that are specific to each size group. The decision between independent and dependent portfolio sorts is another choice for the researcher. Notice that dependent sorts guarantee that portfolios have roughly equal numbers of stocks when breakpoints are computed from all exchanges. However, if breakpoints are based only on NYSE stocks, portfolio counts will generally be uneven — reflecting the large presence of small-cap stocks on NASDAQ and AMEX (see Exercise below).\nTo implement the dependent sorts, we first create the size portfolios by calling assign_portfolio() with sorting_variable=\"me\". Then, we group our data again by month and by the size portfolio before assigning the book-to-market portfolio. The rest of the implementation is the same as before. Finally, we compute the value premium.\n\nvalue_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio_size=assign_portfolio(\n        data=x, sorting_variable=\"size\", n_portfolios=5, exchanges=[\"NYSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"date\", \"portfolio_size\"])\n  .apply(lambda x: x.assign(\n      portfolio_bm=assign_portfolio(\n        data=x, sorting_variable=\"bm\", n_portfolios=5, exchanges=[\"NYSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"date\", \"portfolio_bm\", \"portfolio_size\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nvalue_premium = (value_portfolios\n  .groupby([\"date\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"value_premium\": (\n        x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() -\n          x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean()\n      )\n    })\n  )\n  .aggregate({\"value_premium\": \"mean\"})\n)\n\nThe monthly value premium from dependent sorts is 0.35 percent, which translates to an annualized premium of 4.2 percent per year.\nOverall, we show how to conduct bivariate portfolio sorts in this chapter. In one case, we sort the portfolios independently of each other. Yet we also discuss how to create dependent portfolio sorts. Along the lines of Size Sorts and P-Hacking, we see how many choices a researcher has to make to implement portfolio sorts, and bivariate sorts increase the number of choices.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#key-takeaways",
    "href": "python/value-and-bivariate-sorts.html#key-takeaways",
    "title": "Value and Bivariate Sorts",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nBivariate portfolio sorts assign stocks based on two characteristics, such as firm size and book-to-market ratio, to better capture return patterns in asset pricing.\nIndependent sorts treat each variable separately, while dependent sorts condition the second sort on the first.\nProper handling of accounting data, especially lagging the book-to-market ratio, is essential to avoid look-ahead bias and ensure valid backtesting.\nValue premiums are derived by comparing returns of high versus low book-to-market portfolios, with results sensitive to sorting choices and weighting schemes.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#exercises",
    "href": "python/value-and-bivariate-sorts.html#exercises",
    "title": "Value and Bivariate Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nCalculate the number of stocks in each size–value portfolio under two scenarios: (i) breakpoints based on all exchanges (NYSE, AMEX, NASDAQ) and (ii) breakpoints based on NYSE stocks only. Compare the portfolio counts between the two methods and explain the differences.\nIn Size Sorts and P-Hacking, we examine the distribution of market equity. Repeat this analysis for book equity and the book-to-market ratio (alongside a plot of the breakpoints, i.e., deciles).\nWhen we investigate the portfolios, we focus on the returns exclusively. However, it is also of interest to understand the characteristics of the portfolios. Write a function to compute the average characteristics for size and book-to-market across the 25 independently and dependently sorted portfolios.\nAs for the size premium, also the value premium constructed here does not follow Fama and French (1993). Implement a p-hacking setup as in Size Sorts and P-Hacking to find a premium that comes closest to their HML premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#footnotes",
    "href": "python/value-and-bivariate-sorts.html#footnotes",
    "title": "Value and Bivariate Sorts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe definition of a time lag is another choice a researcher has to make, similar to breakpoint choices as we describe in Size Sorts and P-Hacking.↩︎",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html",
    "href": "python/difference-in-differences.html",
    "title": "Difference in Differences",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we illustrate the concept of difference in differences (DiD) estimators by evaluating the effects of climate change regulation on the pricing of bonds across firms. DiD estimators are typically used to recover the treatment effects of natural or quasi-natural experiments that trigger sharp changes in the environment of a specific group. Instead of looking at differences in just one group (e.g., the effect in the treated group), DiD investigates the treatment effects by looking at the difference between differences in two groups. Such experiments are usually exploited to address endogeneity concerns (e.g., Roberts and Whited 2013). The identifying assumption is that the outcome variable would change equally in both groups without the treatment. This assumption is also often referred to as the assumption of parallel trends. Moreover, we would ideally also want a random assignment to the treatment and control groups. Due to lobbying or other activities, this randomness is often violated in (financial) economics.\nIn the context of our setting, we investigate the impact of the Paris Agreement (PA), signed on December 12, 2015, on the bond yields of polluting firms. We first estimate the treatment effect of the agreement using panel regression techniques that we discuss in Fixed Effects and Clustered Standard Errors. We then present two methods to illustrate the treatment effect over time graphically. Although we demonstrate that the treatment effect of the agreement is anticipated by bond market participants well in advance, the techniques we present below can also be applied to many other settings.\nThe approach we use here replicates the results of Seltzer, Starks, and Zhu (2022) partly. Specifically, we borrow their industry definitions for grouping firms into green and brown types. Overall, the literature on environmental, social, and governance (ESG) effects in corporate bond markets is already large but continues to grow (for recent examples, see, e.g., Halling, Yu, and Zechner (2021), Handler, Jankowitsch, and Pasler (2022), Huynh and Xia (2021), among many others).\nThe current chapter relies on this set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport pyfixest as pf\n\nfrom plotnine import *\nfrom scipy.stats import norm\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\nCompared to previous chapters, we introduce the scipy.stats module from the scipy (Virtanen et al. 2020) for simple retrieval of quantiles of the standard normal distribution.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#data-preparation",
    "href": "python/difference-in-differences.html#data-preparation",
    "title": "Difference in Differences",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use TRACE and Mergent FISD as data sources from our SQLite database introduced in Accessing and Managing Financial Data and TRACE and FISD. \n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfisd = (pd.read_sql_query(\n    sql=\"SELECT complete_cusip, maturity, offering_amt, sic_code FROM fisd\",\n    con=tidy_finance,\n    parse_dates={\"maturity\"})\n  .dropna()\n)\n\ntrace_enhanced = (pd.read_sql_query(\n    sql=(\"SELECT cusip_id, trd_exctn_dt, rptd_pr, entrd_vol_qt, yld_pt \" \n         \"FROM trace_enhanced\"),\n    con=tidy_finance,\n    parse_dates={\"trd_exctn_dt\"})\n  .dropna()\n)\n\nWe start our analysis by preparing the sample of bonds. We only consider bonds with a time to maturity of more than one year to the signing of the PA, so that we have sufficient data to analyze the yield behavior after the treatment date. This restriction also excludes all bonds issued after the agreement. We also consider only the first two digits of the SIC industry code to identify the polluting industries (in line with Seltzer, Starks, and Zhu 2022).\n\ntreatment_date = pd.to_datetime(\"2015-12-12\")\npolluting_industries = [\n  49, 13, 45, 29, 28, 33, 40, 20, 26, 42, 10, 53, 32, 99, 37\n]\n\nbonds = (fisd\n  .query(\"offering_amt &gt; 0 & sic_code != 'None'\")\n  .assign(\n    time_to_maturity=lambda x: (x[\"maturity\"]-treatment_date).dt.days / 365,\n    sic_code=lambda x: x[\"sic_code\"].astype(str).str[:2].astype(int),\n    log_offering_amt=lambda x: np.log(x[\"offering_amt\"])\n  )\n  .query(\"time_to_maturity &gt;= 1\")\n  .rename(columns={\"complete_cusip\": \"cusip_id\"})\n  .get([\"cusip_id\", \"time_to_maturity\", \"log_offering_amt\", \"sic_code\"])\n  .assign(polluter=lambda x: x[\"sic_code\"].isin(polluting_industries))\n  .reset_index(drop=True)\n)\n\nNext, we aggregate the individual transactions as reported in TRACE to a monthly panel of bond yields. We consider bond yields for a bond’s last trading day in a month. Therefore, we first aggregate bond data to daily frequency and apply common restrictions from the literature (see, e.g., Bessembinder et al. 2008). We weigh each transaction by volume to reflect a trade’s relative importance and avoid emphasizing small trades. Moreover, we only consider transactions with reported prices rptd_pr larger than 25 (to exclude bonds that are close to default) and only bond-day observations with more than five trades on a corresponding day (to exclude prices based on too few, potentially non-representative transactions). \n\ntrace_enhanced = (trace_enhanced\n  .query(\"rptd_pr &gt; 25\")\n  .assign(weight=lambda x: x[\"entrd_vol_qt\"]*x[\"rptd_pr\"])\n  .assign(weighted_yield=lambda x: x[\"weight\"]*x[\"yld_pt\"])\n)\n\ntrace_aggregated = (trace_enhanced\n  .groupby([\"cusip_id\", \"trd_exctn_dt\"])\n  .aggregate(\n    weighted_yield_sum=(\"weighted_yield\", \"sum\"),\n    weight_sum=(\"weight\", \"sum\"),\n    trades=(\"rptd_pr\", \"count\")\n  )\n  .reset_index()\n  .assign(avg_yield=lambda x: x[\"weighted_yield_sum\"]/x[\"weight_sum\"])\n  .dropna(subset=[\"avg_yield\"])\n  .query(\"trades &gt;= 5\")\n  .assign(trd_exctn_dt=lambda x: pd.to_datetime(x[\"trd_exctn_dt\"]))\n  .assign(date=lambda x: x[\"trd_exctn_dt\"]-pd.offsets.MonthBegin())\n)\n\ndate_index = (trace_aggregated\n  .groupby([\"cusip_id\", \"date\"])[\"trd_exctn_dt\"]\n  .idxmax()\n)\n\ntrace_aggregated = (trace_aggregated\n  .loc[date_index]\n  .get([\"cusip_id\", \"date\", \"avg_yield\"])\n)\n\nBy combining the bond-specific information from Mergent FISD for our bond sample with the aggregated TRACE data, we arrive at the main sample for our analysis.\n\nbonds_panel = (bonds\n  .merge(trace_aggregated, how=\"inner\", on=\"cusip_id\")\n  .dropna()\n)\n\nBefore we can run the first regression, we need to define the treated indicator,1 which is the product of the post_period (i.e., all months after the signing of the PA) and the polluter indicator defined above.\n\nbonds_panel = (bonds_panel\n  .assign(\n    post_period=lambda x: (\n      x[\"date\"] &gt;= (treatment_date-pd.offsets.MonthBegin())\n    )\n  )\n  .assign(treated=lambda x: x[\"polluter\"] & x[\"post_period\"])\n  .assign(date_cat=lambda x: pd.Categorical(x[\"date\"], ordered=True))\n)\n\nAs usual, we tabulate summary statistics of the variables that enter the regression to check the validity of our variable definitions.\n\nbonds_panel_summary = (bonds_panel\n  .melt(var_name=\"measure\",\n        value_vars=[\"avg_yield\", \"time_to_maturity\", \"log_offering_amt\"])\n  .groupby(\"measure\")\n  .describe(percentiles=[0.05, 0.5, 0.95])\n)\nnp.round(bonds_panel_summary, 2)\n\n\n\n\n\n\n\n\nvalue\n\n\n\ncount\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nmeasure\n\n\n\n\n\n\n\n\n\n\n\n\navg_yield\n127539.0\n4.08\n4.21\n0.06\n1.27\n3.38\n8.11\n127.97\n\n\nlog_offering_amt\n127539.0\n13.27\n0.82\n4.64\n12.21\n13.22\n14.51\n16.52\n\n\ntime_to_maturity\n127539.0\n8.55\n8.42\n1.01\n1.50\n5.81\n27.41\n100.70",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#panel-regressions",
    "href": "python/difference-in-differences.html#panel-regressions",
    "title": "Difference in Differences",
    "section": "Panel Regressions",
    "text": "Panel Regressions\nThe PA is a legally binding international treaty on climate change. It was adopted by 196 parties at COP 21 in Paris on December 12, 2015 and entered into force on November 4, 2016. The PA obliges developed countries to support efforts to build clean, climate-resilient futures. One may thus hypothesize that adopting climate-related policies may affect financial markets. To measure the magnitude of this effect, we first run an ordinary least square (OLS) regression without fixed effects where we include the treated, post_period, and polluter dummies, as well as the bond-specific characteristics log_offering_amt and time_to_maturity. This simple model assumes that there are essentially two periods (before and after the PA) and two groups (polluters and non-polluters). Nonetheless, it should indicate whether polluters have higher yields following the PA compared to non-polluters.\nThe second model follows the typical DiD regression approach by including individual (cusip_id) and time (date) fixed effects. In this model, we do not include any other variables from the simple model because the fixed effects subsume them, and we observe the coefficient of our main variable of interest: treated.\n\nmodel_without_fe = pf.feols(\n  \"avg_yield ~ treated + post_period + polluter + log_offering_amt + time_to_maturity\",\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\nmodel_with_fe = pf.feols(\n  \"avg_yield ~ treated | cusip_id + date\",\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\npf.etable([model_without_fe, model_with_fe], coef_fmt = \"b (t)\")\n\n                                 est1               est2\n----------------  -------------------  -----------------\ndepvar                      avg_yield          avg_yield\n--------------------------------------------------------\nIntercept          10.733*** (57.057)\ntreated              0.453*** (9.135)  0.975*** (30.121)\npost_period        -0.178*** (-6.040)\npolluter            0.486*** (15.426)\nlog_offering_amt  -0.550*** (-38.992)\ntime_to_maturity    0.058*** (41.526)\n--------------------------------------------------------\ndate                                -                  x\ncusip_id                            -                  x\n--------------------------------------------------------\nR2                              0.032              0.648\nS.E. type                         iid                iid\nObservations                   127539             127539\n--------------------------------------------------------\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nFormat of coefficient cell:\nCoefficient (t-stats)\n\n\nBoth models indicate that polluters have significantly higher yields after the PA than non-polluting firms. Note that the magnitude of the treated coefficient varies considerably across models.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#visualizing-parallel-trends",
    "href": "python/difference-in-differences.html#visualizing-parallel-trends",
    "title": "Difference in Differences",
    "section": "Visualizing Parallel Trends",
    "text": "Visualizing Parallel Trends\nEven though the regressions above indicate that there is an impact of the PA on bond yields of polluters, the tables do not tell us anything about the dynamics of the treatment effect. In particular, the models provide no indication about whether the crucial parallel trends assumption is valid. This assumption requires that in the absence of treatment, the difference between the two groups is constant over time. Although there is no well-defined statistical test for this assumption, visual inspection typically provides a good indication.\nTo provide such visual evidence, we revisit the simple OLS model and replace the treated and post_period indicators with month dummies for each group. This approach estimates the average yield change of both groups for each period and provides corresponding confidence intervals. Plotting the coefficient estimates for both groups around the treatment date shows us the dynamics of our panel data.\n\nmodel_without_fe_time = pf.feols(\n  \"avg_yield ~ polluter + date*polluter + time_to_maturity + log_offering_amt\",\n  vcov = \"iid\",\n  data = bonds_panel.assign(date = lambda x: x[\"date\"].astype(str))\n)\n\nmodel_without_fe_coefs = (pd.DataFrame({\n    \"estimate\": model_without_fe_time.coef(),\n    \"std_error\": model_without_fe_time.se()\n  })\n  .reset_index(names=\"term\")\n  .query(\"term.str.contains('date')\")\n  .assign(\n    treatment=lambda x: x[\"term\"].str.contains(\":polluter\"),\n    date=lambda x: x[\"term\"].str.extract(r\"date\\[T\\.(\\d{4}-\\d{2}-\\d{2})\\]\"),\n    ci_up=lambda x: x[\"estimate\"]+norm.ppf(0.975)*x[\"std_error\"],\n    ci_low=lambda x: x[\"estimate\"]+norm.ppf(0.025)*x[\"std_error\"]\n  )\n)\n\nmodel_without_fe_figure = (\n  ggplot(\n    model_without_fe_coefs, \n    aes(x=\"date\", y=\"estimate\", color=\"treatment\",\n        linetype=\"treatment\", shape=\"treatment\")\n  )\n  + geom_vline(xintercept=pd.to_datetime(treatment_date) -\n               pd.offsets.MonthBegin(), linetype=\"dashed\")\n  + geom_hline(yintercept=0, linetype=\"dashed\")\n  + geom_errorbar(aes(ymin=\"ci_low\", ymax=\"ci_up\"), alpha=0.5)\n  + geom_point()\n  + guides(linetype=None)\n  + labs(\n      x=\"\", y=\"Yield\", shape=\"Polluter?\", color=\"Polluter?\",\n      title=\"Polluters respond stronger than green firms\"\n    )\n  + scale_linetype_manual(values=[\"solid\", \"dashed\"])\n  + scale_x_datetime(breaks=date_breaks(\"1 year\"), labels=date_format(\"%Y\")) \n)\nmodel_without_fe_figure.show()\n\n\n\n\n\n\n\nFigure 1: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters and non-polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\n\nFigure 1 shows that throughout most of 2014, the yields of the two groups changed in unison. However, starting at the end of 2014, the yields start to diverge, reaching the highest difference around the signing of the PA. Afterward, the yields for both groups fall again, and the polluters arrive at the same level as at the beginning of 2014. The non-polluters, on the other hand, even experience significantly lower yields than polluters after the signing of the agreement.\nInstead of plotting both groups using the simple model approach, we can also use the fixed-effects model and focus on the polluter’s yield response to the signing relative to the non-polluters. To perform this estimation, we need to replace the treated indicator with separate time dummies for the polluters, each marking a one-month period relative to the treatment date.\n\nbonds_panel_alt = (bonds_panel\n  .assign(\n    diff_to_treatment=lambda x: (\n      np.round(\n        ((x[\"date\"]-(treatment_date- \n            pd.offsets.MonthBegin())).dt.days/365)*12, 0\n      ).astype(int)\n    )\n  )\n)\n\nvariables = (bonds_panel_alt\n  .get([\"diff_to_treatment\", \"date\"])\n  .drop_duplicates()\n  .sort_values(\"date\")\n  .copy()\n  .assign(variable_name=np.nan)\n  .reset_index(drop=True)\n)\n\nIn the next code chunk, we assemble the model formula and regress the monthly yields on the set of time dummies and cusip_id and date fixed effects.\n\nformula = \"avg_yield ~ 1 + \"\n\nfor j in range(variables.shape[0]):\n    if variables[\"diff_to_treatment\"].iloc[j] != 0:\n        old_names=list(bonds_panel_alt.columns)\n        \n        bonds_panel_alt[\"new_var\"] = (\n          bonds_panel_alt[\"diff_to_treatment\"] == \n            variables[\"diff_to_treatment\"].iloc[j]\n        ) & bonds_panel_alt[\"polluter\"]\n        \n        diff_to_treatment_value=variables[\"diff_to_treatment\"].iloc[j]\n        direction=\"lag\" if diff_to_treatment_value &lt; 0 else \"lead\"\n        abs_diff_to_treatment=int(abs(diff_to_treatment_value))\n        new_var_name=f\"{direction}{abs_diff_to_treatment}\"\n        variables.at[j, \"variable_name\"]=new_var_name\n        bonds_panel_alt[new_var_name]=bonds_panel_alt[\"new_var\"]\n        formula += (f\" + {new_var_name}\" if j &gt; 0 else new_var_name)\n\nformula = formula + \" | cusip_id + date \"\n\nmodel_with_fe_time = pf.feols(\n  formula,\n  vcov = \"iid\",\n  data = bonds_panel_alt\n)\n\nWe then collect the regression results into a dataframe that contains the estimates and corresponding 95 percent confidence intervals. Note that we also add a row with zeros for the (omitted) reference point of the time dummies.\n\nlag0_row = pd.DataFrame({\n  \"term\": [\"lag0\"],\n  \"estimate\": [0],\n  \"std_error\": [0],\n  \"ci_up\": [0],\n  \"ci_low\": [0],\n  \"diff_to_treatment\": [0],\n  \"date\": [treatment_date - pd.offsets.MonthBegin()]\n})\n\nmodel_with_fe_time_coefs = (pd.DataFrame({\n    \"estimate\": model_with_fe_time.coef(),\n    \"std_error\": model_with_fe_time.se()\n  })\n  .reset_index(names=\"term\")\n  .assign(\n    ci_up=lambda x: x[\"estimate\"]+norm.ppf(0.975)*x[\"std_error\"],\n    ci_low=lambda x: x[\"estimate\"]+norm.ppf(0.025)*x[\"std_error\"]\n  )\n  .merge(variables, how=\"left\", left_on=\"term\", right_on=\"variable_name\")\n  .drop(columns=\"variable_name\")\n)\n\nmodel_with_fe_time_coefs = pd.concat(\n  [model_with_fe_time_coefs, lag0_row], \n  ignore_index=True\n)\n\nFigure 2 shows the resulting coefficient estimates.\n\nmodel_with_fe_time_figure = (\n  ggplot(\n    model_with_fe_time_coefs,\n    aes(x=\"date\", y=\"estimate\")\n  )\n  + geom_vline(aes(xintercept=treatment_date - pd.offsets.MonthBegin()), \n                   linetype=\"dashed\")\n  + geom_hline(aes(yintercept=0), linetype=\"dashed\")\n  + geom_errorbar(aes(ymin=\"ci_low\", ymax=\"ci_up\"), alpha=0.5)\n  + geom_point(aes(y=\"estimate\"))\n  + labs(\n      x=\"\", y=\"Yield\",\n      title=\"Polluters' yield patterns around Paris Agreement signing\"\n    )\n  + scale_x_datetime(breaks=date_breaks(\"1 year\"), labels=date_format(\"%Y\"))\n)\nmodel_with_fe_time_figure.show()\n\n\n\n\n\n\n\nFigure 2: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\n\n The resulting graph shown in Figure 2 confirms the main conclusion of the previous image: polluters’ yield patterns show a considerable anticipation effect starting toward the end of 2014. Yields only marginally increase after the signing of the agreement. However, as opposed to the simple model, we do not see a complete reversal back to the pre-agreement level. Yields of polluters stay at a significantly higher level even one year after the signing.\nNotice that during the year after the PA was signed, the 45th president of the United States was elected (on November 8, 2016). During his campaign there were some indications of intentions to withdraw the US from the PA, which ultimately happened on November 4, 2020. Hence, reversal effects are potentially driven by these actions.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#key-takeaways",
    "href": "python/difference-in-differences.html#key-takeaways",
    "title": "Difference in Differences",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nDifference-in-differences is a powerful tool for estimating causal effects in financial settings, especially when analyzing the impact of policy changes or shocks.\nIt is important to assess the parallel trends assumption using graphical methods.\nThe pyfixest Python package allows you to implement difference-in-differences regressions and visualize parallel trends.\nBy combining panel data from TRACE and Mergent FISD with fixed effects regressions, you can evaluate how the Paris Agreement influenced corporate bond yields.\nThe application shows that polluting firms experienced significantly higher yields following up to and after the agreement, invalidating the parallel trends assumption.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#exercises",
    "href": "python/difference-in-differences.html#exercises",
    "title": "Difference in Differences",
    "section": "Exercises",
    "text": "Exercises\n\nThe 46th president of the US rejoined the Paris Agreement on February 19, 2021. Repeat the difference in differences analysis for the day of his election victory. Note that you will also have to download new TRACE data. How did polluters’ yields react to this action?\nBased on the exercise on ratings in TRACE and FISD, include ratings as a control variable in the analysis above. Do the results change?",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#footnotes",
    "href": "python/difference-in-differences.html#footnotes",
    "title": "Difference in Differences",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that by using a generic name here, everybody can replace ours with their sample data and run the code to produce standard regression tables and illustrations.↩︎",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/working-with-stock-returns.html",
    "href": "python/working-with-stock-returns.html",
    "title": "Working with Stock Returns",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nThe main aim of this chapter is to familiarize yourself with the core packages for working with stock market data. We focus on downloading and visualizing stock data from data provider Yahoo Finance.\nAt the start of each session, we load the required Python packages. Throughout the entire book, we always use the pandas (McKinney 2010) and numpy (Harris et al. 2020) packages. In this chapter, we also load the tidyfinance package to download stock price data. This package provides a convenient wrapper for various quantitative functions compatible with the core packages and our book.\nYou typically have to install a package once before you can load it into your active Python session. In case you have not done this yet, call, for instance, pip install tidyfinance in your terminal.\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "python/working-with-stock-returns.html#downloading-data",
    "href": "python/working-with-stock-returns.html#downloading-data",
    "title": "Working with Stock Returns",
    "section": "Downloading Data",
    "text": "Downloading Data\nNote that import pandas as pd implies that we can call all pandas functions later with a simple pd.function(). Instead, utilizing from pandas import * is generally discouraged, as it leads to namespace pollution. This statement imports all functions and classes from pandas into your current namespace, potentially causing conflicts with functions you define or those from other imported libraries. Using the pd abbreviation is a very convenient way to prevent this.\nWe first download daily prices for one stock symbol, e.g., the Apple stock (AAPL), directly from the data provider Yahoo Finance. To download the data, you can use the function tf.download_data().\nIn the following code, we request daily data from the beginning of 2000 to the end of the last year, which is a period of more than 20 years.\n\nprices = tf.download_data(\n  domain=\"stock_prices\", \n  symbols=\"AAPL\",\n  start_date=\"2000-01-01\", \n  end_date=\"2023-12-31\"\n)\nprices.head().round(3)\n\n\n\n\n\n\n\n\ndate\nsymbol\nvolume\nopen\nlow\nhigh\nclose\nadjusted_close\n\n\n\n\n0\n2000-01-03\nAAPL\n535796800\n0.936\n0.908\n1.004\n0.999\n0.840\n\n\n1\n2000-01-04\nAAPL\n512377600\n0.967\n0.903\n0.988\n0.915\n0.769\n\n\n2\n2000-01-05\nAAPL\n778321600\n0.926\n0.920\n0.987\n0.929\n0.781\n\n\n3\n2000-01-06\nAAPL\n767972800\n0.948\n0.848\n0.955\n0.848\n0.713\n\n\n4\n2000-01-07\nAAPL\n460734400\n0.862\n0.853\n0.902\n0.888\n0.747\n\n\n\n\n\n\n\n tf.download_data(domain=\"stock_prices\") downloads stock market data from Yahoo Finance. The above code chunk returns a data frame with eight self-explanatory columns: date, symbol, the daily volume (in the number of traded shares), the market prices at the open, low, high, close, and the adjusted_close price in USD. The adjusted prices are corrected for anything that might affect the stock price after the market closes, e.g., stock splits and dividends. These actions affect the quoted prices, but they have no direct impact on the investors who hold the stock. Therefore, we often rely on adjusted prices when it comes to analyzing the returns an investor would have earned by holding the stock continuously.\nNext, we use the plotnine package (Kibirige 2023) to visualize the time series of adjusted prices in Figure 1. This package takes care of visualization tasks based on the principles of the grammar of graphics (Wilkinson 2012). Note that generally, we do not recommend using the * import style. However, we use it here only for the plotting functions, which are distinct to plotnine and have very plotting-related names. So, the risk of misuse through a polluted namespace is marginal.\n\nfrom plotnine import *\n\nCreating figures becomes very intuitive with the Grammar of Graphics, as the following code chunk demonstrates.\n\napple_prices_figure = (\n  ggplot(prices, aes(y=\"adjusted_close\", x=\"date\"))\n  + geom_line()\n  + labs(x=\"\", y=\"\", title=\"Apple stock prices from 2000 to 2023\")\n)\napple_prices_figure.show()\n\n\n\n\n\n\n\nFigure 1: Prices are in USD, adjusted for dividend payments and stock splits.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "python/working-with-stock-returns.html#computing-returns",
    "href": "python/working-with-stock-returns.html#computing-returns",
    "title": "Working with Stock Returns",
    "section": "Computing Returns",
    "text": "Computing Returns\nInstead of analyzing prices, we compute daily returns defined as \\(r_t = p_t / p_{t-1} - 1\\), where \\(p_t\\) is the adjusted price at the end of day \\(t\\). In that context, the function lag() is helpful by returning the previous value.\n\nreturns = (prices\n  .sort_values(\"date\")\n  .assign(ret=lambda x: x[\"adjusted_close\"].pct_change())\n  .get([\"symbol\", \"date\", \"ret\"])\n)\nreturns\n\n\n\n\n\n\n\n\nsymbol\ndate\nret\n\n\n\n\n0\nAAPL\n2000-01-03\nNaN\n\n\n1\nAAPL\n2000-01-04\n-0.084310\n\n\n2\nAAPL\n2000-01-05\n0.014633\n\n\n3\nAAPL\n2000-01-06\n-0.086538\n\n\n4\nAAPL\n2000-01-07\n0.047369\n\n\n...\n...\n...\n...\n\n\n6032\nAAPL\n2023-12-22\n-0.005548\n\n\n6033\nAAPL\n2023-12-26\n-0.002841\n\n\n6034\nAAPL\n2023-12-27\n0.000518\n\n\n6035\nAAPL\n2023-12-28\n0.002226\n\n\n6036\nAAPL\n2023-12-29\n-0.005424\n\n\n\n\n6037 rows × 3 columns\n\n\n\nThe resulting data frame has three columns, the last of which contains the daily returns (ret). Note that the first entry naturally contains a missing value (NaN) because there is no previous price. Obviously, the use of pct_change() would be meaningless if the time series is not ordered by ascending dates. The function sort_values() provides a convenient way to order observations in the correct way for our application. In case you want to order observations by descending dates, you can use the parameter ascending=False.\nFor the upcoming examples, we remove missing values as these would require separate treatment for many applications. For example, missing values can affect sums and averages by reducing the number of valid data points if not properly accounted for. In general, always ensure you understand why NaN values occur and carefully examine if you can simply get rid of these observations.\n\nreturns = returns.dropna() \n\nNext, we visualize the distribution of daily returns in a histogram in Figure 2. Additionally, we draw a dashed line that indicates the historical five percent quantile of the daily returns to the histogram, which is a crude proxy for the worst possible return of the stock with a probability of at most five percent. This quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators. We refer to Tsay (2010) for a more thorough introduction to the stylized facts of financial returns.\n\nfrom mizani.formatters import percent_format\n\nquantile_05 = returns[\"ret\"].quantile(0.05)\n\napple_returns_figure = (\n  ggplot(returns, aes(x=\"ret\"))\n  + geom_histogram(bins=100)\n  + geom_vline(aes(xintercept=quantile_05), linetype=\"dashed\")\n  + labs(x=\"\", y=\"\", title=\"Distribution of daily Apple stock returns\")\n  + scale_x_continuous(labels=percent_format())\n)\napple_returns_figure.show()\n\n\n\n\n\n\n\nFigure 2: The dotted vertical line indicates the historical five percent quantile.\n\n\n\n\n\nHere, bins=100 determines the number of bins used in the illustration and, hence, implicitly sets the width of the bins. Before proceeding, make sure you understand how to use the geom geom_vline() to add a dashed line that indicates the historical five percent quantile of the daily returns. Before proceeding with any data, a typical task is to compute and analyze the summary statistics for the main variables of interest.\n\npd.DataFrame(returns[\"ret\"].describe()).round(3).T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nret\n6036.0\n0.001\n0.025\n-0.519\n-0.01\n0.001\n0.013\n0.139\n\n\n\n\n\n\n\nWe see that the maximum daily return was 13.9 percent. Perhaps not surprisingly, the average daily return is close to but slightly above 0. In line with the illustration above, the large losses on the day with the minimum returns indicate a strong asymmetry in the distribution of returns.\nYou can also compute these summary statistics for each year individually by imposing groupby(returns[\"date\"].dt.year), where the call dt.year returns the year. More specifically, the few lines of code below compute the summary statistics from above for individual groups of data defined by the values of the column year. The summary statistics, therefore, allow an eyeball analysis of the time-series dynamics of the daily return distribution.\n\n(returns[\"ret\"]\n  .groupby(returns[\"date\"].dt.year)\n  .describe()\n  .round(3)\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2000\n251.0\n-0.003\n0.055\n-0.519\n-0.034\n-0.002\n0.027\n0.137\n\n\n2001\n248.0\n0.002\n0.039\n-0.172\n-0.023\n-0.001\n0.027\n0.129\n\n\n2002\n252.0\n-0.001\n0.031\n-0.150\n-0.019\n-0.003\n0.018\n0.085\n\n\n2003\n252.0\n0.002\n0.023\n-0.081\n-0.012\n0.002\n0.015\n0.113\n\n\n2004\n252.0\n0.005\n0.025\n-0.056\n-0.009\n0.003\n0.016\n0.132\n\n\n2005\n252.0\n0.003\n0.024\n-0.092\n-0.010\n0.003\n0.017\n0.091\n\n\n2006\n251.0\n0.001\n0.024\n-0.063\n-0.014\n-0.002\n0.014\n0.118\n\n\n2007\n251.0\n0.004\n0.024\n-0.070\n-0.009\n0.003\n0.018\n0.105\n\n\n2008\n253.0\n-0.003\n0.037\n-0.179\n-0.024\n-0.001\n0.019\n0.139\n\n\n2009\n252.0\n0.004\n0.021\n-0.050\n-0.009\n0.002\n0.015\n0.068\n\n\n2010\n252.0\n0.002\n0.017\n-0.050\n-0.006\n0.002\n0.011\n0.077\n\n\n2011\n252.0\n0.001\n0.017\n-0.056\n-0.009\n0.001\n0.011\n0.059\n\n\n2012\n250.0\n0.001\n0.019\n-0.064\n-0.008\n0.000\n0.012\n0.089\n\n\n2013\n252.0\n0.000\n0.018\n-0.124\n-0.009\n-0.000\n0.011\n0.051\n\n\n2014\n252.0\n0.001\n0.014\n-0.080\n-0.006\n0.001\n0.010\n0.082\n\n\n2015\n252.0\n0.000\n0.017\n-0.061\n-0.009\n-0.001\n0.009\n0.057\n\n\n2016\n252.0\n0.001\n0.015\n-0.066\n-0.006\n0.001\n0.008\n0.065\n\n\n2017\n251.0\n0.002\n0.011\n-0.039\n-0.004\n0.001\n0.007\n0.061\n\n\n2018\n251.0\n-0.000\n0.018\n-0.066\n-0.009\n0.001\n0.009\n0.070\n\n\n2019\n252.0\n0.003\n0.016\n-0.100\n-0.005\n0.003\n0.012\n0.068\n\n\n2020\n253.0\n0.003\n0.029\n-0.129\n-0.010\n0.002\n0.017\n0.120\n\n\n2021\n252.0\n0.001\n0.016\n-0.042\n-0.008\n0.001\n0.012\n0.054\n\n\n2022\n251.0\n-0.001\n0.022\n-0.059\n-0.016\n-0.001\n0.014\n0.089\n\n\n2023\n250.0\n0.002\n0.013\n-0.048\n-0.006\n0.002\n0.009\n0.047",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "python/working-with-stock-returns.html#scaling-up-the-analysis",
    "href": "python/working-with-stock-returns.html#scaling-up-the-analysis",
    "title": "Working with Stock Returns",
    "section": "Scaling Up the Analysis",
    "text": "Scaling Up the Analysis\nAs a next step, we generalize the previous code so that all computations can handle an arbitrary number of symbols (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nThis is where the tidyverse magic starts: Tidy data makes it extremely easy to generalize the computations from before to as many assets or groups as you like. The following code takes any number of symbols, e.g., symbol = [\"AAPL\", \"MMM\", \"BA\"], and automates the download as well as the plot of the price time series. In the end, we create the table of summary statistics for all assets at once. For this example, we analyze data from all current constituents of the Dow Jones Industrial Average index.\nWe first download a table with DOW Jones constituents again using tf.download_data(), but this time with domain=\"constituents\".\n\nsymbols = tf.download_data(\n  domain=\"constituents\", \n  index=\"Dow Jones Industrial Average\"\n)\n\nConveniently, tf.download_data() provides the functionality to get all stock prices from an index for a specific point in time with a single call.\n\nprices_daily = tf.download_data(\n  domain=\"stock_prices\", \n  symbols=symbols[\"symbol\"].tolist(),\n  start_date=\"2000-01-01\", \n  end_date=\"2023-12-31\"\n)\n\nThe resulting data frame contains 177,925 daily observations for 30 different stocks. Figure 3 illustrates the time series of the downloaded adjusted prices for each of the constituents of the Dow index. Make sure you understand every single line of code! What are the arguments of aes()? Which alternative geoms could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes.\n\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\n\nprices_figure = (\n  ggplot(prices_daily, aes(y=\"adjusted_close\", x=\"date\", color=\"symbol\"))\n  + geom_line()\n  + scale_x_datetime(date_breaks=\"5 years\", date_labels=\"%Y\")\n  + labs(x=\"\", y=\"\", color=\"\", title=\"Stock prices of DOW index constituents\")\n  + theme(legend_position=\"none\")\n)\nprices_figure.show()\n\n\n\n\n\n\n\nFigure 3: Prices in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n\nDo you notice the small differences relative to the code we used before? All we needed to do to illustrate all stock symbols simultaneously is to include color = symbol in the ggplot aesthetics. In this way, we generate a separate line for each symbol. Of course, there are simply too many lines on this graph to identify the individual stocks properly, but it illustrates our point of how to generalize a specific analysis to an arbitrage number of subjects quite well.\nThe same holds for stock returns. Before computing the returns, we use groupby(\"symbol\") such that the assign() command is performed for each symbol individually. The same logic also applies to the computation of summary statistics: groupby(\"symbol\") is the key to aggregating the time series into symbol-specific variables of interest.\n\nreturns_daily = (prices_daily\n  .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n  .get([\"symbol\", \"date\", \"ret\"])\n  .dropna(subset=\"ret\")\n)\n\n(returns_daily\n  .groupby(\"symbol\")[\"ret\"]\n  .describe()\n  .round(3)\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\n\n\nAAPL\n6036.0\n0.001\n0.025\n-0.519\n-0.010\n0.001\n0.013\n0.139\n\n\nAMGN\n6036.0\n0.000\n0.019\n-0.134\n-0.009\n0.000\n0.009\n0.151\n\n\nAMZN\n6036.0\n0.001\n0.032\n-0.248\n-0.012\n0.000\n0.014\n0.345\n\n\nAXP\n6036.0\n0.001\n0.023\n-0.176\n-0.009\n0.000\n0.010\n0.219\n\n\nBA\n6036.0\n0.001\n0.022\n-0.238\n-0.010\n0.001\n0.011\n0.243\n\n\nCAT\n6036.0\n0.001\n0.020\n-0.145\n-0.010\n0.001\n0.011\n0.147\n\n\nCRM\n4914.0\n0.001\n0.027\n-0.271\n-0.012\n0.001\n0.014\n0.260\n\n\nCSCO\n6036.0\n0.000\n0.023\n-0.162\n-0.009\n0.000\n0.010\n0.244\n\n\nCVX\n6036.0\n0.001\n0.018\n-0.221\n-0.008\n0.001\n0.009\n0.227\n\n\nDIS\n6036.0\n0.000\n0.019\n-0.184\n-0.009\n0.000\n0.009\n0.160\n\n\nGS\n6036.0\n0.001\n0.023\n-0.190\n-0.010\n0.000\n0.011\n0.265\n\n\nHD\n6036.0\n0.001\n0.019\n-0.287\n-0.008\n0.001\n0.009\n0.141\n\n\nHON\n6036.0\n0.000\n0.019\n-0.174\n-0.008\n0.001\n0.009\n0.282\n\n\nIBM\n6036.0\n0.000\n0.016\n-0.155\n-0.007\n0.000\n0.008\n0.120\n\n\nJNJ\n6036.0\n0.000\n0.012\n-0.158\n-0.005\n0.000\n0.006\n0.122\n\n\nJPM\n6036.0\n0.001\n0.024\n-0.207\n-0.009\n0.000\n0.010\n0.251\n\n\nKO\n6036.0\n0.000\n0.013\n-0.101\n-0.005\n0.000\n0.006\n0.139\n\n\nMCD\n6036.0\n0.001\n0.015\n-0.159\n-0.006\n0.001\n0.007\n0.181\n\n\nMMM\n6036.0\n0.000\n0.015\n-0.129\n-0.007\n0.000\n0.008\n0.126\n\n\nMRK\n6036.0\n0.000\n0.017\n-0.268\n-0.007\n0.000\n0.008\n0.130\n\n\nMSFT\n6036.0\n0.001\n0.019\n-0.156\n-0.008\n0.000\n0.009\n0.196\n\n\nNKE\n6036.0\n0.001\n0.019\n-0.198\n-0.008\n0.001\n0.009\n0.155\n\n\nNVDA\n6036.0\n0.002\n0.038\n-0.352\n-0.016\n0.001\n0.018\n0.424\n\n\nPG\n6036.0\n0.000\n0.013\n-0.302\n-0.005\n0.000\n0.006\n0.120\n\n\nSHW\n6036.0\n0.001\n0.018\n-0.208\n-0.008\n0.001\n0.009\n0.153\n\n\nTRV\n6036.0\n0.001\n0.018\n-0.208\n-0.007\n0.001\n0.008\n0.256\n\n\nUNH\n6036.0\n0.001\n0.020\n-0.186\n-0.008\n0.001\n0.010\n0.348\n\n\nV\n3973.0\n0.001\n0.019\n-0.136\n-0.008\n0.001\n0.009\n0.150\n\n\nVZ\n6036.0\n0.000\n0.015\n-0.118\n-0.007\n0.000\n0.007\n0.146\n\n\nWMT\n6036.0\n0.000\n0.015\n-0.114\n-0.007\n0.000\n0.007\n0.117",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "python/working-with-stock-returns.html#different-frequencies",
    "href": "python/working-with-stock-returns.html#different-frequencies",
    "title": "Working with Stock Returns",
    "section": "Different Frequencies",
    "text": "Different Frequencies\nFinancial data often exists at different frequencies due to varying reporting schedules, trading calendars, and economic data releases. For example, stock prices are typically recorded daily, while macroeconomic indicators such as GDP or inflation are reported monthly or quarterly. Additionally, some datasets are recorded only when transactions occur, resulting in irregular timestamps. To compare data meaningfully, we have to align different frequencies appropriately. For example, to compare returns across different frequencies, we use annualization techniques.\nSo far, we have worked with daily returns, but we can easily convert our data to other frequencies. Let’s create monthly returns from our daily data:\n\nreturns_monthly = (returns_daily\n  .assign(date=returns_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp())\n  .groupby([\"symbol\", \"date\"], as_index=False)\n  .agg(ret=(\"ret\", lambda x: np.prod(1 + x) - 1))\n)\n\nIn this code, we first group the data by symbol and month and then compute monthly returns by compounding the daily returns: \\((1+r_1)(1+r_2)\\ldots(1+r_n)-1\\). To visualize how return characteristics change across different frequencies, we can compare histograms as in Figure 4:\n\napple_daily = (returns_daily\n  .query(\"symbol == 'AAPL'\")\n  .assign(frequency=\"Daily\")\n)\n\napple_monthly = (returns_monthly\n  .query(\"symbol == 'AAPL'\")\n  .assign(frequency=\"Monthly\")\n)\n\napple_returns = pd.concat([apple_daily, apple_monthly], ignore_index=True)\n\napple_returns_figure = (\n  ggplot(apple_returns, aes(x=\"ret\", fill=\"frequency\"))\n  + geom_histogram(position=\"identity\", bins=50)\n  + labs(\n      x=\"\", y=\"\", fill=\"Frequency\",\n      title=\"Distribution of Apple returns across different frequencies\"\n  )\n  + scale_x_continuous(labels=percent_format())\n  + facet_wrap(\"frequency\", scales=\"free\")\n  + theme(legend_position=\"none\")\n)\napple_returns_figure.show()\n\n\n\n\n\n\n\nFigure 4: Returns are based on prices adjusted for dividend payments and stock splits.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "python/working-with-stock-returns.html#other-forms-of-data-aggregation",
    "href": "python/working-with-stock-returns.html#other-forms-of-data-aggregation",
    "title": "Working with Stock Returns",
    "section": "Other Forms of Data Aggregation",
    "text": "Other Forms of Data Aggregation\nOf course, aggregation across variables other than symbol or date can also make sense. For instance, suppose you are interested in answering questions like: Are days with high aggregate trading volume likely followed by days with high aggregate trading volume? To provide some initial analysis on this question, we take the downloaded data and compute aggregate daily trading volume for all Dow index constituents in USD. Recall that the column volume is denoted in the number of traded shares. Thus, we multiply the trading volume with the daily adjusted closing price to get a proxy for the aggregate trading volume in USD. Scaling by 1e-9 (Python can handle scientific notation) denotes daily trading volume in billion USD.\n\ntrading_volume = (prices_daily\n  .assign(trading_volume=lambda x: (x[\"volume\"]*x[\"adjusted_close\"])/1e9)\n  .groupby(\"date\")[\"trading_volume\"]\n  .sum()\n  .reset_index()\n  .assign(trading_volume_lag=lambda x: x[\"trading_volume\"].shift(periods=1))\n)\n\ntrading_volume_figure = (\n  ggplot(trading_volume, aes(x=\"date\", y=\"trading_volume\"))\n  + geom_line()\n  + scale_x_datetime(date_breaks=\"5 years\", date_labels=\"%Y\")\n  + labs(\n      x=\"\", y=\"\",\n      title=\"Aggregate daily trading volume of DOW index constituents in billion USD\"\n    )\n)\ntrading_volume_figure.show()\n\n\n\n\n\n\n\nFigure 5: Total daily trading volume in billion USD.\n\n\n\n\n\nFigure 5 indicates a clear upward trend in aggregated daily trading volume. In particular, since the outbreak of the COVID-19 pandemic, markets have processed substantial trading volumes, as analyzed, for instance, by Goldstein, Koijen, and Mueller (2021). One way to illustrate the persistence of trading volume would be to plot volume on day \\(t\\) against volume on day \\(t-1\\) as in the example below. In Figure 6, we add a dotted 45°-line to indicate a hypothetical one-to-one relation by geom_abline(), addressing potential differences in the axes’ scales.\n\npersistence_figure = (\n  ggplot(trading_volume, aes(x=\"trading_volume_lag\", y=\"trading_volume\"))\n  + geom_point()\n  + geom_abline(aes(intercept=0, slope=1), linetype=\"dashed\")\n  + labs(\n      x=\"Previous day aggregate trading volume\",\n      y=\"Aggregate trading volume\",\n      title=\"Persistence in daily trading volume of DOW constituents in billion USD\"\n    )\n)\npersistence_figure.show()\n\n\n\n\n\n\n\nFigure 6: Total daily trading volume in billion USD.\n\n\n\n\n\nPurely eye-balling reveals that days with high trading volume are often followed by similarly high trading volume days.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "python/working-with-stock-returns.html#key-takeaways",
    "href": "python/working-with-stock-returns.html#key-takeaways",
    "title": "Working with Stock Returns",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nYou can use pandas and numpy in Python to efficiently analyze stock market data using consistent and scalable workflows.\nThe tidyfinance Python package allows seamless downloading of historical stock prices and index data directly from Yahoo Finance.\nTidy data principles enable efficient analysis of financial data.\nAdjusted stock prices provide a more accurate reflection of investor returns by accounting for dividends and stock splits.\nSummary statistics such as mean, standard deviation, and quantiles offer insights into stock return behavior over time.\nVisualizations created with plotnine help identify trends, volatility, and return distributions in financial time series.\nTidy data principles make it easy to scale financial analyses from a single stock to entire indices like the Dow Jones or S&P 500.\nConsistent workflows form the foundation for advanced financial analysis.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "python/working-with-stock-returns.html#exercises",
    "href": "python/working-with-stock-returns.html#exercises",
    "title": "Working with Stock Returns",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily prices for another stock market symbol of your choice from Yahoo Finance using tf.download_data(). Plot two time series of the symbol’s un-adjusted and adjusted closing prices. Explain any visible differences.\nCompute daily net returns for an asset of your choice and visualize the distribution of daily returns in a histogram using 100 bins. Also, use geom_vline() to add a dashed red vertical line that indicates the 5 percent quantile of the daily returns. Compute summary statistics (mean, standard deviation, minimum, and maximum) for the daily returns.\nTake your code from the previous exercises and generalize it such that you can perform all the computations for an arbitrary number of symbols (e.g., symbol = [\"AAPL\", \"MMM\", \"BA\"]). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nTo facilitate the computation of the annualization factor, write a function that takes a vector of return dates as input and determines the frequency before returning the appropriate annualization factor.\nAre days with high aggregate trading volume often also days with large absolute returns? Find an appropriate visualization to analyze the question using the symbol AAPL.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html",
    "href": "python/fixed-effects-and-clustered-standard-errors.html",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we provide an intuitive introduction to the two popular concepts of fixed effects regressions and clustered standard errors. When working with regressions in empirical finance, you will sooner or later be confronted with discussions around how you deal with omitted variables bias and dependence in your residuals. The concepts we introduce in this chapter are designed to address such concerns.\nWe focus on a classical panel regression common to the corporate finance literature (e.g., Fazzari et al. 1988; Erickson and Whited 2012; Gulen and Ion 2015): firm investment modeled as a function that increases in firm cash flow and firm investment opportunities.\nTypically, this investment regression uses quarterly balance sheet data provided via Compustat because it allows for richer dynamics in the regressors and more opportunities to construct variables. As we focus on the implementation of fixed effects and clustered standard errors, we use the annual Compustat data from our previous chapters and leave the estimation using quarterly data as an exercise. We demonstrate below that the regression based on annual data yields qualitatively similar results to estimations based on quarterly data from the literature, namely confirming the positive relationships between investment and the two regressors.\nThe current chapter relies on the following set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport datetime as dt\nimport pyfixest as pf\nCompared to previous chapters, we introduce pyfixest (Fischer 2024), which provides tools for estimating various econometric models such as panel regressions.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and annual Compustat as data sources from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. In particular, Compustat provides balance sheet and income statement data on a firm level, while CRSP provides market valuations. \n\ntidy_finance = sqlite3.connect(\n  database=\"data/tidy_finance_python.sqlite\"\n)\n\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT gvkey, date, mktcap FROM crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\ncompustat = pd.read_sql_query(\n  sql=(\"SELECT datadate, gvkey, year, at, be, capx, oancf, txdb \"\n       \"FROM compustat\"),\n  con=tidy_finance,\n  parse_dates={\"datadate\"}\n)\n\nThe classical investment regressions model is the capital investment of a firm as a function of operating cash flows and Tobin’s q, a measure of a firm’s investment opportunities. We start by constructing investment and cash flows which are usually normalized by lagged total assets of a firm. In the following code chunk, we construct a panel of firm-year observations, so we have both cross-sectional information on firms as well as time-series information for each firm.\n\ndata_investment = (compustat\n  .assign(\n    date=lambda x: (\n      pd.to_datetime(x[\"datadate\"]).dt.to_period(\"M\").dt.to_timestamp()\n    )\n  )\n  .merge(compustat.get([\"gvkey\", \"year\", \"at\"])\n         .rename(columns={\"at\": \"at_lag\"})\n         .assign(year=lambda x: x[\"year\"]+1), \n         on=[\"gvkey\", \"year\"], how=\"left\")\n  .query(\"at &gt; 0 and at_lag &gt; 0\")\n  .assign(investment=lambda x: x[\"capx\"]/x[\"at_lag\"],\n          cash_flows=lambda x: x[\"oancf\"]/x[\"at_lag\"])                   \n)\n\ndata_investment = (data_investment\n  .merge(data_investment.get([\"gvkey\", \"year\", \"investment\"])\n          .rename(columns={\"investment\": \"investment_lead\"})\n          .assign(year=lambda x: x[\"year\"]-1), \n         on=[\"gvkey\", \"year\"], how=\"left\")\n)\n\nTobin’s q is the ratio of the market value of capital to its replacement costs. It is one of the most common regressors in corporate finance applications (e.g., Fazzari et al. 1988; Erickson and Whited 2012). We follow the implementation of Gulen and Ion (2015) and compute Tobin’s q as the market value of equity (mktcap) plus the book value of assets (at) minus book value of equity (be) plus deferred taxes (txdb), all divided by book value of assets (at). Finally, we only keep observations where all variables of interest are non-missing, and the reported book value of assets is strictly positive.\n\ndata_investment = (data_investment\n  .merge(crsp_monthly, on=[\"gvkey\", \"date\"], how=\"left\")\n  .assign(\n    tobins_q=lambda x: (\n      (x[\"mktcap\"]+x[\"at\"]-x[\"be\"]+x[\"txdb\"])/x[\"at\"]\n    )\n  )\n  .get([\"gvkey\", \"year\", \"investment_lead\", \"cash_flows\", \"tobins_q\"])\n  .dropna()\n)\n\nAs the variable construction typically leads to extreme values that are most likely related to data issues (e.g., reporting errors), many papers include winsorization of the variables of interest. Winsorization involves replacing values of extreme outliers with quantiles on the respective end. The following function implements the winsorization for any percentage cut that should be applied on either end of the distributions. In the specific example, we winsorize the main variables (investment, cash_flows, and tobins_q) at the one percent level.1\n\ndef winsorize(x, cut):\n    \"\"\"Winsorize returns at cut level.\"\"\"\n    \n    tmp_x = x.copy()\n    upper_quantile=np.nanquantile(tmp_x, 1 - cut)\n    lower_quantile=np.nanquantile(tmp_x, cut)\n    tmp_x[tmp_x &gt; upper_quantile]=upper_quantile\n    tmp_x[tmp_x &lt; lower_quantile]=lower_quantile\n    \n    return tmp_x\n\ndata_investment = (data_investment\n  .assign(\n    investment_lead=lambda x: winsorize(x[\"investment_lead\"], 0.01),\n    cash_flows=lambda x: winsorize(x[\"cash_flows\"], 0.01),\n    tobins_q=lambda x: winsorize(x[\"tobins_q\"], 0.01)\n  )\n)\n\nBefore proceeding to any estimations, we highly recommend tabulating summary statistics of the variables that enter the regression. These simple tables allow you to check the plausibility of your numerical variables, as well as spot any obvious errors or outliers. Additionally, for panel data, plotting the time series of the variable’s mean and the number of observations is a useful exercise to spot potential problems.\n\ndata_investment_summary = (data_investment\n  .melt(id_vars=[\"gvkey\", \"year\"], var_name=\"measure\",\n        value_vars=[\"investment_lead\", \"cash_flows\", \"tobins_q\"])\n  .get([\"measure\", \"value\"])\n  .groupby(\"measure\")\n  .describe(percentiles=[0.05, 0.5, 0.95])\n)\nnp.round(data_investment_summary, 2)\n\n\n\n\n\n\n\n\nvalue\n\n\n\ncount\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nmeasure\n\n\n\n\n\n\n\n\n\n\n\n\ncash_flows\n133614.0\n0.01\n0.27\n-1.56\n-0.48\n0.06\n0.27\n0.47\n\n\ninvestment_lead\n133614.0\n0.06\n0.08\n0.00\n0.00\n0.03\n0.20\n0.46\n\n\ntobins_q\n133614.0\n1.99\n1.69\n0.56\n0.79\n1.39\n5.35\n10.80",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nTo illustrate fixed effects regressions, we use the pyfixest package, which is both computationally powerful and flexible with respect to model specifications. We start out with the basic investment regression using the simple model \\[ \\text{Investment}_{i,t+1} = \\alpha + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t}, \\tag{1}\\] where \\(\\varepsilon_t\\) is i.i.d. normally distributed across time and firms. We use the PanelOLS()-function to estimate the simple model so that the output has the same structure as the other regressions below.\n\nmodel_ols = pf.feols(\n  \"investment_lead ~ cash_flows + tobins_q\",\n  vcov = \"iid\",\n  data = data_investment\n)\nmodel_ols.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: investment_lead, Fixed effects: \nInference:  iid\nObservations:  133614\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.042 |        0.000 |   130.084 |      0.000 |  0.041 |   0.042 |\n| cash_flows    |      0.049 |        0.001 |    64.342 |      0.000 |  0.047 |   0.050 |\n| tobins_q      |      0.007 |        0.000 |    57.707 |      0.000 |  0.007 |   0.007 |\n---\nRMSE: 0.074 R2: 0.044 \n\n\nAs expected, the regression output shows significant coefficients for both variables. Higher cash flows and investment opportunities are associated with higher investment. However, the simple model actually may have a lot of omitted variables, so our coefficients are most likely biased. As there is a lot of unexplained variation in our simple model (indicated by the rather low adjusted R-squared), the bias in our coefficients is potentially severe, and the true values could be above or below zero. Note that there are no clear cutoffs to decide when an R-squared is high or low, but it depends on the context of your application and on the comparison of different models for the same data.\nOne way to tackle the issue of omitted variable bias is to get rid of as much unexplained variation as possible by including fixed effects; i.e., model parameters that are fixed for specific groups (e.g., Wooldridge 2010). In essence, each group has its own mean in fixed effects regressions. The simplest group that we can form in the investment regression is the firm level. The firm fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t}, \\tag{2}\\] where \\(\\alpha_i\\) is the firm fixed effect and captures the firm-specific mean investment across all years. In fact, you could also compute firms’ investments as deviations from the firms’ average investments and estimate the model without the fixed effects. The idea of the firm fixed effect is to remove the firm’s average investment, which might be affected by firm-specific variables that you do not observe. For example, firms in a specific industry might invest more on average. Or you observe a young firm with large investments but only small concurrent cash flows, which will only happen in a few years. This sort of variation is unwanted because it is related to unobserved variables that can bias your estimates in any direction.\nTo include the firm fixed effect, we use gvkey (Compustat’s firm identifier) as follows:\n\nmodel_fe_firm = pf.feols(\n  \"investment_lead ~ cash_flows + tobins_q | gvkey\",\n  vcov = \"iid\",\n  data = data_investment\n)\nmodel_fe_firm.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: investment_lead, Fixed effects: gvkey\nInference:  iid\nObservations:  133614\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| cash_flows    |      0.014 |        0.001 |    16.639 |      0.000 |  0.012 |   0.015 |\n| tobins_q      |      0.011 |        0.000 |    88.007 |      0.000 |  0.010 |   0.011 |\n---\nRMSE: 0.049 R2: 0.587 R2 Within: 0.056 \n\n\nThe regression output shows a lot of unexplained variation at the firm level that is taken care of by including the firm fixed effect as the adjusted R-squared rises above 50 percent. In fact, it is more interesting to look at the within R-squared that shows the explanatory power of a firm’s cash flow and Tobin’s q on top of the average investment of each firm. We can also see that the coefficients changed slightly in magnitude but not in sign.\nThere is another source of variation that we can get rid of in our setting: average investment across firms might vary over time due to macroeconomic factors that affect all firms, such as economic crises. By including year fixed effects, we can take out the effect of unobservables that vary over time. The two-way fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\alpha_t + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t}, \\tag{3}\\] where \\(\\alpha_t\\) is the time fixed effect. Here you can think of higher investments during an economic expansion with simultaneously high cash flows. You can include a time fixed effects by using “TimeEffects” in the formula of PanelOLS.\n\nmodel_fe_firmyear = pf.feols(\n  \"investment_lead ~ cash_flows + tobins_q | gvkey + year\",\n  vcov = \"iid\",\n  data = data_investment\n)\nmodel_fe_firmyear.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: investment_lead, Fixed effects: gvkey+year\nInference:  iid\nObservations:  133614\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| cash_flows    |      0.017 |        0.001 |    20.798 |      0.000 |  0.015 |   0.018 |\n| tobins_q      |      0.010 |        0.000 |    80.947 |      0.000 |  0.009 |   0.010 |\n---\nRMSE: 0.048 R2: 0.608 R2 Within: 0.049 \n\n\nThe inclusion of time fixed effects did only marginally affect the R-squared and the coefficients, which we can interpret as a good thing as it indicates that the coefficients are not driven by an omitted variable that varies over time.\nHow can we further improve the robustness of our regression results? Ideally, we want to get rid of unexplained variation at the firm-year level, which means we need to include more variables that vary across firm and time and are likely correlated with investment. Note that we cannot include firm-year fixed effects in our setting because then cash flows and Tobin’s q are colinear with the fixed effects, and the estimation becomes void.\nBefore we discuss the properties of our estimation errors, we want to point out that regression tables are at the heart of every empirical analysis, where you compare multiple models. Fortunately, the results.compare() function provides a convenient way to tabulate the regression output (with many parameters to customize and even print the output in LaTeX). We recommend printing \\(t\\)-statistics rather than standard errors in regression tables because the latter are typically very hard to interpret across coefficients that vary in size. We also do not print p-values because they are sometimes misinterpreted to signal the importance of observed effects (Wasserstein and Lazar 2016). The \\(t\\)-statistics provide a consistent way to interpret changes in estimation uncertainty across different model specifications.\n\npf.etable([model_ols, model_fe_firm, model_fe_firmyear], coef_fmt = \"b (t)\")\n\n                            est1               est2               est3\n------------  ------------------  -----------------  -----------------\ndepvar           investment_lead    investment_lead    investment_lead\n----------------------------------------------------------------------\nIntercept     0.042*** (130.084)\ncash_flows     0.049*** (64.342)  0.014*** (16.639)  0.017*** (20.798)\ntobins_q       0.007*** (57.707)  0.011*** (88.007)  0.010*** (80.947)\n----------------------------------------------------------------------\ngvkey                          -                  x                  x\nyear                           -                  -                  x\n----------------------------------------------------------------------\nR2                         0.044              0.587              0.608\nS.E. type                    iid                iid                iid\nObservations              133614             133614             133614\n----------------------------------------------------------------------\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nFormat of coefficient cell:\nCoefficient (t-stats)",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Clustering Standard Errors",
    "text": "Clustering Standard Errors\nApart from biased estimators, we usually have to deal with potentially complex dependencies of our residuals with each other. Such dependencies in the residuals invalidate the i.i.d. assumption of OLS and lead to biased standard errors. With biased OLS standard errors, we cannot reliably interpret the statistical significance of our estimated coefficients.\nIn our setting, the residuals may be correlated across years for a given firm (time-series dependence), or, alternatively, the residuals may be correlated across different firms (cross-section dependence). One of the most common approaches to dealing with such dependence is the use of clustered standard errors (Petersen 2008). The idea behind clustering is that the correlation of residuals within a cluster can be of any form. As the number of clusters grows, the cluster-robust standard errors become consistent (Donald and Lang 2007; Wooldridge 2010). A natural requirement for clustering standard errors in practice is hence a sufficiently large number of clusters. Typically, around at least 30 to 50 clusters are seen as sufficient (Cameron, Gelbach, and Miller 2011).\nInstead of relying on the i.i.d. assumption, we can use the cov_type=\"clustered\" option in the fit()-function as above. The code chunk below applies both one-way clustering by firm as well as two-way clustering by firm and year.\n\nmodel_cluster_firm = pf.feols(\n  \"investment_lead ~ cash_flows + tobins_q | gvkey + year\",\n  vcov = {\"CRV1\": \"gvkey\"},\n  data = data_investment\n)\n\nmodel_cluster_firmyear = pf.feols(\n  \"investment_lead ~ cash_flows + tobins_q | gvkey + year\",\n  vcov = {\"CRV1\": \"gvkey + year\"},\n  data = data_investment\n)\n\n The table below shows the comparison of the different assumptions behind the standard errors. In the first column, we can see highly significant coefficients on both cash flows and Tobin’s q. By clustering the standard errors on the firm level, the \\(t\\)-statistics of both coefficients drop in half, indicating a high correlation of residuals within firms. If we additionally cluster by year, we see a drop, particularly for Tobin’s q, again. Even after relaxing the assumptions behind our standard errors, both coefficients are still comfortably significant as the \\(t\\)-statistics are well above the usual critical values of 1.96 or 2.576 for two-tailed significance tests.\n\npf.etable([model_fe_firmyear, model_cluster_firm, model_cluster_firmyear], coef_fmt = \"b (t)\")\n\n                           est1               est2               est3\n------------  -----------------  -----------------  -----------------\ndepvar          investment_lead    investment_lead    investment_lead\n---------------------------------------------------------------------\ncash_flows    0.017*** (20.798)  0.017*** (11.391)   0.017*** (9.394)\ntobins_q      0.010*** (80.947)  0.010*** (35.765)  0.010*** (14.961)\n---------------------------------------------------------------------\ngvkey                         x                  x                  x\nyear                          x                  x                  x\n---------------------------------------------------------------------\nR2                        0.608              0.608              0.608\nS.E. type                   iid          by: gvkey     by: gvkey+year\nObservations             133614             133614             133614\n---------------------------------------------------------------------\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nFormat of coefficient cell:\nCoefficient (t-stats)\n\n\nInspired by Abadie et al. (2017), we want to close this chapter by highlighting that choosing the right dimensions for clustering is a design problem. Even if the data is informative about whether clustering matters for standard errors, they do not tell you whether you should adjust the standard errors for clustering. Clustering at too aggregate levels can hence lead to unnecessarily inflated standard errors.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#key-takeaways",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#key-takeaways",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFixed effects regressions control for unobserved firm and time-specific factors, reducing omitted variable bias in panel data models.\nThe pyfixest Python package streamlines the estimation of fixed effects and supports clustering standard errors for robust inference.\nClustered standard errors adjust for residual dependence across firms or years, leading to more accurate \\(t\\)-statistics and confidence in significance tests.\nTwo-way clustering by firm and year is commonly used in finance to address both time-series and cross-sectional correlation in residuals.\nCareful model specification, including winsorization and proper clustering choices, enhances the credibility and reliability of empirical finance results.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#exercises",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#exercises",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Exercises",
    "text": "Exercises\n\nEstimate the two-way fixed effects model with two-way clustered standard errors using quarterly Compustat data from WRDS.\nFollowing Peters and Taylor (2017), compute Tobin’s q as the market value of outstanding equity mktcap plus the book value of debt (dltt + dlc) minus the current assets atc and everything divided by the book value of property, plant and equipment ppegt. What is the correlation between the measures of Tobin’s q? What is the impact on the two-way fixed effects regressions?",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#footnotes",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#footnotes",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that in pandas, when you index a dataframee, you receive a reference to the original dataframee. Consequently, modifying a subset will directly impact the initial dataframee. To prevent unintended changes to the original dataframee, it is advisable to use the copy() method as we do here in the winsorize function.↩︎",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html",
    "href": "python/accessing-and-managing-financial-data.html",
    "title": "Accessing and Managing Financial Data",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we suggest a way to organize your financial data. Everybody who has experience with data is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome when using different data formats and across different projects. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages.\nThis chapter shows how to import different open-source datasets. Specifically, our data comes from the application programming interface (API) of Yahoo Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series. We store all the data in a single database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\nFirst, we load the Python packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nimport pandas as pd\nimport numpy as np\nimport io\nimport re\nimport zipfile\nfrom curl_cffi import requests\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on.\nstart_date = \"1960-01-01\"\nend_date = \"2024-12-31\"",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#fama-french-data",
    "href": "python/accessing-and-managing-financial-data.html#fama-french-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Fama-French Data",
    "text": "Fama-French Data\nWe start by downloading some famous Fama-French factors (e.g., Fama and French 1993) and portfolio returns commonly used in empirical asset pricing. The data are freely available from Kenneth French’s Data Library, but the raw files come in a rather idiosyncratic format. If you access the data via the website, the manual raw workflow looks like this:\n\nGo to the website\nFind the right dataset\nDownload a ZIP file\nExtract the CSV inside\nSelect the right data table from the file and import the table into Python\nClean the dates, scale the returns, fix column names, handle missing values, etc.\n\nDoing this once is fine; doing it repeatedly across projects is exactly the type of boilerplate that’s easy to mess up and annoying to maintain. It is therefore natural to automate these steps in Python.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#q-factors",
    "href": "python/accessing-and-managing-financial-data.html#q-factors",
    "title": "Accessing and Managing Financial Data",
    "section": "q-Factors",
    "text": "q-Factors\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the Hou, Xue, and Zhang (2014) q-factor model. We refer to the extended background information provided by the original authors for further information. The q-factors can be downloaded directly from the authors’ homepage from within pd.read_csv(). \nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the “R_”-prescript using regular expressions and write all column names in lowercase. We then query the data to select observations between the start and end dates. Finally, we use the double asterisk (**) notation in the assign function to apply the same transform of dividing by 100 to all four factors by iterating through them. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on try. You can check out style guides available online, e.g., Hadley Wickham’s tidyverse style guide. note that we temporarily adjust the SSL certificate handling behavior in Python’s ssl module when retrieving the \\(q\\)-factors directly from the web, as demonstrated in Working with Stock Returns. This method should be used with caution, which is why we restore the default settings immediately after successfully downloading the data.\n\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\nfactors_q_monthly_link = (\n  \"https://global-q.org/uploads/1/2/2/6/122679606/\"\n  \"q5_factors_monthly_2024.csv\"\n)\n\nfactors_q_monthly = (pd.read_csv(factors_q_monthly_link)\n  .assign(\n    date=lambda x: (\n      pd.to_datetime(x[\"year\"].astype(str) + \"-\" +\n        x[\"month\"].astype(str) + \"-01\"))\n  )\n  .drop(columns=[\"R_F\", \"R_MKT\", \"year\"])\n  .rename(columns=lambda x: x.replace(\"R_\", \"\").lower())\n  .query(f\"date &gt;= '{start_date}' and date &lt;= '{end_date}'\")\n  .assign(\n    **{col: lambda x: x[col]/100 for col in [\"me\", \"ia\", \"roe\", \"eg\"]}\n  )\n)\n\nssl._create_default_https_context = ssl.create_default_context\n\nAgain, you can use the tidyfinance package for a shortcut:\n\ntf.download_data(\n  domain=\"factors_q\",\n  dataset=\"q5_factors_monthly\", \n  start_date=start_date, \n  end_date=end_date\n)\n\n\n\n\n\n\n\n\ndate\nrisk_free\nmkt_excess\nme\nia\nroe\neg\n\n\n\n\n0\n1967-01-01\n0.003927\n0.081852\n0.068122\n-0.029263\n0.018813\n-0.025511\n\n\n1\n1967-02-01\n0.003743\n0.007557\n0.016235\n-0.002915\n0.035399\n0.021792\n\n\n2\n1967-03-01\n0.003693\n0.040169\n0.019836\n-0.016772\n0.018417\n-0.011192\n\n\n3\n1967-04-01\n0.003344\n0.038786\n-0.006700\n-0.028972\n0.010253\n-0.016371\n\n\n4\n1967-05-01\n0.003126\n-0.042807\n0.027457\n0.021864\n0.005901\n0.001191\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n691\n2024-08-01\n0.004419\n0.016518\n-0.040817\n0.004687\n0.018369\n0.008116\n\n\n692\n2024-09-01\n0.004619\n0.016806\n-0.011967\n-0.000010\n0.007408\n-0.032810\n\n\n693\n2024-10-01\n0.003907\n-0.009701\n-0.011261\n-0.011676\n-0.002314\n-0.008335\n\n\n694\n2024-11-01\n0.003955\n0.065002\n0.043985\n-0.049491\n-0.015370\n-0.021420\n\n\n695\n2024-12-01\n0.003663\n-0.031637\n-0.051564\n-0.003684\n-0.021442\n0.049624\n\n\n\n\n696 rows × 7 columns",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "href": "python/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "title": "Accessing and Managing Financial Data",
    "section": "Macroeconomic Predictors",
    "text": "Macroeconomic Predictors\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. Welch and Goyal (2008) comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data on Amit Goyal’s website. Since the data is an XLSX-file stored on a public Google Drive location, we need additional packages to access the data directly from our Python session. Usually, you need to authenticate if you interact with Google drive directly in Python. Since the data is stored via a public link, we can proceed without any authentication.\n\nsheet_id = \"1bM7vCWd3WOt95Sf9qjLPZjoiafgF_8EG\"\nsheet_name = \"macro_predictors.xlsx\"\nmacro_predictors_link = (\n  f\"https://docs.google.com/spreadsheets/d/{sheet_id}\" \n  f\"/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n)\n\nNext, we read in the new data and transform the columns into the variables that we later use:\n\nThe dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (Campbell and Shiller 1988; Campbell and Yogo 2006).\nDividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).\nEarnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index (Campbell and Shiller 1988).\nDividend payout ratio (de), the difference between the log of dividends and the log of earnings (Lamont 1998).\nStock variance (svar), the sum of squared daily returns on the S&P 500 index (Guo 2006).\nBook-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average (Kothari and Shanken 1997).\nNet equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks (Campbell, Hilscher, and Szilagyi 2008).\nTreasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis (Campbell 1987).\nLong-term yield (lty), the long-term government bond yield from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nLong-term rate of returns (ltr), the long-term government bond returns from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nTerm spread (tms), the difference between the long-term yield on government bonds and the Treasury bill (Campbell 1987).\nDefault yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields (Fama and French 1989).\nInflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics (Campbell and Vuolteenaho 2004).\n\nFor variable definitions and the required data transformations, you can consult the material on Amit Goyal’s website.\n\nssl._create_default_https_context = ssl._create_unverified_context\n\nmacro_predictors = (\n  pd.read_csv(macro_predictors_link, thousands=\",\")\n  .assign(\n    date=lambda x: pd.to_datetime(x[\"yyyymm\"], format=\"%Y%m\"),\n    dp=lambda x: np.log(x[\"D12\"])-np.log(x[\"Index\"]),\n    dy=lambda x: np.log(x[\"D12\"])-np.log(x[\"Index\"].shift(1)),\n    ep=lambda x: np.log(x[\"E12\"])-np.log(x[\"Index\"]),\n    de=lambda x: np.log(x[\"D12\"])-np.log(x[\"E12\"]),\n    tms=lambda x: x[\"lty\"]-x[\"tbl\"],\n    dfy=lambda x: x[\"BAA\"]-x[\"AAA\"]\n  )\n  .rename(columns={\"b/m\": \"bm\"})\n  .get([\"date\", \"dp\", \"dy\", \"ep\", \"de\", \"svar\", \"bm\", \n        \"ntis\", \"tbl\", \"lty\", \"ltr\", \"tms\", \"dfy\", \"infl\"])\n  .query(\"date &gt;= @start_date and date &lt;= @end_date\")\n  .dropna()\n)\n\nssl._create_default_https_context = ssl.create_default_context\n\nTo get the equivalent data through tidyfinance, you can call:\n\ntf.download_data(\n  domain=\"macro_predictors\",\n  dataset=\"monthly\",\n  start_date=start_date, \n  end_date=end_date\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "href": "python/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Other Macroeconomic Data",
    "text": "Other Macroeconomic Data\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. As an illustration, we use the tidyfinance package to fetch consumer price index (CPI) data that can be found under the CPIAUCNS key.\n\nseries = \"CPIAUCNS\"\nurl = f\"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series}\"\n\nWe can then use the requests module to request the CSV, extract the data from the response body, and convert the columns to a tidy format:\n\nresp = requests.get(url)\nresp_csv = pd.io.common.StringIO(resp.text)\n\ncpi_monthly = (pd.read_csv(resp_csv)\n  .assign(\n    date=lambda x: pd.to_datetime(x[\"observation_date\"]),\n    value=lambda x: pd.to_numeric(\n      x[series], errors=\"coerce\"\n    ),\n      series=series,\n   )\n  .get([\"date\", \"series\", \"value\"])\n  .query(\"date &gt;= @start_date & date &lt;= @end_date\")\n  .assign(cpi=lambda x: x[\"value\"] / x[\"value\"].iloc[-1])\n)\n\nThe last line sets the current (latest) price level as the reference price level.\nThe tidyfinance package can, of course, also fetch the same index data and many more data series:\n\ntf.download_data(\n  domain=\"fred\",\n  series = \"CPIAUCNS\",\n  start_date = start_date,\n  end_date = end_date\n)\n\n\n\n\n\n\n\n\ndate\nseries\nvalue\n\n\n\n\n0\n1960-01-01\nCPIAUCNS\n29.300\n\n\n1\n1960-02-01\nCPIAUCNS\n29.400\n\n\n2\n1960-03-01\nCPIAUCNS\n29.400\n\n\n3\n1960-04-01\nCPIAUCNS\n29.500\n\n\n4\n1960-05-01\nCPIAUCNS\n29.500\n\n\n...\n...\n...\n...\n\n\n775\n2024-08-01\nCPIAUCNS\n314.796\n\n\n776\n2024-09-01\nCPIAUCNS\n315.301\n\n\n777\n2024-10-01\nCPIAUCNS\n315.664\n\n\n778\n2024-11-01\nCPIAUCNS\n315.493\n\n\n779\n2024-12-01\nCPIAUCNS\n315.605\n\n\n\n\n780 rows × 3 columns\n\n\n\nTo download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the PCU2122212122210 key. If your desired time series is not supported through tidyfinance, we recommend working with the fredapi package. Note that you need to get an API key to use its functionality. We refer to the package documentation for details.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#setting-up-a-database",
    "href": "python/accessing-and-managing-financial-data.html#setting-up-a-database",
    "title": "Accessing and Managing Financial Data",
    "section": "Setting Up a Database",
    "text": "Setting Up a Database\nNow that we have downloaded some (freely available) data from the web into the memory of our Python session, let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code.\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an SQLite-database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured SQL database engine. Note that SQL (Structured Query Language) is a standard language for accessing and manipulating databases.\n\nimport sqlite3\n\nAn SQLite-database is easily created - the code below is really all there is. You do not need any external software. Otherwise, date columns are stored and retrieved as integers. We will use the file tidy_finance_r.sqlite, located in the data subfolder, to retrieve data for all subsequent chapters. The initial part of the code ensures that the directory is created if it does not already exist.\n\nimport os\n\nif not os.path.exists(\"data\"):\n  os.makedirs(\"data\")\n    \ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the pandas function to_sql(), which copies the data to our SQLite-database.\n\n(factors_ff3_monthly\n  .to_sql(name=\"factors_ff3_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\nNow, if we want to have the whole table in memory, we need to call pd.read_sql_query() with the corresponding query. You will see that we regularly load the data into the memory in the next chapters.\n\npd.read_sql_query(\n  sql=\"SELECT date, risk_free FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\n\n\n\n\n\n\n\ndate\nrisk_free\n\n\n\n\n0\n1960-01-01\n0.0033\n\n\n1\n1960-02-01\n0.0029\n\n\n2\n1960-03-01\n0.0035\n\n\n3\n1960-04-01\n0.0019\n\n\n4\n1960-05-01\n0.0027\n\n\n...\n...\n...\n\n\n775\n2024-08-01\n0.0048\n\n\n776\n2024-09-01\n0.0040\n\n\n777\n2024-10-01\n0.0039\n\n\n778\n2024-11-01\n0.0040\n\n\n779\n2024-12-01\n0.0037\n\n\n\n\n780 rows × 2 columns\n\n\n\nThe last couple of code chunks are really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages.\nBefore we move on to the next data source, let us also store the other six tables in our new SQLite database.\n\ndata_dict = {\n  \"factors_ff5_monthly\": factors_ff5_monthly,\n  \"factors_ff3_daily\": factors_ff3_daily,\n  \"industries_ff_monthly\": industries_ff_monthly, \n  \"factors_q_monthly\": factors_q_monthly,\n  \"macro_predictors\": macro_predictors,\n  \"cpi_monthly\": cpi_monthly\n}\n\nfor key, value in data_dict.items():\n    value.to_sql(name=key,\n                 con=tidy_finance, \n                 if_exists=\"replace\",\n                 index=False)\n\nFrom now on, all you need to do to access data that is stored in the database is to follow two steps: (i) Establish the connection to the SQLite-database and (ii) execute the query to fetch the data. For your convenience, the following steps show all you need in a compact fashion.\n\nimport pandas as pd\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_q_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM factors_q_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "href": "python/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "title": "Accessing and Managing Financial Data",
    "section": "Managing SQLite Databases",
    "text": "Managing SQLite Databases\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\nTo optimize the database file, you can run the VACUUM command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the execute() function.\n\ntidy_finance.execute(\"VACUUM\")\n\nThe VACUUM command actually performs a couple of additional cleaning steps, which you can read about in this tutorial.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#key-takeaways",
    "href": "python/accessing-and-managing-financial-data.html#key-takeaways",
    "title": "Accessing and Managing Financial Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nImporting Fama-French factors, q-factors, macroeconomic indicators, and CPI data is simplified through API calls, CSV parsing, and web scraping techniques.\nThe tidyfinance Python package offers pre-processed access to financial datasets, reducing manual data cleaning and saving valuable time.\nCreating a centralized SQLite database helps manage and organize data efficiently across projects, while maintaining reproducibility.\nStructured database storage supports scalable data access, which is essential for long-term academic projects and collaborative work in finance.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#exercises",
    "href": "python/accessing-and-managing-financial-data.html#exercises",
    "title": "Accessing and Managing Financial Data",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the monthly Fama-French factors manually from Kenneth French’s data library and read them in via pd.read_csv(). Validate that you get the same data as via the tf.download_data() package.\nDownload the daily Fama-French 5 factors using the tf.download_data() function. After the successful download and conversion to the column format that we used above, compare the risk_free, mkt_excess, smb, and hml columns of factors_ff3_daily to factors_ff5_daily. Discuss any differences you might find.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/changelog.html",
    "href": "python/changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "You can find every single change in our commit history. We collect the most important changes for Tidy Finance with Python in the list below.\n\nAugust 4, 2024, Commit e30cff9: We switched to pyfixest instead of linearmodels for fixed effects regressions for better alignment with the R version.\nAugust 4, 2024, Commit 524bdd1: We added an additional filter to the Compustat download to exclude non-US companies in WRDS, CRSP, and Compustat.\nAugust 1, 2024, Commit 2980cf2: We updated the data until 2023-12-31 in all chapters.\nJuly 29, 2024, Commit cedec3e: We removed the month column from all chapters because it was misleading and consistently introduced date.\nJuly 16, 2024, Commit f4bbd00: We improved the documentation with respect to delisting returns in WRDS, CRSP, and Compustat. June 3, 2024, Commit 23d379f: We fixed a bug in Univaritate Portfolio Sorts, which led to wrong annual returns in Figure 3.\nMay 15, 2024, Commit 2bb2e07: We added a new subsection about creating environment variables to Setting Up Your Environment.\nMay 15, 2024, Commit adccfc9: We updated the filters in CRSP download, so that correct historical information is used and daily and monthly data are aligned.\nApril 17, 2024, Commit b8c32aa: Corrects a typo in the TRACE download.\nApril 15, 2024, Commit c0f5cc0: Fixes the definition of the dividend yield.",
    "crumbs": [
      "R",
      "Appendix",
      "Changelog"
    ]
  },
  {
    "objectID": "python/proofs.html",
    "href": "python/proofs.html",
    "title": "Proofs",
    "section": "",
    "text": "The minimum variance portfolio weights are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega= 1,\\] where \\(\\iota\\) is an \\((N \\times 1)\\) vector of ones. The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1).\\] We can solve the first-order conditions of the Lagrangian equation: \\[\n\\begin{aligned}\n& \\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega} = 0 \\Leftrightarrow 2\\Sigma \\omega = \\lambda\\iota \\Rightarrow \\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota \\\\ \\end{aligned}\n\\] Next, the constraint that weights have to sum up to one delivers: \\(1 = \\iota'\\omega = \\frac{\\lambda}{2}\\iota'\\Sigma^{-1}\\iota \\Rightarrow \\lambda = \\frac{2}{\\iota'\\Sigma^{-1}\\iota}.\\) Finally, plug-in the derived value of \\(\\lambda\\) to get \\[\n\\begin{aligned}\n\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}.\n\\end{aligned}\n\\]\n\n\n\nConsider an investor who aims to achieve minimum variance given a desired expected return \\(\\bar{\\mu}\\), that is: \\[\\omega_\\text{eff}\\left(\\bar{\\mu}\\right) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1) - \\tilde{\\lambda}(\\omega'\\mu - \\bar{\\mu}). \\] We can solve the first-order conditions to get \\[\n\\begin{aligned}\n2\\Sigma \\omega &= \\lambda\\iota + \\tilde\\lambda \\mu\\\\\n\\Rightarrow\\omega &= \\frac{\\lambda}{2}\\Sigma^{-1}\\iota + \\frac{\\tilde\\lambda}{2}\\Sigma^{-1}\\mu.\n\\end{aligned}\n\\]\nNext, the two constraints (\\(w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}\\)) imply \\[\n\\begin{aligned}\n1 &= \\iota'\\omega = \\frac{\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\iota}_{C} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\mu}_D\\\\\n\\Rightarrow \\lambda&= \\frac{2 - \\tilde\\lambda D}{C}\\\\\n\\bar\\mu &= \\mu'\\omega = \\frac{\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\iota}_{D} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\mu}_E = \\frac{1}{2}\\left(\\frac{2 - \\tilde\\lambda D}{C}\\right)D+\\frac{\\tilde\\lambda}{2}E  \\\\&=\\frac{D}{C}+\\frac{\\tilde\\lambda}{2}\\left(E - \\frac{D^2}{C}\\right)\\\\\n\\Rightarrow \\tilde\\lambda &= 2\\frac{\\bar\\mu - D/C}{E-D^2/C}.\n\\end{aligned}\n\\] As a result, the efficient portfolio weight takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right).\\] Thus, the efficient portfolio allocates wealth in the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and a levered (self-financing) portfolio to increase the expected return.\nNote that the portfolio weights sum up to one as \\[\\iota'\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) = D - D = 0\\text{ so }\\iota'\\omega_\\text{eff} = \\iota'\\omega_\\text{mvp} = 1.\\] Finally, the expected return of the efficient portfolio is \\[\\mu'\\omega_\\text{eff} = \\frac{D}{C} + \\bar\\mu - \\frac{D}{C} = \\bar\\mu.\\]\n\n\n\nWe argue that an investor with a quadratic utility function with certainty equivalent \\[\\max_\\omega CE(\\omega) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen with the aim to minimize volatility given a pre-specified level or expected returns \\[\\min_\\omega \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1.\\] Note the difference: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines their optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return they want to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk-averse investors (higher \\(\\gamma\\)) will chose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nTo proof this equivalence, consider first the optimal portfolio weights for a certainty equivalent maximizing investor. The first-order condition reads \\[\n\\begin{aligned}\n\\mu - \\lambda \\iota &= \\gamma \\Sigma \\omega \\\\\n\\Leftrightarrow \\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\lambda\\iota\\right)\n\\end{aligned}\n\\] Next, we make use of the constraint \\(\\iota'\\omega = 1\\). \\[\n\\begin{aligned}\n\\iota'\\omega &= 1 = \\frac{1}{\\gamma}\\left(\\iota'\\Sigma^{-1}\\mu - \\lambda\\iota'\\Sigma^{-1}\\iota\\right)\\\\\n\\Rightarrow \\lambda &= \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right).\n\\end{aligned}\n\\] Plugging in the value of \\(\\lambda\\) reveals the desired portfolio for an investor with risk aversion \\(\\gamma\\). \\[\n\\begin{aligned}\n\\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right)\\right) \\\\\n\\Rightarrow \\omega &= \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\iota'\\Sigma^{-1}\\right)\\mu\\\\\n&= \\omega_\\text{mvp} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1}\\mu - \\frac{\\iota'\\Sigma^{-1}\\mu}{\\iota'\\Sigma^{-1}\\iota}\\Sigma^{-1}\\iota\\right).\n\\end{aligned}\n\\] The resulting weights correspond to the efficient portfolio with desired return \\(\\bar r\\) such that (in the notation of book) \\[\\frac{1}{\\gamma} = \\frac{\\tilde\\lambda}{2} = \\frac{\\bar\\mu - D/C}{E - D^2/C}\\] which implies that the desired return is just \\[\\bar\\mu = \\frac{D}{C} + \\frac{1}{\\gamma}\\left({E - D^2/C}\\right)\\] which is \\(\\frac{D}{C} = \\mu'\\omega_\\text{mvp}\\) for \\(\\gamma\\rightarrow \\infty\\) as expected. For instance, letting \\(\\gamma \\rightarrow \\infty\\) implies \\(\\bar\\mu = \\frac{D}{C} = \\omega_\\text{mvp}'\\mu\\).",
    "crumbs": [
      "R",
      "Appendix",
      "Proofs"
    ]
  },
  {
    "objectID": "python/proofs.html#optimal-portfolio-choice",
    "href": "python/proofs.html#optimal-portfolio-choice",
    "title": "Proofs",
    "section": "",
    "text": "The minimum variance portfolio weights are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega= 1,\\] where \\(\\iota\\) is an \\((N \\times 1)\\) vector of ones. The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1).\\] We can solve the first-order conditions of the Lagrangian equation: \\[\n\\begin{aligned}\n& \\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega} = 0 \\Leftrightarrow 2\\Sigma \\omega = \\lambda\\iota \\Rightarrow \\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota \\\\ \\end{aligned}\n\\] Next, the constraint that weights have to sum up to one delivers: \\(1 = \\iota'\\omega = \\frac{\\lambda}{2}\\iota'\\Sigma^{-1}\\iota \\Rightarrow \\lambda = \\frac{2}{\\iota'\\Sigma^{-1}\\iota}.\\) Finally, plug-in the derived value of \\(\\lambda\\) to get \\[\n\\begin{aligned}\n\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}.\n\\end{aligned}\n\\]\n\n\n\nConsider an investor who aims to achieve minimum variance given a desired expected return \\(\\bar{\\mu}\\), that is: \\[\\omega_\\text{eff}\\left(\\bar{\\mu}\\right) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1) - \\tilde{\\lambda}(\\omega'\\mu - \\bar{\\mu}). \\] We can solve the first-order conditions to get \\[\n\\begin{aligned}\n2\\Sigma \\omega &= \\lambda\\iota + \\tilde\\lambda \\mu\\\\\n\\Rightarrow\\omega &= \\frac{\\lambda}{2}\\Sigma^{-1}\\iota + \\frac{\\tilde\\lambda}{2}\\Sigma^{-1}\\mu.\n\\end{aligned}\n\\]\nNext, the two constraints (\\(w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}\\)) imply \\[\n\\begin{aligned}\n1 &= \\iota'\\omega = \\frac{\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\iota}_{C} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\mu}_D\\\\\n\\Rightarrow \\lambda&= \\frac{2 - \\tilde\\lambda D}{C}\\\\\n\\bar\\mu &= \\mu'\\omega = \\frac{\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\iota}_{D} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\mu}_E = \\frac{1}{2}\\left(\\frac{2 - \\tilde\\lambda D}{C}\\right)D+\\frac{\\tilde\\lambda}{2}E  \\\\&=\\frac{D}{C}+\\frac{\\tilde\\lambda}{2}\\left(E - \\frac{D^2}{C}\\right)\\\\\n\\Rightarrow \\tilde\\lambda &= 2\\frac{\\bar\\mu - D/C}{E-D^2/C}.\n\\end{aligned}\n\\] As a result, the efficient portfolio weight takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right).\\] Thus, the efficient portfolio allocates wealth in the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and a levered (self-financing) portfolio to increase the expected return.\nNote that the portfolio weights sum up to one as \\[\\iota'\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) = D - D = 0\\text{ so }\\iota'\\omega_\\text{eff} = \\iota'\\omega_\\text{mvp} = 1.\\] Finally, the expected return of the efficient portfolio is \\[\\mu'\\omega_\\text{eff} = \\frac{D}{C} + \\bar\\mu - \\frac{D}{C} = \\bar\\mu.\\]\n\n\n\nWe argue that an investor with a quadratic utility function with certainty equivalent \\[\\max_\\omega CE(\\omega) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen with the aim to minimize volatility given a pre-specified level or expected returns \\[\\min_\\omega \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1.\\] Note the difference: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines their optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return they want to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk-averse investors (higher \\(\\gamma\\)) will chose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nTo proof this equivalence, consider first the optimal portfolio weights for a certainty equivalent maximizing investor. The first-order condition reads \\[\n\\begin{aligned}\n\\mu - \\lambda \\iota &= \\gamma \\Sigma \\omega \\\\\n\\Leftrightarrow \\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\lambda\\iota\\right)\n\\end{aligned}\n\\] Next, we make use of the constraint \\(\\iota'\\omega = 1\\). \\[\n\\begin{aligned}\n\\iota'\\omega &= 1 = \\frac{1}{\\gamma}\\left(\\iota'\\Sigma^{-1}\\mu - \\lambda\\iota'\\Sigma^{-1}\\iota\\right)\\\\\n\\Rightarrow \\lambda &= \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right).\n\\end{aligned}\n\\] Plugging in the value of \\(\\lambda\\) reveals the desired portfolio for an investor with risk aversion \\(\\gamma\\). \\[\n\\begin{aligned}\n\\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right)\\right) \\\\\n\\Rightarrow \\omega &= \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\iota'\\Sigma^{-1}\\right)\\mu\\\\\n&= \\omega_\\text{mvp} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1}\\mu - \\frac{\\iota'\\Sigma^{-1}\\mu}{\\iota'\\Sigma^{-1}\\iota}\\Sigma^{-1}\\iota\\right).\n\\end{aligned}\n\\] The resulting weights correspond to the efficient portfolio with desired return \\(\\bar r\\) such that (in the notation of book) \\[\\frac{1}{\\gamma} = \\frac{\\tilde\\lambda}{2} = \\frac{\\bar\\mu - D/C}{E - D^2/C}\\] which implies that the desired return is just \\[\\bar\\mu = \\frac{D}{C} + \\frac{1}{\\gamma}\\left({E - D^2/C}\\right)\\] which is \\(\\frac{D}{C} = \\mu'\\omega_\\text{mvp}\\) for \\(\\gamma\\rightarrow \\infty\\) as expected. For instance, letting \\(\\gamma \\rightarrow \\infty\\) implies \\(\\bar\\mu = \\frac{D}{C} = \\omega_\\text{mvp}'\\mu\\).",
    "crumbs": [
      "R",
      "Appendix",
      "Proofs"
    ]
  },
  {
    "objectID": "python/beta-estimation.html",
    "href": "python/beta-estimation.html",
    "title": "Beta Estimation",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we introduce an important concept in financial economics: The exposure of an individual stock to changes in the market portfolio. According to the Capital Asset Pricing Model (CAPM) of Sharpe (1964), Lintner (1965), and Mossin (1966), cross-sectional variation in expected asset returns should be a function of the covariance between the excess return of the asset and the excess return on the market portfolio. The regression coefficient of excess market returns on excess stock returns is usually called the market beta. We show an estimation procedure for the market betas. We do not go into details about the foundations of market beta but simply refer to any treatment of the CAPM for further information. Instead, we provide details about all the functions that we use to compute the results. In particular, we leverage useful computational concepts: Rolling-window estimation and parallelization.\nWe use the following Python packages throughout this chapter:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n\nfrom plotnine import *\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import percent_format, date_format\nfrom joblib import Parallel, delayed, cpu_count\nfrom itertools import product\nCompared to previous chapters, we introduce statsmodels (Seabold and Perktold 2010) for regression analysis and for sliding-window regressions and joblib (Team 2023) for parallelization.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#estimating-beta-using-monthly-returns",
    "href": "python/beta-estimation.html#estimating-beta-using-monthly-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta Using Monthly Returns",
    "text": "Estimating Beta Using Monthly Returns\nThe estimation procedure is based on a rolling-window estimation, where we may use either monthly or daily returns and different window lengths. First, let us start with loading the monthly CRSP data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=\"SELECT permno, date, industry, ret_excess FROM crsp_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .dropna()\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n)\n\nTo estimate the CAPM regression coefficients\n\\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{i, t},\n\\tag{1}\\]\nwe regress stock excess returns ret_excess on excess returns of the market portfolio mkt_excess.\nPython provides a simple solution to estimate (linear) models with the function smf.ols(). The function requires a formula as input that is specified in a compact symbolic form. An expression of the form y ~ model is interpreted as a specification that the response y is modeled by a linear predictor specified symbolically by model. Such a model consists of a series of terms separated by + operators. In addition to standard linear models, smf.ols() provides a lot of flexibility. You should check out the documentation for more information. To start, we restrict the data only to the time series of observations in CRSP that correspond to Apple’s stock (i.e., to permno 14593 for Apple) and compute \\(\\hat\\alpha_i\\) as well as \\(\\hat\\beta_i\\).\n\nmodel_fit = smf.ols(\n    formula=\"ret_excess ~ mkt_excess\", data=crsp_monthly.query(\"permno == 14593\")\n).fit()\ncoefficients = model_fit.summary2().tables[1]\ncoefficients\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.010093\n0.004976\n2.028154\n4.305973e-02\n0.000316\n0.019869\n\n\nmkt_excess\n1.387103\n0.109121\n12.711648\n2.207232e-32\n1.172726\n1.601481\n\n\n\n\n\n\n\nsmf.ols() returns an object of class RegressionModel, which contains all the information we usually care about with linear models. summary2() returns information about the estimated parameters. The output above indicates that Apple moves excessively with the market as the estimated \\(\\hat\\beta_i\\) is above one (\\(\\hat\\beta_i \\approx 1.4\\)).",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#rolling-window-estimation",
    "href": "python/beta-estimation.html#rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Rolling-Window Estimation",
    "text": "Rolling-Window Estimation\nAfter we estimated the regression coefficients on an example, we scale the estimation of \\(\\beta_i\\) to a whole different level and perform rolling-window estimations for the entire CRSP sample. The following function implements the CAPM regression for a data frame (or a part thereof) containing at least min_obs observations to avoid huge fluctuations if the time series is too short. The function conveniently returns the regression results as a data frame, which ensures that our approach is scalable. If the min_obs-condition is violated, that is, the time series is too short, the function returns an empty data frame for consistency.\n\ndef estimate_capm(data, min_obs=1):\n    if data.shape[0] &lt; min_obs:\n        capm = pd.DataFrame()\n    else:\n        fit = smf.ols(formula=\"ret_excess ~ mkt_excess\", data=data).fit()\n        coefficients = fit.summary2().tables[1]\n\n        capm = pd.DataFrame(\n            {\n                \"coefficient\": coefficients.index,\n                \"estimate\": coefficients[\"Coef.\"],\n                \"t_statistic\": coefficients[\"t\"],\n            }\n        ).assign(\n            coefficient=lambda x: np.where(\n                x[\"coefficient\"] == \"Intercept\", \"alpha\", x[\"coefficient\"]\n            )\n        )\n\n    return capm\n\nNext, we define a function that does the rolling estimation. We use a simple for-loop to implement the sliding window estimation in a straightforward manner. The following function takes input data and slides across the date vector, considering only a total of look_back months. The function essentially performs three steps: (i) arrange all rows, (ii) compute betas by sliding across months, and (iii) return a tibble with months and corresponding parameter estimates. As we demonstrate further below, we can also apply the same function to daily returns data.\n\ndef roll_capm_estimation(data, look_back=60, min_obs=48):\n    results = []\n    dates = data[\"date\"].sort_values().drop_duplicates()\n\n    for i in range(look_back - 1, len(dates)):\n        start_date = dates.iloc[i - look_back + 1]\n        end_date = dates.iloc[i]\n\n        window_data = data.query(\"date &gt;= @start_date & date &lt;= @end_date\")\n\n        result = estimate_capm(window_data)\n        result[\"date\"] = np.max(window_data[\"date\"])\n        results.append(result)\n\n    if results:\n        rolling_capm_estimation = pd.concat(results, ignore_index=True)\n    else:\n        rolling_capm_estimation = pd.DataFrame()\n\n    return rolling_capm_estimation\n\nBefore we approach the whole CRSP sample, let us focus on a couple of examples for well-known firms.\n\nexamples = pd.DataFrame({\n    \"permno\": [14593, 10107, 93436, 17778],\n    \"company\": [\"Apple\", \"Microsoft\", \"Tesla\", \"Berkshire Hathaway\"]\n})\n\nThe main idea is to apply the function to each stock individually and then combine the results into a single data frame. First, we nest the data by permno. Nested data means we now have a list of permno with corresponding grouped time series data. We get one row of output for each unique combination of non-nested variables which is only permno in this case.\n\ncapm_examples_nested = (crsp_monthly\n    .query(\"permno in @examples['permno']\")\n    .groupby(\"permno\", group_keys=True)\n)\ncapm_examples_nested\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x15fc16680&gt;\n\n\nNext, we want to apply the roll_capm_estimation() function to each stock. This situation is an ideal use case for apply(), which takes a list or vector as input and returns an object of the same length as the input. In our case, apply() returns a single data frame with a time series of beta estimates for each stock. Therefore, we use reset_index() to transform the list of outputs to a tidy data frame.\n\ncapm_examples = (capm_examples_nested\n    .apply(lambda x: roll_capm_estimation(x), include_groups=False)\n    .reset_index()\n    .get([\"permno\", \"date\", \"coefficient\", \"estimate\", \"t_statistic\"])\n)\ncapm_examples\n\n\n\n\n\n\n\n\npermno\ndate\ncoefficient\nestimate\nt_statistic\n\n\n\n\n0\n10107\n1991-03-01\nalpha\n0.042753\n2.805863\n\n\n1\n10107\n1991-03-01\nmkt_excess\n1.414250\n5.018872\n\n\n2\n10107\n1991-04-01\nalpha\n0.038412\n2.534832\n\n\n3\n10107\n1991-04-01\nmkt_excess\n1.434267\n5.115538\n\n\n4\n10107\n1991-05-01\nalpha\n0.039112\n2.583179\n\n\n...\n...\n...\n...\n...\n...\n\n\n2917\n93436\n2023-10-01\nmkt_excess\n2.326019\n5.545174\n\n\n2918\n93436\n2023-11-01\nalpha\n0.037527\n1.586507\n\n\n2919\n93436\n2023-11-01\nmkt_excess\n2.305606\n5.587578\n\n\n2920\n93436\n2023-12-01\nalpha\n0.032742\n1.373479\n\n\n2921\n93436\n2023-12-01\nmkt_excess\n2.358772\n5.573925\n\n\n\n\n2922 rows × 5 columns\n\n\n\nFigure 1 displays the resulting beta estimates, focusing exclusively on the coefficient fo \"mkt_excess\".\n\nbeta_examples_sub = (capm_examples\n    .merge(examples, how=\"left\", on=\"permno\")\n    .query(\"coefficient == 'mkt_excess'\")\n)\n\nbeta_figure = (\n    ggplot(\n        beta_examples_sub, \n        aes(x=\"date\", y=\"estimate\", color=\"company\", linetype=\"company\")\n    )\n    + geom_line()\n    + labs(\n        x=\"\",\n        y=\"\",\n        color=\"\",\n        linetype=\"\",\n        title=\"Monthly beta estimates for example stocks using 5 years of data\",\n    )\n    + scale_x_datetime(breaks=date_breaks(width=\"5 year\"), labels=date_format(\"%Y\"))\n)\nbeta_figure.show()\n\n\n\n\n\n\n\nFigure 1: The figure shows monthly beta estimates for example stocks using five years of data. The CAPM betas are estimated with monthly data and a rolling window of length five years based on adjusted excess returns from CRSP. We use market excess returns from Kenneth French data library.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#parallelized-rolling-window-estimation",
    "href": "python/beta-estimation.html#parallelized-rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Parallelized Rolling-Window Estimation",
    "text": "Parallelized Rolling-Window Estimation\nEven though we could now just apply the function using .groubby() on the whole CRSP sample, we advise against doing it as it is computationally quite expensive. Remember that we have to perform rolling-window estimations across all stocks and time periods. However, this estimation problem is an ideal scenario to employ the power of parallelization. Parallelization means that we split the tasks which perform rolling-window estimations across different workers (or cores on your local machine).\nIf you have a Windows or Mac machine, it makes most sense use the default parallelization backend of joblib, which means that separate Python processes are running in the background on the same machine to perform the individual jobs. If you check out the documentation of joblib.parallel_config(), you can also see other ways to resolve the parallelization in different environments. Note that we use availableCores() to determine the number of cores available for parallelization, but keep one core free for other tasks. Some machines might freeze if all cores are busy with Python jobs. \n\nn_cores = cpu_count() - 1\n\nUsing eight cores, the estimation for our sample of around 25k stocks takes around 20 minutes. Of course, you can speed up things considerably by having more cores available to share the workload or by having more powerful cores. Instead of using .apply() on groups, we use Parallel() to execute multiple tasks concurrently and delayed() to wrap each function call, allowing the calls to be queued and distributed to worker processes rather than executed immediately.\n\ncrsp_monthly_nested = (crsp_monthly\n    .groupby(\"permno\", group_keys=False)\n)\n\ncapm_monthly = pd.concat(\n    Parallel(n_jobs=n_cores)(\n        delayed(lambda name, group: roll_capm_estimation(group).assign(permno=name))(\n            name, group\n        )\n        for name, group in crsp_monthly_nested\n    )\n).get([\"permno\", \"date\", \"coefficient\", \"estimate\", \"t_statistic\"])\ncapm_monthly\n\n\n\n\n\n\n\n\npermno\ndate\ncoefficient\nestimate\nt_statistic\n\n\n\n\n0\n10001\n1991-01-01\nalpha\n0.008016\n1.222418\n\n\n1\n10001\n1991-01-01\nmkt_excess\n0.098174\n0.813336\n\n\n2\n10001\n1991-02-01\nalpha\n0.007914\n1.206700\n\n\n3\n10001\n1991-02-01\nmkt_excess\n0.095534\n0.791486\n\n\n4\n10001\n1991-03-01\nalpha\n0.007375\n1.122804\n\n\n...\n...\n...\n...\n...\n...\n\n\n201\n93436\n2023-10-01\nmkt_excess\n2.326019\n5.545174\n\n\n202\n93436\n2023-11-01\nalpha\n0.037527\n1.586507\n\n\n203\n93436\n2023-11-01\nmkt_excess\n2.305606\n5.587578\n\n\n204\n93436\n2023-12-01\nalpha\n0.032742\n1.373479\n\n\n205\n93436\n2023-12-01\nmkt_excess\n2.358772\n5.573925\n\n\n\n\n4256430 rows × 5 columns",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#estimating-beta-using-daily-returns",
    "href": "python/beta-estimation.html#estimating-beta-using-daily-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta Using Daily Returns",
    "text": "Estimating Beta Using Daily Returns\nBefore we provide some descriptive statistics of our beta estimates, we implement the estimation for the daily CRSP sample as well. Depending on the application, you might either use longer horizon beta estimates based on monthly data or shorter horizon estimates based on daily returns. As loading the full daily CRSP data requires relatively large amounts of memory, we split the beta estimation into smaller chunks. The logic follows the approach that we use to download the daily CRSP data (see WRDS, CRSP, and Compustat).\nFirst, we load the daily Fama-French market excess returns and extract the vector of dates.\n\nfactors_ff3_daily = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_daily\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nWe use the stocks from the monthly CRSP dataset as our reference point and process them in batches of 500. To estimate the CAPM over a consistent lookback window while accommodating different return frequencies, we adjust the minimum required number of observations accordingly. Specifically, we require at least 1,000 daily returns over a five‑year period for a valid estimation. This threshold is consistent with the monthly requirement of 48 observations out of 60 months, given that there are roughly 252 trading days in a year.\n\npermnos = list(crsp_monthly[\"permno\"].unique().astype(str))\n\nbatch_size = 500\nbatches = np.ceil(len(permnos)/batch_size).astype(int)\nmin_obs = 1000\n\nWe then proceed to perform the same steps as with the monthly CRSP data, just in batches: Load in daily returns, nest the data by stock, and parallelize the beta estimation across stocks. Note that we also convert the daily date to the beginning of the month\n\ncapm_daily = []\n\nfor j in range(1, batches+1):  \n    permno_batch = permnos[\n      ((j-1)*batch_size):(min(j*batch_size, len(permnos)))\n    ]\n    \n    permno_batch_formatted = (\n      \", \".join(f\"'{permno}'\" for permno in permno_batch)\n    )\n    permno_string = f\"({permno_batch_formatted})\"\n    \n    crsp_daily_sub_query = (\n      \"SELECT permno, date, ret_excess \"\n        \"FROM crsp_daily \"\n       f\"WHERE permno IN {permno_string}\" \n    )\n      \n    crsp_daily_sub = pd.read_sql_query(\n      sql=crsp_daily_sub_query,\n      con=tidy_finance,\n      dtype={\"permno\": int},\n      parse_dates={\"date\"}\n    )\n    \n    crsp_daily_sub_nested = (crsp_daily_sub\n      .merge(factors_ff3_daily, how=\"inner\", on=\"date\")\n      .assign(\n          date = lambda x: \n            x[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n        )\n      .groupby(\"permno\", group_keys=False)\n    )\n    \n    capm_daily_sub = pd.concat(\n        Parallel(n_jobs=n_cores)(\n            delayed(lambda name, group: roll_capm_estimation(group).assign(permno=name))(\n            name, group\n            )\n            for name, group in crsp_daily_sub_nested\n        )\n    ).get([\"permno\", \"date\", \"coefficient\", \"estimate\", \"t_statistic\"])\n    \n    capm_daily.append(capm_daily_sub)\n              \n    print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")\n  \ncapm_daily = pd.concat(capm_daily)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#comparing-beta-estimates",
    "href": "python/beta-estimation.html#comparing-beta-estimates",
    "title": "Beta Estimation",
    "section": "Comparing Beta Estimates",
    "text": "Comparing Beta Estimates\nWhat is a typical value for stock betas? First, let us extract the relevant estimates from our CAPM results based on monthly returns.\n\nbeta_monthly = (capm_monthly\n    .query(\"coefficient == 'mkt_excess'\")\n    .get([\"permno\", \"date\", \"estimate\"])\n    .rename(columns={\"estimate\": \"beta\"})\n    .assign(return_type=\"monthly\")\n)\n\nTo get some feeling, we illustrate the dispersion of the estimated \\(\\hat\\beta_i\\) across different industries and across time below. Figure 2 shows that typical business models across industries imply different exposure to the general market economy. However, there are barely any firms that exhibit a negative exposure to the market factor.\n\nbeta_industries = (beta_monthly\n    .merge(crsp_monthly, how=\"inner\", on=[\"permno\", \"date\"])\n    .dropna(subset=\"beta\")\n    .groupby([\"industry\",\"permno\"])[\"beta\"]\n    .aggregate(\"mean\")\n    .reset_index()\n)\n\nindustry_order = (beta_industries\n    .groupby(\"industry\")[\"beta\"]\n    .aggregate(\"median\")\n    .sort_values()\n    .index.tolist()\n)\n\nbeta_industries_figure = (\n    ggplot(\n        beta_industries, \n        aes(x=\"industry\", y=\"beta\")\n    )\n    + geom_boxplot()\n    + coord_flip()\n    + labs(\n        x=\"\",\n        y=\"\", \n        title=\"Firm-specific beta distributions by industry\"\n        )\n    + scale_x_discrete(limits=industry_order)\n)\nbeta_industries_figure.show()\n\n\n\n\n\n\n\nFigure 2: The box plots show the average firm-specific beta estimates by industry.\n\n\n\n\n\nNext, we illustrate the time-variation in the cross-section of estimated betas. Figure 3 shows the monthly deciles of estimated betas (based on monthly data) and indicates an interesting pattern: First, betas seem to vary over time in the sense that during some periods, there is a clear trend across all deciles. Second, the sample exhibits periods where the dispersion across stocks increases in the sense that the lower decile decreases and the upper decile increases, which indicates that for some stocks the correlation with the market increases while for others it decreases. Note also here: stocks with negative betas are a rare exception.\n\nbeta_quantiles = (\n    beta_monthly\n    .groupby(\"date\")[\"beta\"]\n    .quantile(q=np.arange(0.1, 1.0, 0.1))\n    .reset_index()\n    .rename(columns={\"level_1\": \"quantile\"})\n    .assign(quantile=lambda x: (x[\"quantile\"] * 100).astype(int))\n    .dropna()\n)\n\nlinetypes = [\"-\", \"--\", \"-.\", \":\"]\nn_quantiles = beta_quantiles[\"quantile\"].nunique()\n\nbeta_quantiles_figure = (\n    ggplot(\n        beta_quantiles,\n        aes(x=\"date\", y=\"beta\", color=\"factor(quantile)\", linetype=\"factor(quantile)\")\n    )\n    + geom_line()\n    + labs(\n        x=\"\", y=\"\", color=\"\", linetype=\"\",\n        title=\"Monthly deciles of estimated betas\"\n    )\n    + scale_x_datetime(breaks=date_breaks(\"5 year\"), labels=date_format(\"%Y\"))\n    + scale_linetype_manual(\n        values=[linetypes[l % len(linetypes)] for l in range(n_quantiles)]\n    )\n)\nbeta_quantiles_figure.show()\n\n\n\n\n\n\n\nFigure 3: The figure shows monthly deciles of estimated betas. Each line corresponds to the monthly cross-sectional quantile of the estimated CAPM beta.\n\n\n\n\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table.\n\nbeta_daily = (capm_daily\n    .query(\"coefficient == 'mkt_excess'\")\n    .get([\"permno\", \"date\", \"estimate\"])\n    .rename(columns={\"estimate\": \"beta\"})\n    .assign(return_type=\"daily\")\n)\n\nbeta = pd.concat([beta_monthly, beta_daily], ignore_index=True)\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table. Then, we use the table to plot a comparison of beta estimates for our example stocks in Figure 4.\n\nbeta_comparison = (beta\n    .merge(examples, how=\"inner\", on=\"permno\")\n)\n\nbeta_comparison_figure = (\n  ggplot(\n    beta_comparison,\n    aes(x=\"date\", y=\"beta\", color=\"return_type\", linetype = \"return_type\")\n  )\n  + geom_line()\n  + facet_wrap(\"~company\", ncol=1)\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Comparison of beta estimates using monthly and daily data\"\n    )\n  + scale_x_datetime(breaks=date_breaks(\"10 years\"), labels=date_format(\"%Y\"))\n  + theme(figure_size=(6.4, 6.4))\n)\nbeta_comparison_figure.show()\n\n\n\n\n\n\n\nFigure 4: The figure shows the comparison of beta estimates using monthly and daily data. CAPM betas are computed using five years of monthly or three months of daily data. The two lines show the monthly estimates based on a rolling window for few exemplary stocks.\n\n\n\n\n\nThe estimates in Figure 4 look as expected. As you can see, it really depends on the data frequency how your beta estimates turn out because the estimates based on daily data are much smoother due to the higher number of observations in each regression.\nFinally, we write the estimates to our database so that we can use them in later chapters.\n\n(beta.to_sql(\n  name=\"beta\", \n  con=tidy_finance, \n  if_exists=\"replace\",\n  index=False\n  )\n)\n\nWhenever you perform some kind of estimation, it also makes sense to do rough plausibility tests. A possible check is to plot the share of stocks with beta estimates over time. This descriptive helps us discover potential errors in our data preparation or estimation procedure. For instance, suppose there was a gap in our output where we do not have any betas. In this case, we would have to go back and check all previous steps to find out what went wrong.\n\nreturn_types = pd.DataFrame({\"return_type\": [\"monthly\", \"daily\"]})\n\nbeta_coverage = (crsp_monthly\n    .merge(return_types, how=\"cross\")\n    .merge(beta, on=[\"permno\", \"date\", \"return_type\"], how=\"left\") \n    .groupby([\"date\", \"return_type\"], as_index=False)\n    .apply(lambda x: pd.Series({\"share\": x[\"beta\"].notna().sum() / len(x)}))\n)\n\nbeta_coverage_figure = (\n  ggplot(\n    beta_coverage, \n    aes(x=\"date\", y=\"share\", color=\"return_type\", linetype=\"return_type\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"End-of-month share of securities with beta estimates\"\n    )\n  + scale_y_continuous(labels=percent_format())\n  + scale_x_datetime(breaks=date_breaks(\"10 year\"), labels=date_format(\"%Y\")) \n)\nbeta_coverage_figure.show()\n\n\n\n\n\n\n\nFigure 5: The figure shows end-of-month share of securities with beta estimates. The two lines show the share of securities with beta estimates using five years of monthly or three months of daily data.\n\n\n\n\n\nFigure 5 shows no issues, as the two coverage lines track each other closely, so we can proceed to the next check.\nWe also encourage everyone to always look at the distributional summary statistics of variables. You can easily spot outliers or weird distributions when looking at such tables.\n\n(beta\n    .groupby(\"return_type\")[\"beta\"]\n    .describe()\n    .round(2)\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nreturn_type\n\n\n\n\n\n\n\n\n\n\n\n\ndaily\n2158046.0\n0.79\n0.49\n-3.66\n0.41\n0.75\n1.12\n4.40\n\n\nmonthly\n2128215.0\n1.10\n0.71\n-9.01\n0.64\n1.04\n1.47\n10.35\n\n\n\n\n\n\n\nThe summary statistics also look plausible for the two estimation procedures.\nFinally, since we have two different estimators for the same theoretical object, we expect the estimators to be at least positively correlated (although not perfectly as the estimators are based on different sample periods and frequencies).\n\n(beta\n    .pivot_table(index=[\"permno\", \"date\"], columns=\"return_type\", values=\"beta\")\n    .reset_index()\n    .get([\"monthly\", \"daily\"])\n    .corr()\n    .round(2)\n)\n\n\n\n\n\n\n\nreturn_type\nmonthly\ndaily\n\n\nreturn_type\n\n\n\n\n\n\nmonthly\n1.00\n0.62\n\n\ndaily\n0.62\n1.00\n\n\n\n\n\n\n\nIndeed, we find a positive correlation between our beta estimates. In the subsequent chapters, we mainly use the estimates based on monthly data, as most readers should be able to replicate them due to potential memory limitations that might arise with the daily data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#key-takeaways",
    "href": "python/beta-estimation.html#key-takeaways",
    "title": "Beta Estimation",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nCAPM betas can be estimated using rolling-window estimation and processed in parallel via joblib.\nBoth monthly and daily return data can be used to estimate betas with different frequencies and window lengths, depending on the application.\nSummary statistics, visualization, and plausibility checks help to validate beta estimates across time and industries.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#exercises",
    "href": "python/beta-estimation.html#exercises",
    "title": "Beta Estimation",
    "section": "Exercises",
    "text": "Exercises\n\nCompute beta estimates based on monthly data using one, three, and five years of data and impose a minimum number of observations of 10, 28, and 48 months with return data, respectively. How strongly correlated are the estimated betas?\nCompute beta estimates based on monthly data using five years of data and impose different numbers of minimum observations. How does the share of permno-date observations with successful beta estimates vary across the different requirements? Do you find a high correlation across the estimated betas?\nInstead of using joblib, perform the beta estimation in a loop (using either monthly or daily data) for a subset of 100 permnos of your choice. Verify that you get the same results as with the parallelized code from above.\nFilter out the stocks with negative betas. Do these stocks frequently exhibit negative betas, or do they resemble estimation errors?\nCompute beta estimates for multi-factor models such as the Fama-French three-factor model by extending the estimate_capm() function with a model parameter. In particular, your regression should support the model \\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\sum\\limits_{j=1}^k\\beta_{i,k}(r_{j, t}-r_{f,t})+\\varepsilon_{i, t}\n\\tag{2}\\] where \\(r_{i, t}\\) are the \\(k\\) factor returns. Thus, you estimate four parameters (\\(\\alpha_i\\) and the slope coefficients). Provide some summary statistics of the cross-section of firms and their exposure to the different factors.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html",
    "href": "python/setting-up-your-environment.html",
    "title": "Setting Up Your Environment",
    "section": "",
    "text": "We aim to lower the bar for starting empirical research in financial economics. We want to make using Python easy for you. However, given that Tidy Finance is a platform that supports multiple programming languages, we also consider the possibility that you are unfamiliar with Python. Maybe you are transitioning from R to Python, i.e., following the journey of Tidy Finance, which started in R. Hence, we provide you with a simple guide to get started with Python. If you have not used Python before, you will be able to use it after reading this chapter.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#python-environment",
    "href": "python/setting-up-your-environment.html#python-environment",
    "title": "Setting Up Your Environment",
    "section": "Python Environment",
    "text": "Python Environment\nA Python environment is a self-contained directory or folder containing a specific version of the Python installation with a set of packages and dependencies. In order to isolate and manage the specific dependencies of the Tidy Finance with Python project, a so-called virtual environment is a reliable way to ensure that it will work consistently and reliably on different systems and over time.\nThere are many ways to install a Python version and environments on your system. We present two ways that we found most convenient to write this book and maintain our website: (i) Installation via Anaconda along with using Python in Spyder and (ii) installation via RStudio.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#installation-via-anaconda",
    "href": "python/setting-up-your-environment.html#installation-via-anaconda",
    "title": "Setting Up Your Environment",
    "section": "Installation via Anaconda",
    "text": "Installation via Anaconda\nFirst, we need to install Python itself via Anaconda. You can download the latest version of Anaconda from the official Anaconda website. After downloading the respective version for your operating system, follow the installation instructions.\nSecond, we now describe how to set up a Python virtual environment specific to Tidy Finance on your local system. This book uses Python version 3.10.11 to set up the environment for both Windows and Mac. As we write this book, it is not the latest version of Python. The reason for this is that we wanted (i) a stable code base and (ii) the content of the book to be usable for all kinds of users, especially for those who might rely on corporate version controls and are not able to install new Python distributions.\nFor the installation, we use the Anaconda Python distribution you downloaded in the step before.1 Additionally, you need the packages listed in the provided requirements.txt-file in a dedicated folder for the project. You can find the detailed list of packages in the Colophon.\nWe recommend you start with the package installation right away. After you have prepared your system, you can open the Anaconda prompt and install your virtual environment with the following commands:\n\nconda create -p C:\\Apps\\Anaconda3\\envs\\tidy_finance_environment python==3.10.11 (Confirm with y)\nconda activate C:\\Apps\\Anaconda3\\envs\\tidy_finance_environment\npip install -r \"&lt;Tidy-Finance-with-Python Folder&gt;\\requirements.txt\"\n\nAll other packages found with the command pip list are installed automatically as dependencies with the required packages in the file requirements.txt. Note that we make reference to two distinct folders. The first one, C:\\Apps\\Anaconda3\\envs\\tidy_finance_environment refers to the location of your Python environment used for Tidy Finance. Apart from that, you should store your data, program codes, and scripts in another location: Your Tidy Finance working folder.\nNow, you are basically ready to go. However, you will now need a Python integrated development environment (IDE) to make your coding experience pleasant.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#python-ide",
    "href": "python/setting-up-your-environment.html#python-ide",
    "title": "Setting Up Your Environment",
    "section": "Python IDE",
    "text": "Python IDE\nIf you are new to coding, you will not have an IDE for Python. We recommend using Spyder if you plan to code only in Python as it comes with Anaconda. If you don’t use Anaconda, you can download the software for your operating system from the official website. Then, follow the installation instructions. To add the previously created virtual environment to Spyder, Go to Tools → Preferences → Python Interpreter → “Use the following interpreter” and add C:\\Apps\\Anaconda3\\envs\\tidy_finance_environment\\python.exe.\nAnother increasingly popular code editor for data analysis is Visual Studio Code (VS Code), as it supports a variety of programming languages, including Python and R. We refer to this tutorial if you want to get started with VS Code. There are many more ways to set up a Python IDE, so we refer to this page in the Python wiki for more inspiration.\nIf you also plan to try R, you should get a multipurpose tool: RStudio. You can get your RStudio version from Posit (i.e., the company that created RStudio, which was previously called RStudio itself). When you follow the instructions, you will see that Posit asks you to install R; you need to do so to make RStudio feasible for Python. Then, select the virtual environment in RStudio. Alternatively, you can also start with the installation guide starting from RStudio, which we present below.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#installation-via-rstudio",
    "href": "python/setting-up-your-environment.html#installation-via-rstudio",
    "title": "Setting Up Your Environment",
    "section": "Installation via RStudio",
    "text": "Installation via RStudio\nYou can also install Python and set up your environment directly from RStudio. This approach has the big advantage that you can switch between R and Python code smoothly. We believe that being able to switch between different programming languages is a tremendously valuable skill, so we set up a repository containing all the files that you need to achieve this goal: Tidy Finance Environment. To set up this environment locally, follow these steps:\n\nInstall R and RStudio.\nClone the Tidy Finance Environment repository directly in RStudio by clicking on File/New Project/ and selecting Version Control. Then, click Git and provide the repository address https://github.com/tidy-finance/environment. RStudio will then automatically open the project in the new environment.\nInstall the reticulate R package: install.packages(\"reticulate\").\nUse reticulate to install Python: reticulate::install_python(version = \"3.10.11\", force = TRUE).\nTell renv to use Python: renv::use_python(\"PATH\").\n\n\"PATH\" on Mac: \"~/.pyenv/versions/3.10.11/bin/python\".\n\"PATH\" on Windows: \"C:/Users/&lt;User&gt;/AppData/Local/r-reticulate/ r-reticulate/pyenv/pyenv-win/versions/3.10.11/python.exe\" where &lt;User&gt; is your username.\n\nTell renv to install all required packages: renv::restore().\n\nNow you are ready to execute all code that you can find in this book or its sibling Tidy Finance with R.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#creating-environment-variables",
    "href": "python/setting-up-your-environment.html#creating-environment-variables",
    "title": "Setting Up Your Environment",
    "section": "Creating Environment Variables",
    "text": "Creating Environment Variables\nIf you plan to share your own code with collaborators or the public, you may encounter the situation that your projects require sensitive information, such as login credentials, that you don’t want to publish. Environment variables are widely used in software development projects because they provide a flexible and secure way to configure applications and store secrets. In later chapters, we use such environment variables to store private login data for a remote database.\nYou can use .env-files to store environment variables. Upon startup, Python projects often use libraries like python-dotenv to load these environment variables from a .env-file. .env-files can be placed at the project level and are not meant to be committed to version control, ensuring that sensitive information remains private.\nFirst, you need to install the python-dotenv library if you haven’t already:\n\npip install python-dotenv\n\nThen, create a .env-file in your project directory. You can add variables to this file. For the purpose of this book, we create and save the following variables (where user and password are our private login credentials):\nWRDS_USER=user\nWRDS_PASSWORD=password\nTo access these environment variables in your Python code, load the environment variables at the start of your Python script using python-dotenv:\n\nfrom dotenv import load_dotenv\nimport os\nload_dotenv()\n\nwrds_user = os.getenv(\"WRDS_USER\")\nwrds_password = os.getenv(\"WRDS_PASSWORD\")\n\nNote that you can also store other login credentials, API keys, or file paths in the same environment file.\nIf you use version control, make sure that the .env-file is included in your .gitignore to avoid committing sensitive information to your repository.",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#footnotes",
    "href": "python/setting-up-your-environment.html#footnotes",
    "title": "Setting Up Your Environment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that you can also install a newer version of Python. We only require the environment set up in the previous step to use Python version 3.10.11. The neat aspect is Python’s capability to accommodate version control in this respect.↩︎",
    "crumbs": [
      "R",
      "Prerequisits",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Support Tidy Finance",
    "section": "",
    "text": "Tidy Finance is and will remain an open-source project. We are grateful for all the support we have received so far. Of course, we do not force anybody to support us, but every gesture is very much appreciated. We have three options if you want to give something back and support our efforts. Moreover, most options come at no additional cost to you, i.e., they just increase our share of the pie. Who does not appreciate a little more pie?"
  },
  {
    "objectID": "support.html#get-your-copy-of-the-books",
    "href": "support.html#get-your-copy-of-the-books",
    "title": "Support Tidy Finance",
    "section": "Get your copy of the books",
    "text": "Get your copy of the books\nYou can read the free online versions of Tidy Finance on this website. However, you can also get your own physical copies! The books come with many perks, such as the joy of holding something in your hand, a fresh smell, and it certainly looks good in your library. If you decide to buy your own copies, please consider using our affiliate link from Routledge for Tidy Finance with R and Tidy Finance with Python. No extra cost to you, just some pie for us.\nNote that some affiliate links track your behavior on the site and can be flagged as suspicious by your browser. We exclusively use the official paths provided by the respective vendor. Alternatively, our books are also available on Amazon and other retailers."
  },
  {
    "objectID": "support.html#spread-the-word",
    "href": "support.html#spread-the-word",
    "title": "Support Tidy Finance",
    "section": "Spread the word",
    "text": "Spread the word\nThe project grows with the attention it receives from the community. Therefore, making people aware of Tidy Finance is a great way to support it. There are certainly many possibilities how you can spread the word. For example, you could\n\nContribute to the Tidy Finance blog\nCite Tidy Finance with R or Tidy Finance with Python in one of your projects\nUse Tidy Finance as a teaching resource and let us know\nConnect with us and share posts about Tidy Finance via social media\nYou can also buy Tidy Finance Swag\n\nThese are just a few suggestions, yet highly effective. In any case, we rely on your support to share Tidy Finance within your own community."
  },
  {
    "objectID": "support.html#buy-us-a-coffee",
    "href": "support.html#buy-us-a-coffee",
    "title": "Support Tidy Finance",
    "section": "Buy us a coffee",
    "text": "Buy us a coffee\nEvery task requires some fuel. In particular, one key ingredient to completing the mental efforts that culminate in Tidy Finance is, of course, coffee. Hence, if you appreciate Tidy Finance, let us have a coffee. We are grateful for every small contribution to sustain our caffeine levels. Moreover, higher caffeine levels positively correlate with new content on Tidy Finance. It is a win-win situation!"
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html",
    "href": "r/size-sorts-and-p-hacking.html",
    "title": "Size Sorts and p-Hacking",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we continue with portfolio sorts in a univariate setting. Yet, we consider firm size as a sorting variable, which gives rise to a well-known return factor: the size premium. The size premium arises from buying small stocks and selling large stocks. Prominently, Fama and French (1993) include it as a factor in their three-factor model. Apart from that, asset managers commonly include size as a key firm characteristic when making investment decisions.\nWe also introduce new choices in the formation of portfolios. In particular, we discuss listing exchanges, industries, weighting regimes, and periods. These choices matter for the portfolio returns and result in different size premiums (see Hasler (2021), Soebhag, Van Vliet, and Verwijmeren (2022), and Walter, Weber, and Weiss (2022) for more insights into decision nodes and their effect on premiums). Exploiting these ideas to generate favorable results is called p-hacking. There is arguably a thin line between p-hacking and conducting robustness tests. Our purpose here is to illustrate the substantial variation that can arise along the evidence-generating process.\nThe chapter relies on the following set of packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(furrr)\nlibrary(rlang)\nCompared to previous chapters, we introduce the rlang package (Henry and Wickham 2022) for more advanced parsing of functional expressions.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#data-preparation",
    "href": "r/size-sorts-and-p-hacking.html#data-preparation",
    "title": "Size Sorts and p-Hacking",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we retrieve the relevant data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. Firm size is defined as market equity in most asset pricing applications that we retrieve from CRSP. We further use the Fama-French factor returns for performance evaluation.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  collect()\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(smb) |&gt;\n  collect()",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#size-distribution",
    "href": "r/size-sorts-and-p-hacking.html#size-distribution",
    "title": "Size Sorts and p-Hacking",
    "section": "Size Distribution",
    "text": "Size Distribution\nBefore we build our size portfolios, we investigate the distribution of the variable firm size. Visualizing the data is a valuable starting point to understand the input to the analysis. Figure 8.1 shows the fraction of total market capitalization concentrated in the largest firm. To produce this graph, we create monthly indicators that track whether a stock belongs to the largest x percent of the firms. Then, we aggregate the firms within each bucket and compute the buckets’ share of total market capitalization.\nFigure 1 shows that the largest 1 percent of firms cover up to 50 percent of the total market capitalization, and holding just the 25 percent largest firms in the CRSP universe essentially replicates the market portfolio. The distribution of firm size thus implies that the largest firms of the market dominate many small firms whenever we use value-weighted benchmarks.\n\ncrsp_monthly |&gt;\n  group_by(date) |&gt;\n  mutate(\n    top01 = if_else(mktcap &gt;= quantile(mktcap, 0.99), 1, 0),\n    top05 = if_else(mktcap &gt;= quantile(mktcap, 0.95), 1, 0),\n    top10 = if_else(mktcap &gt;= quantile(mktcap, 0.90), 1, 0),\n    top25 = if_else(mktcap &gt;= quantile(mktcap, 0.75), 1, 0)\n  ) |&gt;\n  summarize(\n    total_market_cap =  sum(mktcap),\n    `Largest 1% of stocks` = sum(mktcap[top01 == 1]) / total_market_cap,\n    `Largest 5% of stocks` = sum(mktcap[top05 == 1]) / total_market_cap,\n    `Largest 10% of stocks` = sum(mktcap[top10 == 1]) / total_market_cap,\n    `Largest 25% of stocks` = sum(mktcap[top25 == 1]) / total_market_cap,\n    .groups = \"drop\"\n  ) |&gt;\n  select(-total_market_cap) |&gt; \n  pivot_longer(cols = -date) |&gt;\n  mutate(name = factor(name, levels = c(\n    \"Largest 1% of stocks\", \"Largest 5% of stocks\",\n    \"Largest 10% of stocks\", \"Largest 25% of stocks\"\n  ))) |&gt;\n  ggplot(aes(\n    x = date, \n    y = value, \n    color = name,\n    linetype = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent, limits = c(0, 1)) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Percentage of total market capitalization in largest stocks\"\n  )\n\n\n\n\n\n\n\nFigure 1: We report the aggregate market capitalization of all stocks that belong to the 1, 5, 10, and 25 percent quantile of the largest firms in the monthly cross-section relative to the market capitalization of all stocks during the month.\n\n\n\n\n\nNext, firm sizes also differ across listing exchanges. Stocks’ primary listings were important in the past and are potentially still relevant today. Figure 2 shows that the New York Stock Exchange (NYSE) was and still is the largest listing exchange in terms of market capitalization. More recently, NASDAQ has gained relevance as a listing exchange. Do you know what the small peak in NASDAQ’s market cap around the year 2000 was?\n\ncrsp_monthly |&gt;\n  group_by(date, exchange) |&gt;\n  summarize(mktcap = sum(mktcap),\n            .groups = \"drop_last\") |&gt;\n  mutate(share = mktcap / sum(mktcap)) |&gt;\n  ggplot(aes(\n    x = date, \n    y = share, \n    fill = exchange, \n    color = exchange)) +\n  geom_area(\n    position = \"stack\",\n    stat = \"identity\",\n    alpha = 0.5\n  ) +\n  geom_line(position = \"stack\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, fill = NULL, color = NULL,\n    title = \"Share of total market capitalization per listing exchange\"\n  )\n\n\n\n\n\n\n\nFigure 2: Years are on the horizontal axis and the corresponding share of total market capitalization per listing exchange on the vertical axis.\n\n\n\n\n\nFinally, we consider the distribution of firm size across listing exchanges and create summary statistics. The function summary() does not include all statistics we are interested in, which is why we create the function create_summary() that adds the standard deviation and the number of observations. Then, we apply it to the most current month of our CRSP data on each listing exchange. We also add a row with add_row() with the overall summary statistics.\nThe resulting table shows that firms listed on NYSE in December 2023 are significantly larger on average than firms listed on the other exchanges. Moreover, NASDAQ lists the largest number of firms. This discrepancy between firm sizes across listing exchanges motivated researchers to form breakpoints exclusively on the NYSE sample and apply those breakpoints to all stocks. In the following, we use this distinction to update our portfolio sort procedure.\n\ncreate_summary &lt;- function(data, column_name) {\n  data |&gt;\n    select(value = {{ column_name }}) |&gt;\n    summarize(\n      mean = mean(value),\n      sd = sd(value),\n      min = min(value),\n      q05 = quantile(value, 0.05),\n      q50 = quantile(value, 0.50),\n      q95 = quantile(value, 0.95),\n      max = max(value),\n      n = n()\n    )\n}\n\ncrsp_monthly |&gt;\n  filter(date == max(date)) |&gt;\n  group_by(exchange) |&gt;\n  create_summary(mktcap) |&gt;\n  add_row(crsp_monthly |&gt;\n            filter(date == max(date)) |&gt;\n            create_summary(mktcap) |&gt;\n            mutate(exchange = \"Overall\"))\n\n# A tibble: 4 × 9\n  exchange   mean      sd    min    q05    q50    q95      max     n\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1 AMEX       208.    537.  1.61    3.55   46.9   930.    3921.   155\n2 NASDAQ   12535. 141802.  0.767   5.17  310.  20238. 3785304.  2382\n3 NYSE     21862.  63583. 15.3   179.   4048.  90119.  732872.  1256\n4 Overall  15120. 118288.  0.767   6.80  725.  47107. 3785304.  3793",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "href": "r/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "title": "Size Sorts and p-Hacking",
    "section": "Univariate Size Portfolios with Flexible Breakpoints",
    "text": "Univariate Size Portfolios with Flexible Breakpoints\nIn Univariate Portfolio Sorts, we construct portfolios with a varying number of breakpoints and different sorting variables. Here, we extend the framework such that we compute breakpoints on a subset of the data, for instance, based on selected listing exchanges. In published asset pricing articles, many scholars compute sorting breakpoints only on NYSE-listed stocks. These NYSE-specific breakpoints are then applied to the entire universe of stocks.\nTo replicate the NYSE-centered sorting procedure, we introduce exchanges as an argument in our assign_portfolio() function. The exchange-specific argument then enters in the filter filter(exchange %in% exchanges). For example, if exchanges = 'NYSE' is specified, only stocks listed on NYSE are used to compute the breakpoints. Alternatively, you could specify exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"), which keeps all stocks listed on either of these exchanges. Overall, regular expressions are a powerful tool, and we only touch on a specific case here.\n\nassign_portfolio &lt;- function(\n  data,\n  n_portfolios,\n  exchanges\n) {\n  breakpoints &lt;- data |&gt;\n    filter(exchange %in% exchanges) |&gt;\n    pull(mktcap_lag) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(\n      portfolio = findInterval(\n        mktcap_lag,\n        breakpoints,\n        all.inside = TRUE\n      )\n    ) |&gt;\n    pull(portfolio)\n  \n  assigned_portfolios\n}\n\nNote that the tidyfinance package also provides an assing_portfolio() function, albeit with more flexibility. For the sake of simplicity, we continue to use the function that we just defined.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "href": "r/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "title": "Size Sorts and p-Hacking",
    "section": "Weighting Schemes for Portfolios",
    "text": "Weighting Schemes for Portfolios\nApart from computing breakpoints on different samples, researchers often use different portfolio weighting schemes. So far, we weighted each portfolio constituent by its relative market equity of the previous period. This protocol is called value-weighting. The alternative protocol is equal-weighting, which assigns each stock’s return the same weight, i.e., a simple average of the constituents’ returns. Notice that equal-weighting is difficult in practice as the portfolio manager needs to rebalance the portfolio monthly while value-weighting is a truly passive investment.\nWe implement the two weighting schemes in the function compute_portfolio_returns() that takes a logical argument to weight the returns by firm value. The statement if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)) generates value-weighted returns if value_weighted = TRUE. Additionally, the long-short portfolio is long in the smallest firms and short in the largest firms, consistent with research showing that small firms outperform their larger counterparts. Apart from these two changes, the function is similar to the procedure in Univariate Portfolio Sorts.\n\ncompute_portfolio_returns &lt;- function(\n  n_portfolios = 10,\n  exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n  value_weighted = TRUE,\n  data = crsp_monthly\n) {\n  portfolio_returns &lt;- data |&gt;\n    group_by(date) |&gt;\n    mutate(\n      portfolio = assign_portfolio(\n        data = pick(everything()),\n        n_portfolios = n_portfolios,\n        exchanges = exchanges\n      )\n    ) |&gt;\n    group_by(date, portfolio) |&gt;\n    summarize(\n      ret = if_else(\n        value_weighted,\n        weighted.mean(ret_excess, mktcap_lag),\n        mean(ret_excess)\n      ),\n      .groups = \"drop_last\"\n    ) |&gt;\n    summarize(\n      size_premium = ret[portfolio == min(portfolio)] -\n        ret[portfolio == max(portfolio)]\n    ) |&gt;\n    summarize(size_premium = mean(size_premium))\n  \n  portfolio_returns\n}\n\nTo see how the function compute_portfolio_returns() works, we consider a simple median breakpoint example with value-weighted returns. We are interested in the effect of restricting listing exchanges on the estimation of the size premium. In the first function call, we compute returns based on breakpoints from all listing exchanges. Then, we computed returns based on breakpoints from NYSE-listed stocks.\n\nret_all &lt;- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\nret_nyse &lt;- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\ntibble(\n  Exchanges = c(\"NYSE, NASDAQ & AMEX\", \"NYSE\"),\n  Premium = as.numeric(c(ret_all, ret_nyse)) * 100\n)\n\n# A tibble: 2 × 2\n  Exchanges           Premium\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 NYSE, NASDAQ & AMEX  0.0410\n2 NYSE                 0.129 \n\n\nThe table shows that the size premium is more than 60 percent larger if we consider only stocks from NYSE to form the breakpoint each month. The NYSE-specific breakpoints are larger, and there are more than 50 percent of the stocks in the entire universe in the resulting small portfolio because NYSE firms are larger on average. The impact of this choice is not negligible.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "href": "r/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "title": "Size Sorts and p-Hacking",
    "section": "P-Hacking and Non-standard Errors",
    "text": "P-Hacking and Non-standard Errors\nSince the choice of the listing exchange has a significant impact, the next step is to investigate the effect of other data processing decisions researchers have to make along the way. In particular, any portfolio sort analysis has to decide at least on the number of portfolios, the listing exchanges to form breakpoints, and equal- or value-weighting. Further, one may exclude firms that are active in the finance industry or restrict the analysis to some parts of the time series. All of the variations of these choices that we discuss here are part of scholarly articles published in the top finance journals. We refer to Walter, Weber, and Weiss (2022) for an extensive set of other decision nodes at the discretion of researchers.\nThe intention of this application is to show that the different ways to form portfolios result in different estimated size premiums. Despite the effects of this multitude of choices, there is no correct way. It should also be noted that none of the procedures is wrong, the aim is simply to illustrate the changes that can arise due to the variation in the evidence-generating process (Menkveld et al., n.d.). The term non-standard errors refers to the variation due to (suitable) choices made by researchers. Interestingly, in a large scale study, Menkveld et al. (n.d.) find that the magnitude of non-standard errors are similar than the estimation uncertainty based on a chosen model which shows how important it is to adjust for the seemingly innocent choices in the data preparation and evaluation workflow. \nFrom a malicious perspective, these modeling choices give the researcher multiple chances to find statistically significant results. Yet this is considered p-hacking, which renders the statistical inference due to multiple testing invalid (Harvey, Liu, and Zhu 2016).\nNevertheless, the multitude of options creates a problem since there is no single correct way of sorting portfolios. How should a researcher convince a reader that their results do not come from a p-hacking exercise? To circumvent this dilemma, academics are encouraged to present evidence from different sorting schemes as robustness tests and report multiple approaches to show that a result does not depend on a single choice. Thus, the robustness of premiums is a key feature.\nBelow we conduct a series of robustness tests which could also be interpreted as a p-hacking exercise. To do so, we examine the size premium in different specifications presented in the table p_hacking_setup. The function expand_grid() produces a table of all possible permutations of its arguments. Note that we use the argument data to exclude financial firms and truncate the time series.\n\np_hacking_setup &lt;- expand_grid(\n  n_portfolios = c(2, 5, 10),\n  exchanges = list(\"NYSE\", c(\"NYSE\", \"NASDAQ\", \"AMEX\")),\n  value_weighted = c(TRUE, FALSE),\n  data = parse_exprs(\n    'crsp_monthly; \n     crsp_monthly |&gt; filter(industry != \"Finance\");\n     crsp_monthly |&gt; filter(date &lt; \"1990-06-01\");\n     crsp_monthly |&gt; filter(date &gt;=\"1990-06-01\")'\n  )\n)\n\nTo speed the computation up we parallelize the (many) different sorting procedures, as in Beta Estimation. Finally, we report the resulting size premiums in descending order. There are indeed substantial size premiums possible in our data, in particular when we use equal-weighted portfolios.\n\nn_cores = availableCores() - 1\nplan(multisession, workers = n_cores)\n\np_hacking_setup &lt;- p_hacking_setup |&gt;\n  mutate(size_premium = future_pmap(\n    .l = list(\n      n_portfolios,\n      exchanges,\n      value_weighted,\n      data\n    ),\n    .f = ~ compute_portfolio_returns(\n      n_portfolios = ..1,\n      exchanges = ..2,\n      value_weighted = ..3,\n      data = eval_tidy(..4)\n    )\n  ))\n\np_hacking_results &lt;- p_hacking_setup |&gt;\n  mutate(data = map_chr(data, deparse)) |&gt;\n  unnest(size_premium) |&gt;\n  arrange(desc(size_premium))\np_hacking_results\n\n# A tibble: 48 × 5\n  n_portfolios exchanges value_weighted data             size_premium\n         &lt;dbl&gt; &lt;list&gt;    &lt;lgl&gt;          &lt;chr&gt;                   &lt;dbl&gt;\n1           10 &lt;chr [3]&gt; FALSE          \"filter(crsp_mo…       0.0164\n2           10 &lt;chr [3]&gt; FALSE          \"filter(crsp_mo…       0.0150\n3           10 &lt;chr [3]&gt; FALSE          \"crsp_monthly\"         0.0146\n4           10 &lt;chr [3]&gt; FALSE          \"filter(crsp_mo…       0.0141\n5           10 &lt;chr [3]&gt; TRUE           \"filter(crsp_mo…       0.0110\n# ℹ 43 more rows",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#size-premium-variation",
    "href": "r/size-sorts-and-p-hacking.html#size-premium-variation",
    "title": "Size Sorts and p-Hacking",
    "section": "Size-Premium Variation",
    "text": "Size-Premium Variation\nWe provide a graph in Figure 3 that shows the different premiums. The figure also shows the relation to the average Fama-French SMB (small minus big) premium used in the literature which we include as a dotted vertical line.\n\np_hacking_results |&gt;\n  ggplot(aes(x = size_premium)) +\n  geom_histogram(bins = nrow(p_hacking_results)) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of size premiums for different sorting choices\"\n  ) +\n  geom_vline(aes(xintercept = mean(factors_ff3_monthly$smb)),\n    linetype = \"dashed\"\n  ) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\nFigure 3: The dashed vertical line indicates the average Fama-French SMB premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#key-takeaways",
    "href": "r/size-sorts-and-p-hacking.html#key-takeaways",
    "title": "Size Sorts and p-Hacking",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFirm size is a crucial factor in asset pricing, and sorting stocks by size reveals the size premium, where small-cap stocks tend to outperform large-cap stocks.\nPortfolio returns are sensitive to research design choices like exchange filters, weighting schemes, and the number of portfolios—decisions that can meaningfully shift results.\nMethodological flexibility can lead to non-standard errors and potential p-hacking.\nValidate results by varying assumptions and show that findings hold across multiple specifications.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#exercises",
    "href": "r/size-sorts-and-p-hacking.html#exercises",
    "title": "Size Sorts and p-Hacking",
    "section": "Exercises",
    "text": "Exercises\n\nWe gained several insights on the size distribution above. However, we did not analyze the average size across listing exchanges and industries. Which listing exchanges/industries have the largest firms? Plot the average firm size for the three listing exchanges over time. What do you conclude?\nWe compute breakpoints but do not take a look at them in the exposition above. This might cover potential data errors. Plot the breakpoints for ten size portfolios over time. Then, take the difference between the two extreme portfolios and plot it. Describe your results.\nThe returns that we analyze above do not account for differences in the exposure to market risk, i.e., the CAPM beta. Change the function compute_portfolio_returns() to output the CAPM alpha or beta instead of the average excess return.\nWhile you saw the spread in returns from the p-hacking exercise, we did not show which choices led to the largest effects. Find a way to investigate which choice variable has the largest impact on the estimated size premium.\nWe computed several size premiums, but they do not follow the definition of Fama and French (1993). Which of our approaches comes closest to their SMB premium?",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html",
    "href": "r/replicating-fama-and-french-factors.html",
    "title": "Replicating Fama-French Factors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we provide a replication of the famous Fama-French factor portfolios. The Fama-French factor models are a cornerstone of empirical asset pricing Fama and French (2015). On top of the market factor represented by the traditional CAPM beta, the three-factor model includes the size and value factors to explain the cross section of returns. Its successor, the five-factor model, additionally includes profitability and investment as explanatory factors.\nWe start with the three-factor model. We already introduced the size and value factors in Value and Bivariate Sorts, and their definition remains the same: size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nAfter the replication of the three-factor model, we move to the five factors by constructing the profitability factor RMW (robust-minus-weak) as the difference between the returns of firms with high and low operating profitability and the investment factor CMA (conservative-minus-aggressive) as the difference between firms with high versus low investment rates.\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#data-preparation",
    "href": "r/replicating-fama-and-french-factors.html#data-preparation",
    "title": "Replicating Fama-French Factors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and Compustat as data sources, as we need the same variables to compute the factors in the way Fama and French do it. Hence, there is nothing new below, and we only load data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.1 \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(\n    permno, gvkey, date, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n    select(gvkey, datadate, be, op, inv) |&gt;\n    collect() \n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(date, smb, hml) |&gt;\n  collect()\n\nfactors_ff5_monthly &lt;- tbl(tidy_finance, \"factors_ff5_monthly\") |&gt;\n  select(date, smb, hml, rmw, cma) |&gt;\n  collect()\n\nYet when we start merging our dataset for computing the premiums, there are a few differences to Value and Bivariate Sorts. First, Fama and French form their portfolios in June of year \\(t\\), whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year \\(t+1\\).\nSecond, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year \\(t - 1\\) and the book equity reported in year \\(t-1\\), i.e., the datadate is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity. The other sorting variables are analogously to book equity taken from year \\(t-1\\).\nTo implement all these time lags, we again employ the temporary sorting_date-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year. We ensure this by a call of distinct() at the end of the chunk below.\n\nsize &lt;- crsp_monthly |&gt;\n  filter(month(date) == 6) |&gt;\n  mutate(sorting_date = date %m+% months(1)) |&gt;\n  select(permno, exchange, sorting_date, size = mktcap)\n\nmarket_equity &lt;- crsp_monthly |&gt;\n  filter(month(date) == 12) |&gt;\n  mutate(sorting_date = ymd(str_c(year(date) + 1, \"0701)\"))) |&gt;\n  select(permno, gvkey, sorting_date, me = mktcap)\n\nbook_to_market &lt;- compustat |&gt;\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |&gt;\n  select(gvkey, sorting_date, be) |&gt;\n  inner_join(market_equity, join_by(gvkey, sorting_date)) |&gt;\n  mutate(bm = be / me) |&gt;\n  select(permno, sorting_date, me, bm)\n\nsorting_variables &lt;- size |&gt;\n  inner_join(\n    book_to_market, join_by(permno, sorting_date)\n    ) |&gt;\n  drop_na() |&gt;\n  distinct(permno, sorting_date, .keep_all = TRUE)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "href": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "title": "Replicating Fama-French Factors",
    "section": "Portfolio Sorts",
    "text": "Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on NYSE-specific breakpoints to independently form two portfolios in the size dimension at the median and three portfolios in the dimension of book-to-market at the 30- and 70-percentiles. The sorts for book-to-market require an adjustment to the function in Value and Bivariate Sorts because it would not produce the right breakpoints. Instead of n_portfolios, we now specify percentiles, which takes the sequence of breakpoints as an object specified in the function’s call. Specifically, we give percentiles = c(0.3, 0.7) to the function. Additionally, we perform an inner_join() with our return data to ensure that we only use traded stocks when computing the breakpoints as a first step.\n\nassign_portfolio &lt;- function(\n  data, \n  sorting_variable, \n  percentiles\n) {\n  breakpoints &lt;- data |&gt;\n    filter(exchange == \"NYSE\") |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = c(0, percentiles, 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  assigned_portfolios\n}\n\nportfolios &lt;- sorting_variables |&gt;\n  group_by(sorting_date) |&gt;\n  mutate(\n    portfolio_size = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"size\",\n      percentiles = c(0.5)\n    ),\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"bm\",\n      percentiles = c(0.3, 0.7)\n    )\n  ) |&gt;\n  ungroup() |&gt; \n  select(permno, sorting_date, \n         portfolio_size, portfolio_bm)\n\nAlternatively, you can implement the same portfolio sorts using the assign_portfolio() function from the tidyfinance package, which we omit to avoid repeating almost the same code chunk as above.\nNext, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column sorting_date in our return data by setting the date to sort on to July of \\(t-1\\) if the month is June (of year \\(t\\)) or earlier or to July of year \\(t\\) if the month is July or later.\n\nportfolios &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = case_when(\n    month(date) &lt;= 6 ~ ymd(str_c(year(date) - 1, \"0701\")),\n    month(date) &gt;= 7 ~ ymd(str_c(year(date), \"0701\"))\n  )) |&gt;\n  inner_join(portfolios, join_by(permno, sorting_date))",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#fama-french-three-factor-model",
    "href": "r/replicating-fama-and-french-factors.html#fama-french-three-factor-model",
    "title": "Replicating Fama-French Factors",
    "section": "Fama-French Three-Factor Model",
    "text": "Fama-French Three-Factor Model\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama-French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally.\n\nfactors_replicated &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_bm, date) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\"\n  ) |&gt;\n  group_by(date) |&gt;\n  summarize(\n    smb_replicated = mean(ret[portfolio_size == 1]) -\n      mean(ret[portfolio_size == 2]),\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "href": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "title": "Replicating Fama-French Factors",
    "section": "Replication Evaluation",
    "text": "Replication Evaluation\nIn the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using lm(). If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to 1 (indicating a high correlation), and an adjusted R-squared close to 1 (indicating a high proportion of explained variance).\n\ntest &lt;- factors_ff3_monthly |&gt;\n  inner_join(factors_replicated, join_by(date)) |&gt;\n  mutate(\n    across(c(smb_replicated, hml_replicated), ~round(., 4))\n  )\n\nTo test the success of the SMB factor, we hence run the following regression:\n\nmodel_smb &lt;- lm(smb ~ smb_replicated, data = test)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.020202 -0.001373  0.000019  0.001461  0.015537 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.000165   0.000125   -1.32     0.19    \nsmb_replicated  0.979113   0.004110  238.20   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00346 on 760 degrees of freedom\nMultiple R-squared:  0.987, Adjusted R-squared:  0.987 \nF-statistic: 5.67e+04 on 1 and 760 DF,  p-value: &lt;2e-16\n\n\nThe results for the SMB factor are really convincing, as all three criteria outlined above are met and the coefficient is 0.98 and the R-squared is at 99 percent.\n\nmodel_hml &lt;- lm(hml ~ hml_replicated, data = test)\nsummary(model_hml)\n\n\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02360 -0.00247 -0.00018  0.00202  0.03114 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000407   0.000209    1.95    0.052 .  \nhml_replicated 0.955243   0.006893  138.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00574 on 760 degrees of freedom\nMultiple R-squared:  0.962, Adjusted R-squared:  0.962 \nF-statistic: 1.92e+04 on 1 and 760 DF,  p-value: &lt;2e-16\n\n\nThe replication of the HML factor is also a success, although at a slightly lower coefficient of 0.96 and an R-squared around 96 percent.\nThe evidence hence allows us to conclude that we did a relatively good job in replicating the original Fama-French size and value premiums, although we do not know their underlying code. From our perspective, a perfect match is only possible with additional information from the maintainers of the original data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#fama-french-five-factor-model",
    "href": "r/replicating-fama-and-french-factors.html#fama-french-five-factor-model",
    "title": "Replicating Fama-French Factors",
    "section": "Fama-French Five-Factor Model",
    "text": "Fama-French Five-Factor Model\nNow, let us move to the replication of the five-factor model. We extend the other_sorting_variables table from above with the additional characteristics operating profitability op and investment inv. Note that the drop_na() statement yields different sample sizes as some firms with be values might not have op or inv values.\n\nother_sorting_variables &lt;- compustat |&gt;\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |&gt;\n  select(gvkey, sorting_date, be, op, inv) |&gt;\n  inner_join(market_equity, \n             join_by(gvkey, sorting_date)) |&gt;\n  mutate(bm = be / me) |&gt;\n  select(permno, sorting_date, me, be, bm, op, inv)\n\nsorting_variables &lt;- size |&gt;\n  inner_join(\n    other_sorting_variables, \n    join_by(permno, sorting_date)\n    ) |&gt;\n  drop_na() |&gt;\n  distinct(permno, sorting_date, .keep_all = TRUE)\n\nIn each month, we independently sort all stocks into the two size portfolios. The value, profitability, and investment portfolios, on the other hand, are the results of dependent sorts based on the size portfolios. We then merge the portfolios to the return data for the rest of the year just as above.\n\nportfolios &lt;- sorting_variables |&gt;\n  group_by(sorting_date) |&gt;\n  mutate(\n    portfolio_size = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"size\",\n      percentiles = c(0.5)\n    )) |&gt; \n  group_by(sorting_date, portfolio_size) |&gt; \n  mutate(\n    across(\n      c(bm, op, inv), \n      ~assign_portfolio(\n        data = pick(everything()), \n        sorting_variable = ., \n        percentiles = c(0.3, 0.7)\n      ),\n      .names = \"portfolio_{.col}\"\n    )\n  ) |&gt;\n  ungroup() |&gt; \n  select(\n    permno, sorting_date, \n    portfolio_size, portfolio_bm, portfolio_op, portfolio_inv\n  )\n\nportfolios &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = case_when(\n    month(date) &lt;= 6 ~ ymd(str_c(year(date) - 1, \"0701\")),\n    month(date) &gt;= 7 ~ ymd(str_c(year(date), \"0701\"))\n  )) |&gt;\n  inner_join(portfolios, join_by(permno, sorting_date))\n\nNow, we want to construct each of the factors, but this time the size factor actually comes last because it is the result of averaging across all other factor portfolios. This dependency is the reason why we keep the table with value-weighted portfolio returns as a separate object that we reuse later. We construct the value factor, HML, as above by going long the two portfolios with high book-to-market ratios and shorting the two portfolios with low book-to-market.\n\nportfolios_value &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_bm, date) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), \n    .groups = \"drop\"\n  )\n\nfactors_value &lt;- portfolios_value |&gt;\n  group_by(date) |&gt;\n  summarize(\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )\n\nFor the profitability factor, RMW, we take a long position in the two high profitability portfolios and a short position in the two low profitability portfolios.\n\nportfolios_profitability &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_op, date) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), \n    .groups = \"drop\"\n  ) \n\nfactors_profitability &lt;- portfolios_profitability |&gt;\n  group_by(date) |&gt;\n  summarize(\n    rmw_replicated = mean(ret[portfolio_op == 3]) -\n      mean(ret[portfolio_op == 1])\n  )\n\nFor the investment factor, CMA, we go long the two low investment portfolios and short the two high investment portfolios.\n\nportfolios_investment &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_inv, date) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), \n    .groups = \"drop\"\n  )\n\nfactors_investment &lt;- portfolios_investment |&gt;\n  group_by(date) |&gt;\n  summarize(\n    cma_replicated = mean(ret[portfolio_inv == 1]) -\n      mean(ret[portfolio_inv == 3])\n  )\n\nFinally, the size factor, SMB, is constructed by going long the nine small portfolios and short the nine large portfolios.\n\nfactors_size &lt;- bind_rows(\n  portfolios_value,\n  portfolios_profitability,\n  portfolios_investment\n) |&gt; \n  group_by(date) |&gt;\n  summarize(\n    smb_replicated = mean(ret[portfolio_size == 1]) -\n      mean(ret[portfolio_size == 2])\n  )\n\nWe then join all factors together into one dataframe and construct again a suitable table to run tests for evaluating our replication.\n\nfactors_replicated &lt;- factors_size |&gt;\n  full_join(factors_value, join_by(date)) |&gt;\n  full_join(factors_profitability, join_by(date)) |&gt;\n  full_join(factors_investment, join_by(date))\n\ntest &lt;- factors_ff5_monthly |&gt;\n  inner_join(factors_replicated, join_by(date)) |&gt;\n  mutate(\n    across(c(smb_replicated, hml_replicated, \n             rmw_replicated, cma_replicated), ~round(., 4))\n  )\n\nLet us start the replication evaluation again with the size factor:\n\nmodel_smb &lt;- lm(smb ~ smb_replicated, data = test)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.019396 -0.001875  0.000164  0.001940  0.013435 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.000219   0.000129   -1.71    0.088 .  \nsmb_replicated  0.959323   0.004085  234.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00349 on 736 degrees of freedom\nMultiple R-squared:  0.987, Adjusted R-squared:  0.987 \nF-statistic: 5.51e+04 on 1 and 736 DF,  p-value: &lt;2e-16\n\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient is 0.96 and the R-squared is at 99 percent.\n\nmodel_hml &lt;- lm(hml ~ hml_replicated, data = test)\nsummary(model_hml)\n\n\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.04341 -0.00409 -0.00030  0.00384  0.03453 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000609   0.000287    2.12    0.034 *  \nhml_replicated 0.980391   0.009799  100.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00778 on 736 degrees of freedom\nMultiple R-squared:  0.932, Adjusted R-squared:  0.931 \nF-statistic: 1e+04 on 1 and 736 DF,  p-value: &lt;2e-16\n\n\nThe replication of the HML factor is also a success, although at a slightly higher coefficient of 0.98 and an R-squared around 93 percent.\n\nmodel_rmw &lt;- lm(rmw ~ rmw_replicated, data = test)\nsummary(model_rmw)\n\n\nCall:\nlm(formula = rmw ~ rmw_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.018496 -0.003050 -0.000024  0.003275  0.019058 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.69e-05   1.99e-04    0.19     0.85    \nrmw_replicated 9.50e-01   8.72e-03  108.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00536 on 736 degrees of freedom\nMultiple R-squared:  0.942, Adjusted R-squared:  0.942 \nF-statistic: 1.19e+04 on 1 and 736 DF,  p-value: &lt;2e-16\n\n\nWe are also able to replicate the RMW factor quite well with a coefficient of 0.95 and an R-squared around 94 percent.\n\nmodel_cma &lt;- lm(cma ~ cma_replicated, data = test)\nsummary(model_cma)\n\n\nCall:\nlm(formula = cma ~ cma_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02876 -0.00274  0.00004  0.00260  0.02121 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000523   0.000172    3.05   0.0024 ** \ncma_replicated 0.955140   0.008140  117.33   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00464 on 736 degrees of freedom\nMultiple R-squared:  0.949, Adjusted R-squared:  0.949 \nF-statistic: 1.38e+04 on 1 and 736 DF,  p-value: &lt;2e-16\n\n\nFinally, the CMA factor also replicates well with a coefficient of 0.96 and an R-squared around 95 percent.\nOverall, our approach seems to replicate the Fama-French five-factor models just as well as the three factors.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#key-takeaways",
    "href": "r/replicating-fama-and-french-factors.html#key-takeaways",
    "title": "Replicating Fama-French Factors",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe three-factor model adds size (SMB) and value (HML) to the traditional CAPM, while the five-factor model extends this with profitability (RMW) and investment (CMA) factors.\nThe portfolio construction follows the original Fama-French methodology, including NYSE breakpoints, specific time lags, and sorting rules based on firm characteristics.\nThe quality of replication can be evaluated using regression analysis and confirms strong alignment with the original Fama-French data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#exercises",
    "href": "r/replicating-fama-and-french-factors.html#exercises",
    "title": "Replicating Fama-French Factors",
    "section": "Exercises",
    "text": "Exercises\n\nReplicate the market factor mkt_excess from the factors_ff3_monthly data as the value-weight return of all CRSP firms incorporated in the US and listed on the NYSE, AMEX, or NASDAQ that have a CRSP share code of 10 or 11. Assess your replication effort using linear regressions as above.\nFama and French (1993) claim that their sample excludes firms until they have appeared in Compustat for two years. Implement this additional filter and compare the improvements of your replication effort.\nOn his homepage, Kenneth French provides instructions on how to construct the most common variables used for portfolio sorts. Try to replicate the univariate portfolio sort return time series for E/P (earnings/price) provided on his homepage and evaluate your replication effort using regressions.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#footnotes",
    "href": "r/replicating-fama-and-french-factors.html#footnotes",
    "title": "Replicating Fama-French Factors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that Fama and French (1992) claim to exclude financial firms. To a large extent this happens through using industry format “INDL”, as we do in WRDS, CRSP, and Compustat. Neither the original paper, nor Ken French’s website, or the WRDS replication contains any indication that financial companies are excluded using additional filters such as industry codes.↩︎",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html",
    "href": "r/trace-and-fisd.html",
    "title": "TRACE and FISD",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we dive into the US corporate bond market. Bond markets are far more diverse than stock markets, as most issuers have multiple bonds outstanding simultaneously with potentially very different indentures. This market segment is exciting due to its size (roughly 10 trillion USD outstanding), heterogeneity of issuers (as opposed to government bonds), market structure (mostly over-the-counter trades), and data availability. We introduce how to use bond characteristics from FISD and trade reports from TRACE and provide code to download and clean TRACE in R.\nMany researchers study liquidity in the US corporate bond market O’Hara and Zhou (2021). We do not cover bond returns here, but you can compute them from TRACE data. Instead, we refer to studies on the topic such as Bessembinder et al. (2008), Bai, Bali, and Wen (2019), and Kelly, Palhares, and Pruitt (2021) and a survey by Huang and Shi (2021). Moreover, WRDS includes bond returns computed from TRACE data at a monthly frequency.\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(dbplyr)\nlibrary(RSQLite)\nlibrary(RPostgres)\nCompared to previous chapters, we load the devtools package (Wickham et al. 2022) to source code that we provided to the public via gist.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#bond-data-from-wrds",
    "href": "r/trace-and-fisd.html#bond-data-from-wrds",
    "title": "TRACE and FISD",
    "section": "Bond Data from WRDS",
    "text": "Bond Data from WRDS\nBoth bond databases we need are available on WRDS to which we establish the RPostgres connection described in WRDS, CRSP, and Compustat. Additionally, we connect to our local SQLite-database to store the data we download.\n\nwrds &lt;- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"WRDS_USER\"),\n  password = Sys.getenv(\"WRDS_PASSWORD\")\n)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#mergent-fisd",
    "href": "r/trace-and-fisd.html#mergent-fisd",
    "title": "TRACE and FISD",
    "section": "Mergent FISD",
    "text": "Mergent FISD\nFor research on US corporate bonds, the Mergent Fixed Income Securities Database (FISD) is the primary resource for bond characteristics. There is a detailed manual on WRDS, so we only cover the necessary subjects here. FISD data comes in two main variants, namely, centered on issuers or issues. In either case, the most useful identifiers are CUSIPs. 9-digit CUSIPs identify securities issued by issuers. The issuers can be identified from the first six digits of a security CUSIP, which is also called 6-digit CUSIP. Both stocks and bonds have CUSIPs. This connection would, in principle, allow matching them easily, but due to changing issuer details, this approach only yields small coverage.\nWe use the issue-centered version of FISD to identify the subset of US corporate bonds that meet the standard criteria (Bessembinder, Maxwell, and Venkataraman 2006). The WRDS table fisd_mergedissue contains most of the information we need on a 9-digit CUSIP level. Due to the diversity of corporate bonds, details in the indenture vary significantly. We focus on common bonds that make up the majority of trading volume in this market without diverging too much in indentures.\nThe following chunk connects to the data and selects the bond sample to remove certain bond types that are less commonly used (see, e.g., Dick-Nielsen, Feldhütter, and Lando 2012; O’Hara and Zhou 2021, among many others). In particular, we use the filters listed below. Note that we also treat missing values in these flags.\n\nKeep only senior bonds (security_level = 'SEN').\nExclude bonds which are secured lease obligations (slob = 'N' OR slob IS NULL).\nExclude secured bonds (security_pledge IS NULL).\nExclude asset-backed bonds (asset_backed = 'N' OR asset_backed IS NULL).\nExclude defeased bonds ((defeased = 'N' OR defeased IS NULL) AND defeased_date IS NULL).\nKeep only the bond types US Corporate Debentures ('CDEB'), US Corporate Medium Term Notes ('CMTN'), US Corporate Zero Coupon Notes and Bonds ('CMTZ', 'CZ'), and US Corporate Bank Note ('USBN').\nExclude bonds that are payable in kind ((pay_in_kind != 'Y' OR pay_in_kind IS NULL) AND pay_in_kind_exp_date IS NULL).\nExclude foreign (yankee == \"N\" OR is.na(yankee)) and Canadian issuers (canadian = 'N' OR canadian IS NULL).\nExclude bonds denominated in foreign currency (foreign_currency = 'N').\nKeep only fixed (F) and zero (Z) coupon bonds with additional requirements of fix_frequency IS NULL, coupon_change_indicator = 'N' and annual, semi-annual, quarterly, or monthly interest frequencies.\nExclude bonds that were issued under SEC Rule 144A (rule_144a = 'N').\nExlcude privately placed bonds (private_placement = 'N' OR private_placement IS NULL).\nExclude defaulted bonds (defaulted = 'N' AND filing_date IS NULL AND settlement IS NULL).\nExclude convertible (convertible = 'N'), putable (putable = 'N' OR putable IS NULL), exchangeable (exchangeable = 'N' OR exchangeable IS NULL), perpetual (perpetual = 'N'), or preferred bonds (preferred_security = 'N' OR preferred_security IS NULL).\nExclude unit deal bonds ((unit_deal = 'N' OR unit_deal IS NULL)).\n\n\nfisd_mergedissue_db &lt;- tbl(wrds, I(\"fisd.fisd_mergedissue\"))\n\nfisd &lt;- fisd_mergedissue_db |&gt;\n  filter(\n    security_level == \"SEN\",\n    slob == \"N\" | is.na(slob),\n    is.na(security_pledge),\n    asset_backed == \"N\" | is.na(asset_backed),\n    defeased == \"N\" | is.na(defeased),\n    is.na(defeased_date),\n    bond_type %in% c(\n      \"CDEB\",\n      \"CMTN\",\n      \"CMTZ\",\n      \"CZ\",\n      \"USBN\"\n    ), \n    pay_in_kind != \"Y\" | is.na(pay_in_kind),\n    is.na(pay_in_kind_exp_date),\n    yankee == \"N\" | is.na(yankee),\n    canadian == \"N\" | is.na(canadian),\n    foreign_currency == \"N\",\n    coupon_type %in% c(\n      \"F\",\n      \"Z\"\n    ), \n    is.na(fix_frequency),\n    coupon_change_indicator == \"N\",\n    interest_frequency %in% c(\n      \"0\",\n      \"1\",\n      \"2\",\n      \"4\",\n      \"12\"\n    ),\n    rule_144a == \"N\",\n    private_placement == \"N\" | is.na(private_placement),\n    defaulted == \"N\",\n    is.na(filing_date),\n    is.na(settlement),\n    convertible == \"N\",\n    is.na(exchange),\n    putable == \"N\" | is.na(putable),\n    unit_deal == \"N\" | is.na(unit_deal),\n    exchangeable == \"N\" | is.na(exchangeable),\n    perpetual == \"N\",\n    preferred_security == \"N\" | is.na(preferred_security)\n  ) |&gt; \n  select(\n    complete_cusip, maturity,\n    offering_amt, offering_date,\n    dated_date, \n    interest_frequency, coupon,\n    last_interest_date, \n    issue_id, issuer_id\n  ) |&gt;\n  collect()\n\nWe also pull issuer information from fisd_mergedissuer regarding the industry and country of the firm that issued a particular bond. Then, we filter to include only US-domiciled firms’ bonds. We match the data by issuer_id.\n\nfisd_mergedissuer_db &lt;- tbl(wrds, I(\"fisd.fisd_mergedissuer\")) \n\nfisd_issuer &lt;- fisd_mergedissuer_db |&gt;\n  select(issuer_id, sic_code, country_domicile) |&gt;\n  collect()\n\nfisd &lt;- fisd |&gt;\n  inner_join(fisd_issuer, join_by(issuer_id)) |&gt;\n  filter(country_domicile == \"USA\") |&gt;\n  select(-country_domicile)\n\nTo download the FISD data with the above filters and processing steps, you can use the tidyfinance package. Note that you might have to set the login credentials for WRDS first using set_wrds_credentials().\n\ndownload_data(\"wrds_fisd\")\n\nFinally, we save the bond characteristics to our local database. This selection of bonds also constitutes the sample for which we will collect trade reports from TRACE below.\n\ndbWriteTable(\n  conn = tidy_finance,\n  name = \"fisd\",\n  value = fisd,\n  overwrite = TRUE\n)\n\nThe FISD database also contains other data. The issue-based file contains information on covenants, i.e., restrictions included in bond indentures to limit specific actions by firms (e.g., Handler, Jankowitsch, and Weiss 2021). Moreover, FISD also provides information on bond ratings. We do not need either here.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#trace",
    "href": "r/trace-and-fisd.html#trace",
    "title": "TRACE and FISD",
    "section": "TRACE",
    "text": "TRACE\nThe Financial Industry Regulatory Authority (FINRA) provides the Trade Reporting and Compliance Engine (TRACE). In TRACE, dealers that trade corporate bonds must report such trades individually. Hence, we observe trade messages in TRACE that contain information on the bond traded, the trade time, price, and volume. TRACE comes in two variants: standard and enhanced TRACE. We show how to download and clean enhanced TRACE as it contains uncapped volume, a crucial quantity missing in the standard distribution. Moreover, enhanced TRACE also provides information on the respective parties’ roles and the direction of the trade report. These items become essential in cleaning the messages.\nWhy do we repeatedly talk about cleaning TRACE? Trade messages are submitted within a short time window after a trade is executed (less than 15 minutes). These messages can contain errors, and the reporters subsequently correct them or they cancel a trade altogether. The cleaning needs are described by Dick-Nielsen (2009) in detail, and Dick-Nielsen (2014) shows how to clean the enhanced TRACE data using SAS. We do not go into the cleaning steps here, since the code is lengthy and serves no educational purpose. However, downloading and cleaning enhanced TRACE data is straightforward with our setup.\nThe TRACE database is considerably large. Therefore, we only download subsets of data at once. Specifying too many CUSIPs over a long time horizon will result in very long download times and a potential failure due to the size of the request to WRDS. The size limit depends on many parameters, and we cannot give you a guideline here. If we were working with the complete TRACE data for all CUSIPs above, splitting the data into 100 parts takes roughly two hours using our setup. For the applications in this book, we need data around the Paris Agreement in December 2015 and download the data in ten sets, which we define below.\n\nfisd_cusips &lt;- fisd |&gt;\n  pull(complete_cusip)\n\nfisd_parts &lt;- split(\n  fisd_cusips,\n  rep(1:10, \n      length.out = length(fisd_cusips))\n)\n\nFinally, we run a loop in the same style as in WRDS, CRSP, and Compustat where we download daily returns from CRSP. For each of the CUSIP sets defined above, we call the cleaning function and save the resulting output. We add new data to the existing table for batch two and all following batches.\n\nbatches &lt;- length(fisd_parts)\n\nfor (j in 1:batches) {\n  trace_enhanced &lt;- download_data(\n    type = \"wrds_trace_enhanced\",\n    cusips = fisd_parts[[j]],\n    start_date = ymd(\"2014-01-01\"),\n    end_date = ymd(\"2016-11-30\")\n  )\n\n  dbWriteTable(\n    conn = tidy_finance,\n    name = \"trace_enhanced\",\n    value = trace_enhanced,\n    overwrite = ifelse(j == 1, TRUE, FALSE),\n    append = ifelse(j != 1, TRUE, FALSE)\n  )\n  \n  message(\"Batch \", j, \" out of \", batches, \" done (\", \n          round(j / batches, 2) * 100, \"%)\\n\")\n}\n\nIf you want to download the prepared enhanced TRACE data for selected bonds via the tidyfinance package, you can call, e.g.:\n\ndownload_data(\n  \"wrds_trace_enhanced\",\n  cusips = c(\"00101JAH9\"),\n  start_date = \"2019-01-01\", \n  end_date = \"2021-12-31\"\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#insights-into-corporate-bonds",
    "href": "r/trace-and-fisd.html#insights-into-corporate-bonds",
    "title": "TRACE and FISD",
    "section": "Insights into Corporate Bonds",
    "text": "Insights into Corporate Bonds\nWhile many news outlets readily provide information on stocks and the underlying firms, corporate bonds are not covered frequently. Additionally, the TRACE database contains trade-level information, potentially new to students. Therefore, we provide you with some insights by showing some summary statistics.\nWe start by looking into the number of bonds outstanding over time and compare it to the number of bonds traded in our sample. First, we compute the number of bonds outstanding for each quarter around the Paris Agreement from 2014 to 2016.\n\nbonds_outstanding &lt;- expand_grid(\n  \"date\" = seq(ymd(\"2014-01-01\"), ymd(\"2016-11-30\"), by = \"quarter\"), \n  \"complete_cusip\" = fisd$complete_cusip\n) |&gt; \n  left_join(fisd |&gt; \n              select(complete_cusip, offering_date, maturity), \n            join_by(complete_cusip)) |&gt; \n  mutate(offering_date = floor_date(offering_date),\n         maturity = floor_date(maturity)) |&gt; \n  filter(date &gt;= offering_date & date &lt;= maturity) |&gt; \n  count(date) |&gt; \n  mutate(type = \"Outstanding\")\n\nNext, we look at the bonds traded each quarter in the same period. Notice that we load the complete trace table from our database, as we only have a single part of it in the environment from the download loop above.\n\ntrace_enhanced &lt;- tbl(tidy_finance, \"trace_enhanced\") |&gt;\n  collect()\n\nbonds_traded &lt;- trace_enhanced |&gt; \n  mutate(date = floor_date(trd_exctn_dt, \"quarters\")) |&gt; \n  group_by(date) |&gt; \n  summarize(n = length(unique(cusip_id)),\n            type = \"Traded\",\n            .groups = \"drop\") \n\nFinally, we plot the two time series in Figure 1.\n\nbonds_outstanding |&gt; \n  bind_rows(bonds_traded) |&gt; \n  ggplot(aes(\n    x = date, \n    y = n, \n    color = type, \n    linetype = type\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Number of bonds outstanding and traded each quarter\"\n  )\n\n\n\n\n\n\n\nFigure 1: The number of corporate bonds outstanding each quarter as reported by Mergent FISD and the number of traded bonds from enhanced TRACE between 2014 and end of 2016.\n\n\n\n\n\nWe see that the number of bonds outstanding increases steadily between 2014 and 2016. During our sample period of trade data, we see that the fraction of bonds trading each quarter is roughly 60 percent. The relatively small number of traded bonds means that many bonds do not trade through an entire quarter. This lack of trading activity illustrates the generally low level of liquidity in the corporate bond market, where it can be hard to trade specific bonds. Does this lack of liquidity mean that corporate bond markets are irrelevant in terms of their size? With over 7,500 traded bonds each quarter, it is hard to say that the market is small. However, let us also investigate the characteristics of issued corporate bonds. In particular, we consider maturity (in years), coupon, and offering amount (in million USD).\n\nfisd |&gt;\n  mutate(maturity = as.numeric(maturity - offering_date) / 365,\n         offering_amt = offering_amt / 10^3) |&gt; \n  pivot_longer(cols = c(maturity, coupon, offering_amt),\n               names_to = \"measure\") |&gt;\n  drop_na() |&gt; \n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value)\n  )\n\n# A tibble: 3 × 8\n  measure        mean     sd    min   q05    q50    q95    max\n  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 coupon         5.96   2.66  0     1.95    6      10      39 \n2 maturity       9.57   9.11 -6.24  1.03    7.05   30.0   101.\n3 offering_amt 372.   564.    0.001 0.632 200    1324.  15000 \n\n\nWe see that the average bond in our sample period has an offering amount of over 357 million USD with a median of 200 million USD, which both cannot be considered small. The average bond has a maturity of 10 years and pays around 6 percent in coupons.\nFinally, let us compute some summary statistics for the trades in this market. To this end, we show a summary based on aggregate information daily. In particular, we consider the trade size (in million USD) and the number of trades.\n\ntrace_enhanced |&gt; \n  group_by(trd_exctn_dt) |&gt; \n  summarize(trade_size = sum(entrd_vol_qt * rptd_pr / 100) / 10^6,\n            trade_number = n(),\n            .groups = \"drop\") |&gt; \n  pivot_longer(cols = c(trade_size, trade_number),\n               names_to = \"measure\") |&gt; \n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value)\n  )\n\n# A tibble: 2 × 8\n  measure        mean    sd   min    q05    q50    q95    max\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 trade_number 25914. 5444. 439   17844. 26051  34383. 40839 \n2 trade_size   12968. 3577.  17.2  6128. 13421. 17850. 21312.\n\n\nOn average, nearly 26 billion USD of corporate bonds are traded daily in nearly 13,000 transactions. We can hence conclude that the corporate bond market is indeed significant in terms of trading volume and activity.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#key-takeaways",
    "href": "r/trace-and-fisd.html#key-takeaways",
    "title": "TRACE and FISD",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe US corporate bond market is large, diverse, and primarily trades over-the-counter, making it an important yet complex subject of financial research.\nThe Mergent FISD database on WRDS provides detailed bond characteristics, which are essential for selecting a representative sample of US corporate bonds.\nEnhanced TRACE data includes uncapped trade volumes and dealer roles, offering valuable insights into bond market liquidity and trade execution.\nCleaning TRACE data is crucial, as trades may be corrected or canceled shortly after reporting, but automated functions in the tidyfinance R package simplify this task.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#exercises",
    "href": "r/trace-and-fisd.html#exercises",
    "title": "TRACE and FISD",
    "section": "Exercises",
    "text": "Exercises\n\nCompute the amount outstanding across all bonds over time. Make sure to subtract all matured bonds. How would you describe the resulting plot?\nCompute the number of days each bond is traded (accounting for the bonds’ maturities and issuances). Start by looking at the number of bonds traded each day in a graph similar to the one above. How many bonds trade on more than 75 percent of trading days?\nWRDS provides more information from Mergent FISD such as ratings in the table fisd_ratings. Download the ratings table and plot the distribution of ratings for the different rating providers. How would you map the different providers to a common numeric rating scale?",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html",
    "href": "r/discounted-cash-flow-analysis.html",
    "title": "Discounted Cash Flow Analysis",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we address a fundamental question: What is the value of a company? Company valuation is a critical tool that helps us determine the economic value of a business. Understanding a company’s value is essential, whether it is for investment decisions, mergers and acquisitions, or financial reporting. Valuation is not just about assigning a number, as it is also a framework for making informed decisions. For example, investors use valuation to identify whether a stock is under- or overvalued, and companies rely on valuation for strategic decisions, like pricing an acquisition or preparing for an IPO.\nCompany valuation methods broadly fall into three categories:\nWe focus on Discounted Cash Flow (DCF) analysis, an income-based approach, because it captures three crucial aspects of valuation: First, DCF explicitly accounts for the time value of money - the principle that a dollar today is worth more than a dollar in the future. By discounting future cash flows to present value with the appropriate discount rate, we incorporate both time preferences and risk. Second, DCF is inherently forward-looking, making it particularly suitable for companies where historical performance may not reflect future potential. This characteristic is especially relevant when valuing growth companies or analyzing new business opportunities. Third, DCF analysis is flexible enough to accommodate various business models and capital structures, making it applicable across different industries and company sizes.\nIn our exposition of the DCF valuation framework, we focus on its three key components:\nWe make a few simplifying assumptions due to the diverse nature of businesses, which a simple model cannot cover. After all, there are entire textbooks written just on valuation. In particular, we assume that firms only conduct operating activities (i.e., financial statements do not include non-operating items), implying that firms do not own any assets that do not produce operating cash flows. Otherwise, you must value these non-operating activities separately for many real-world examples.\nIn this chapter, we rely on the following packages to build a simple DCF analysis:\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(scales)\nlibrary(fmpapi)",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#prepare-data",
    "href": "r/discounted-cash-flow-analysis.html#prepare-data",
    "title": "Discounted Cash Flow Analysis",
    "section": "Prepare Data",
    "text": "Prepare Data\nBefore we can perform a DCF analysis, we need historical financial data to inform our forecasts. We use the Financial Modeling Prep (FMP) API to download financial statements. The fmpapi package provides a convenient interface for accessing this data in a tidy format.\n\nsymbol &lt;- \"MSFT\"\n\nincome_statements &lt;- fmp_get(\n  \"income-statement\",\n  symbol,\n  list(period = \"annual\", limit = 5)\n)\n\ncash_flow_statements &lt;- fmp_get(\n  \"cash-flow-statement\",\n  symbol,\n  list(period = \"annual\", limit = 5)\n)\n\nOur analysis centers on Free Cash Flow (FCF), which represents the cash available to all investors after the company has covered its operational needs and capital investments. We calculate FCF using the following formula:\n\\[\\text{FCF} = \\text{EBIT} + \\text{Depreciation \\& Amortization} - \\text{Taxes} + \\Delta \\text{Working Capital} - \\text{CAPEX}\\]\nEach component of this formula serves a specific purpose in capturing the company’s cash-generating ability:\n\nEBIT (Earnings Before Interest and Taxes) measures core operating profit\nDepreciation & Amortization accounts for non-cash expenses\nTaxes reflect actual cash payments to tax authorities\nChanges in Working Capital capture cash tied up in operations\nCapital Expenditures (CAPEX) represent investments in long-term assets\n\nWe can implement this calculation by combining and transforming our financial statement data. Note that we also arrange by year, which is important for some of the following code chunks.\n\ndcf_data &lt;- income_statements |&gt;\n  mutate(\n    fiscal_year = as.integer(fiscal_year),\n    ebit = net_income + income_tax_expense + interest_expense - interest_income\n  ) |&gt;\n  select(\n    year = fiscal_year,\n    ebit,\n    revenue,\n    depreciation_and_amortization,\n    taxes = income_tax_expense\n  ) |&gt;\n  left_join(\n    cash_flow_statements |&gt;\n      mutate(\n        fiscal_year = as.integer(fiscal_year)\n      ) |&gt;\n      select(\n        year = fiscal_year,\n        delta_working_capital = change_in_working_capital,\n        capex = capital_expenditure\n      ),\n    join_by(year)\n  ) |&gt;\n  mutate(\n    fcf = ebit +\n      depreciation_and_amortization -\n      taxes +\n      delta_working_capital -\n      capex\n  ) |&gt;\n  arrange(year)",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#forecasting-free-cash-flow",
    "href": "r/discounted-cash-flow-analysis.html#forecasting-free-cash-flow",
    "title": "Discounted Cash Flow Analysis",
    "section": "Forecasting Free Cash Flow",
    "text": "Forecasting Free Cash Flow\nAfter calculating historical FCF, we need to project it into the future. While historical data provides a foundation, forecasting requires both quantitative analysis and qualitative judgment. We use a ratio-based approach that links all FCF components to revenue growth, making our forecasts more tractable. This is another crucial assumption for our exposition that will not hold in reality. Thus, you must put more thought into these forecast ratios and their dynamics over time.\nFirst, we express each FCF component as a ratio relative to revenue. This standardization helps identify trends and makes forecasting more systematic. Figure 1 shows the historical evolution of these key financial ratios.\n\ndcf_data &lt;- dcf_data |&gt;\n  mutate(\n    revenue_growth = revenue / lag(revenue) - 1,\n    operating_margin = ebit / revenue,\n    da_margin = depreciation_and_amortization / revenue,\n    taxes_to_revenue = taxes / revenue,\n    delta_working_capital_to_revenue = delta_working_capital / revenue,\n    capex_to_revenue = capex / revenue\n  )\n\ndcf_data |&gt;\n  pivot_longer(cols = c(operating_margin:capex_to_revenue)) |&gt;\n  ggplot(aes(x = year, y = value, color = name)) +\n  geom_line() +\n  scale_x_continuous(breaks = pretty_breaks()) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Key financial ratios of Microsoft between 2021 and 2025\"\n  )\n\n\n\n\n\n\n\nFigure 1: Ratios are based on financial statements provided through the FMP API.\n\n\n\n\n\nThe operating margin, for instance, represents how much of each revenue dollar translates into operating profit (EBIT), while the CAPEX-to-revenue ratio indicates the company’s investment intensity.\nFor our DCF analysis, we need to project these ratios into the future. These projections should reflect both historical patterns and forward-looking considerations such as: Industry trends and competitive dynamics, company-specific growth initiatives, expected operational efficiency improvements, planned capital investments, or working capital management strategies. Overall, there is a lot to consider in practice. However, forecast ratios make this process tractable.\nAnother crucial question is the length of the forecast period. We use five years below ad-hoc, but you want to make detailed projections for each year when the company undergoes significant changes. As long as forecast ratios change, you need to make explicit forecasts. Once the company reaches a steady state, you can switch to computing the continuation value discussed later.\nWe demonstrate this forecasting approach in Figure 2. Note that our forecast ratio dynamics just serve as an example and are not grounded in deeper economic analysis.\n\ndcf_data_forecast_ratios &lt;- tribble(\n  ~year, ~operating_margin, ~da_margin, ~taxes_to_revenue, ~delta_working_capital_to_revenue, ~capex_to_revenue,\n  2026, 0.41, 0.09, 0.08, 0.001, -0.2,\n  2027, 0.42, 0.09, 0.07, 0.001, -0.22,\n  2028, 0.43, 0.09, 0.06, 0.001, -0.2,\n  2029, 0.44, 0.09, 0.06, 0.001, -0.18,\n  2030, 0.45, 0.09, 0.06, 0.001, -0.16\n) |&gt; \n  mutate(type = \"Forecast\")\n\ndcf_data &lt;- dcf_data |&gt;\n  mutate(type = \"Realized\") |&gt;\n  bind_rows(dcf_data_forecast_ratios) |&gt;\n  mutate(type = factor(type, levels = c(\"Realized\", \"Forecast\")))\n\ndcf_data |&gt;\n  pivot_longer(cols = c(operating_margin:capex_to_revenue)) |&gt;\n  ggplot(aes(x = year, y = value, color = name, linetype = type)) +\n  geom_line() +\n  scale_x_continuous(breaks = pretty_breaks()) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"Key financial ratios and ad-hoc forecasts of Microsoft between 2021 and 2030\"\n  ) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\nFigure 2: Historical ratios are based on financial statements provided through the FMP API, while forecasts are manually defined.\n\n\n\n\n\nThe final step in our FCF forecast is projecting revenue growth. While there are multiple approaches to this task, we demonstrate a GDP-based method that links company growth to macroeconomic forecasts.\nWe use GDP growth forecasts from the IMF World Economic Outlook (WEO) database as our baseline. The WEO provides comprehensive economic projections, though it is important to note that these forecasts are periodically revised as new data becomes available. We manually collect these forecasts and store them in a tibble.\n\ngdp_growth &lt;- tibble(\n  year = 2021:2030,\n  gdp_growth = c(\n    0.06055,\n    0.02512,\n    0.02887,\n    0.02765,\n    0.02153,\n    0.02028,\n    0.02120,\n    0.02122,\n    0.02122,\n    0.02122\n  )\n)\n\ndcf_data &lt;- dcf_data |&gt;\n  left_join(gdp_growth, join_by(year))\n\nOur approach models revenue growth as a linear function of GDP growth. This relation captures the intuition that company revenues often move in tandem with broader economic activity, though usually with different sensitivity. Let us estimate the model with historical data.\n\nrevenue_growth_model &lt;- dcf_data |&gt;\n  lm(revenue_growth ~ gdp_growth, data = _) |&gt;\n  coefficients()\n\ndcf_data &lt;- dcf_data |&gt;\n  mutate(\n    revenue_growth_modeled = revenue_growth_model[1] +\n      revenue_growth_model[2] * gdp_growth,\n    revenue_growth = if_else(\n      type == \"Forecast\",\n      revenue_growth_modeled,\n      revenue_growth\n    )\n  )\n\nThe model estimates two parameters: (i) an intercept that captures the company’s baseline growth, and (ii) a slope coefficient that measures the company’s sensitivity to GDP changes. Then, we can use the model and the growth forecasts to make revenue growth projections. We visualize the historical and projected growth rates using this approach in Figure 3:\n\ndcf_data |&gt;\n  filter(year &gt;= 2021) |&gt;\n  pivot_longer(cols = c(revenue_growth, gdp_growth)) |&gt;\n  ggplot(aes(x = year, y = value, color = name, linetype = type)) +\n  geom_line() +\n  scale_x_continuous(breaks = pretty_breaks()) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"GDP growth and Microsoft's revenue growth and forecasts between 2021 and 2030\"\n  ) +\n  theme(legend.position = \"right\")\n\nWarning: Removed 1 row containing missing values or values outside the scale\nrange (`geom_line()`).\n\n\n\n\n\n\n\n\nFigure 3: Realized revenue growth rates are based on financial statements provided through the FMP API, while forecasts are modeled unsing IMF WEO forecasts.\n\n\n\n\n\nWhile more sophisticated approaches exist (e.g., proprietary analyst forecasts or bottom-up market analyses), this method provides a transparent and data-driven starting point for revenue projections.\nWith all components in place - revenue growth projections and forecast ratios - we can now calculate our FCF forecasts. We must first convert our growth rates into revenue projections and then apply our forecast ratios to compute each FCF component.\n\ndcf_data$revenue_growth[1] &lt;- 0\ndcf_data$revenue &lt;- dcf_data$revenue[1] * cumprod(1 + dcf_data$revenue_growth)\n\ndcf_data &lt;- dcf_data |&gt;\n  mutate(\n    ebit = operating_margin * revenue,\n    depreciation_and_amortization = da_margin * revenue,\n    taxes = taxes_to_revenue * revenue,\n    delta_working_capital = delta_working_capital_to_revenue * revenue,\n    capex = capex_to_revenue * revenue,\n    fcf = ebit +\n      depreciation_and_amortization -\n      taxes +\n      delta_working_capital -\n      capex\n  )\n\nWe visualize the resulting FCF projections in Figure 4.\n\ndcf_data |&gt;\n  ggplot(aes(x = year, y = fcf / 1e9)) +\n  geom_col(aes(fill = type)) +\n  scale_x_continuous(breaks = pretty_breaks()) +\n  labs(\n    x = NULL,\n    y = \"Free Cash Flow (in B USD)\",\n    fill = NULL,\n    title = \"Actual and predicted free cash flow for Microsoft from 2021 to 2030\"\n  )\n\n\n\n\n\n\n\nFigure 4: Realized growth rates are based on financial statements provided through the FMP API, while forecasts are manually defined.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#continuation-value",
    "href": "r/discounted-cash-flow-analysis.html#continuation-value",
    "title": "Discounted Cash Flow Analysis",
    "section": "Continuation Value",
    "text": "Continuation Value\nA key component of DCF analysis is the continuation value (or terminal value), representing the company’s value beyond the explicit forecast period. This value often constitutes the majority of the total valuation, making its estimation particularly important. You can compute the continuation value once the company has reached its steady state.\nThe most common approach is the Perpetuity Growth Model (or Gordon Growth Model), which assumes FCF grows at a constant rate indefinitely. The formula for this model is:\n\\[TV_{T} = \\frac{FCF_{T+1}}{r - g},\\]\nwhere \\(TV_{T}\\) is the terminal value at time \\(T\\), \\(FCF_{T+1}\\) is the free cash flow in the first year after our forecast period, \\(r\\) is the discount rate (typically WACC, see below), and \\(g\\) is the perpetual growth rate. A common mistake is to ignore the time shift in the model. You must compute \\(FCF_{T+1}\\), which is equal to \\(FCF_{T}\\cdot(1+g)\\)\nThe perpetual growth rate \\(g\\) should reflect the long-term economic growth potential. A common benchmark is the long-term GDP growth rate, as few companies can sustainably grow faster than the overall economy indefinitely. Exceeding GDP growth indefinitely also implies that the whole economy eventually consists of one company. For example, the last 20 years of GDP growth is a sensible assumption (the nominal growth rate is 4% for the US).\nLet us implement the Perpetuity Growth Model:\n\ncompute_terminal_value &lt;- function(last_fcf, growth_rate, discount_rate) {\n  last_fcf * (1 + growth_rate) / (discount_rate - growth_rate)\n}\n\nlast_fcf &lt;- tail(dcf_data$fcf, 1)\nterminal_value &lt;- compute_terminal_value(last_fcf, 0.04, 0.08)\nterminal_value / 1e9\n\n[1] 10607\n\n\nNote that while we use the Perpetuity Growth Model here, practitioners often cross-check their estimates with alternative methods like the exit multiple approach, which bases the terminal value on comparable company valuations.1",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#discount-rate",
    "href": "r/discounted-cash-flow-analysis.html#discount-rate",
    "title": "Discounted Cash Flow Analysis",
    "section": "Discount Rate",
    "text": "Discount Rate\nThe final component of our DCF analysis involves discounting future cash flows to present value. We typically use the Weighted Average Cost of Capital (WACC) as the discount rate, representing the blended cost of financing for all company stakeholders. The WACC is the correct rate to discount cash flows distributed to both debt and equity holders, which is the case in our model as we use free cash flows.\nThe WACC formula combines the costs of equity and debt financing:\n\\[WACC = \\frac{E}{D+E} \\cdot r^E + \\frac{D}{D+E} \\cdot r^D \\cdot (1 - \\tau),\\]\nwhere \\(E\\) is the market value of the company’s equity with required return \\(r^E\\), \\(D\\) is the market value of the company’s debt with pre-tax return \\(r^D\\), and \\(\\tau\\) is the tax rate.\nWhile you can often find estimates of WACC from financial databases or analysts’ reports, you may need to calculate it yourself. Let us walk through the practical steps to estimate WACC using real-world data:\n\n\\(E\\) is typically measured as the market value of the company’s equity. One common approach is to use market capitalization from the stock exchange.\n\\(D\\) is often measured using the book value of the company’s debt. While this might not perfectly reflect market conditions, it is a practical starting point when market data is unavailable. Moreover, it is close to correct without default.\nThe Capital Asset Pricing Model (CAPM) is a popular method to estimate the cost of equity \\(r^E\\). It considers the risk-free rate, the equity risk premium, and the company’s risk exposure (i.e., beta). For a detailed guide estimating the CAPM, we refer to Chapter Capital Asset Pricing Model.\nThe return on debt \\(r^D\\) can also be estimated in different ways. For instance, effective interest rates can be calculated as the ratio of interest expense to total debt from financial statements. This gives you a real-world measure of what the company is currently paying. Alternatively, you can look up corporate bond spreads for companies in the same rating group.\n\nIf you would rather not estimate WACC manually, resources are available to help you find industry-specific discount rates. One of the most widely used sources is Aswath Damodaran’s database. He maintains an extensive database that provides a wealth of financial data, including estimated discount rates, cash flows, growth rates, multiples, and more. For example, if you are analyzing a company in the Computer Services sector, as we do here, you can look up the industry’s average WACC and use it as a benchmark for your analysis. The following code chunk downloads the WACC data and extracts the value for this industry:\n\nlibrary(readxl)\n\nfile &lt;- tempfile(fileext = \"xls\")\n\nurl &lt;- \"https://pages.stern.nyu.edu/~adamodar/pc/datasets/wacc.xls\"\ndownload.file(url, file)\nwacc_raw &lt;- read_xls(file, sheet = 2, skip = 18)\nunlink(file)\n\nwacc &lt;- wacc_raw |&gt;\n  filter(`Industry Name` == \"Computer Services\") |&gt;\n  pull(`Cost of Capital`)",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#computing-enterprise-value",
    "href": "r/discounted-cash-flow-analysis.html#computing-enterprise-value",
    "title": "Discounted Cash Flow Analysis",
    "section": "Computing Enterprise Value",
    "text": "Computing Enterprise Value\nHaving established all components, we can now compute the total company value (given that there are no non-operating activities). The enterprise value combines two elements:\n\nThe present value of cash flows during the explicit forecast period and\nThe present value of the continuation (or terminal) value.\n\nThis is expressed mathematically as:\n\\[\n\\text{Total DCF Value} = \\sum_{t=1}^{T} \\frac{\\text{FCF}t}{(1 + \\text{WACC})^t} + \\frac{\\text{TV}_{T}}{(1 + \\text{WACC})^T},\n\\]\nwhere \\(T\\) is the length of our forecast period. Let us implement this calculation in a simple function that takes the WACC and growth rate as input. Then, we present an example at the values discussed above.\n\ncompute_dcf &lt;- function(wacc, growth_rate) {\n  free_cash_flow &lt;- dcf_data$fcf\n  last_fcf &lt;- tail(free_cash_flow, 1)\n  terminal_value &lt;- compute_terminal_value(last_fcf, growth_rate, wacc)\n\n  years &lt;- length(free_cash_flow)\n  present_value_fcf &lt;- free_cash_flow / (1 + wacc)^(1:years)\n  present_value_tv &lt;- terminal_value / (1 + wacc)^years\n  total_dcf_value &lt;- sum(present_value_fcf) + present_value_tv\n  total_dcf_value\n}\n\ncompute_dcf(wacc, 0.04) / 1e9\n\n[1] 5169\n\n\nNote that this valuation represents an enterprise value - the total value of the company’s operations. To arrive at the equity value, we need to subtract net debt (total debt minus cash and equivalents).",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#sensitivity-analysis",
    "href": "r/discounted-cash-flow-analysis.html#sensitivity-analysis",
    "title": "Discounted Cash Flow Analysis",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\nDCF valuation is only as robust as its underlying assumptions. Given the inherent uncertainty in forecasting, it is crucial to understand how changes in key inputs affect our valuation.\nWhile we could examine sensitivity to various inputs like operating margins or capital expenditure ratios, we focus on two critical drivers for our exposition:\n\nThe perpetual growth rate, which determines long-term value creation and\nThe WACC, which affects how we value future cash flows.\n\nLet us implement a sensitivity analysis that varies these two parameters:\n\nwacc_range &lt;- seq(0.06, 0.08, by = 0.01)\ngrowth_rate_range &lt;- seq(0.02, 0.04, by = 0.01)\n\nsensitivity &lt;- expand_grid(\n  wacc = wacc_range,\n  growth_rate = growth_rate_range\n) |&gt;\n  mutate(value = pmap_dbl(list(wacc, growth_rate), compute_dcf))\n\nsensitivity |&gt;\n  mutate(value = round(value / 1e9, 0)) |&gt;\n  ggplot(aes(x = wacc, y = growth_rate, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = comma(value)), color = \"white\") +\n  scale_x_continuous(labels = percent) +\n  scale_y_continuous(labels = percent) +\n  scale_fill_continuous(labels = comma) +\n  labs(\n    x = \"WACC\",\n    y = \"Perpetual growth rate\",\n    fill = \"Enterprise value\",\n    title = \"Enterprise value of Microsoft for different WACC and growth rate scenarios\"\n  ) +\n  guides(fill = guide_colorbar(barwidth = 15, barheight = 0.5))\n\n\n\n\n\n\n\nFigure 5: The enterprise values combine data from FMP API, ad-hoc forecasts of financial ratios, and IMF WEO growth forecasts.\n\n\n\n\n\nFigure 5 reveals several key insights about our valuation: The valuation is highly sensitive to both WACC and growth assumptions, as small changes in either parameter can lead to substantial changes in enterprise value. Moreover, the relation between these parameters and company value is non-linear as the impact of growth rate changes becomes more pronounced at lower WACCs.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#from-enterprise-to-equity-value",
    "href": "r/discounted-cash-flow-analysis.html#from-enterprise-to-equity-value",
    "title": "Discounted Cash Flow Analysis",
    "section": "From Enterprise to Equity Value",
    "text": "From Enterprise to Equity Value\nAs we have discussed, our DCF analysis yields the value of the company’s operations. We have assumed that there are no non-operating assets. Now, we explicitly consider their existence and show you how to compute the equity value belonging to shareholders.\nNon-operating assets are not essential to operations but could, in some cases, generate income (e.g., marketable securities, vacant land, idle equipment). If they exist, you must restate the financial statements to exclude the impact of these non-operating assets. With their effects removed, you can conduct the DCF analysis presented above. Afterwards, you value these non-operating assets and add them to the DCF value to arrive at the company’s enterprise value.\nSecond, we want to discuss equity value. As you saw in the computation of the WACC, free cash flows go to both debt and equity holders. Hence, we must consider the share of the enterprise value that goes to debt. In theory, the value of debt is the market value of total debt, but in practice, typically book debt. This value has to be subtracted from enterprise value.\nCombining these adjustments, we can compute the equity value that belongs to shareholders as:\n\\[\\text{Equity Value} = \\text{DCF Value} + \\text{Non-Operating Assets} - \\text{Value of Debt}\\]\nWe leave it as an exercise to calculate the equity value using the DCF value from above.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#key-takeaways",
    "href": "r/discounted-cash-flow-analysis.html#key-takeaways",
    "title": "Discounted Cash Flow Analysis",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nFree Cash Flow can be calculated by transforming financial statement data into standardized ratios linked to company revenue.\nForecasting future cash flows requires both historical financial data and macroeconomic projections such as GDP growth rates.\nThe terminal value represents long-term company value and can be estimated using the perpetuity growth model.\nThe WACC is used as the discount rate and reflects the cost of financing from both equity and debt.\nA DCF model combines present values of projected free cash flows and terminal value to estimate enterprise value.\nSensitivity analysis reveals how small changes in WACC or growth assumptions can significantly impact company valuation.\nTo determine equity value, subtract net debt from enterprise value and adjust for any non-operating assets or liabilities.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#exercises",
    "href": "r/discounted-cash-flow-analysis.html#exercises",
    "title": "Discounted Cash Flow Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nDownload financial statements for another company of your choice and compute its historical Free Cash Flow. Compare the results with the Microsoft example from this chapter.\nCreate a function that automatically generates FCF forecasts using different sets of ratio assumptions. Use it to create alternative scenarios for Microsoft.\nImplement an exit multiple approach for terminal value calculation and compare the results with the perpetuity growth method.\nExtend the sensitivity analysis to include operating margin assumptions. Create a visualization showing how changes in margins affect the final valuation.\nCalculate Microsoft’s equity value by adjusting the DCF value as described above.",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/discounted-cash-flow-analysis.html#footnotes",
    "href": "r/discounted-cash-flow-analysis.html#footnotes",
    "title": "Discounted Cash Flow Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee (corporatefinanceinstitute.com/)[https://corporatefinanceinstitute.com/resources/valuation/exit-multiple/)] for an intuitive explanation of the exit multiple approach.↩︎",
    "crumbs": [
      "R",
      "Getting Started",
      "Discounted Cash Flow Analysis"
    ]
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "This website is the online version of Tidy Finance with R, a book published via Chapman & Hall/CRC. The book is the result of a joint effort of Christoph Scheuch, Stefan Voigt, and Patrick Weiss.\nWe are grateful for any kind of feedback on every aspect of the book. So please get in touch with us via contact@tidy-finance.org if you spot typos, discover any issues that deserve more attention, or if you have suggestions for additional chapters and sections. Additionally, let us know if you found the text helpful. We look forward to hearing from you!\n\n\n\n\n\n\nSupport Tidy Finance\n\n\n\nBuy our book via your preferred vendor or support us with coffee here.\n\n\n\n\nFinancial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future.\n\n\n\nWe write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from the undergraduate to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed.\n\n\n\n\nThe book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common datasets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet, the data chapters provide an important background necessary for data management in all other chapters.\n\n\n\nThis book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham, Çetinkaya-Rundel, and Grolemund (2023) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).\n\n\n\n\nWe believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019).\n\n\n\nAs you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80 percent of data analysis is spent on preparing data. By tidying data, we want to structure datasets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021), and lubridate (Grolemund and Wickham 2011).\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book we use the native pipe |&gt;, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %&gt;% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to this blog post second edition by Hadley Wickham.\n\n\n\n\n\n\nWe met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is an independent data science and business intelligence expert, currently serving as an external lecturer at Humboldt University of Berlin and as a summer school instructor at the Barcelona School of Economics. Previously, he was the Head of AI, Director of Product, and Head of BI & Data Science at the social trading platform wikifolio.com. He also was an external lecturer at the Vienna University of Economics and Business (WU), where he obtained his PhD in finance as part of the Vienna Graduate School of Finance (VGSF).\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in leading journals in financial economics.\n\n\n\n\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows:\n\nScheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237.\n\n@book{Scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org/r/},\n  doi = {https://doi.org/10.1201/b23237}\n}\n\n\n\nThis book represents a snapshot of research practices and available data at a particular time. However, time does not stop. As you read this text, there is new data, packages used here have changed, and research practices might be updated. We as authors of Tidy Finance are committed to staying up-to-date and keeping up with the newest developments. Therefore, you can expect updates to Tidy Finance on a continuous basis. The best way for you to monitor the ongoing developments, is to check our online Changelog frequently.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#why-does-this-book-exist",
    "href": "r/index.html#why-does-this-book-exist",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "Financial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#who-should-read-this-book",
    "href": "r/index.html#who-should-read-this-book",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "We write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from the undergraduate to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#what-will-you-learn",
    "href": "r/index.html#what-will-you-learn",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "The book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common datasets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet, the data chapters provide an important background necessary for data management in all other chapters.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#what-wont-you-learn",
    "href": "r/index.html#what-wont-you-learn",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "This book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham, Çetinkaya-Rundel, and Grolemund (2023) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#why-r",
    "href": "r/index.html#why-r",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "We believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019).",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#why-tidy",
    "href": "r/index.html#why-tidy",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "As you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80 percent of data analysis is spent on preparing data. By tidying data, we want to structure datasets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021), and lubridate (Grolemund and Wickham 2011).\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book we use the native pipe |&gt;, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %&gt;% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to this blog post second edition by Hadley Wickham.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#about-the-authors",
    "href": "r/index.html#about-the-authors",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "We met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is an independent data science and business intelligence expert, currently serving as an external lecturer at Humboldt University of Berlin and as a summer school instructor at the Barcelona School of Economics. Previously, he was the Head of AI, Director of Product, and Head of BI & Data Science at the social trading platform wikifolio.com. He also was an external lecturer at the Vienna University of Economics and Business (WU), where he obtained his PhD in finance as part of the Vienna Graduate School of Finance (VGSF).\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in leading journals in financial economics.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#license",
    "href": "r/index.html#license",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "This book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows:\n\nScheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237.\n\n@book{Scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org/r/},\n  doi = {https://doi.org/10.1201/b23237}\n}",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#future-updates-and-changes",
    "href": "r/index.html#future-updates-and-changes",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "This book represents a snapshot of research practices and available data at a particular time. However, time does not stop. As you read this text, there is new data, packages used here have changed, and research practices might be updated. We as authors of Tidy Finance are committed to staying up-to-date and keeping up with the newest developments. Therefore, you can expect updates to Tidy Finance on a continuous basis. The best way for you to monitor the ongoing developments, is to check our online Changelog frequently.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/hex-sticker.html",
    "href": "r/hex-sticker.html",
    "title": "Hex Sticker",
    "section": "",
    "text": "library(hexSticker)\n\nsticker(\"images/logo-website.png\", \n        package = \"Tidy Finance\", \n        p_size = 20, p_color = \"black\",\n        s_x = 1, s_y = 0.75, s_width = 0.7, s_height = 0.7, asp = 0.9,\n        h_color = \"#3b9ab2\",\n        h_fill = \"white\",\n        url = \"tidy-finance.org\",\n        filename = \"images/hex-sticker.png\")\n\n\n\n\nTidy Finance HEX Sticker",
    "crumbs": [
      "R",
      "Appendix",
      "Hex Sticker"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html",
    "href": "r/difference-in-differences.html",
    "title": "Difference in Differences",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we illustrate the concept of difference in differences (DiD) estimators by evaluating the effects of climate change regulation on the pricing of bonds across firms. DiD estimators are typically used to recover the treatment effects of natural or quasi-natural experiments that trigger sharp changes in the environment of a specific group. Instead of looking at differences in just one group (e.g., the effect in the treated group), DiD investigates the treatment effects by looking at the difference between differences in two groups. Such experiments are usually exploited to address endogeneity concerns (e.g., Roberts and Whited 2013). The identifying assumption is that the outcome variable would change equally in both groups without the treatment. This assumption is also often referred to as the assumption of parallel trends. Moreover, we would ideally also want a random assignment to the treatment and control groups. Due to lobbying or other activities, this randomness is often violated in (financial) economics.\nIn the context of our setting, we investigate the impact of the Paris Agreement (PA), signed on December 12, 2015, on the bond yields of polluting firms. We first estimate the treatment effect of the agreement using panel regression techniques that we discuss in Fixed Effects and Clustered Standard Errors. We then present two methods to illustrate the treatment effect over time graphically. Although we demonstrate that the treatment effect of the agreement is anticipated by bond market participants well in advance, the techniques we present below can also be applied to many other settings.\nThe approach we use here replicates the results of Seltzer, Starks, and Zhu (2022) partly. Specifically, we borrow their industry definitions for grouping firms into green and brown types. Overall, the literature on environmental, social, and governance (ESG) effects in corporate bond markets is already large but continues to grow (for recent examples, see, e.g., Halling, Yu, and Zechner (2021), Handler, Jankowitsch, and Pasler (2022), Huynh and Xia (2021), among many others).\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(fixest)\nlibrary(broom)",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html#data-preparation",
    "href": "r/difference-in-differences.html#data-preparation",
    "title": "Difference in Differences",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use TRACE and Mergent FISD as data sources from our SQLite-database introduced in Accessing and Managing Financial Data and TRACE and FISD. \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfisd &lt;- tbl(tidy_finance, \"fisd\") |&gt;\n  select(complete_cusip, maturity, offering_amt, sic_code) |&gt;\n  collect() |&gt; \n  drop_na()\n\ntrace_enhanced &lt;- tbl(tidy_finance, \"trace_enhanced\") |&gt;\n  select(cusip_id, trd_exctn_dt, rptd_pr, entrd_vol_qt, yld_pt)|&gt;\n  collect() |&gt; \n  drop_na()\n\nWe start our analysis by preparing the sample of bonds. We only consider bonds with a time to maturity of more than one year to the signing of the PA, so that we have sufficient data to analyze the yield behavior after the treatment date. This restriction also excludes all bonds issued after the agreement. We also consider only the first two digits of the SIC industry code to identify the polluting industries (in line with Seltzer, Starks, and Zhu 2022).\n\ntreatment_date &lt;- ymd(\"2015-12-12\")\n\npolluting_industries &lt;- c(\n  49, 13, 45, 29, 28, 33, 40, 20,\n  26, 42, 10, 53, 32, 99, 37\n)\n\nbonds &lt;- fisd |&gt;\n  filter(offering_amt &gt; 0) |&gt; \n  mutate(\n    time_to_maturity = as.numeric(maturity - treatment_date) / 365,\n    sic_code = as.integer(substr(sic_code, 1, 2)),\n    log_offering_amt = log(offering_amt)\n  ) |&gt;\n  filter(time_to_maturity &gt;= 1) |&gt;\n  select(\n    cusip_id = complete_cusip,\n    time_to_maturity, log_offering_amt, sic_code\n  ) |&gt;\n  mutate(polluter = sic_code %in% polluting_industries)\n\nNext, we aggregate the individual transactions as reported in TRACE to a monthly panel of bond yields. We consider bond yields for a bond’s last trading day in a month. Therefore, we first aggregate bond data to daily frequency and apply common restrictions from the literature (see, e.g., Bessembinder et al. 2008). We weigh each transaction by volume to reflect a trade’s relative importance and avoid emphasizing small trades. Moreover, we only consider transactions with reported prices rptd_pr larger than 25 (to exclude bonds that are close to default) and only bond-day observations with more than five trades on a corresponding day (to exclude prices based on too few, potentially non-representative transactions). \n\ntrace_aggregated &lt;- trace_enhanced |&gt;\n  filter(rptd_pr &gt; 25) |&gt;\n  group_by(cusip_id, trd_exctn_dt) |&gt;\n  summarize(\n    avg_yield = weighted.mean(yld_pt, entrd_vol_qt * rptd_pr),\n    trades = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  drop_na(avg_yield) |&gt;\n  filter(trades &gt;= 5) |&gt;\n  mutate(date = floor_date(trd_exctn_dt, \"month\")) |&gt;\n  group_by(cusip_id, date) |&gt;\n  slice_max(trd_exctn_dt) |&gt;\n  ungroup() |&gt;\n  select(cusip_id, date, avg_yield)\n\nBy combining the bond-specific information from Mergent FISD for our bond sample with the aggregated TRACE data, we arrive at the main sample for our analysis.\n\nbonds_panel &lt;- bonds |&gt;\n  inner_join(trace_aggregated, join_by(cusip_id), multiple = \"all\") |&gt;\n  drop_na()\n\nBefore we can run the first regression, we need to define the treated indicator, which is the product of the post_period (i.e., all months after the signing of the PA) and the polluter indicator defined above.\n\nbonds_panel &lt;- bonds_panel |&gt;\n  mutate(post_period = date &gt;= floor_date(treatment_date, \"months\")) |&gt;\n  mutate(treated = polluter & post_period)\n\nAs usual, we tabulate summary statistics of the variables that enter the regression to check the validity of our variable definitions.\n\nbonds_panel |&gt;\n  pivot_longer(\n    cols = c(avg_yield, time_to_maturity, log_offering_amt),\n    names_to = \"measure\"\n  ) |&gt;\n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 9\n  measure           mean    sd    min   q05   q50   q95   max      n\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;\n1 avg_yield         4.08 4.21  0.0595  1.27  3.38  8.10 128.  127523\n2 log_offering_amt 13.3  0.823 4.64   12.2  13.2  14.5   16.5 127523\n3 time_to_maturity  8.55 8.41  1.01    1.50  5.81 27.4  101.  127523",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html#panel-regressions",
    "href": "r/difference-in-differences.html#panel-regressions",
    "title": "Difference in Differences",
    "section": "Panel Regressions",
    "text": "Panel Regressions\nThe PA is a legally binding international treaty on climate change. It was adopted by 196 parties at COP 21 in Paris on December 12, 2015 and entered into force on November 4, 2016. The PA obliges developed countries to support efforts to build clean, climate-resilient futures. One may thus hypothesize that adopting climate-related policies may affect financial markets. To measure the magnitude of this effect, we first run an ordinary least square (OLS) regression without fixed effects where we include the treated, post_period, and polluter dummies, as well as the bond-specific characteristics log_offering_amt and time_to_maturity. This simple model assumes that there are essentially two periods (before and after the PA) and two groups (polluters and non-polluters). Nonetheless, it should indicate whether polluters have higher yields following the PA compared to non-polluters.\nThe second model follows the typical DiD regression approach by including individual (cusip_id) and time (date) fixed effects. In this model, we do not include any other variables from the simple model because the fixed effects subsume them, and we observe the coefficient of our main variable of interest: treated.\n\nmodel_without_fe &lt;- feols(\n  avg_yield ~ treated + post_period + polluter +\n    log_offering_amt + time_to_maturity,\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\nmodel_with_fe &lt;- feols(\n  avg_yield ~ treated | cusip_id + date,\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\netable(\n  model_without_fe, model_with_fe, \n  coefstat = \"tstat\", digits = 3, digits.stats = 3\n)\n\n                  model_without_fe   model_with_fe\nDependent Var.:          avg_yield       avg_yield\n                                                  \nConstant            10.7*** (57.0)                \ntreatedTRUE        0.462*** (9.31) 0.983*** (29.5)\npost_periodTRUE  -0.174*** (-5.92)                \npolluterTRUE       0.481*** (15.3)                \nlog_offering_amt -0.551*** (-39.0)                \ntime_to_maturity   0.058*** (41.6)                \nFixed-Effects:   ----------------- ---------------\ncusip_id                        No             Yes\ndate                            No             Yes\n________________ _________________ _______________\nVCOV type                      IID             IID\nObservations               127,523         127,523\nR2                           0.032           0.647\nWithin R2                       --           0.007\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth models indicate that polluters have significantly higher yields after the PA than non-polluting firms. Note that the magnitude of the treated coefficient varies considerably across models.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html#visualizing-parallel-trends",
    "href": "r/difference-in-differences.html#visualizing-parallel-trends",
    "title": "Difference in Differences",
    "section": "Visualizing Parallel Trends",
    "text": "Visualizing Parallel Trends\nEven though the regressions above indicate that there is an impact of the PA on bond yields of polluters, the tables do not tell us anything about the dynamics of the treatment effect. In particular, the models provide no indication about whether the crucial parallel trends assumption is valid. This assumption requires that in the absence of treatment, the difference between the two groups is constant over time. Although there is no well-defined statistical test for this assumption, visual inspection typically provides a good indication.\nTo provide such visual evidence, we revisit the simple OLS model and replace the treated and post_period indicators with month dummies for each group. This approach estimates the average yield change of both groups for each period and provides corresponding confidence intervals. Plotting the coefficient estimates for both groups around the treatment date shows us the dynamics of our panel data.\n\nmodel_without_fe_time &lt;- feols(\n  avg_yield ~ polluter + date:polluter +\n    time_to_maturity + log_offering_amt,\n  vcov = \"iid\",\n  data = bonds_panel |&gt;\n    mutate(date = factor(date))\n)\n\nmodel_without_fe_coefs &lt;- tidy(model_without_fe_time) |&gt;\n  filter(str_detect(term, \"date\")) |&gt;\n  mutate(\n    date = ymd(substr(term, nchar(term) - 9, nchar(term))),\n    treatment = str_detect(term, \"TRUE\"),\n    ci_up = estimate + qnorm(0.975) * std.error,\n    ci_low = estimate + qnorm(0.025) * std.error\n  )\n\nmodel_without_fe_coefs |&gt;\n  ggplot(\n    aes(date, \n        color = treatment,\n        linetype = treatment,\n        shape = treatment\n    )) +\n  geom_vline(\n    aes(xintercept = floor_date(treatment_date, \"month\")),\n    linetype = \"dashed\"\n  ) +\n  geom_hline(\n    aes(yintercept = 0),\n    linetype = \"dashed\"\n  ) +\n  geom_errorbar(\n    aes(ymin = ci_low, ymax = ci_up),\n    alpha = 0.5\n  ) +\n  guides(linetype = \"none\") + \n  geom_point(aes(y = estimate)) +\n  labs(\n    x = NULL,\n    y = \"Yield\",\n    shape = \"Polluter?\",\n    color = \"Polluter?\",\n    title = \"Polluters respond stronger to Paris Agreement than green firms\"\n  )\n\n\n\n\n\n\n\nFigure 1: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters and non-polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\n\nFigure 1 shows that throughout most of 2014, the yields of the two groups changed in unison. However, starting at the end of 2014, the yields start to diverge, reaching the highest difference around the signing of the PA. Afterward, the yields for both groups fall again, and the polluters arrive at the same level as at the beginning of 2014. The non-polluters, on the other hand, even experience significantly lower yields than polluters after the signing of the agreement.\nInstead of plotting both groups using the simple model approach, we can also use the fixed-effects model and focus on the polluter’s yield response to the signing relative to the non-polluters. To perform this estimation, we need to replace the treated indicator with separate time dummies for the polluters, each marking a one-month period relative to the treatment date. We then regress the monthly yields on the set of time dummies and cusip_id and date fixed effects.\n\nbonds_panel_alt &lt;- bonds_panel |&gt;\n  mutate(\n    diff_to_treatment = interval(\n      floor_date(treatment_date, \"month\"), date\n    ) %/% months(1)\n  )\n\nvariables &lt;- bonds_panel_alt |&gt;\n  distinct(diff_to_treatment, date) |&gt;\n  arrange(date) |&gt;\n  mutate(variable_name = as.character(NA))\n\nformula &lt;- \"avg_yield ~ \"\n\nfor (j in 1:nrow(variables)) {\n  if (variables$diff_to_treatment[j] != 0) {\n    old_names &lt;- names(bonds_panel_alt)\n    bonds_panel_alt &lt;- bonds_panel_alt |&gt;\n      mutate(new_var = diff_to_treatment == variables$diff_to_treatment[j] & \n               polluter)\n    new_var_name &lt;- ifelse(variables$diff_to_treatment[j] &lt; 0,\n      str_c(\"lag\", abs(variables$diff_to_treatment[j])),\n      str_c(\"lead\", variables$diff_to_treatment[j])\n    )\n    variables$variable_name[j] &lt;- new_var_name\n    names(bonds_panel_alt) &lt;- c(old_names, new_var_name)\n    formula &lt;- str_c(\n      formula,\n      ifelse(j == 1,\n        new_var_name,\n        str_c(\"+\", new_var_name)\n      )\n    )\n  }\n}\nformula &lt;- str_c(formula, \"| cusip_id + date\")\n\nmodel_with_fe_time &lt;- feols(\n  as.formula(formula),\n  vcov = \"iid\",\n  data = bonds_panel_alt\n)\n\nmodel_with_fe_time_coefs &lt;- tidy(model_with_fe_time) |&gt;\n  mutate(\n    term = str_remove(term, \"TRUE\"),\n    ci_up = estimate + qnorm(0.975) * std.error,\n    ci_low = estimate + qnorm(0.025) * std.error\n  ) |&gt;\n  left_join(\n    variables,\n    join_by(term == variable_name)\n  ) |&gt;\n  bind_rows(tibble(\n    term = \"lag0\",\n    estimate = 0,\n    ci_up = 0,\n    ci_low = 0,\n    date = floor_date(treatment_date, \"month\")\n  ))\n\nmodel_with_fe_time_coefs |&gt;\n  ggplot(aes(x = date, y = estimate)) +\n  geom_vline(\n    aes(xintercept = floor_date(treatment_date, \"month\")),\n    linetype = \"dashed\"\n  ) +\n  geom_hline(\n    aes(yintercept = 0),\n    linetype = \"dashed\"\n  ) +\n  geom_errorbar(\n    aes(ymin = ci_low, ymax = ci_up),\n    alpha = 0.5\n  ) +\n  geom_point(aes(y = estimate)) +\n  labs(\n    x = NULL,\n    y = \"Yield\",\n    title = \"Polluters' yield patterns around Paris Agreement signing\"\n  )\n\n\n\n\n\n\n\nFigure 2: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\n\n The resulting graph shown in Figure 2 confirms the main conclusion of the previous image: polluters’ yield patterns show a considerable anticipation effect starting toward the end of 2014. Yields only marginally increase after the signing of the agreement. However, as opposed to the simple model, we do not see a complete reversal back to the pre-agreement level. Yields of polluters stay at a significantly higher level even one year after the signing.\nNotice that during the year after the PA was signed, Donald Trump, the 45th president of the United States, was elected on November 8, 2016. During his campaign there were some indications of intentions to withdraw the US from the PA, which ultimately happened on November 4, 2020. Hence, reversal effects are potentially driven by these actions.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html#key-takeaways",
    "href": "r/difference-in-differences.html#key-takeaways",
    "title": "Difference in Differences",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nDifference-in-differences is a powerful tool for estimating causal effects in financial settings, especially when analyzing the impact of policy changes or shocks.\nIt is important to assess the parallel trends assumption using graphical methods.\nThe fixest R package allows you to implement difference-in-differences regressions and visualize parallel trends.\nBy combining panel data from TRACE and Mergent FISD with fixed effects regressions, you can evaluate how the Paris Agreement influenced corporate bond yields.\nThe application shows that polluting firms experienced significantly higher yields following up to and after the agreement, invalidating the parallel trends assumption.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html#exercises",
    "href": "r/difference-in-differences.html#exercises",
    "title": "Difference in Differences",
    "section": "Exercises",
    "text": "Exercises\n\nThe 46th President of the US, Joe Biden, rejoined the Paris Agreement on February 19, 2021. Repeat the difference in differences analysis for the day of his election victory. Note that you will also have to download new TRACE data. How did polluters’ yields react to this action?\nBased on the exercise on ratings in TRACE and FISD, include ratings as a control variable in the analysis above. Do the results change?",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/working-with-stock-returns.html",
    "href": "r/working-with-stock-returns.html",
    "title": "Working with Stock Returns",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nThe main aim of this chapter is to familiarize yourself with the tidyverse for working with stock market data. We focus on downloading and visualizing stock data from data provider Yahoo Finance.\nAt the start of each session, we load the required R packages. Throughout the entire book, we always use the tidyverse (Wickham et al. 2019) package. In this chapter, we also load the tidyfinance package to download stock price data. This package provides a convenient wrapper for various quantitative functions compatible with the tidyverse and our book. Finally, the package scales (Wickham and Seidel 2022) provides nice formatting for axis labels in visualizations.\nYou typically have to install a package once before you can load it into your active R session. In case you have not done this yet, call, for instance, install.packages(\"tidyfinance\").\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(scales)",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "r/working-with-stock-returns.html#downloading-data",
    "href": "r/working-with-stock-returns.html#downloading-data",
    "title": "Working with Stock Returns",
    "section": "Downloading Data",
    "text": "Downloading Data\nWe first download daily prices for one stock symbol, e.g., the Apple stock (AAPL), directly from the data provider Yahoo Finance. To download the data, you can use the function download_data. In case this is the first time you use the package tidyfinance, you may be asked once to install some additional packages in the process of downloading the data. If you do not know how to use it, make sure you read the help file by calling ?download_data. We especially recommend taking a look at the examples section of the documentation.\nIn the following code, we request daily data from the beginning of 2000 to the end of the last year, which is a period of more than 20 years.\n\nprices &lt;- download_data(\n  type = \"stock_prices\",\n  symbols = \"AAPL\",\n  start_date = \"2000-01-01\",\n  end_date = \"2024-12-31\"\n)\nprices\n\n# A tibble: 6,288 × 8\n  symbol date          volume  open   low  high close adjusted_close\n  &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1 AAPL   2000-01-03 535796800 0.936 0.908 1.00  0.999          0.840\n2 AAPL   2000-01-04 512377600 0.967 0.903 0.988 0.915          0.769\n3 AAPL   2000-01-05 778321600 0.926 0.920 0.987 0.929          0.781\n4 AAPL   2000-01-06 767972800 0.948 0.848 0.955 0.848          0.713\n5 AAPL   2000-01-07 460734400 0.862 0.853 0.902 0.888          0.747\n# ℹ 6,283 more rows\n\n\n download_data(type = \"stock_prices\") downloads stock market data from Yahoo Finance. The function returns a data frame with eight self-explanatory columns: symbol, date, the daily volume (in the number of traded shares), the market prices at the open, high, low, close, and the adjusted price in USD. The adjusted prices are corrected for anything that might affect the stock price after the market closes, e.g., stock splits and dividends. These actions affect the quoted prices, but they have no direct impact on the investors who hold the stock. Therefore, we often rely on adjusted prices when it comes to analyzing the returns an investor would have earned by holding the stock continuously.\nNext, we use the ggplot2 package (Wickham 2016) to visualize the time series of adjusted prices in Figure 1. This package takes care of visualization tasks based on the principles of the grammar of graphics (Wilkinson 2012).\n\nprices |&gt;\n  ggplot(aes(x = date, y = adjusted_close)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Apple stock prices between beginning of 2000 and end of 2024\"\n  )\n\n\n\n\n\n\n\nFigure 1: Prices are in USD, adjusted for dividend payments and stock splits.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "r/working-with-stock-returns.html#computing-returns",
    "href": "r/working-with-stock-returns.html#computing-returns",
    "title": "Working with Stock Returns",
    "section": "Computing Returns",
    "text": "Computing Returns\nInstead of analyzing prices, we compute daily returns defined as \\(r_t = p_t / p_{t-1} - 1\\), where \\(p_t\\) is the adjusted price at the end of day \\(t\\). In that context, the function lag() is helpful by returning the previous value.\n\nreturns &lt;- prices |&gt;\n  arrange(date) |&gt;\n  mutate(ret = adjusted_close / lag(adjusted_close) - 1) |&gt;\n  select(symbol, date, ret)\nreturns\n\n# A tibble: 6,288 × 3\n  symbol date           ret\n  &lt;chr&gt;  &lt;date&gt;       &lt;dbl&gt;\n1 AAPL   2000-01-03 NA     \n2 AAPL   2000-01-04 -0.0843\n3 AAPL   2000-01-05  0.0146\n4 AAPL   2000-01-06 -0.0865\n5 AAPL   2000-01-07  0.0474\n# ℹ 6,283 more rows\n\n\nThe resulting data frame has three columns, the last of which contains the daily returns (ret). Note that the first entry naturally contains a missing value (NA) because there is no previous price. Obviously, the use of lag() would be meaningless if the time series is not ordered by ascending dates. The command arrange() provides a convenient way to order observations in the correct way for our application. If you want to order observations by descending values, you could, for instance, use arrange(desc(ret)). Always check that your data has the desired structure before calling lag() or similar functions.\nFor the upcoming examples, we remove missing values as these would require separate treatment for many applications. For example, missing values can affect sums and averages by reducing the number of valid data points if not properly accounted for. In general, always ensure you understand why NA values occur and carefully examine if you can simply get rid of these observations.\n\nreturns &lt;- returns |&gt;\n  drop_na(ret)\n\nNext, we visualize the distribution of daily returns in a histogram in Figure 2. Additionally, we draw a dashed line that indicates the historical five percent quantile of the daily returns to the histogram, which is a crude proxy for the worst possible return of the stock with a probability of at most five percent. This quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators. We refer to Tsay (2010) for a more thorough introduction to the stylized facts of financial returns.\n\nquantile_05 &lt;- quantile(returns$ret, probs = 0.05)\nreturns |&gt;\n  ggplot(aes(x = ret)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of daily Apple stock returns\"\n  ) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\nFigure 2: The dotted vertical line indicates the historical five percent quantile.\n\n\n\n\n\nHere, bins = 100 determines the number of bins used in the illustration and, hence, implicitly sets the width of the bins. Before proceeding, make sure you understand how to use the geom geom_vline() to add a dashed line that indicates the historical five percent quantile of the daily returns. Before proceeding with any data, a typical task is to compute and analyze the summary statistics for the main variables of interest.\n\nreturns |&gt;\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  ))\n\n# A tibble: 1 × 4\n  ret_daily_mean ret_daily_sd ret_daily_min ret_daily_max\n           &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        0.00122       0.0244        -0.519         0.139\n\n\nWe see that the maximum daily return was 13.905 percent. Perhaps not surprisingly, the average daily return is close to but slightly above 0. In line with the illustration above, the large losses on the day with the minimum returns indicate a strong asymmetry in the distribution of returns.\nYou can also compute these summary statistics for each year individually by imposing group_by(year = year(date)), where the call year(date) returns the year. More specifically, the few lines of code below compute the summary statistics from above for individual groups of data defined by the values of the column year. The summary statistics, therefore, allow an eyeball analysis of the time-series dynamics of the daily return distribution.\n\nreturns |&gt;\n  group_by(year = year(date)) |&gt;\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |&gt;\n  print(n = Inf)\n\n# A tibble: 25 × 5\n    year daily_mean daily_sd daily_min daily_max\n   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2000 -0.00346     0.0549   -0.519     0.137 \n 2  2001  0.00233     0.0393   -0.172     0.129 \n 3  2002 -0.00121     0.0305   -0.150     0.0846\n 4  2003  0.00186     0.0234   -0.0814    0.113 \n 5  2004  0.00470     0.0255   -0.0558    0.132 \n 6  2005  0.00349     0.0245   -0.0921    0.0912\n 7  2006  0.000949    0.0243   -0.0633    0.118 \n 8  2007  0.00366     0.0238   -0.0702    0.105 \n 9  2008 -0.00265     0.0367   -0.179     0.139 \n10  2009  0.00382     0.0214   -0.0502    0.0676\n11  2010  0.00183     0.0169   -0.0496    0.0769\n12  2011  0.00104     0.0165   -0.0559    0.0589\n13  2012  0.00130     0.0186   -0.0644    0.0887\n14  2013  0.000472    0.0180   -0.124     0.0514\n15  2014  0.00145     0.0136   -0.0799    0.0820\n16  2015  0.0000199   0.0168   -0.0612    0.0574\n17  2016  0.000575    0.0147   -0.0657    0.0650\n18  2017  0.00164     0.0111   -0.0388    0.0610\n19  2018 -0.0000573   0.0181   -0.0663    0.0704\n20  2019  0.00266     0.0165   -0.0996    0.0683\n21  2020  0.00281     0.0294   -0.129     0.120 \n22  2021  0.00131     0.0158   -0.0417    0.0539\n23  2022 -0.000970    0.0225   -0.0587    0.0890\n24  2023  0.00168     0.0128   -0.0480    0.0469\n25  2024  0.00120     0.0143   -0.0482    0.0726\n\n\nIn case you wonder, the additional argument .names = \"{.fn}\" in across() determines how to name the output columns. It acts as a placeholder that gets replaced by the name of the function being applied (e.g., mean, sd, min, max) when creating new column names. The specification is rather flexible and allows almost arbitrary column names, which can be useful for reporting. The print() function controls the R console’s output options.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "r/working-with-stock-returns.html#scaling-up-the-analysis",
    "href": "r/working-with-stock-returns.html#scaling-up-the-analysis",
    "title": "Working with Stock Returns",
    "section": "Scaling Up the Analysis",
    "text": "Scaling Up the Analysis\nAs a next step, we generalize the previous code so that all computations can handle an arbitrary number of symbols (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nThis is where the tidyverse magic starts: Tidy data makes it extremely easy to generalize the computations from before to as many assets or groups as you like. The following code takes any number of symbols, e.g., symbol &lt;- c(\"AAPL\", \"MMM\", \"BA\"), and automates the download as well as the plot of the price time series. In the end, we create the table of summary statistics for all assets at once. For this example, we analyze data from all current constituents of the Dow Jones Industrial Average index.\n\nsymbols &lt;- download_data(\n  type = \"constituents\", \n  index = \"Dow Jones Industrial Average\"\n) \nsymbols\n\n# A tibble: 30 × 5\n  symbol name                    location           exchange currency\n  &lt;chr&gt;  &lt;chr&gt;                   &lt;chr&gt;              &lt;chr&gt;    &lt;chr&gt;   \n1 GS     GOLDMAN SACHS GROUP INC Vereinigte Staaten New Yor… USD     \n2 MSFT   MICROSOFT CORP          Vereinigte Staaten NASDAQ   USD     \n3 CAT    CATERPILLAR INC         Vereinigte Staaten New Yor… USD     \n4 HD     HOME DEPOT INC          Vereinigte Staaten New Yor… USD     \n5 SHW    SHERWIN WILLIAMS        Vereinigte Staaten New Yor… USD     \n# ℹ 25 more rows\n\n\nConveniently, tidyfinance provides the functionality to get all stock prices from an index for a specific point in time with a single call.\n\nprices_daily &lt;- download_data(\n  type = \"stock_prices\",\n  symbols = symbols$symbol,\n  start_date = \"2000-01-01\",\n  end_date = \"2023-12-31\"\n)\n\nThe resulting data frame contains 177925 daily observations for GS, MSFT, CAT, HD, SHW, V, AXP, MCD, UNH, JPM, AMGN, TRV, CRM, IBM, BA, AAPL, AMZN, HON, JNJ, NVDA, CVX, PG, MMM, DIS, WMT, MRK, NKE, KO, CSCO, VZ different stocks. Figure 3 illustrates the time series of the downloaded adjusted prices for each of the constituents of the Dow index. Make sure you understand every single line of code! What are the arguments of aes()? Which alternative geoms could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes.\n\nprices_daily |&gt;\n  ggplot(aes(\n    x = date,\n    y = adjusted_close,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Stock prices of Dow Jones index constituents\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 3: Prices in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n\nDo you notice the small differences relative to the code we used before? All we needed to do to illustrate all stock symbols simultaneously is to include color = symbol in the ggplot aesthetics. In this way, we generate a separate line for each symbol. Of course, there are simply too many lines on this graph to identify the individual stocks properly, but it illustrates our point of how to generalize a specific analysis to an arbitrage number of subjects quite well.\nThe same holds for stock returns. Before computing the returns, we use group_by(symbol) such that the mutate() command is performed for each symbol individually. The same logic also applies to the computation of summary statistics: group_by(symbol) is the key to aggregating the time series into symbol-specific variables of interest.\n\nreturns_daily &lt;- prices_daily |&gt;\n  group_by(symbol) |&gt;\n  mutate(ret = adjusted_close / lag(adjusted_close) - 1) |&gt;\n  select(symbol, date, ret) |&gt;\n  drop_na(ret)\n\nreturns_daily |&gt;\n  group_by(symbol) |&gt;\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |&gt;\n  print(n = Inf)\n\n# A tibble: 30 × 5\n   symbol daily_mean daily_sd daily_min daily_max\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 AAPL     0.00122    0.0247    -0.519     0.139\n 2 AMGN     0.000493   0.0194    -0.134     0.151\n 3 AMZN     0.00107    0.0315    -0.248     0.345\n 4 AXP      0.000544   0.0227    -0.176     0.219\n 5 BA       0.000628   0.0222    -0.238     0.243\n 6 CAT      0.000724   0.0203    -0.145     0.147\n 7 CRM      0.00119    0.0266    -0.271     0.260\n 8 CSCO     0.000322   0.0234    -0.162     0.244\n 9 CVX      0.000511   0.0175    -0.221     0.227\n10 DIS      0.000414   0.0194    -0.184     0.160\n11 GS       0.000557   0.0229    -0.190     0.265\n12 HD       0.000544   0.0192    -0.287     0.141\n13 HON      0.000497   0.0191    -0.174     0.282\n14 IBM      0.000297   0.0163    -0.155     0.120\n15 JNJ      0.000379   0.0121    -0.158     0.122\n16 JPM      0.000606   0.0238    -0.207     0.251\n17 KO       0.000318   0.0131    -0.101     0.139\n18 MCD      0.000536   0.0145    -0.159     0.181\n19 MMM      0.000363   0.0151    -0.129     0.126\n20 MRK      0.000371   0.0166    -0.268     0.130\n21 MSFT     0.000573   0.0193    -0.156     0.196\n22 NKE      0.000708   0.0193    -0.198     0.155\n23 NVDA     0.00175    0.0376    -0.352     0.424\n24 PG       0.000362   0.0133    -0.302     0.120\n25 SHW      0.000860   0.0180    -0.208     0.153\n26 TRV      0.000555   0.0181    -0.208     0.256\n27 UNH      0.000948   0.0196    -0.186     0.348\n28 V        0.000933   0.0185    -0.136     0.150\n29 VZ       0.000238   0.0151    -0.118     0.146\n30 WMT      0.000323   0.0148    -0.114     0.117\n\n\nNote that you are now also equipped with all tools to download price data for each symbol listed in the S&P 500 index with the same number of lines of code. Just use symbol &lt;- download_data(type = \"constituents\", index = \"S&P 500\"), which provides you with a data frame that contains each symbol that is (currently) part of the S&P 500. However, don’t try this if you are not prepared to wait for a couple of minutes because this is quite some data to download!",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "r/working-with-stock-returns.html#different-frequencies",
    "href": "r/working-with-stock-returns.html#different-frequencies",
    "title": "Working with Stock Returns",
    "section": "Different Frequencies",
    "text": "Different Frequencies\nFinancial data often exists at different frequencies due to varying reporting schedules, trading calendars, and economic data releases. For example, stock prices are typically recorded daily, while macroeconomic indicators such as GDP or inflation are reported monthly or quarterly. Additionally, some datasets are recorded only when transactions occur, resulting in irregular timestamps. To compare data meaningfully, we have to align different frequencies appropriately. For example, to compare returns across different frequencies, we use annualization techniques.\nSo far, we have worked with daily returns, but we can easily convert our data to other frequencies. Let’s create monthly returns from our daily data:\n\nreturns_monthly &lt;- returns_daily |&gt;\n  mutate(date = floor_date(date, \"month\")) |&gt;\n  group_by(symbol, date) |&gt;\n  summarize(\n    ret = prod(1 + ret) - 1,\n    .groups = \"drop\"\n  )\n\nIn this code, we first group the data by symbol and month and then compute monthly returns by compounding the daily returns: \\((1+r_1)(1+r_2)\\ldots(1+r_n)-1\\). To visualize how return characteristics change across different frequencies, we can compare histograms as in Figure 4:\n\napple_returns &lt;- bind_rows(\n  returns_daily |&gt; \n    filter(symbol == \"AAPL\") |&gt; \n    mutate(frequency = \"Daily\"),\n  returns_monthly |&gt; \n    filter(symbol == \"AAPL\") |&gt; \n    mutate(frequency = \"Monthly\")\n)\n\napple_returns |&gt;\n  ggplot(aes(x = ret, fill = frequency)) +\n  geom_histogram(position = \"identity\", bins = 50) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = \"Frequency\",\n    title = \"Distribution of Apple returns across different frequencies\"\n  ) +\n  scale_x_continuous(labels = percent) +\n  theme_minimal() +\n  facet_wrap(~ frequency, scales = \"free\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 4: Returns are based on prices adjusted for dividend payments and stock splits.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "r/working-with-stock-returns.html#other-forms-of-data-aggregation",
    "href": "r/working-with-stock-returns.html#other-forms-of-data-aggregation",
    "title": "Working with Stock Returns",
    "section": "Other Forms of Data Aggregation",
    "text": "Other Forms of Data Aggregation\nOf course, aggregation across variables other than symbol or date can also make sense. For instance, suppose you are interested in answering questions like: Are days with high aggregate trading volume likely followed by days with high aggregate trading volume? To provide some initial analysis on this question, we take the downloaded data and compute aggregate daily trading volume for all Dow index constituents in USD. Recall that the column volume is denoted in the number of traded shares. Thus, we multiply the trading volume with the daily adjusted closing price to get a proxy for the aggregate trading volume in USD. Scaling by 1e-9 (R can handle scientific notation) denotes daily trading volume in billion USD.\n\ntrading_volume &lt;- prices_daily |&gt;\n  group_by(date) |&gt;\n  summarize(trading_volume = sum(volume * adjusted_close))\n\ntrading_volume |&gt;\n  ggplot(aes(x = date, y = trading_volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume of Dow Jones index constitutens\"\n  ) +\n  scale_y_continuous(labels = unit_format(unit = \"B\", scale = 1e-9))\n\n\n\n\n\n\n\nFigure 5: Total daily trading volume in billion USD.\n\n\n\n\n\nFigure 5 indicates a clear upward trend in aggregated daily trading volume. In particular, since the outbreak of the COVID-19 pandemic, markets have processed substantial trading volumes, as analyzed, for instance, by Goldstein, Koijen, and Mueller (2021). One way to illustrate the persistence of trading volume would be to plot volume on day \\(t\\) against volume on day \\(t-1\\) as in the example below. In Figure 6, we add a dotted 45°-line to indicate a hypothetical one-to-one relation by geom_abline(), addressing potential differences in the axes’ scales.\n\ntrading_volume |&gt;\n  ggplot(aes(x = lag(trading_volume), y = trading_volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = \"Previous day aggregate trading volume\",\n    y = \"Aggregate trading volume\",\n    title = \"Persistence in daily trading volume of Dow Jones index constituents\"\n  ) + \n  scale_x_continuous(labels = unit_format(unit = \"B\", scale = 1e-9)) +\n  scale_y_continuous(labels = unit_format(unit = \"B\", scale = 1e-9))\n\nWarning: Removed 1 row containing missing values or values outside the scale\nrange (`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 6: Total daily trading volume in billion USD.\n\n\n\n\n\nDo you understand where the warning ## Warning: Removed 1 rows containing missing values (geom_point). comes from and what it means? Purely eye-balling reveals that days with high trading volume are often followed by similarly high trading volume days.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "r/working-with-stock-returns.html#key-takeaways",
    "href": "r/working-with-stock-returns.html#key-takeaways",
    "title": "Working with Stock Returns",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nYou can use the tidyverse in R to efficiently analyze stock market data using consistent and scalable workflows.\nThe tidyfinance R package allows seamless downloading of historical stock prices and index data directly from Yahoo Finance.\nTidy data principles enable efficient analysis of financial data.\nAdjusted stock prices provide a more accurate reflection of investor returns by accounting for dividends and stock splits.\nSummary statistics such as mean, standard deviation, and quantiles offer insights into stock return behavior over time.\nVisualizations created with ggplot2 help identify trends, volatility, and return distributions in financial time series.\nTidy data principles make it easy to scale financial analyses from a single stock to entire indices like the Dow Jones or S&P 500.\nConsistent workflows form the foundation for advanced financial analysis.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "r/working-with-stock-returns.html#exercises",
    "href": "r/working-with-stock-returns.html#exercises",
    "title": "Working with Stock Returns",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily prices for another stock market symbol of your choice from Yahoo Finance using download_data() from the tidyfinance package. Plot two time series of the symbol’s un-adjusted and adjusted closing prices. Explain any visible differences.\nCompute daily net returns for an asset of your choice and visualize the distribution of daily returns in a histogram using 100 bins. Also, use geom_vline() to add a dashed red vertical line that indicates the 5 percent quantile of the daily returns. Compute summary statistics (mean, standard deviation, minimum, and maximum) for the daily returns.\nTake your code from the previous exercises and generalize it such that you can perform all the computations for an arbitrary number of symbols (e.g., symbol &lt;- c(\"AAPL\", \"MMM\", \"BA\")). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nTo facilitate the computation of the annualization factor, write a function that takes a vector of return dates as input and determines the frequency before returning the appropriate annualization factor.\nAre days with high aggregate trading volume often also days with large absolute returns? Find an appropriate visualization to analyze the question using the symbol AAPL.",
    "crumbs": [
      "R",
      "Getting Started",
      "Working with Stock Returns"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html",
    "href": "r/fixed-effects-and-clustered-standard-errors.html",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we provide an intuitive introduction to the two popular concepts of fixed effects regressions and clustered standard errors. When working with regressions in empirical finance, you will sooner or later be confronted with discussions around how you deal with omitted variables bias and dependence in your residuals. The concepts we introduce in this chapter are designed to address such concerns.\nWe focus on a classical panel regression common to the corporate finance literature (e.g., Fazzari et al. 1988; Erickson and Whited 2012; Gulen and Ion 2015): firm investment modeled as a function that increases in firm cash flow and firm investment opportunities.\nTypically, this investment regression uses quarterly balance sheet data provided via Compustat because it allows for richer dynamics in the regressors and more opportunities to construct variables. As we focus on the implementation of fixed effects and clustered standard errors, we use the annual Compustat data from our previous chapters and leave the estimation using quarterly data as an exercise. We demonstrate below that the regression based on annual data yields qualitatively similar results to estimations based on quarterly data from the literature, namely confirming the positive relationships between investment and the two regressors.\nThe current chapter relies on the following set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(fixest)\nCompared to previous chapters, we introduce fixest (Bergé 2018) for the fixed effects regressions, the implementation of standard error clusters, and tidy estimation output.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and annual Compustat as data sources from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. In particular, Compustat provides balance sheet and income statement data on a firm level, while CRSP provides market valuations. \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(gvkey, date, mktcap) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n  select(datadate, gvkey, year, at, be, capx, oancf, txdb) |&gt;\n  collect()\n\nThe classical investment regressions model the capital investment of a firm as a function of operating cash flows and Tobin’s q, a measure of a firm’s investment opportunities. We start by constructing investment and cash flows which are usually normalized by lagged total assets of a firm. In the following code chunk, we construct a panel of firm-year observations, so we have both cross-sectional information on firms as well as time-series information for each firm.\n\ndata_investment &lt;- compustat |&gt;\n  mutate(date = floor_date(datadate, \"month\")) |&gt;\n  left_join(compustat |&gt;\n    select(gvkey, year, at_lag = at) |&gt;\n    mutate(year = year + 1),\n  join_by(gvkey, year)\n  ) |&gt;\n  filter(at &gt; 0, at_lag &gt; 0) |&gt;\n  mutate(\n    investment = capx / at_lag,\n    cash_flows = oancf / at_lag\n  )\n\ndata_investment &lt;- data_investment |&gt;\n  left_join(data_investment |&gt;\n    select(gvkey, year, investment_lead = investment) |&gt;\n    mutate(year = year - 1),\n    join_by(gvkey, year)\n  )\n\nTobin’s q is the ratio of the market value of capital to its replacement costs. It is one of the most common regressors in corporate finance applications (e.g., Fazzari et al. 1988; Erickson and Whited 2012). We follow the implementation of Gulen and Ion (2015) and compute Tobin’s q as the market value of equity (mktcap) plus the book value of assets (at) minus book value of equity (be) plus deferred taxes (txdb), all divided by book value of assets (at). Finally, we only keep observations where all variables of interest are non-missing, and the reported book value of assets is strictly positive.\n\ndata_investment &lt;- data_investment |&gt;\n  left_join(crsp_monthly, join_by(gvkey, date)) |&gt;\n  mutate(tobins_q = (mktcap + at - be + txdb) / at) |&gt;\n  select(gvkey, year, investment_lead, cash_flows, tobins_q) |&gt;\n  drop_na()\n\nAs the variable construction typically leads to extreme values that are most likely related to data issues (e.g., reporting errors), many papers include winsorization of the variables of interest. Winsorization involves replacing values of extreme outliers with quantiles on the respective end. The following function implements the winsorization for any percentage cut that should be applied on either end of the distributions. In the specific example, we winsorize the main variables (investment, cash_flows, and tobins_q) at the one percent level.\n\nwinsorize &lt;- function(x, cut) {\n  x &lt;- replace(\n    x,\n    x &gt; quantile(x, 1 - cut, na.rm = T),\n    quantile(x, 1 - cut, na.rm = T)\n  )\n  x &lt;- replace(\n    x,\n    x &lt; quantile(x, cut, na.rm = T),\n    quantile(x, cut, na.rm = T)\n  )\n  return(x)\n}\n\ndata_investment &lt;- data_investment |&gt;\n  mutate(across(\n    c(investment_lead, cash_flows, tobins_q),\n    ~ winsorize(., 0.01)\n  ))\n\nBefore proceeding to any estimations, we highly recommend tabulating summary statistics of the variables that enter the regression. These simple tables allow you to check the plausibility of your numerical variables, as well as spot any obvious errors or outliers. Additionally, for panel data, plotting the time series of the variable’s mean and the number of observations is a useful exercise to spot potential problems.\n\ndata_investment |&gt;\n  pivot_longer(\n    cols = c(investment_lead, cash_flows, tobins_q),\n    names_to = \"measure\"\n  ) |&gt;\n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 9\n  measure      mean     sd    min      q05    q50   q95    max      n\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;\n1 cash_flo… 0.00860 0.275  -1.56  -4.80e-1 0.0625 0.271  0.475 133614\n2 investme… 0.0563  0.0760  0      5.66e-4 0.0317 0.202  0.457 133614\n3 tobins_q  1.99    1.69    0.563  7.90e-1 1.39   5.35  10.8   133614",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nTo illustrate fixed effects regressions, we use the fixest package, which is both computationally powerful and flexible with respect to model specifications. We start out with the basic investment regression using the simple model \\[ \\text{Investment}_{i,t+1} = \\alpha + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t},\\] where \\(\\varepsilon_t\\) is i.i.d. normally distributed across time and firms. We use the feols()-function to estimate the simple model so that the output has the same structure as the other regressions below, but you could also use lm().\n\nmodel_ols &lt;- feols(\n  fml = investment_lead ~ cash_flows + tobins_q,\n  vcov = \"iid\",\n  data = data_investment\n)\nmodel_ols\n\nOLS estimation, Dep. Var.: investment_lead\nObservations: 133,614\nStandard-errors: IID \n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  0.04170   0.000321   130.1 &lt; 2.2e-16 ***\ncash_flows   0.04891   0.000760    64.3 &lt; 2.2e-16 ***\ntobins_q     0.00715   0.000124    57.7 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.074325   Adj. R2: 0.043695\n\n\nAs expected, the regression output shows significant coefficients for both variables. Higher cash flows and investment opportunities are associated with higher investment. However, the simple model actually may have a lot of omitted variables, so our coefficients are most likely biased. As there is a lot of unexplained variation in our simple model (indicated by the rather low adjusted R-squared), the bias in our coefficients is potentially severe, and the true values could be above or below zero. Note that there are no clear cutoffs to decide when an R-squared is high or low, but it depends on the context of your application and on the comparison of different models for the same data.\nOne way to tackle the issue of omitted variable bias is to get rid of as much unexplained variation as possible by including fixed effects; i.e., model parameters that are fixed for specific groups (e.g., Wooldridge 2010). In essence, each group has its own mean in fixed effects regressions. The simplest group that we can form in the investment regression is the firm level. The firm fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t},\\] where \\(\\alpha_i\\) is the firm fixed effect and captures the firm-specific mean investment across all years. In fact, you could also compute firms’ investments as deviations from the firms’ average investments and estimate the model without the fixed effects. The idea of the firm fixed effect is to remove the firm’s average investment, which might be affected by firm-specific variables that you do not observe. For example, firms in a specific industry might invest more on average. Or you observe a young firm with large investments but only small concurrent cash flows, which will only happen in a few years. This sort of variation is unwanted because it is related to unobserved variables that can bias your estimates in any direction.\nTo include the firm fixed effect, we use gvkey (Compustat’s firm identifier) as follows:\n\nmodel_fe_firm &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey,\n  vcov = \"iid\",\n  data = data_investment\n)\nmodel_fe_firm\n\nOLS estimation, Dep. Var.: investment_lead\nObservations: 133,614\nFixed-effects: gvkey: 14,695\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(&gt;|t|)    \ncash_flows   0.0138   0.000878    15.7 &lt; 2.2e-16 ***\ntobins_q     0.0105   0.000127    83.0 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.048854     Adj. R2: 0.535774\n                 Within R2: 0.055835\n\n\nThe regression output shows a lot of unexplained variation at the firm level that is taken care of by including the firm fixed effect as the adjusted R-squared rises above 50 percent. In fact, it is more interesting to look at the within R-squared that shows the explanatory power of a firm’s cash flow and Tobin’s q on top of the average investment of each firm. We can also see that the coefficients changed slightly in magnitude but not in sign.\nThere is another source of variation that we can get rid of in our setting: average investment across firms might vary over time due to macroeconomic factors that affect all firms, such as economic crises. By including year fixed effects, we can take out the effect of unobservables that vary over time. The two-way fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\alpha_t + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t},\\] where \\(\\alpha_t\\) is the time fixed effect. Here you can think of higher investments during an economic expansion with simultaneously high cash flows.\n\nmodel_fe_firmyear &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  vcov = \"iid\",\n  data = data_investment\n)\nmodel_fe_firmyear\n\nOLS estimation, Dep. Var.: investment_lead\nObservations: 133,614\nFixed-effects: gvkey: 14,695,  year: 37\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(&gt;|t|)    \ncash_flows  0.01683   0.000858    19.6 &lt; 2.2e-16 ***\ntobins_q    0.00958   0.000126    76.4 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.04756     Adj. R2: 0.559914\n                Within R2: 0.04889 \n\n\nThe inclusion of time fixed effects did only marginally affect the R-squared and the coefficients, which we can interpret as a good thing as it indicates that the coefficients are not driven by an omitted variable that varies over time.\nHow can we further improve the robustness of our regression results? Ideally, we want to get rid of unexplained variation at the firm-year level, which means we need to include more variables that vary across firm and time and are likely correlated with investment. Note that we cannot include firm-year fixed effects in our setting because then cash flows and Tobin’s q are colinear with the fixed effects, and the estimation becomes void.\nBefore we discuss the properties of our estimation errors, we want to point out that regression tables are at the heart of every empirical analysis, where you compare multiple models. Fortunately, the etable() function provides a convenient way to tabulate the regression output (with many parameters to customize and even print the output in LaTeX). We recommend printing \\(t\\)-statistics rather than standard errors in regression tables because the latter are typically very hard to interpret across coefficients that vary in size. We also do not print p-values because they are sometimes misinterpreted to signal the importance of observed effects (Wasserstein and Lazar 2016). The \\(t\\)-statistics provide a consistent way to interpret changes in estimation uncertainty across different model specifications.\n\netable(\n  model_ols, model_fe_firm, model_fe_firmyear,\n  coefstat = \"tstat\", digits = 3, digits.stats = 3\n)\n\n                       model_ols   model_fe_firm model_fe_firm..\nDependent Var.:  investment_lead investment_lead investment_lead\n                                                                \nConstant        0.042*** (130.1)                                \ncash_flows       0.049*** (64.3) 0.014*** (15.7) 0.017*** (19.6)\ntobins_q         0.007*** (57.7) 0.011*** (83.0) 0.010*** (76.4)\nFixed-Effects:  ---------------- --------------- ---------------\ngvkey                         No             Yes             Yes\nyear                          No              No             Yes\n_______________ ________________ _______________ _______________\nVCOV type                    IID             IID             IID\nObservations             133,614         133,614         133,614\nR2                         0.044           0.587           0.608\nWithin R2                     --           0.056           0.049\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Clustering Standard Errors",
    "text": "Clustering Standard Errors\nApart from biased estimators, we usually have to deal with potentially complex dependencies of our residuals with each other. Such dependencies in the residuals invalidate the i.i.d. assumption of OLS and lead to biased standard errors. With biased OLS standard errors, we cannot reliably interpret the statistical significance of our estimated coefficients.\nIn our setting, the residuals may be correlated across years for a given firm (time-series dependence), or, alternatively, the residuals may be correlated across different firms (cross-section dependence). One of the most common approaches to dealing with such dependence is the use of clustered standard errors (Petersen 2008). The idea behind clustering is that the correlation of residuals within a cluster can be of any form. As the number of clusters grows, the cluster-robust standard errors become consistent (Donald and Lang 2007; Wooldridge 2010). A natural requirement for clustering standard errors in practice is hence a sufficiently large number of clusters. Typically, around at least 30 to 50 clusters are seen as sufficient (Cameron, Gelbach, and Miller 2011).\nInstead of relying on the iid assumption, we can use the cluster option in the feols-function as above. The code chunk below applies both one-way clustering by firm as well as two-way clustering by firm and year.\n\nmodel_cluster_firm &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = \"gvkey\",\n  data = data_investment\n)\n\nmodel_cluster_firmyear &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = c(\"gvkey\", \"year\"),\n  data = data_investment\n)\n\n The table below shows the comparison of the different assumptions behind the standard errors. In the first column, we can see highly significant coefficients on both cash flows and Tobin’s q. By clustering the standard errors on the firm level, the \\(t\\)-statistics of both coefficients drop in half, indicating a high correlation of residuals within firms. If we additionally cluster by year, we see a drop, particularly for Tobin’s q, again. Even after relaxing the assumptions behind our standard errors, both coefficients are still comfortably significant as the \\(t\\)-statistics are well above the usual critical values of 1.96 or 2.576 for two-tailed significance tests.\n\netable(\n  model_fe_firmyear, model_cluster_firm, model_cluster_firmyear,\n  coefstat = \"tstat\", digits = 3, digits.stats = 3\n)\n\n                model_fe_firm.. model_cluster.. model_cluster...1\nDependent Var.: investment_lead investment_lead   investment_lead\n                                                                 \ncash_flows      0.017*** (19.6) 0.017*** (11.4)   0.017*** (9.39)\ntobins_q        0.010*** (76.4) 0.010*** (35.8)   0.010*** (15.0)\nFixed-Effects:  --------------- ---------------   ---------------\ngvkey                       Yes             Yes               Yes\nyear                        Yes             Yes               Yes\n_______________ _______________ _______________   _______________\nVCOV type                   IID       by: gvkey  by: gvkey & year\nObservations            133,614         133,614           133,614\nR2                        0.608           0.608             0.608\nWithin R2                 0.049           0.049             0.049\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInspired by Abadie et al. (2017), we want to close this chapter by highlighting that choosing the right dimensions for clustering is a design problem. Even if the data is informative about whether clustering matters for standard errors, they do not tell you whether you should adjust the standard errors for clustering. Clustering at too aggregate levels can hence lead to unnecessarily inflated standard errors.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#key-takeaways",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#key-takeaways",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFixed effects regressions control for unobserved firm and time-specific factors, reducing omitted variable bias in panel data models.\nThe fixest R package streamlines the estimation of fixed effects and supports clustering standard errors for robust inference.\nClustered standard errors adjust for residual dependence across firms or years, leading to more accurate \\(t\\)-statistics and confidence in significance tests.\nTwo-way clustering by firm and year is commonly used in finance to address both time-series and cross-sectional correlation in residuals.\nCareful model specification, including winsorization and proper clustering choices, enhances the credibility and reliability of empirical finance results.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#exercises",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#exercises",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Exercises",
    "text": "Exercises\n\nEstimate the two-way fixed effects model with two-way clustered standard errors using quarterly Compustat data from WRDS. Note that you can access quarterly data via tbl(wrds, I(\"comp.fundq\")).\nFollowing Peters and Taylor (2017), compute Tobin’s q as the market value of outstanding equity mktcap plus the book value of debt (dltt + dlc) minus the current assets atc and everything divided by the book value of property, plant and equipment ppegt. What is the correlation between the measures of Tobin’s q? What is the impact on the two-way fixed effects regressions?",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html",
    "href": "r/accessing-and-managing-financial-data.html",
    "title": "Accessing and Managing Financial Data",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we suggest a way to organize your financial data. Everybody who has experience with data is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome in the case of using different data formats, both across different projects and across different programming languages. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages.\nThis chapter shows how to import different open source data sets. Specifically, our data comes from the application programming interface (API) of Yahoo Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series that can be scraped directly from a website. We show how to process these raw data, as well as how to take a shortcut using the tidyfinance package, which provides a consistent interface to tidy financial data. We store all the data in a single database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\nFirst, we load the global R packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(scales)\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on.\nstart_date &lt;- ymd(\"1960-01-01\")\nend_date &lt;- ymd(\"2024-12-31\")",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#fama-french-data",
    "href": "r/accessing-and-managing-financial-data.html#fama-french-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Fama-French Data",
    "text": "Fama-French Data\nWe start by downloading some famous Fama-French factors (e.g., Fama and French 1993) and portfolio returns commonly used in empirical asset pricing. Fortunately, there is a neat package by Nelson Areal that allows us to access the data easily: the frenchdata package provides functions to download and read data sets from Prof. Kenneth French finance data library (Areal 2021). \n\nlibrary(frenchdata)\n\nWe can use the download_french_data() function of the package to download monthly Fama-French factors. The set Fama/French 3 Factors contains the return time series of the market mkt_excess, size smb and value hml alongside the risk-free rates rf. Note that we have to do some manual work to correctly parse all the columns and scale them appropriately, as the raw Fama-French data comes in a very unpractical data format. For precise descriptions of the variables, we suggest consulting Prof. Kenneth French’s finance data library directly. If you are on the website, check the raw data files to appreciate the time you can save thanks to frenchdata.\n\nfactors_ff3_monthly_raw &lt;- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff3_monthly &lt;- factors_ff3_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) |&gt; \n  filter(date &gt;= start_date & date &lt;= end_date)\n\nWe also download the set 5 Factors (2x3), which additionally includes the return time series of the profitability rmw and investment cma factors. We demonstrate how the monthly factors are constructed in the chapter Replicating Fama and French Factors.\n\nfactors_ff5_monthly_raw &lt;- download_french_data(\"Fama/French 5 Factors (2x3)\")\n\nfactors_ff5_monthly &lt;- factors_ff5_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    across(c(RF, `Mkt-RF`, SMB, HML, RMW, CMA), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) |&gt; \n  filter(date &gt;= start_date & date &lt;= end_date)\n\nIt is straightforward to download the corresponding daily Fama-French factors with the same function.\n\nfactors_ff3_daily_raw &lt;- download_french_data(\"Fama/French 3 Factors [Daily]\")\n\nfactors_ff3_daily &lt;- factors_ff3_daily_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = ymd(date),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date)\n\nIn a subsequent chapter, we also use the 10 monthly industry portfolios, so let us fetch that data, too.\n\nindustries_ff_monthly_raw &lt;- download_french_data(\"10 Industry Portfolios\")\n\nindustries_ff_monthly &lt;- industries_ff_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(date = floor_date(ymd(str_c(date, \"01\")), \"month\")) |&gt;\n  mutate(across(where(is.numeric), ~ . / 100)) |&gt;\n  select(date, everything()) |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date) |&gt; \n  rename_with(str_to_lower)\n\nIt is worth taking a look at all available portfolio return time series from Kenneth French’s homepage. You should check out the other sets by calling get_french_data_list().\nTo automatically download and process Fama-French data, you can also use the tidyfinance package with type = \"factors_ff_3_monthly\" or similar, e.g.:\n\ndownload_data(\n  type = \"factors_ff_3_monthly\", \n  start_date = start_date, \n  end_date = end_date\n)\n\nThe tidyfinance package implements the processing steps as above and returns the same cleaned data frame. The list of supported Fama-French data types can be called as follows:\n\nlist_supported_types(domain = \"Fama-French\")",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#q-factors",
    "href": "r/accessing-and-managing-financial-data.html#q-factors",
    "title": "Accessing and Managing Financial Data",
    "section": "q-Factors",
    "text": "q-Factors\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the Hou, Xue, and Zhang (2014) q-factor model. We refer to the extended background information provided by the original authors for further information. The q factors can be downloaded directly from the authors’ homepage from within read_csv().\nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the “R_”-prescript using regular expressions and write all column names in lowercase. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on try. You can check out style guides available online, e.g., Hadley Wickham’s tidyverse style guide.\n\nfactors_q_monthly_link &lt;-\n  \"https://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2023.csv\"\n\nfactors_q_monthly &lt;- read_csv(factors_q_monthly_link) |&gt;\n  mutate(date = ymd(str_c(year, month, \"01\", sep = \"-\"))) |&gt;\n  rename_with(~str_remove(., \"R_\")) |&gt;\n  rename_with(str_to_lower) |&gt;\n  mutate(across(-date, ~. / 100)) |&gt;\n  select(date, risk_free = f, mkt_excess = mkt, everything()) |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date)\n\nAgain, you can use the tidyfinance package for a shortcut:\n\ndownload_data(\n  type = \"factors_q5_monthly\", \n  start_date = start_date, \n  end_date = end_date\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "href": "r/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "title": "Accessing and Managing Financial Data",
    "section": "Macroeconomic Predictors",
    "text": "Macroeconomic Predictors\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. Welch and Goyal (2008) comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data updated to 2022 on Amit Goyal’s website. The data is an XLSX-file stored on a public Google drive location and we directly export a CSV file.\n\nsheet_id &lt;- \"1bM7vCWd3WOt95Sf9qjLPZjoiafgF_8EG\"\nsheet_name &lt;- \"Monthly\"\nmacro_predictors_url &lt;- paste0(\n  \"https://docs.google.com/spreadsheets/d/\", sheet_id,\n  \"/gviz/tq?tqx=out:csv&sheet=\", sheet_name\n)\nmacro_predictors_raw &lt;- read_csv(macro_predictors_url)\n\nNext, we transform the columns into the variables that we later use:\n\nThe dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (Campbell and Shiller 1988; Campbell and Yogo 2006).\nDividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).\nEarnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index (Campbell and Shiller 1988).\nDividend payout ratio (de), the difference between the log of dividends and the log of earnings (Lamont 1998).\nStock variance (svar), the sum of squared daily returns on the S&P 500 index (Guo 2006).\nBook-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average (Kothari and Shanken 1997).\nNet equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks (Campbell, Hilscher, and Szilagyi 2008).\nTreasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis (Campbell 1987).\nLong-term yield (lty), the long-term government bond yield from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nLong-term rate of returns (ltr), the long-term government bond returns from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nTerm spread (tms), the difference between the long-term yield on government bonds and the Treasury bill (Campbell 1987).\nDefault yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields (Fama and French 1989).\nInflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics (Campbell and Vuolteenaho 2004).\n\nFor variable definitions and the required data transformations, you can consult the material on Amit Goyal’s website.\n\nmacro_predictors &lt;- macro_predictors_raw |&gt;\n  mutate(date = ym(yyyymm)) |&gt;\n  mutate(across(where(is.character), as.numeric)) |&gt;\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend yield\n    ep = log(E12) - log(Index), # Earnings price ratio\n    de = log(D12) - log(E12), # Dividend payout ratio\n    tms = lty - tbl, # Term spread\n    dfy = BAA - AAA # Default yield spread\n  ) |&gt;\n  select(\n    date, rp_div, dp, dy, ep, de, svar,\n    bm = `b/m`, ntis, tbl, lty, ltr,\n    tms, dfy, infl\n  ) |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date) |&gt;\n  drop_na()\n\nTo get the equivalent data through tidyfinance, you can call:\n\ndownload_data(\n  type = \"macro_predictors_monthly\",\n  start_date = start_date,\n  end_date = end_date\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "href": "r/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Other Macroeconomic Data",
    "text": "Other Macroeconomic Data\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. The data can be downloaded directly from FRED by constructing the appropriate URL. For instance, let us consider the consumer price index (CPI) data that can be found under the CPIAUCNS:\n\nseries &lt;- \"CPIAUCNS\"\ncpi_url &lt;- paste0(\n  \"https://fred.stlouisfed.org/graph/fredgraph.csv?id=\", series\n)\n\nWe can then use the httr2 (Wickham 2024) package to request the CSV, extract the data from the response body, and convert the columns to a tidy format:\n\nlibrary(httr2)\n\nresp &lt;- request(cpi_url) |&gt; \n  req_perform()\nresp_csv &lt;- resp |&gt; \n  resp_body_string() \n\ncpi_monthly &lt;- resp_csv |&gt; \n  read_csv() |&gt;\n  mutate(\n    date = as.Date(observation_date),\n    value = as.numeric(.data[[series]]),\n    series = series,\n    .keep = \"none\"\n  ) |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date) |&gt; \n  mutate(\n    cpi = value / value[date == max(date)]\n  )\n\nThe last line sets the current (latest) price level as the reference price level.\nThe tidyfinance package can, of course, also fetch the same index data and many more data series:\n\ndownload_data(\n  type = \"fred\",\n  series = \"CPIAUCNS\",\n  start_date = start_date,\n  end_date = end_date\n)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: date &lt;date&gt;, value &lt;dbl&gt;, series &lt;chr&gt;\n\n\nTo download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the PCU2122212122210 key. If your desired time series is not supported through tidyfinance, we recommend working with the fredr package (Boysel and Vaughan 2021). Note that you need to get an API key to use its functionality. We refer to the package documentation for details.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#setting-up-a-database",
    "href": "r/accessing-and-managing-financial-data.html#setting-up-a-database",
    "title": "Accessing and Managing Financial Data",
    "section": "Setting Up a Database",
    "text": "Setting Up a Database\nNow that we have downloaded some (freely available) data from the web into the memory of our R session let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code.\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an SQLite database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that SQL (Structured Query Language) is a standard language for accessing and manipulating databases and heavily inspired the dplyr functions. We refer to this tutorial for more information on SQL.\nThere are two packages that make working with SQLite in R very simple: RSQLite (Müller et al. 2022) embeds the SQLite database engine in R, and dbplyr (Wickham, Girlich, and Ruiz 2022) is the database back-end for dplyr. These packages allow to set up a database to remotely store tables and use these remote database tables as if they are in-memory data frames by automatically converting dplyr into SQL. Check out the RSQLite and dbplyr vignettes for more information.\n\nlibrary(RSQLite)\nlibrary(dbplyr)\n\nAn SQLite database is easily created - the code below is really all there is. You do not need any external software. Note that we use the extended_types = TRUE option to enable date types when storing and fetching data. Otherwise, date columns are stored and retrieved as integers. We will use the file tidy_finance_r.sqlite, located in the data subfolder, to retrieve data for all subsequent chapters. The initial part of the code ensures that the directory is created if it does not already exist.\n\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")\n}\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the function dbWriteTable(), which copies the data to our SQLite-database.\n\ndbWriteTable(\n  tidy_finance,\n  \"factors_ff3_monthly\",\n  value = factors_ff3_monthly,\n  overwrite = TRUE\n)\n\nWe can use the remote table as an in-memory data frame by building a connection via tbl().\n\nfactors_ff3_monthly_db &lt;- tbl(tidy_finance, \"factors_ff3_monthly\")\n\nAll dplyr calls are evaluated lazily, i.e., the data is not in our R session’s memory, and the database does most of the work. You can see that by noticing that the output below does not show the number of rows. In fact, the following code chunk only fetches the top 10 rows from the database for printing.\n\nfactors_ff3_monthly_db |&gt;\n  select(date, rf)\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [data/tidy_finance_r.sqlite]\n  date           rf\n  &lt;date&gt;      &lt;dbl&gt;\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# ℹ more rows\n\n\nIf we want to have the whole table in memory, we need to collect() it. You will see that we regularly load the data into the memory in the next chapters.\n\nfactors_ff3_monthly_db |&gt;\n  select(date, rf) |&gt;\n  collect()\n\n# A tibble: 780 × 2\n  date           rf\n  &lt;date&gt;      &lt;dbl&gt;\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# ℹ 775 more rows\n\n\nThe last couple of code chunks is really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages.\nBefore we move on to the next data source, let us also store the other five tables in our new SQLite database.\n\ndbWriteTable(\n  tidy_finance,\n  \"factors_ff5_monthly\",\n  value = factors_ff5_monthly,\n  overwrite = TRUE\n)\n\ndbWriteTable(\n  tidy_finance,\n  \"factors_ff3_daily\",\n  value = factors_ff3_daily,\n  overwrite = TRUE\n)\n\ndbWriteTable(\n  tidy_finance,\n  \"industries_ff_monthly\",\n  value = industries_ff_monthly,\n  overwrite = TRUE\n)\n\ndbWriteTable(\n  tidy_finance,\n  \"factors_q_monthly\",\n  value = factors_q_monthly,\n  overwrite = TRUE\n)\n\ndbWriteTable(\n  tidy_finance,\n  \"macro_predictors\",\n  value = macro_predictors,\n  overwrite = TRUE\n)\n\ndbWriteTable(\n  tidy_finance,\n  \"cpi_monthly\",\n  value = cpi_monthly,\n  overwrite = TRUE\n)\n\nFrom now on, all you need to do to access data that is stored in the database is to follow three steps: (i) Establish the connection to the SQLite database, (ii) call the table you want to extract, and (iii) collect the data. For your convenience, the following steps show all you need in a compact fashion.\n\nlibrary(tidyverse)\nlibrary(RSQLite)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_q_monthly &lt;- tbl(tidy_finance, \"factors_q_monthly\")\nfactors_q_monthly &lt;- factors_q_monthly |&gt; collect()",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "href": "r/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "title": "Accessing and Managing Financial Data",
    "section": "Managing SQLite Databases",
    "text": "Managing SQLite Databases\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\nTo optimize the database file, you can run the VACUUM command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the dbSendQuery() function.\n\nres &lt;- dbSendQuery(tidy_finance, \"VACUUM\")\nres\n\n&lt;SQLiteResult&gt;\n  SQL  VACUUM\n  ROWS Fetched: 0 [complete]\n       Changed: 0\n\n\nThe VACUUM command actually performs a couple of additional cleaning steps, which you can read about in this tutorial. \nWe store the result of the above query in res because the database keeps the result set open. To close open results and avoid warnings going forward, we can use dbClearResult().\n\ndbClearResult(res)\n\nApart from cleaning up, you might be interested in listing all the tables that are currently in your database. You can do this via the dbListTables() function.\n\ndbListTables(tidy_finance)\n\n [1] \"beta\"                  \"compustat\"            \n [3] \"cpi_monthly\"           \"crsp_daily\"           \n [5] \"crsp_monthly\"          \"factors_ff3_daily\"    \n [7] \"factors_ff3_monthly\"   \"factors_ff5_monthly\"  \n [9] \"factors_q_monthly\"     \"fisd\"                 \n[11] \"industries_ff_monthly\" \"macro_predictors\"     \n[13] \"trace_enhanced\"       \n\n\nThis function comes in handy if you are unsure about the correct naming of the tables in your database.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#key-takeaways",
    "href": "r/accessing-and-managing-financial-data.html#key-takeaways",
    "title": "Accessing and Managing Financial Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nImporting Fama-French factors, q-factors, macroeconomic indicators, and CPI data is simplified through API calls, CSV parsing, and web scraping techniques.\nThe tidyfinance R package offers pre-processed access to financial datasets, reducing manual data cleaning and saving valuable time.\nCreating a centralized SQLite database helps manage and organize data efficiently across projects, while maintaining reproducibility.\nStructured database storage supports scalable data access, which is essential for long-term academic projects and collaborative work in finance.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#exercises",
    "href": "r/accessing-and-managing-financial-data.html#exercises",
    "title": "Accessing and Managing Financial Data",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the monthly Fama-French factors manually from Ken French’s data library and read them in via read_csv(). Validate that you get the same data as via the frenchdata package.\nDownload the daily Fama-French 5 factors using the frenchdata package. Use get_french_data_list() to find the corresponding table name. After the successful download and conversion to the column format that we used above, compare the rf, mkt_excess, smb, and hml columns of factors_ff3_daily to factors_ff5_daily. Discuss any differences you might find.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/changelog.html",
    "href": "r/changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "You can find every single change in our commit history. We collect the most important changes for Tidy Finance with R in the list below.\n\nMarch 4, 2025, Commit 0b2680: we revised the introductory sections of Tidy Finance with R entirely and included new chapters on Financial Statement Analysis and Discount Cash Flow analysis.\nSeptember 13, 2024, Commit 6464f94: we introduced the tidyfinance R package into the book. The package is available on CRAN.\nAugust 4, 2024, Commit 524bdd1: We added an additional filter to the Compustat download to exclude non-US companies in WRDS, CRSP, and Compustat.\nAugust 1, 2024, Commit 2980cf2: We updated the data until 2023-12-31 in all chapters.\nJuly 29, 2024, Commit cedec3e: We removed the month column from all chapters because it was misleading and consistently introduced date.\nJuly 16, 2024, Commit f4bbd00: We improved the documentation with respect to delisting returns in WRDS, CRSP, and Compustat.\nJune 3, 2024, Commit 23d379f: We fixed a bug in Univaritate Portfolio Sorts, which led to wrong annual returns in Figure 3.\nMay 15, 2024, Commit 2bb2e07: We added a new subsection about creating environment variables to Setting Up Your Environment.\nMay 15, 2024, Commit adccfc9: We updated the filters in CRSP download, so that correct historical information is used and daily and monthly data are aligned.\nApril 19, 2024, Commit d8c4de3: We updated to dbplyr version 2.5.0 and switched to the new I() instead of in_schema() syntax.\nMarch 4, 2024, Commit 6acb50b: We updated the download of monthly and daily CRSP data to the new official file format as distributed by WRDS, see additional information on the WRDS website.\nFeb 13, 2024, Commit 7871900: We updated the function for cleaning enhanced TRACE used in TRACE and FISD and shown in Appendix Clean Enhanced TRACE with R to reflect the correct time zone (i.e., New York, ET) and require less dependencies. We also updated the respective gist.\nFeb 13, 2024, Commit 5fce497: We removed the depedency on googledrive in Accessing and Managing Financial Data because frequently encountered failed downloads due to quota limits on the Google API.\nJan 4, 2024, Commit e9ab1a3: We updated the syntax of *_join() functions to use join_by() instead of by.\nDec 10, 2023, Commit 9814a2f: We added handling of delisting returns to daily CRSP download.\nOct 14, 2023 Commit b5a7495: We changed the download of daily CRSP data from individual stocks to batches in WRDS, CRSP, and Compustat.\nOct 12, 2023, Commit 48b6b29: We migrated from keras to torch in Option Pricing via Machine Learning for improved environment management.\nOct 4, 2023, Commit d4e0717: We added a new chapter Setting Up Your Environment.\nSep 28, 2023, Commit 290a612: We updated all data sources until 2022-12-31.\nSep 23, 2023, Commit f88f6c9: We switched from alabama and quadprog to nloptr in Constrained Optimization and Backtesting to be more consistent with the optimization in Python and to provide more flexibility with respect to constraints.\nJune 15, 2023, Commit 47dbb30: We moved the first usage of broom:tidy() from Fama-Macbeth Regressions to Univariate Portfolio Sorts to clean up the CAPM estimation.\nJune 12, 2023, Commit e008622: We fixed some inconsencies in notation of portfolio weights. Now, we refer to portfolio weights with \\(\\omega\\) throughout the complete book.\nJune 12, 2023, Commit 186ec7b2: We fixed a typo in the discussion of the elastic net in Chapter Factor Selection via Machine Learning.\nMay 23, 2023, Commit d5e355c: We update the workflow to collect() tables from tidy_finance.sqlite: To make variable selection more obvious, we now explicitly select() columns before collecting. As part of the pull request Commit 91d3077, we now select excess returns instead of net returns in the Chapter Fama-MacBeth Regressions.\nMay 20, 2023, Commit be0f0b4: We include NA-observations in the Mergent filters in Chapter TRACE and FISD.\nMay 17, 2023, Commit 2209bb1: We changed the assign_portfolio()-functions in Chapters Univariate Portfolio Sorts, Size Sorts and p-Hacking, Value and Bivariate Sorts, and Replicating Fama and French Factors. Additionally, we added a small explanation to potential issues with the function for clustered sorting variables in Chapter Univariate Portfolio Sorts.\nMay 12, 2023, Commit 54b76d7: We removed magic numbers in Chapter Introduction to Tidy Finance and introduced the scales packages already in the introduction chapter to reduce scaling issues in figures.\nMar. 30, 2023, Issue 29: We upgraded to tidyverse 2.0.0 and R 4.2.3 and removed all explicit loads of lubridate.\nFeb. 15, 2023, Commit bfda6af: We corrected an error in the calculation of the annualized average return volatility in the Chapter Introduction to Tidy Finance.\nMar. 06, 2023, Commit 857f0f5: We corrected an error in the label of Figure 6 in Chapter Introduction to Tidy Finance, which wrongly claimed to show the efficient tangency portfolio.\nMar. 09, 2023, Commit fae4ac3: We corrected a typo in the definition of the power utility function in Chapter Portfolio Performance. The utility function implemented in the code is now consistent with the text.",
    "crumbs": [
      "R",
      "Appendix",
      "Changelog"
    ]
  },
  {
    "objectID": "r/clean-enhanced-trace-with-r.html",
    "href": "r/clean-enhanced-trace-with-r.html",
    "title": "Clean Enhanced TRACE with R",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\n\n\nThis appendix contains code to clean enhanced TRACE with R. It is also available via the following GitHub gist. Hence, you could also source the function with devtools::source_gist(\"3a05b3ab281563b2e94858451c2eb3a4\"). We need this function in Chapter TRACE and FISD to download and clean enhanced TRACE trade messages following Dick-Nielsen (2009) and Dick-Nielsen (2014) for enhanced TRACE specifically. Relatedly, WRDS provides SAS code and there is Python code available by the project Open Source Bond Asset Pricing.\nThe function takes a vector of CUSIPs (in cusips), a connection to WRDS (connection) explained in Chapter 3, and a start and end date (start_date and end_date, respectively). Specifying too many CUSIPs will result in very slow downloads and a potential failure due to the size of the request to WRDS. The dates should be within the coverage of TRACE itself, i.e., starting after 2002, and the dates should be supplied using the class date. The output of the function contains all valid trade messages for the selected CUSIPs over the specified period.\n\nclean_enhanced_trace &lt;- function(cusips,\n                                 connection,\n                                 start_date = as.Date(\"2002-01-01\"),\n                                 end_date = today()) {\n\n  # Packages (required)\n  library(dplyr)\n  library(lubridate)\n  library(dbplyr)\n  library(RPostgres)\n\n  # Function checks ---------------------------------------------------------\n  # Input parameters\n  ## Cusips\n  if (length(cusips) == 0 | any(is.na(cusips))) stop(\"Check cusips.\")\n\n  ## Dates\n  if (!is.Date(start_date) | !is.Date(end_date)) stop(\"Dates needed\")\n  if (start_date &lt; as.Date(\"2002-01-01\")) stop(\"TRACE starts later.\")\n  if (end_date &gt; today()) stop(\"TRACE does not predict the future.\")\n  if (start_date &gt;= end_date) stop(\"Date conflict.\")\n\n  ## Connection\n  if (!dbIsValid(connection)) stop(\"Connection issue.\")\n\n  # Enhanced Trace ----------------------------------------------------------\n  trace_enhanced_db &lt;- tbl(connection, I(\"trace.trace_enhanced\"))\n  \n  # Main file\n   trace_all &lt;- trace_enhanced_db |&gt;\n    filter(\n      cusip_id %in% cusips,\n      between(trd_exctn_dt, start_date, end_date)\n    ) |&gt;\n    select(cusip_id, msg_seq_nb, orig_msg_seq_nb,\n           entrd_vol_qt, rptd_pr, yld_pt, rpt_side_cd, cntra_mp_id,\n           trd_exctn_dt, trd_exctn_tm, trd_rpt_dt, trd_rpt_tm,\n           pr_trd_dt, trc_st, asof_cd, wis_fl,\n           days_to_sttl_ct, stlmnt_dt, spcl_trd_fl) |&gt;\n    collect()\n\n  # Enhanced Trace: Post 06-02-2012 -----------------------------------------\n  # Trades (trc_st = T) and correction (trc_st = R)\n  trace_post_TR &lt;- trace_all |&gt;\n    filter((trc_st == \"T\" | trc_st == \"R\"),\n           trd_rpt_dt &gt;= as.Date(\"2012-02-06\"))\n\n  # Cancellations (trc_st = X) and correction cancellations (trc_st = C)\n  trace_post_XC &lt;- trace_all |&gt;\n    filter((trc_st == \"X\" | trc_st == \"C\"),\n           trd_rpt_dt &gt;= as.Date(\"2012-02-06\"))\n\n  # Cleaning corrected and cancelled trades\n  trace_post_TR &lt;- trace_post_TR |&gt;\n    anti_join(trace_post_XC,\n              by = join_by(cusip_id, msg_seq_nb, entrd_vol_qt,\n                           rptd_pr, rpt_side_cd, cntra_mp_id,\n                           trd_exctn_dt, trd_exctn_tm))\n\n  # Reversals (trc_st = Y)\n  trace_post_Y &lt;- trace_all |&gt;\n    filter(trc_st == \"Y\",\n           trd_rpt_dt &gt;= as.Date(\"2012-02-06\"))\n\n  # Clean reversals\n  ## match the orig_msg_seq_nb of the Y-message to\n  ## the msg_seq_nb of the main message\n  trace_post &lt;- trace_post_TR |&gt;\n    anti_join(trace_post_Y,\n              by = join_by(cusip_id, msg_seq_nb == orig_msg_seq_nb,\n                           entrd_vol_qt, rptd_pr, rpt_side_cd,\n                           cntra_mp_id, trd_exctn_dt, trd_exctn_tm))\n\n\n  # Enhanced TRACE: Pre 06-02-2012 ------------------------------------------\n  # Cancellations (trc_st = C)\n  trace_pre_C &lt;- trace_all |&gt;\n    filter(trc_st == \"C\",\n           trd_rpt_dt &lt; as.Date(\"2012-02-06\"))\n\n  # Trades w/o cancellations\n  ## match the orig_msg_seq_nb of the C-message\n  ## to the msg_seq_nb of the main message\n  trace_pre_T &lt;- trace_all |&gt;\n    filter(trc_st == \"T\",\n           trd_rpt_dt &lt; as.Date(\"2012-02-06\")) |&gt;\n    anti_join(trace_pre_C,\n              by = join_by(cusip_id, msg_seq_nb == orig_msg_seq_nb,\n                           entrd_vol_qt, rptd_pr, rpt_side_cd,\n                           cntra_mp_id, trd_exctn_dt, trd_exctn_tm))\n\n  # Corrections (trc_st = W) - W can also correct a previous W\n  trace_pre_W &lt;- trace_all |&gt;\n    filter(trc_st == \"W\",\n           trd_rpt_dt &lt; as.Date(\"2012-02-06\"))\n\n  # Implement corrections in a loop\n  ## Correction control\n  correction_control &lt;- nrow(trace_pre_W)\n  correction_control_last &lt;- nrow(trace_pre_W)\n\n  ## Correction loop\n  while (correction_control &gt; 0) {\n    # Corrections that correct some msg\n    trace_pre_W_correcting &lt;- trace_pre_W |&gt;\n      semi_join(trace_pre_T,\n                by = join_by(cusip_id, trd_exctn_dt,\n                             orig_msg_seq_nb == msg_seq_nb))\n\n    # Corrections that do not correct some msg\n    trace_pre_W &lt;- trace_pre_W |&gt;\n      anti_join(trace_pre_T,\n                by = join_by(cusip_id, trd_exctn_dt,\n                             orig_msg_seq_nb == msg_seq_nb))\n\n    # Delete msgs that are corrected and add correction msgs\n    trace_pre_T &lt;- trace_pre_T |&gt;\n      anti_join(trace_pre_W_correcting,\n                by = join_by(cusip_id, trd_exctn_dt,\n                             msg_seq_nb == orig_msg_seq_nb)) |&gt;\n      union_all(trace_pre_W_correcting)\n\n    # Escape if no corrections remain or they cannot be matched\n    correction_control &lt;- nrow(trace_pre_W)\n\n    if (correction_control == correction_control_last) {\n\n      correction_control &lt;- 0\n\n    }\n\n    correction_control_last &lt;- nrow(trace_pre_W)\n\n  }\n\n\n  # Clean reversals\n  ## Record reversals\n  trace_pre_R &lt;- trace_pre_T |&gt;\n    filter(asof_cd == 'R') |&gt;\n    group_by(cusip_id, trd_exctn_dt, entrd_vol_qt,\n             rptd_pr, rpt_side_cd, cntra_mp_id) |&gt;\n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |&gt;\n    mutate(seq = row_number()) |&gt;\n    ungroup()\n\n  ## Remove reversals and the reversed trade\n  trace_pre &lt;- trace_pre_T |&gt;\n    filter(is.na(asof_cd) | !(asof_cd %in% c('R', 'X', 'D'))) |&gt;\n    group_by(cusip_id, trd_exctn_dt, entrd_vol_qt,\n             rptd_pr, rpt_side_cd, cntra_mp_id) |&gt;\n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |&gt;\n    mutate(seq = row_number()) |&gt;\n    ungroup() |&gt;\n    anti_join(trace_pre_R,\n              by = join_by(cusip_id, trd_exctn_dt, entrd_vol_qt,\n                           rptd_pr, rpt_side_cd, cntra_mp_id, seq)) |&gt;\n    select(-seq)\n\n\n  # Agency trades -----------------------------------------------------------\n  # Combine pre and post trades\n  trace_clean &lt;- trace_post |&gt;\n    union_all(trace_pre)\n\n  # Keep angency sells and unmatched agency buys\n  ## Agency sells\n  trace_agency_sells &lt;- trace_clean |&gt;\n    filter(cntra_mp_id == \"D\",\n           rpt_side_cd == \"S\")\n\n  # Agency buys that are unmatched\n  trace_agency_buys_filtered &lt;- trace_clean |&gt;\n    filter(cntra_mp_id == \"D\",\n           rpt_side_cd == \"B\") |&gt;\n    anti_join(trace_agency_sells,\n              by = join_by(cusip_id, trd_exctn_dt,\n                           entrd_vol_qt, rptd_pr))\n\n  # Agency clean\n  trace_clean &lt;- trace_clean |&gt;\n    filter(cntra_mp_id == \"C\")  |&gt;\n    union_all(trace_agency_sells) |&gt;\n    union_all(trace_agency_buys_filtered)\n\n\n  # Additional Filters ------------------------------------------------------\n  trace_add_filters &lt;- trace_clean |&gt;\n    mutate(days_to_sttl_ct2 = stlmnt_dt - trd_exctn_dt) |&gt;\n    filter(is.na(days_to_sttl_ct) | as.numeric(days_to_sttl_ct) &lt;= 7,\n           is.na(days_to_sttl_ct2) | as.numeric(days_to_sttl_ct2) &lt;= 7,\n           wis_fl == \"N\",\n           is.na(spcl_trd_fl) | spcl_trd_fl == \"\",\n           is.na(asof_cd) | asof_cd == \"\")\n\n\n  # Output ------------------------------------------------------------------\n  # Only keep necessary columns\n  trace_final &lt;- trace_add_filters |&gt;\n    arrange(cusip_id, trd_exctn_dt, trd_exctn_tm) |&gt;\n    select(cusip_id, trd_exctn_dt, trd_exctn_tm,\n           rptd_pr, entrd_vol_qt, yld_pt, rpt_side_cd, cntra_mp_id) |&gt;\n    mutate(trd_exctn_tm = format(as_datetime(trd_exctn_tm, tz = \"America/New_York\"), \"%H:%M:%S\"))\n\n  trace_final\n}\n\n\n\n\n\nReferences\n\nDick-Nielsen, Jens. 2009. “Liquidity biases in TRACE.” The Journal of Fixed Income 19 (2): 43–55. https://doi.org/10.3905/jfi.2009.19.2.043.\n\n\n———. 2014. “How to clean enhanced TRACE data.” Working Paper. https://ssrn.com/abstract=2337908.",
    "crumbs": [
      "R",
      "Appendix",
      "Clean Enhanced TRACE with R"
    ]
  },
  {
    "objectID": "r/cover-and-logo-design.html",
    "href": "r/cover-and-logo-design.html",
    "title": "Cover and Logo Design",
    "section": "",
    "text": "The cover of the book is inspired by the fast growing generative art community in R. Generative art refers to art that in whole or in part has been created with the use of an autonomous system. Instead of creating random dynamics, we rely on what is core to the book: The evolution of financial markets. Each circle in the cover figure corresponds to daily market return within one year of our sample. Deviations from the circle line indicate positive or negative returns. The colors are determined by the standard deviation of market returns during the particular year. The few lines of code below replicate the entire figure. We use the Wes Andersen color palette (also throughout the entire book), provided by the package wesanderson (Ram and Wickham 2018)\n\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff3_daily &lt;- tbl(\n  tidy_finance,\n  \"factors_ff3_daily\"\n) |&gt;\n  collect()\n\ndata_plot &lt;- factors_ff3_daily |&gt;\n  select(date, mkt_excess) |&gt;\n  group_by(year = floor_date(date, \"year\")) |&gt;\n  mutate(group_id = cur_group_id())\n\ndata_plot &lt;- data_plot |&gt;\n  group_by(group_id) |&gt;\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) |&gt;\n  filter(year &gt;= \"1962-01-01\" & year &lt;= \"2021-12-31\")\n\nlevels &lt;- data_plot |&gt;\n  distinct(group_id, vola) |&gt;\n  arrange(vola) |&gt;\n  pull(vola)\n\ncp &lt;- coord_polar(\n  direction = -1,\n  clip = \"on\"\n)\n\ncp$is_free &lt;- function() TRUE\ncolors &lt;- wes_palette(\"Zissou1\",\n  n_groups(data_plot),\n  type = \"continuous\"\n)\n\ncover &lt;- data_plot |&gt;\n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    group = group_id,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id,\n    ncol = 10,\n    scales = \"free\"\n  ) +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\nggsave(\n plot = cover,\n width = 10,\n height = 6,\n filename = \"images/cover.png\",\n bg = \"white\"\n)\n\nTo generate our logo, we focus on year 2021 - the end of the sample period at the time we published tidy-finance.org for the first time.\n\nlogo &lt;- data_plot |&gt;\n  ungroup() |&gt; \n  filter(year == \"2021-01-01\") |&gt; \n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) +\n  scale_fill_manual(values =  \"white\") \n\nggsave(\n plot = logo,\n width = 840,\n height = 840,\n units = \"px\",\n filename = \"images/logo-website-white.png\",\n)\n\nggsave(\n plot = logo +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 840,\n height = 840,\n units = \"px\",\n filename = \"images/logo-website.png\",\n)\n\nHere is the code to generate the vector graphics for our buttons.\n\nbutton_r &lt;- data_plot |&gt;\n  ungroup() |&gt; \n  filter(year == \"2000-01-01\") |&gt; \n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) \n\nggsave(\n plot = button_r +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-r-blue.svg\",\n)\n\nggsave(\n plot = button_r +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[4]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-r-orange.svg\",\n)\n\nbutton_python &lt;- data_plot |&gt;\n  ungroup() |&gt; \n  filter(year == \"1991-01-01\") |&gt; \n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) \n\nggsave(\n plot = button_python +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-python-blue.svg\",\n)\n\nggsave(\n plot = button_python +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[4]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-python-orange.svg\",\n)\n\n\n\n\n\nReferences\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson palette generator. https://CRAN.R-project.org/package=wesanderson.",
    "crumbs": [
      "R",
      "Appendix",
      "Cover and Logo Design"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html",
    "href": "r/constrained-optimization-and-backtesting.html",
    "title": "Constrained Optimization and Backtesting",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we conduct portfolio backtesting in a realistic setting by including transaction costs and investment constraints such as no-short-selling rules. We start with standard mean-variance efficient portfolios and introduce constraints in a step-by-step manner. To do so, we rely on numerical optimization procedures in R. We conclude the chapter by providing an out-of-sample backtesting procedure for the different strategies that we introduce in this chapter.\nThroughout this chapter, we use the following R packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(nloptr)\nCompared to previous chapters, we introduce the nloptr package (Johnson 2007) to perform numerical constrained optimization for portfolio choice problems.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#data-preparation",
    "href": "r/constrained-optimization-and-backtesting.html#data-preparation",
    "title": "Constrained Optimization and Backtesting",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start by loading the required data from our SQLite-database introduced in Accessing and Managing Financial Data. For simplicity, we restrict our investment universe to the monthly Fama-French industry portfolio returns in the following application. \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nindustry_returns &lt;- tbl(tidy_finance, \"industries_ff_monthly\") |&gt;\n  select(-date) |&gt;\n  collect() |&gt; \n  drop_na()",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "href": "r/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Recap of Portfolio Choice",
    "text": "Recap of Portfolio Choice\nA common objective for portfolio optimization is to find mean-variance efficient portfolio weights, i.e., the allocation which delivers the lowest possible return variance for a given minimum level of expected returns. In the most extreme case, where the investor is only concerned about portfolio variance, they may choose to implement the minimum variance portfolio (MVP) weights which are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1\\] where \\(\\Sigma\\) is the \\((N \\times N)\\) covariance matrix of the returns. The optimal weights \\(\\omega_\\text{mvp}\\) can be found analytically and are \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). In terms of code, the math is equivalent to the following chunk. \n\nn_industries &lt;- ncol(industry_returns)\n\nSigma &lt;- cov(industry_returns)\nw_mvp &lt;- solve(Sigma) %*% rep(1, n_industries)\nw_mvp &lt;- as.vector(w_mvp / sum(w_mvp))\n\nmu &lt;- colMeans(industry_returns)\n\nNext, consider an investor who aims to achieve minimum variance given a required expected portfolio return \\(\\bar{\\mu}\\) such that she chooses \\[\\omega_\\text{eff}({\\bar{\\mu}}) =\\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] We leave it as an exercise below to show that the portfolio choice problem can equivalently be formulated for an investor with mean-variance preferences and risk aversion factor \\(\\gamma\\). That means the investor aims to choose portfolio weights as the solution to \\[ \\omega^*_\\gamma = \\arg\\max \\omega' \\mu - \\frac{\\gamma}{2}\\omega'\\Sigma \\omega\\quad \\text{ s.t. } \\omega'\\iota = 1.\\] The solution to the optimal portfolio choice problem is: \\[\\omega^*_{\\gamma}  = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu  + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota.\\] Empirically, this classical solution imposes many problems. In particular, the estimates of \\(\\mu\\) are noisy over short horizons, the (\\(N \\times N\\)) matrix \\(\\Sigma\\) contains \\(N(N-1)/2\\) distinct elements and thus, estimation error is huge. Seminal papers on the effect of ignoring estimation uncertainty, among others, are Brown (1976), Jobson and Korkie (1980), Jorion (1986), and Chopra and Ziemba (1993).\nEven worse, if the asset universe contains more assets than available time periods \\((N &gt; T)\\), the sample covariance matrix is no longer positive definite such that the inverse \\(\\Sigma^{-1}\\) does not exist anymore. To address estimation issues for vast-dimensional covariance matrices, regularization techniques are a popular tool (see, e.g., Ledoit and Wolf 2003, 2004, 2012; Fan, Fan, and Lv 2008).\nWhile the uncertainty associated with estimated parameters is challenging, the data-generating process is also unknown to the investor. In other words, model uncertainty reflects that it is ex-ante not even clear which parameters require estimation (for instance, if returns are driven by a factor model, selecting the universe of relevant factors imposes model uncertainty). Wang (2005) and Garlappi, Uppal, and Wang (2007) provide theoretical analysis on optimal portfolio choice under model and estimation uncertainty. In the most extreme case, Pflug, Pichler, and Wozabal (2012) shows that the naive portfolio which allocates equal wealth to all assets is the optimal choice for an investor averse to model uncertainty.\nOn top of the estimation uncertainty, transaction costs are a major concern. Rebalancing portfolios is costly, and, therefore, the optimal choice should depend on the investor’s current holdings. In the presence of transaction costs, the benefits of reallocating wealth may be smaller than the costs associated with turnover. This aspect has been investigated theoretically, among others, for one risky asset by Magill and Constantinides (1976) and Davis and Norman (1990). Subsequent extensions to the case with multiple assets have been proposed by Balduzzi and Lynch (1999) and Balduzzi and Lynch (2000). More recent papers on empirical approaches which explicitly account for transaction costs include Gârleanu and Pedersen (2013), and DeMiguel, Nogales, and Uppal (2014), and DeMiguel, Martín-Utrera, and Nogales (2015).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "href": "r/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "title": "Constrained Optimization and Backtesting",
    "section": "Estimation Uncertainty and Transaction Costs",
    "text": "Estimation Uncertainty and Transaction Costs\nThe empirical evidence regarding the performance of a mean-variance optimization procedure in which you simply plug in some sample estimates \\(\\hat \\mu\\) and \\(\\hat \\Sigma\\) can be summarized rather briefly: mean-variance optimization performs poorly! The literature discusses many proposals to overcome these empirical issues. For instance, one may impose some form of regularization of \\(\\Sigma\\), rely on Bayesian priors inspired by theoretical asset pricing models (Kan and Zhou 2007) or use high-frequency data to improve forecasting (Hautsch, Kyj, and Malec 2015). One unifying framework that works easily, effectively (even for large dimensions), and is purely inspired by economic arguments is an ex-ante adjustment for transaction costs (Hautsch and Voigt 2019).\nAssume that returns are from a multivariate normal distribution with mean \\(\\mu\\) and variance-covariance matrix \\(\\Sigma\\), \\(N(\\mu,\\Sigma)\\). Additionally, we assume quadratic transaction costs which penalize rebalancing such that \\[\n\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) = \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\\end{aligned}\\] with cost parameter \\(\\beta&gt;0\\) and \\(\\omega_{t^+} = {\\omega_t \\circ  (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\). \\(\\omega_{t^+}\\) denotes the portfolio weights just before rebalancing. Note that \\(\\omega_{t^+}\\) differs mechanically from \\(\\omega_t\\) due to the returns in the past period. Intuitively, transaction costs penalize portfolio performance when the portfolio is shifted from the current holdings \\(\\omega_{t^+}\\) to a new allocation \\(\\omega_{t+1}\\). In this setup, transaction costs do not increase linearly. Instead, larger rebalancing is penalized more heavily than small adjustments. Then, the optimal portfolio choice for an investor with mean variance preferences is \\[\\begin{aligned}\\omega_{t+1} ^* &=  \\arg\\max \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\text{ s.t. } \\iota'\\omega = 1\\\\\n&=\\arg\\max\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t.} \\iota'\\omega=1,\\end{aligned}\\] where \\[\\mu^*=\\mu+\\beta \\omega_{t^+} \\quad  \\text{and} \\quad \\Sigma^*=\\Sigma + \\frac{\\beta}{\\gamma} I_N.\\] As a result, adjusting for transaction costs implies a standard mean-variance optimal portfolio choice with adjusted return parameters \\(\\Sigma^*\\) and \\(\\mu^*\\): \\[\\omega^*_{t+1} = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^*  + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota.\\]\nAn alternative formulation of the optimal portfolio can be derived as follows: \\[\\omega_{t+1} ^*=\\arg\\max\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t. } \\iota'\\omega=1.\\] The optimal weights correspond to a mean-variance portfolio, where the vector of expected returns is such that assets that currently exhibit a higher weight are considered as delivering a higher expected return.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "href": "r/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Optimal Portfolio Choice",
    "text": "Optimal Portfolio Choice\nThe function below implements the efficient portfolio weight in its general form, allowing for transaction costs (conditional on the holdings before reallocation). For \\(\\beta=0\\), the computation resembles the standard mean-variance efficient framework. gamma denotes the coefficient of risk aversion \\(\\gamma\\), beta is the transaction cost parameter \\(\\beta\\) and w_prev are the weights before rebalancing \\(\\omega_{t^+}\\).\n\ncompute_efficient_weight &lt;- function(Sigma,\n                                     mu,\n                                     gamma = 2,\n                                     beta = 0, # transaction costs\n                                     w_prev = rep(\n                                       1 / ncol(Sigma),\n                                       ncol(Sigma)\n                                     )) {\n  iota &lt;- rep(1, ncol(Sigma))\n  Sigma_processed &lt;- Sigma + beta / gamma * diag(ncol(Sigma))\n  mu_processed &lt;- mu + beta * w_prev\n\n  Sigma_inverse &lt;- solve(Sigma_processed)\n\n  w_mvp &lt;- Sigma_inverse %*% iota\n  w_mvp &lt;- as.vector(w_mvp / sum(w_mvp))\n  w_opt &lt;- w_mvp + 1 / gamma *\n    (Sigma_inverse - w_mvp %*% t(iota) %*% Sigma_inverse) %*%\n      mu_processed\n  \n  return(as.vector(w_opt))\n}\n\nw_efficient &lt;- compute_efficient_weight(Sigma, mu)\nround(w_efficient, 3)\n\n [1]  1.192  0.239 -1.666  0.616  0.453 -0.437  0.755  0.359 -0.047\n[10] -0.464\n\n\nThe portfolio weights above indicate the efficient portfolio for an investor with risk aversion coefficient \\(\\gamma=2\\) in absence of transaction costs. Some of the positions are negative which implies short-selling, most of the positions are rather extreme. For instance, a position of \\(-1\\) implies that the investor takes a short position worth their entire wealth to lever long positions in other assets. What is the effect of transaction costs or different levels of risk aversion on the optimal portfolio choice? The following few lines of code analyze the distance between the minimum variance portfolio and the portfolio implemented by the investor for different values of the transaction cost parameter \\(\\beta\\) and risk aversion \\(\\gamma\\).\n\ngammas &lt;- c(2, 4, 8, 20)\nbetas &lt;- 20 * qexp((1:99) / 100)\n                   \ntransaction_costs &lt;- expand_grid(\n  gamma = gammas,\n  beta = betas\n) |&gt;\n  mutate(\n    weights = map2(\n      gamma,\n      beta,\n      \\(x, y) compute_efficient_weight(\n        Sigma,\n        mu,\n        gamma = x,\n        beta = y / 10000,\n        w_prev = w_mvp\n      )\n    ),\n    concentration = map_dbl(weights, \\(x) sum(abs(x - w_mvp)))\n  )\n\nThe code chunk above computes the optimal weight in presence of transaction cost for different values of \\(\\beta\\) and \\(\\gamma\\) but with the same initial allocation, the theoretical optimal minimum variance portfolio. Starting from the initial allocation, the investor chooses their optimal allocation along the efficient frontier to reflect their own risk preferences. If transaction costs would be absent, the investor would simply implement the mean-variance efficient allocation. If transaction costs make it costly to rebalance, their optimal portfolio choice reflects a shift toward the efficient portfolio, whereas their current portfolio anchors their investment.\n\ntransaction_costs |&gt;\n  mutate(risk_aversion = as_factor(gamma)) |&gt;\n  ggplot(aes(\n    x = beta,\n    y = concentration,\n    color = risk_aversion,\n    linetype = risk_aversion\n  )) +\n  geom_line() +\n  guides(linetype = \"none\") + \n  labs(\n    x = \"Transaction cost parameter\",\n    y = \"Distance from MVP\",\n    color = \"Risk aversion\",\n    title = \"Portfolio weights for different risk aversion and transaction cost\"\n  )\n\n\n\n\n\n\n\nFigure 1: The horizontal axis indicates the distance from the empirical minimum variance portfolio weight, measured by the sum of the absolute deviations of the chosen portfolio from the benchmark.\n\n\n\n\n\nFigure 1 shows rebalancing from the initial portfolio (which we always set to the minimum variance portfolio weights in this example). The higher the transaction costs parameter \\(\\beta\\), the smaller is the rebalancing from the initial portfolio. In addition, if risk aversion \\(\\gamma\\) increases, the efficient portfolio is closer to the minimum variance portfolio weights such that the investor desires less rebalancing from the initial holdings.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#constrained-optimization",
    "href": "r/constrained-optimization-and-backtesting.html#constrained-optimization",
    "title": "Constrained Optimization and Backtesting",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\nNext, we introduce constraints to the above optimization procedure. Very often, typical constraints such as short-selling restrictions prevent analytical solutions for optimal portfolio weights (short-selling restrictions simply imply that negative weights are not allowed such that we require that \\(w_i \\geq 0\\quad \\forall i\\)). However, numerical optimization allows computing the solutions to such constrained problems.\nWe rely on the powerful nloptr package, which provides a common interface to a number of different optimization routines. In particular, we employ the Sequential Least-Squares Quadratic Programming (SLSQP) algorithm of Kraft (1994) because it is able to hand multiple equality and inequality constraints at the same time and typically used for problems where the objective function and the constraints are twice continuously differentiable. We hence have to provide the algorithm with the objective function and its gradient, as well as the constraints and their Jacobian.\nWe illustrate the use of the nloptr() function by replicating the analytical solutions for the minimum variance and efficient portfolio weights from above. Note that the equality constraint for both solutions is given by the requirement that the weights must sum up to one. In addition, we supply a vector of equal weights as an initial value for the algorithm in all applications. We verify that the output is equal to the above solution. Note that near() is a safe way to compare two vectors for pairwise equality. The alternative == is sensitive to small differences that may occur due to the representation of floating points on a computer, while near() has a built-in tolerance.\n\nw_initial &lt;- rep(1 / n_industries, n_industries)\n\nobjective_mvp &lt;- function(w) {\n  0.5 * t(w) %*% Sigma %*% w\n}\n\ngradient_mvp &lt;- function(w) {\n  Sigma %*% w\n}\n\nequality_constraint &lt;- function(w) {\n  sum(w) - 1\n}\n\njacobian_equality &lt;- function(w) {\n  rep(1, n_industries)\n}\n\noptions &lt;- list(\n  \"xtol_rel\"=1e-20, \n  \"algorithm\" = \"NLOPT_LD_SLSQP\", \n  \"maxeval\" = 10000\n)\n\nw_mvp_numerical &lt;- nloptr(\n  x0 = w_initial, \n  eval_f = objective_mvp, \n  eval_grad_f = gradient_mvp,\n  eval_g_eq = equality_constraint,\n  eval_jac_g_eq = jacobian_equality,\n  opts = options\n)\n\nall(near(w_mvp, w_mvp_numerical$solution))\n\n[1] TRUE\n\nobjective_efficient &lt;- function(w) {\n  2 * 0.5 * t(w) %*% Sigma %*% w - sum((1 + mu) * w)\n}\n\ngradient_efficient &lt;- function(w) {\n  2 * Sigma %*% w - (1 + mu)\n}\n\nw_efficient_numerical &lt;- nloptr(\n  x0 = w_initial, \n  eval_f = objective_efficient, \n  eval_grad_f = gradient_efficient,\n  eval_g_eq = equality_constraint, \n  eval_jac_g_eq = jacobian_equality,\n  opts = options\n)\n\nall(near(w_efficient, w_efficient_numerical$solution))\n\n[1] TRUE\n\n\nThe result above shows that indeed the numerical procedure recovered the optimal weights for a scenario, where we already know the analytic solution. For more complex optimization routines, R’s optimization task view provides an overview of the vast optimization landscape. \nNext, we approach problems where no analytical solutions exist. First, we additionally impose short-sale constraints, which implies \\(N\\) inequality constraints of the form \\(\\omega_i &gt;=0\\). We can implement the short-sale constraints by imposing a vector of lower bounds lb = rep(0, n_industries).\n\nw_no_short_sale &lt;- nloptr(\n  x0 = w_initial, \n  eval_f = objective_efficient, \n  eval_grad_f = gradient_efficient,\n  eval_g_eq = equality_constraint, \n  eval_jac_g_eq = jacobian_equality,\n  lb = rep(0, n_industries),\n  opts = options\n)\n\nround(w_no_short_sale$solution, 3)\n\n [1] 0.280 0.000 0.000 0.198 0.000 0.000 0.339 0.182 0.000 0.000\n\n\nAs expected, the resulting portfolio weights are all positive (up to numerical precision). Typically, the holdings in the presence of short-sale constraints are concentrated among way fewer assets than for the unrestricted case. You can verify that sum(w_no_short_sale$solution) returns 1. In other words, nloptr() provides the numerical solution to a portfolio choice problem for a mean-variance investor with risk aversion gamma = 2, where negative holdings are forbidden.\nnloptr() can also handle more complex problems. As an example, we show how to compute optimal weights, subject to the so-called Regulation-T constraint, which requires that the sum of all absolute portfolio weights is smaller than 1.5, that is \\(\\sum_{i=1}^N |\\omega_i| \\leq 1.5\\). The constraint enforces that a maximum of 50 percent of the allocated wealth can be allocated to short positions, thus implying an initial margin requirement of 50 percent. Imposing such a margin requirement reduces portfolio risks because extreme portfolio weights are not attainable anymore. The implementation of Regulation-T rules is numerically interesting because the margin constraints imply a non-linear constraint on the portfolio weights. \n\nreg_t &lt;- 1.5\n\ninequality_constraint &lt;- function(w) {\n  sum(abs(w)) - reg_t\n}\n\njacobian_inequality &lt;- function(w) {\n  sign(w)\n}\n\nobjective_reg_t &lt;- function(w) {\n  - t(w) %*% (1 + mu) +\n    2 * 0.5 * t(w) %*% Sigma %*% w\n}\n\ngradient_reg_t &lt;- function(w) {\n  - (1 + mu) + 2 * Sigma %*% w\n}\n\nw_reg_t &lt;- nloptr(\n  x0 = w_initial,\n  eval_f = objective_reg_t, \n  eval_grad_f = gradient_reg_t,\n  eval_g_eq = equality_constraint, \n  eval_jac_g_eq = jacobian_equality, \n  eval_g_ineq = inequality_constraint, \n  eval_jac_g_ineq = jacobian_inequality,\n  opts = options\n)\n\nround(w_reg_t$solution, 3)\n\n [1]  0.359  0.000 -0.250  0.245  0.051  0.000  0.401  0.195  0.000\n[10]  0.000\n\n\nFigure 2 shows the optimal allocation weights across all 10 industries for the four different strategies considered so far: minimum variance, efficient portfolio with \\(\\gamma\\) = 2, efficient portfolio with short-sale constraints, and the Regulation-T constrained portfolio.\n\ntibble(\n  `No short-sale` = w_no_short_sale$solution,\n  `Minimum Variance` = w_mvp,\n  `Efficient portfolio` = compute_efficient_weight(Sigma, mu),\n  `Regulation-T` = w_reg_t$solution,\n  Industry = colnames(industry_returns)\n) |&gt;\n  pivot_longer(-Industry,\n    names_to = \"Strategy\",\n    values_to = \"weights\"\n  ) |&gt;\n  ggplot(aes(\n    fill = Strategy,\n    y = weights,\n    x = Industry\n  )) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  coord_flip() +\n  labs(\n    y = \"Allocation weight\", fill = NULL,\n    title = \"Optimal allocations for different strategies\"\n  ) +\n  scale_y_continuous(labels = percent)\n\n\n\n\n\n\n\nFigure 2: Optimal allocation weights for the 10 industry portfolios and the 4 different allocation strategies.\n\n\n\n\n\nThe results clearly indicate the effect of imposing additional constraints: the extreme holdings the investor implements if they follow the (theoretically optimal) efficient portfolio vanish under, e.g., the Regulation-T constraint. You may wonder why an investor would deviate from what is theoretically the optimal portfolio by imposing potentially arbitrary constraints. The short answer is: the efficient portfolio is only efficient if the true parameters of the data-generating process correspond to the estimated parameters \\(\\hat\\Sigma\\) and \\(\\hat\\mu\\). Estimation uncertainty may thus lead to inefficient allocations. By imposing restrictions, we implicitly shrink the set of possible weights and prevent extreme allocations, which could result from error-maximization due to estimation uncertainty (Jagannathan and Ma 2003).\nBefore we move on, we want to propose a final allocation strategy, which reflects a somewhat more realistic structure of transaction costs instead of the quadratic specification used above. The function below computes efficient portfolio weights while adjusting for transaction costs of the form \\(\\beta\\sum_{i=1}^N |(\\omega_{i, t+1} - \\omega_{i, t^+})|\\). No closed-form solution exists, and we rely on non-linear optimization procedures.\n\ncompute_efficient_weight_L1_TC &lt;- function(mu,\n                                           Sigma,\n                                           gamma,\n                                           beta,\n                                           initial_weights) {\n  objective &lt;- function(w) {\n    -t(w) %*% mu +\n      gamma / 2 * t(w) %*% Sigma %*% w +\n      (beta / 10000) / 2 * sum(abs(w - initial_weights))\n  }\n  \n  gradient &lt;- function(w) {\n    -mu + gamma * Sigma %*% w + \n      (beta / 10000) * 0.5 * sign(w - initial_weights)\n  }\n\n  w_optimal &lt;- nloptr(\n    x0 = initial_weights,\n    eval_f = objective, \n    eval_grad_f = gradient,\n    eval_g_eq = equality_constraint, \n    eval_jac_g_eq = jacobian_equality, \n    opts = options\n  )\n\n  return(w_optimal$solution)\n}",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "href": "r/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "title": "Constrained Optimization and Backtesting",
    "section": "Out-of-Sample Backtesting",
    "text": "Out-of-Sample Backtesting\nFor the sake of simplicity, we committed one fundamental error in computing portfolio weights above: we used the full sample of the data to determine the optimal allocation (Arnott, Harvey, and Markowitz 2019). To implement this strategy at the beginning of the 2000s, you will need to know how the returns will evolve until 2021. While interesting from a methodological point of view, we cannot evaluate the performance of the portfolios in a reasonable out-of-sample fashion. We do so next in a backtesting application for three strategies. For the backtest, we recompute optimal weights just based on past available data.\nThe few lines below define the general setup. We consider 120 periods from the past to update the parameter estimates before recomputing portfolio weights. Then, we update portfolio weights which is costly and affects the performance. The portfolio weights determine the portfolio return. A period later, the current portfolio weights have changed and form the foundation for transaction costs incurred in the next period. We consider three different competing strategies: the mean-variance efficient portfolio, the mean-variance efficient portfolio with ex-ante adjustment for transaction costs, and the naive portfolio, which allocates wealth equally across the different assets.\n\nwindow_length &lt;- 120\nperiods &lt;- nrow(industry_returns) - window_length\n\nbeta &lt;- 50\ngamma &lt;- 2\n\nperformance_values &lt;- matrix(NA,\n  nrow = periods,\n  ncol = 3\n)\ncolnames(performance_values) &lt;- c(\"raw_return\", \"turnover\", \"net_return\")\n\nperformance_values &lt;- list(\n  \"MV (TC)\" = performance_values,\n  \"Naive\" = performance_values,\n  \"MV\" = performance_values\n)\n\nw_prev_1 &lt;- w_prev_2 &lt;- w_prev_3 &lt;- rep(\n  1 / n_industries,\n  n_industries\n)\n\nWe also define two helper functions: one to adjust the weights due to returns and one for performance evaluation, where we compute realized returns net of transaction costs.\n\nadjust_weights &lt;- function(w, next_return) {\n  w_prev &lt;- 1 + w * next_return\n  as.numeric(w_prev / sum(as.vector(w_prev)))\n}\n\nevaluate_performance &lt;- function(w, w_previous, next_return, beta = 50) {\n  raw_return &lt;- as.matrix(next_return) %*% w\n  turnover &lt;- sum(abs(w - w_previous))\n  net_return &lt;- raw_return - beta / 10000 * turnover\n  c(raw_return, turnover, net_return)\n}\n\nThe following code chunk performs a rolling-window estimation, which we implement in a loop. In each period, the estimation window contains the returns available up to the current period. Note that we use the sample variance-covariance matrix and ignore the estimation of \\(\\hat\\mu\\) entirely, but you might use more advanced estimators in practice.\n\nfor (p in 1:periods) {\n  returns_window &lt;- industry_returns[p:(p + window_length - 1), ]\n  next_return &lt;- industry_returns[p + window_length, ] |&gt; as.matrix()\n\n  Sigma &lt;- cov(returns_window)\n  mu &lt;- 0 * colMeans(returns_window)\n\n  # Transaction-cost adjusted portfolio\n  w_1 &lt;- compute_efficient_weight_L1_TC(\n    mu = mu,\n    Sigma = Sigma,\n    beta = beta,\n    gamma = gamma,\n    initial_weights = w_prev_1\n  )\n\n  performance_values[[1]][p, ] &lt;- evaluate_performance(w_1,\n    w_prev_1,\n    next_return,\n    beta = beta\n  )\n\n  w_prev_1 &lt;- adjust_weights(w_1, next_return)\n\n  # Naive portfolio\n  w_2 &lt;- rep(1 / n_industries, n_industries)\n\n  performance_values[[2]][p, ] &lt;- evaluate_performance(\n    w_2,\n    w_prev_2,\n    next_return\n  )\n\n  w_prev_2 &lt;- adjust_weights(w_2, next_return)\n\n  # Mean-variance efficient portfolio (w/o transaction costs)\n  w_3 &lt;- compute_efficient_weight(\n    Sigma = Sigma,\n    mu = mu,\n    gamma = gamma\n  )\n\n  performance_values[[3]][p, ] &lt;- evaluate_performance(\n    w_3,\n    w_prev_3,\n    next_return\n  )\n\n  w_prev_3 &lt;- adjust_weights(w_3, next_return)\n}\n\nFinally, we get to the evaluation of the portfolio strategies net-of-transaction costs. Note that we compute annualized returns and standard deviations.\n\nperformance &lt;- lapply(\n  performance_values,\n  as_tibble\n) |&gt;\n  bind_rows(.id = \"strategy\")\n\nlength_year &lt;- 12\n\nperformance_table &lt;- performance |&gt;\n  group_by(Strategy = strategy) |&gt;\n  summarize(\n    Mean = length_year * mean(100 * net_return),\n    SD = sqrt(length_year) * sd(100 * net_return),\n    `Sharpe ratio` = if_else(Mean &gt; 0,\n      Mean / SD,\n      NA_real_\n    ),\n    Turnover = 100 * mean(turnover)\n  )\n\nperformance_table |&gt; \n  mutate(across(-Strategy, ~round(., 4)))\n\n# A tibble: 3 × 5\n  Strategy  Mean    SD `Sharpe ratio` Turnover\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1 MV       -1.06  12.6         NA     210.    \n2 MV (TC)  12.1   15.1          0.798   0.0186\n3 Naive    12.1   15.1          0.796   0.236 \n\n\nThe results clearly speak against mean-variance optimization. Turnover is huge when the investor only considers their portfolio’s expected return and variance. Effectively, the mean-variance portfolio generates a negative annualized return after adjusting for transaction costs. At the same time, the naive portfolio turns out to perform very well. In fact, the performance gains of the transaction-cost adjusted mean-variance portfolio are small. The out-of-sample Sharpe ratio is slightly higher than for the naive portfolio. Note the extreme effect of turnover penalization on turnover: MV (TC) effectively resembles a buy-and-hold strategy which only updates the portfolio once the estimated parameters \\(\\hat\\mu_t\\) and \\(\\hat\\Sigma_t\\) indicate that the current allocation is too far away from the optimal theoretical portfolio.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#key-takeaways",
    "href": "r/constrained-optimization-and-backtesting.html#key-takeaways",
    "title": "Constrained Optimization and Backtesting",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe nloptr R package can be used to solve constrained portfolio optimization problems that cannot be addressed analytically, including margin and regulatory constraints.\nTransaction costs can be modeled in both quadratic and absolute terms, showing how rebalancing penalties influence portfolio allocations and reduce excessive turnover.\nAn out-of-sample backtesting framework demonstrates that naive portfolios often outperform classical mean-variance strategies once transaction costs are considered.\nThe findings highlight the practical trade-offs between theoretical optimality and robust, implementable investment strategies under uncertainty.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#exercises",
    "href": "r/constrained-optimization-and-backtesting.html#exercises",
    "title": "Constrained Optimization and Backtesting",
    "section": "Exercises",
    "text": "Exercises\n\nConsider the portfolio choice problem for transaction-cost adjusted certainty equivalent maximization with risk aversion parameter \\(\\gamma\\) \\[\\omega_{t+1} ^* =  \\arg\\max_{\\omega \\in \\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega\\] where \\(\\Sigma\\) and \\(\\mu\\) are (estimators of) the variance-covariance matrix of the returns and the vector of expected returns. Assume for now that transaction costs are quadratic in rebalancing and proportional to stock illiquidity such that \\[\\nu_t\\left(\\omega, B\\right) = \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right)\\] where \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) is a diagonal matrix, where \\(ill_1, \\ldots, ill_N\\). Derive a closed-form solution for the mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based on the transaction cost specification above. Discuss the effect of illiquidity \\(ill_i\\) on the individual portfolio weights relative to an investor that myopically ignores transaction costs in their decision.\nUse the solution from the previous exercise to update the function compute_efficient_weight() such that you can compute optimal weights conditional on a matrix \\(B\\) with illiquidity measures.\nIllustrate the evolution of the optimal weights from the naive portfolio to the efficient portfolio in the mean-standard deviation diagram.\nIs it always optimal to choose the same \\(\\beta\\) in the optimization problem than the value used in evaluating the portfolio performance? In other words, can it be optimal to choose theoretically sub-optimal portfolios based on transaction cost considerations that do not reflect the actual incurred costs? Evaluate the out-of-sample Sharpe ratio after transaction costs for a range of different values of imposed \\(\\beta\\) values.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html",
    "href": "r/univariate-portfolio-sorts.html",
    "title": "Univariate Portfolio Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we dive into portfolio sorts, one of the most widely used statistical methodologies in empirical asset pricing (e.g., Bali, Engle, and Murray 2016). The key application of portfolio sorts is to examine whether one or more variables can predict future excess returns. In general, the idea is to sort individual stocks into portfolios, where the stocks within each portfolio are similar with respect to a sorting variable, such as firm size. The different portfolios then represent well-diversified investments that differ in the level of the sorting variable. You can then attribute the differences in the return distribution to the impact of the sorting variable. We start by introducing univariate portfolio sorts (which sort based on only one characteristic) and tackle bivariate sorting in Value and Bivariate Sorts.\nA univariate portfolio sort considers only one sorting variable \\(x_{t-1,i}\\). Here, \\(i\\) denotes the stock and \\(t-1\\) indicates that the characteristic is observable by investors at time \\(t\\).\nThe objective is to assess the cross-sectional relation between \\(x_{t-1,i}\\) and, typically, stock excess returns \\(r_{t,i}\\) at time \\(t\\) as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas from the previous chapter as our sorting variable.\nThe current chapter relies on the following set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(lmtest)\nlibrary(broom)\nlibrary(sandwich)\nCompared to previous chapters, we introduce lmtest (Zeileis and Hothorn 2002) for inference for estimated coefficients, broom package (Robinson, Hayes, and Couch 2022) to tidy the estimation output of many estimated linear models, and sandwich (Zeileis 2006) for different covariance matrix estimators",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#data-preparation",
    "href": "r/univariate-portfolio-sorts.html#data-preparation",
    "title": "Univariate Portfolio Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start with loading the required data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. In particular, we use the monthly CRSP sample as our asset universe. Once we form our portfolios, we use the Fama-French market factor returns to compute the risk-adjusted performance (i.e., alpha). beta is the tibble with market betas computed in the previous chapter.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, date, ret_excess, mktcap_lag) |&gt;\n  collect()\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(date, mkt_excess) |&gt;\n  collect()\n\nbeta &lt;- tbl(tidy_finance, \"beta\") |&gt;\n  filter(return_type == \"monthly\") |&gt; \n  select(permno, date, beta) |&gt;\n  collect()",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "href": "r/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "title": "Univariate Portfolio Sorts",
    "section": "Sorting by Market Beta",
    "text": "Sorting by Market Beta\nNext, we merge our sorting variable with the return data. We use the one-month lagged betas as a sorting variable to ensure that the sorts rely only on information available when we create the portfolios. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month \\(t\\) information is available in month \\(t+1\\). You may be tempted to simply use a call such as crsp_monthly |&gt; group_by(permno) |&gt; mutate(beta_lag = lag(beta))) instead. This procedure, however, does not work correctly if there are non-explicit missing values in the time series.\n\nbeta_lag &lt;- beta |&gt;\n  mutate(date = date %m+% months(1)) |&gt;\n  select(permno, date, beta_lag = beta) |&gt;\n  drop_na()\n\ndata_for_sorts &lt;- crsp_monthly |&gt;\n  inner_join(beta_lag, join_by(permno, date))\n\nThe first step to conduct portfolio sorts is to calculate periodic breakpoints that you can use to group the stocks into portfolios. For simplicity, we start with the median lagged market beta as the single breakpoint. We then compute the value-weighted returns for each of the two resulting portfolios, which means that the lagged market capitalization determines the weight in weighted.mean().\n\nbeta_portfolios &lt;- data_for_sorts |&gt;\n  group_by(date) |&gt;\n  mutate(\n    breakpoint = median(beta_lag),\n    portfolio = case_when(\n      beta_lag &lt;= breakpoint ~ \"low\",\n      beta_lag &gt; breakpoint ~ \"high\"\n    )\n  ) |&gt;\n  group_by(date, portfolio) |&gt;\n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), \n            .groups = \"drop\")",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#performance-evaluation",
    "href": "r/univariate-portfolio-sorts.html#performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "Performance Evaluation",
    "text": "Performance Evaluation\nWe can construct a long-short strategy based on the two portfolios: buy the high-beta portfolio and, at the same time, short the low-beta portfolio. Thereby, the overall position in the market is net-zero, i.e., you do not need to invest money to realize this strategy in the absence of frictions.\n\nbeta_longshort &lt;- beta_portfolios |&gt;\n  pivot_wider(id_cols = date, names_from = portfolio, values_from = ret) |&gt;\n  mutate(long_short = high - low)\n\nWe compute the average return and the corresponding standard error to test whether the long-short portfolio yields on average positive or negative excess returns. In the asset pricing literature, one typically adjusts for autocorrelation by using Whitney K. Newey and West (1987) \\(t\\)-statistics to test the null hypothesis that average portfolio excess returns are equal to zero. One necessary input for Newey-West standard errors is a chosen bandwidth based on the number of lags employed for the estimation. While it seems that researchers often default on choosing a pre-specified lag length of 6 months, we instead recommend a data-driven approach. This automatic selection is advocated by Whitney K. Newey and West (1994) and available in the sandwich package. To implement this test, we compute the average return via lm() and then employ the coeftest() function. If you want to implement the typical 6-lag default setting, you can enforce it by passing the arguments lag = 6, prewhite = FALSE to the coeftest() function in the code below and it passes them on to NeweyWest().\n\nmodel_fit &lt;- lm(long_short ~ 1, data = beta_longshort)\ncoeftest(model_fit, vcov = NeweyWest)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 0.000397   0.001257    0.32     0.75\n\n\nThe results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint hence does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM yields that the high beta stocks should yield higher expected returns. Our portfolio sort implicitly mimics an investment strategy that finances high beta stocks by shorting low beta stocks. Therefore, one should expect that the average excess returns yield a return that is above the risk-free rate.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "href": "r/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "title": "Univariate Portfolio Sorts",
    "section": "Functional Programming for Portfolio Sorts",
    "text": "Functional Programming for Portfolio Sorts\nNow we take portfolio sorts to the next level. We want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming is very handy: we employ the curly-curly-operator to give us flexibility concerning which variable to use for the sorting, denoted by sorting_variable. We use quantile() to compute breakpoints for n_portfolios. Then, we assign portfolios to stocks using the findInterval() function. The output of the following function is a new column that contains the number of the portfolio to which a stock belongs.\nIn some applications, the variable used for the sorting might be clustered (e.g., at a lower bound of 0). Then, multiple breakpoints may be identical, leading to empty portfolios. Similarly, some portfolios might have a very small number of stocks at the beginning of the sample. Cases where the number of portfolio constituents differs substantially due to the distribution of the characteristics require careful consideration and, depending on the application, might require customized sorting approaches.\n\nassign_portfolio &lt;- function(\n  data, \n  sorting_variable, \n  n_portfolios\n) {\n  breakpoints &lt;- data |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  assigned_portfolios\n}\n\nWe can use the above function to sort stocks into ten portfolios each month using lagged betas and compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.\n\nbeta_portfolios &lt;- data_for_sorts |&gt;\n  group_by(date) |&gt;\n  mutate(\n    portfolio = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = beta_lag,\n      n_portfolios = 10\n    ),\n    portfolio = as.factor(portfolio)\n  ) |&gt;\n  group_by(portfolio, date) |&gt;\n  summarize(\n    ret_excess = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )|&gt;\n  left_join(factors_ff3_monthly, join_by(date))",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#more-performance-evaluation",
    "href": "r/univariate-portfolio-sorts.html#more-performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "More Performance Evaluation",
    "text": "More Performance Evaluation\nIn the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns.\n\nbeta_portfolios_summary &lt;- beta_portfolios |&gt;\n  nest(data = c(date, ret_excess, mkt_excess)) |&gt;\n  mutate(estimates = map(\n    data, \\(x) tidy(lm(ret_excess ~ 1 + mkt_excess, data = x))\n  )) |&gt;\n  unnest(estimates) |&gt; \n  select(portfolio, term, estimate) |&gt; \n  pivot_wider(names_from = term, values_from = estimate) |&gt; \n  rename(alpha = `(Intercept)`, beta = mkt_excess) |&gt; \n  left_join(\n    beta_portfolios |&gt; \n      group_by(portfolio) |&gt; \n      summarize(ret_excess = mean(ret_excess),\n                .groups = \"drop\"), join_by(portfolio)\n  )\n\nFigure 1 illustrates the CAPM alphas of beta-sorted portfolios. It shows that low beta portfolios tend to exhibit positive alphas, while high beta portfolios exhibit negative alphas.\n\nbeta_portfolios_summary |&gt;\n  ggplot(aes(x = portfolio, y = alpha, fill = portfolio)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"CAPM alphas of beta-sorted portfolios\",\n    x = \"Portfolio\",\n    y = \"CAPM alpha\",\n    fill = \"Portfolio\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  theme(legend.position = \"None\")\n\n\n\n\n\n\n\nFigure 1: Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the entire CRSP period.\n\n\n\n\n\nThese results suggest a negative relation between beta and future stock returns, which contradicts the predictions of the CAPM. According to the CAPM, returns should increase with beta across the portfolios and risk-adjusted returns should be statistically indistinguishable from zero.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#the-security-market-line-and-beta-portfolios",
    "href": "r/univariate-portfolio-sorts.html#the-security-market-line-and-beta-portfolios",
    "title": "Univariate Portfolio Sorts",
    "section": "The Security Market Line and Beta Portfolios",
    "text": "The Security Market Line and Beta Portfolios\nThe CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time. Figure 2 illustrates the security market line: We see that (not surprisingly) the high beta portfolio returns have a high correlation with the market returns. However, it seems like the average excess returns for high beta stocks are lower than what the security market line implies would be an “appropriate” compensation for the high market risk.\n\nsml_capm &lt;- lm(ret_excess ~ 1 + beta, data = beta_portfolios_summary)$coefficients\n\nbeta_portfolios_summary |&gt;\n  ggplot(aes(\n    x = beta, \n    y = ret_excess, \n    color = portfolio\n  )) +\n  geom_point() +\n  geom_abline(\n    intercept = 0,\n    slope = mean(factors_ff3_monthly$mkt_excess),\n    linetype = \"solid\"\n  ) +\n  geom_abline(\n    intercept = sml_capm[1],\n    slope = sml_capm[2],\n    linetype = \"dashed\"\n  ) +\n  scale_y_continuous(\n    labels = percent,\n    limit = c(0, mean(factors_ff3_monthly$mkt_excess) * 2)\n  ) +\n  scale_x_continuous(limits = c(0, 2)) +\n  labs(\n    x = \"Beta\", y = \"Excess return\", color = \"Portfolio\",\n    title = \"Average portfolio excess returns and average beta estimates\"\n  )\n\n\n\n\n\n\n\nFigure 2: Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\n\nTo provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio.\n\nbeta_longshort &lt;- beta_portfolios |&gt;\n  mutate(portfolio = case_when(\n    portfolio == max(as.numeric(portfolio)) ~ \"high\",\n    portfolio == min(as.numeric(portfolio)) ~ \"low\"\n  )) |&gt;\n  filter(portfolio %in% c(\"low\", \"high\")) |&gt;\n  pivot_wider(id_cols = date, \n              names_from = portfolio, \n              values_from = ret_excess) |&gt;\n  mutate(long_short = high - low) |&gt;\n  left_join(factors_ff3_monthly, join_by(date))\n\nAgain, the resulting long-short strategy does not exhibit statistically significant returns.\n\ncoeftest(lm(long_short ~ 1, data = beta_longshort),\n  vcov = NeweyWest\n)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.00252    0.00320    0.79     0.43\n\n\nHowever, the long-short portfolio yields a statistically significant negative CAPM-adjusted alpha, although, controlling for the effect of beta, the average excess stock returns should be zero according to the CAPM. The results thus provide no evidence in support of the CAPM. The negative value has been documented as the so-called betting against beta factor (Frazzini and Pedersen 2014). Betting against beta corresponds to a strategy that shorts high beta stocks and takes a (levered) long position in low beta stocks. If borrowing constraints prevent investors from taking positions on the SML they are instead incentivized to buy high beta stocks, which leads to a relatively higher price (and therefore lower expected returns than implied by the CAPM) for such high beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital constraint investors with lower risk aversion.\n\ncoeftest(lm(long_short ~ 1 + mkt_excess, data = beta_longshort),\n  vcov = NeweyWest\n)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.00423    0.00249    -1.7     0.09 .  \nmkt_excess   1.16367    0.08695    13.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFigure 3 shows the annual returns of the extreme beta portfolios we are mainly interested in. The figure illustrates no consistent striking patterns over the last years; each portfolio exhibits periods with positive and negative annual returns.\n\nbeta_longshort |&gt;\n  group_by(year = year(date)) |&gt;\n  summarize(\n    low = prod(1 + low) - 1,\n    high = prod(1 + high) - 1,\n    long_short = prod(1 + long_short) - 1\n  ) |&gt;\n  pivot_longer(cols = -year) |&gt;\n  ggplot(aes(x = year, y = value, fill = name)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~name, ncol = 1) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Annual returns of beta portfolios\",\n    x = NULL, y = NULL\n  )\n\n\n\n\n\n\n\nFigure 3: We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.\n\n\n\n\n\nOverall, this chapter shows how functional programming can be leveraged to form an arbitrary number of portfolios using any sorting variable and how to evaluate the performance of the resulting portfolios. In the next chapter, we dive deeper into the many degrees of freedom that arise in the context of portfolio analysis.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#key-takeaways",
    "href": "r/univariate-portfolio-sorts.html#key-takeaways",
    "title": "Univariate Portfolio Sorts",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nUnivariate portfolio sorts assess whether a single firm characteristic, like lagged market beta, can predict future excess returns.\nPortfolios are formed each month using quantile breakpoints, with returns computed using value-weighted averages to reflect realistic investment strategies.\nA long-short strategy based on beta-sorted portfolios fails to generate significant positive excess returns, contradicting CAPM predictions that higher beta should yield higher returns.\nThe analysis highlights the “betting against beta” anomaly, where low-beta portfolios deliver higher alphas than high-beta portfolios, providing evidence against the CAPM\nThe functional programming capabilities of R enable scalable and flexible portfolio sorting, making it easy to analyze multiple characteristics and portfolio configurations.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#exercises",
    "href": "r/univariate-portfolio-sorts.html#exercises",
    "title": "Univariate Portfolio Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nTake the two long-short beta strategies based on different numbers of portfolios and compare the returns. Is there a significant difference in returns? How do the Sharpe ratios compare between the strategies? Find one additional portfolio evaluation statistic and compute it.\nWe plotted the alphas of the ten beta portfolios above. Write a function that tests these estimates for significance. Which portfolios have significant alphas?\nThe analysis here is based on betas from monthly returns. However, we also computed betas from daily returns. Re-run the analysis and point out differences in the results.\nGiven the results in this chapter, can you define a long-short strategy that yields positive abnormal returns (i.e., alphas)? Plot the cumulative excess return of your strategy and the market excess return for comparison.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "blog/fast-portfolio-sorts/index.html",
    "href": "blog/fast-portfolio-sorts/index.html",
    "title": "Fast Portfolio Sorts",
    "section": "",
    "text": "Implementing standard tasks like portfolio sorts in R can be approached in various ways, including using base R, dplyr, or data.table. For researchers and data analysts, it’s crucial that these implementations are both correct and efficient. Even though portfolio sorting is a relatively simple task, the need to sort portfolios in numerous ways due to the variety of sorting variables and methodological uncertainties can make computational efficiency critical. This blog post will benchmark the performance of different sorting methods in R, focusing on execution time, to provide insights for data analysts and portfolio managers on choosing the most efficient approach.\nWe’ll dive into the following sorting approaches:\nThroughout this blog post, I’ll use the following packages. Notably, bench is used to create benchmarking results.\nlibrary(dplyr)\nlibrary(dtplyr)\nlibrary(data.table)\nlibrary(bench)\nlibrary(purrr)\nlibrary(RSQLite)\nlibrary(ggplot2)"
  },
  {
    "objectID": "blog/fast-portfolio-sorts/index.html#data-preparation",
    "href": "blog/fast-portfolio-sorts/index.html#data-preparation",
    "title": "Fast Portfolio Sorts",
    "section": "Data preparation",
    "text": "Data preparation\nFirst, I start by loading the monthly CRSP data from our database (see WRDS, CRSP, and Compustat for details). The dataset has about 3 million rows and contains monthly returns between 1960 and 2023 for about 26,000 stocks. I also make sure that the data comes as a tibble for dplyr, a data.frame for base, two data.tables for the two data.table approaches, and a ‘lazy’ data table for dtplyr because I want to avoid any conversion issues in the portfolio assignments.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"../../data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly_dplyr &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, month, ret_excess, mktcap_lag) |&gt;\n  collect()\n\ncrsp_monthly_base &lt;- as.data.frame(crsp_monthly_dplyr)\n\ncrsp_monthly_dt &lt;- copy(as.data.table(crsp_monthly_dplyr))\n\ncrsp_monthly_dtip &lt;- copy(as.data.table(crsp_monthly_dplyr))\n\ncrsp_monthly_dtplyr &lt;- lazy_dt(copy(crsp_monthly_dt))\n\nNote data.table in R uses reference semantics, which means that modifying one data.table object could potentially modify another if they share the same underlying data. Therefore, copy() ensures that crsp_monthly_dt is an independent copy of the data, preventing unintentional side effects from modifications in subsequent operations and ensuring a fair comparison."
  },
  {
    "objectID": "blog/fast-portfolio-sorts/index.html#defining-portfolio-sorts",
    "href": "blog/fast-portfolio-sorts/index.html#defining-portfolio-sorts",
    "title": "Fast Portfolio Sorts",
    "section": "Defining portfolio sorts",
    "text": "Defining portfolio sorts\nAs a common denominator across approaches, I introduce a stripped down version of assign_portfolio() that can also be found in the tidyfinance package.\n\nassign_portfolio &lt;- function(data, sorting_variable, n_portfolios) {\n  \n  breakpoints &lt;- quantile(\n    data[[sorting_variable]], \n    probs = seq(0, 1, length.out = n_portfolios + 1), \n    na.rm = TRUE, names = FALSE\n  )\n\n  findInterval(\n    data[[sorting_variable]], breakpoints, all.inside = TRUE\n  )\n}\n\nThe goal is to apply this function to the cross-section of stocks in each month and then compute average excess returns for each portfolio across all months.\nIf we want to apply the function above to each month using only base, then we have to first split the data.frame into multiple parts and lapply() the function to each part. After we combined the parts again to one big data.frame, we can use aggregate() to compute the average excess returns.\n\nsort_base &lt;- function() {\n  crsp_monthly_base$portfolio &lt;- with(\n    crsp_monthly_base, \n    ave(mktcap_lag, month, FUN = function(x) assign_portfolio(data.frame(mktcap_lag = x), \"mktcap_lag\", n_portfolios = 10))\n  )\n  \n  mean_ret_excess &lt;- with(\n    crsp_monthly_base, \n    tapply(ret_excess, portfolio, mean)\n  )\n\n  data.frame(\n    portfolio = names(mean_ret_excess), \n    ret = unlist(mean_ret_excess)\n  )\n}\nbench::system_time(sort_base())\n\nprocess    real \n  3.08s   3.04s \n\n\nThis approach takes about 3 seconds per execution on my machine and is in fact more than 8-times slower than the other approaches! To create a more nuanced picture for the fast and arguably more interesting approaches, I’ll drop the base approach going forward.\nIf we want to perform the same logic using dplyr, we can use the following approach. Note that I use as.data.frame() for all approaches to ensure that the output format is the same for all approaches - a necessary requirement for a meaningful benchmark (otherwise code would not be equivalent).\n\nsort_dplyr &lt;- function() {\n  crsp_monthly_dplyr |&gt; \n    group_by(month) |&gt; \n    mutate(\n      portfolio = assign_portfolio(\n        pick(everything()), \"mktcap_lag\", n_portfolios = 10),\n      by = \"month\"\n    ) |&gt; \n    group_by(portfolio) |&gt; \n    summarize(ret = mean(ret_excess)) |&gt; \n    as.data.frame()\n}\nsort_dplyr()\n\n   portfolio     ret\n1          1 0.02270\n2          2 0.00512\n3          3 0.00482\n4          4 0.00515\n5          5 0.00548\n6          6 0.00608\n7          7 0.00637\n8          8 0.00680\n9          9 0.00647\n10        10 0.00579\n\n\nThe equivalent approach in data.table looks as follows. Note that I deliberately don’t use any pipe or intermediate assignments as to avoid any performance overhead that these might introduce. I also avoid using the in-place modifier := because it would create a new permanent column in crsp_monthly_dt, which I don’t need for the on-the-fly aggregation and it also doesn’t happen in dplyr.\n\nsort_dt &lt;- function() {\n  as.data.frame(crsp_monthly_dt[\n    , .(portfolio = assign_portfolio(.SD, \"mktcap_lag\", n_portfolios = 10), month, ret_excess), by = .(month)][\n      , .(ret = mean(ret_excess)), keyby = .(portfolio)\n      ])\n}\nsort_dt()\n\n   portfolio     ret\n1          1 0.02270\n2          2 0.00512\n3          3 0.00482\n4          4 0.00515\n5          5 0.00548\n6          6 0.00608\n7          7 0.00637\n8          8 0.00680\n9          9 0.00647\n10        10 0.00579\n\n\nHowever, as the performance benefit of data.table may manifest itself through its in-place modification capabilties, I also introduce a second version of the data.table expression. Note that in this version crsp_monthly_dtip gets a permanent column portfolio that is overwritten in each iteration.\n\nsort_dtip &lt;- function() {\n  as.data.frame(crsp_monthly_dtip[\n    , portfolio := assign_portfolio(.SD, \"mktcap_lag\", n_portfolios = 10), by = .(month)][\n      , .(ret = mean(ret_excess)), keyby = .(portfolio)\n      ])\n}\nsort_dtip()\n\n   portfolio     ret\n1          1 0.02270\n2          2 0.00512\n3          3 0.00482\n4          4 0.00515\n5          5 0.00548\n6          6 0.00608\n7          7 0.00637\n8          8 0.00680\n9          9 0.00647\n10        10 0.00579\n\n\nLastly, I add the dtplyr implementation that also takes a data.table as input and internally converts dplyr code to data.table syntax. Note that the final as.data.frame() call is used to access the results and ensure that the result format is consistent with the other approaches.\n\nsort_dtplyr &lt;- function() {\n  crsp_monthly_dtplyr |&gt; \n    group_by(month) |&gt; \n    mutate(\n      portfolio = assign_portfolio(\n        pick(everything()), \"mktcap_lag\", n_portfolios = 10),\n      by = \"month\"\n    )  |&gt; \n    group_by(portfolio) |&gt; \n    summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n    as.data.frame()\n}\nsort_dtplyr()\n\n   portfolio     ret\n1          1 0.02270\n2          2 0.00512\n3          3 0.00482\n4          4 0.00515\n5          5 0.00548\n6          6 0.00608\n7          7 0.00637\n8          8 0.00680\n9          9 0.00647\n10        10 0.00579\n\n\nNow that we have verified that all code chunks create the same average excess returns per portfolio, we can proceed to the performance evaluation."
  },
  {
    "objectID": "blog/fast-portfolio-sorts/index.html#benchmarking-results",
    "href": "blog/fast-portfolio-sorts/index.html#benchmarking-results",
    "title": "Fast Portfolio Sorts",
    "section": "Benchmarking results",
    "text": "Benchmarking results\nThe bench package is a great utility for benchmarking and timing expressions in R. It provides functions that allow you to measure the execution time of expressions or code chunks. This can be useful for comparing the performance of different approaches or implementations, or for identifying potential bottlenecks in your code. The following code evaluates each approach from above a 100 times and collects the results.\n\niterations &lt;- 100\n\nresults &lt;- bench::mark(\n  sort_dplyr(), sort_dt(), sort_dtip(), sort_dtplyr(), \n  iterations = iterations\n)\n\nThe following plot shows the distributions of execution times as violin plots. You can see that dplyr takes the lead and is followed closely by both data.table variants, while dtplyr takes the third place.\n\nautoplot(results, type = \"violin\") +\n  labs(y = NULL, x = NULL, \n       title = \"Execution time of porfolio sorts using dplyr, data.table, and dtplyr\",\n       subtitle = \"'dt' refers to data.table without in-place modification and 'dtip' with in-place modification\")\n\n\n\n\n\n\n\n\nNote that all three methods are quite fast and take less than 1 second, given that the task is to assign 10 portfolios across up to 26,000 stocks for 755 months. In fact, dplyr yields the fastest execution times, followed by both data.table implementations and dtplyr.\nWhy is data.table slower than dplyr? It is generally believed that data.table is faster than dplyr for data manipulation tasks. The example above shows that it actually depends on the application. On the one hand, the data set might be ‘too small’ for the performance benefits of data.table to kick in. On the other hand, sorting the portfolios using the assign_portfolio() function might be better suited for the dplyr execution backend than the data.table backend.\nWhy is dtplyr slower than data.table? On the one hand, dtplyr translates dplyr operations into data.table syntax. This translation process introduces some overhead, as dtplyr needs to interpret the dplyr code and convert it into equivalent data.table operations. On the other hand, dtplyr does not modify in place by default, so it typcially makes a copy that would not be necessary if you were using data.table directly."
  },
  {
    "objectID": "blog/fast-portfolio-sorts/index.html#concluding-remarks",
    "href": "blog/fast-portfolio-sorts/index.html#concluding-remarks",
    "title": "Fast Portfolio Sorts",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nThe key takeway is that neither of the libraries is strictly more efficient than the other. If you really search for performance among R libraries, you have to carefully choose a library for your specific application and think hard about optimizing the logic of your code to the chosen library.\nIf you have ideas how to optimize any of the approaches, please reach out to us! In particular, we’d love to optimize base sufficiently for it to be included in the benchmark tests."
  },
  {
    "objectID": "blog/op-ed-tidy-finance/index.html",
    "href": "blog/op-ed-tidy-finance/index.html",
    "title": "What is Tidy Finance?",
    "section": "",
    "text": "Empirical finance can be tedious. Many standard tasks, such as cleaning data or forming factor portfolios, require a lot of effort. The code to produce even seminal results is typically opaque. Why should researchers have to reinvent the wheel over and over again?\nTidy Finance with R is our take on how to conduct empirical research in financial economics from scratch. Whether you are an industry professional looking to expand your quant skills, a graduate student diving into the finance world, or an academic researcher, this book shows you how to use R to master applications in asset pricing, portfolio optimization, risk management, and option pricing.\nWe wrote this book to provide a comprehensive guide to using R for financial analysis. Our book collects all the tools we wish we would have had at hand at the beginning of our graduate studies in finance. Without transparent code for standard procedures, numerous replication efforts (and their failures) feel like a waste of resources. We have been there, as probably everybody working with data has. Since we kicked off our careers, we have constantly updated our methods, coding styles, and workflows. Our book reflects our lessons learned. By sharing them, we aim to help others avoid dead ends.\nWorking on problems that countless others have already solved in secrecy is not just tedious, it even may have detrimental effects. In a recent study1 together with hundreds of research teams from across the globe, Albert J. Menkveld, the best-publishing Dutch economist according to Economentop 40, shows that without a standard path to do empirical analysis, results may vary substantially. Even if teams set out to analyze the same research question based on the same data, implementation details are important and deserve more than treatment as subtleties.\nThere will always be multiple acceptable ways to test relevant research questions. So why should it matter that our book lifts the curtain on reproducible finance by providing a fully transparent code base for many typical financial applications? First and foremost, we hope to inspire others to make their research truly reproducible. This is not a purely theoretical exercise: our examples start with data preparation and conclude with communicating results to get readers to do empirical analysis on real data as fast as possible. We believe that the need for precise academic writing does not stop where the code begins. Understanding and agreeing on standard procedures hopefully frees up resources to focus on what matters: a novel research project, a seminar paper, or a thorough analysis for your employer. If our book helps to provide a foundation for discussions on which determinants render code useful, we have achieved much more than we were hoping for at the beginning of this project.\nUnlike typical stylized examples, our book starts with the problems of any serious research project. The often overlooked challenge behind empirical research may seem trivial at first glance: we need data to conduct our analyses. Finance is not an exception: raw data, often hidden behind proprietary financial data sources, requires cleaning before there is any hope of extracting valuable insights from it. While you can despise data cleaning, you cannot ignore it.\nWe describe and provide the code to prepare typical open-source and proprietary financial data sources (e.g., CRSP, Compustat, Mergent FISD, TRACE). We reuse these data in all the subsequent chapters, which we keep as self-contained as possible. The empirical applications range from key concepts of empirical asset pricing (beta estimation, portfolio sorts, performance analysis, Fama-French factors) to modeling and machine learning applications (fixed effects estimation, clustering standard errors, difference-in-difference estimators, ridge regression, Lasso, Elastic net, random forests, neural networks) and portfolio optimization techniques.\nNecessarily, our book reflects our opinionated perspective on how to perform empirical analyses. From our experience as researchers and instructors, we believe in the value of the workflows we teach and apply daily. The entire book rests on two core concepts: coding principles using the tidyverse family of R packages and tidy data.\nWe base our book entirely on the open-source programming language R. R and the tidyverse community provide established tools to perform typical data science tasks, ranging from cleaning and manipulation to plotting and modeling. R is hence the ideal environment to develop an accessible code base for future finance students. The concept of tidy data refers to organizing financial data in a structured and consistent way, allowing for easy analysis and understanding.2 Taken together, tidy data and code help achieve the ultimate goal: to provide a fundamentally human-centered experience that makes it easier to teach, learn, and replicate the code of others – or even your own!\nWe are convinced that empirical research in finance is in desperate need of reproducible code to form standards for otherwise repetitive tasks. Instructors and researchers have already reached out to us with grateful words about our book. Tidy Finance finds its way into lecture halls across the globe already today. Various recent developments support our call for increased transparency. For instance, Cam Harvey, the former editor of the Journal of Finance, and a former president of the American Finance Association, openly argues that the profession needs to tackle the replication crisis.3 Top journals in financial economics increasingly adopt code and data-sharing policies to increase transparency. The industry and academia are aware and concerned (if not alarmed) about these issues, which is why we believe that the timing for publishing Tidy Finance with R could not be better."
  },
  {
    "objectID": "blog/op-ed-tidy-finance/index.html#footnotes",
    "href": "blog/op-ed-tidy-finance/index.html#footnotes",
    "title": "What is Tidy Finance?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMenkveld, A. J. et al. (2022). “Non-standard Errors”. http://dx.doi.org/10.2139/ssrn.3961574↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nWigglesworth, R. (2021). The hidden ‘replication crisis’ of finance. Financial Times. https://www.ft.com/content/9025393f-76da-4b8f-9436-4341485c75d0↩︎"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html",
    "href": "blog/tidy-market-microstructure/index.html",
    "title": "Tidy Market Microstructure",
    "section": "",
    "text": "Anyone active in market microstructure research knows that the devil is in the details. When clocks tick in microseconds and prices move in cents, a brief delay or a small fee discount can make a huge difference for traders. In fact, they can even be the business model of an exchange. But as much as such institutional detail is fascinating, the empirical implementation of even the most conventional microstructure concepts can be frustrating. Referring to the interest of brevity, many journal articles often defer the implementation details to an appendix, and even there they tend to be vague or incomplete. With the additional challenge of vast data sets, new entrants into this field face a steep challenge.\nWe provide a beginner’s guide to market quality measurement in high-frequency data, aiming to lower the barriers to entry into empirical market microstructure. We discuss economic considerations and show step-by-step how to code the most common measures of market liquidity and market efficiency. Because virtually all securities now trade at multiple venues, we also emphasize how market quality can account for market fragmentation.\nIs this guide really needed? Well, a recent paper by Menkveld et al., (2023)1 shows in full clarity that even small variations in methodology can lead to large differences in market quality measures. The authors assigned the same set of market microstructure hypotheses and the same data to 164 research teams. They found that the variation in results across teams, the non-standard error, was of a magnitude similar to the standard error. Many teams included seasoned professors, but past publication performance and seniority did not reduce the non-standard error.\nAnother question is if the cumbersome high-frequency data analysis is really worth the effort? After all, there are numerous liquidity proxies based on daily data. The answer depends on the research question. First, low-frequency proxies are designed for low-frequency applications. For example, Amihud’s (2002)2 popular proxy was originally proposed to be measured as an annual average. Most microstructure applications require liquidity measures at higher frequencies than that. Furthermore, recent evidence by Jahan-Parvar and Zikes (2023)3 show that many low-frequency proxies capture volatility rather than liquidity.\nIf we convinced you to take on the high-frequency data, here’s what we offer. Table 1 lists the market quality measures that we cover, as well as their underlying data type. For some measures, we include several versions and discuss the differences between them. We organize the text by the data type, as we think that is a natural work flow. We start with liquidity measures based on tick-by-tick quote data, followed by measures based on both trade and quote data. Finally, we look into a set of measures of efficiency and volatility that require equispaced quote data.\nMarket quality variables and data types\n\n\nVariable name\nVariable type\nData type\n\n\n\n\nQuoted bid-ask spread\nLiquidity\nQuote tick data\n\n\nQuoted depth\nLiquidity\nQuote tick data\n\n\nEffective bid-ask spread\nLiquidity\nQuote and trade tick data\n\n\nTrade volume\nVolume\nTrade tick data\n\n\nPrice impact\nLiquidity\nQuote and trade tick data\n\n\nRealized spread\nLiquidity\nQuote and trade tick data\n\n\nReturn autocorrelation\nEfficiency\nEquispaced quote data\n\n\nRealized volatility\nVolatility\nEquispaced quote data\n\n\nVariance ratio\nEfficiency\nEquispaced quote data\nOur focus is on the practice of empirical market microstructure. As such, we often motivate the coding choices with economic concepts. Other than that, however, readers interested in the economics underlying each metric are referred to introductory texts such as Campbell, Lo and MacKinlay (1997)4, Foucault, Pagano & Röell (2013)5 and Hasbrouck (2007)6."
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#quote-data-inspection-and-preparation",
    "href": "blog/tidy-market-microstructure/index.html#quote-data-inspection-and-preparation",
    "title": "Tidy Market Microstructure",
    "section": "Quote data inspection and preparation",
    "text": "Quote data inspection and preparation\nLet’s dive into it. After loading the required packages, the following code shows how to import and preview the data. We get an overview of the data by simply typing the name of the data frame in the console. It automatically abbreviates the content to show only a subset of the data.\nThe data can be downloaded directly from within R.\n\nquotes_url &lt;- \"http://tinyurl.com/pruquotes\"\n\n\ndata.tabledtplyr\n\n\nTo load the data we use the function ‘fread’ which is similar to read.csv, but much faster.\n\n# Install and load the `data.table` package\n# install.packages(\"data.table\")\nlibrary(data.table)\n\n# Load the view the quote data\nquotes &lt;- fread(quotes_url)\n\nThe raw data looks as follows.\n\nquotes\n\n           #RIC       Domain           Date-Time GMT Offset  Type Bid Price\n     1:   PRU.L Market Price 2021-06-07 04:00:03          1 Quote    1450.0\n     2:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote    1450.0\n     3:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote    1450.0\n     4:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote    1450.0\n     5:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote    1450.0\n    ---                                                                    \n263969: PRUl.TQ Market Price 2021-06-11 15:29:59          1 Quote    1488.5\n263970: PRUl.TQ Market Price 2021-06-11 15:29:59          1 Quote    1488.5\n263971: PRUl.TQ Market Price 2021-06-11 15:29:59          1 Quote        NA\n263972: PRUl.TQ Market Price 2021-06-11 15:30:00          1 Quote        NA\n263973: PRUl.TQ Market Price 2021-06-11 15:30:00          1 Quote        NA\n        Bid Size Ask Price Ask Size          Exch Time\n     1:      300    1550.0      700 04:00:02.983920000\n     2:     1028    1550.0      700 06:50:00.016246000\n     3:     1028    1510.5     1000 06:50:00.251314000\n     4:     1028    1504.5     1000 06:50:00.402760000\n     5:     1028    1504.5     1009 06:50:00.547739000\n    ---                                               \n263969:      476    1496.0      472 15:29:59.623000000\n263970:      476    1600.0      425 15:29:59.798000000\n263971:       NA    1600.0      425 15:29:59.833000000\n263972:       NA    1600.0      325 15:30:00.104000000\n263973:       NA        NA       NA 15:30:00.104000000\n\n\n\n\n\n# install.packages(\"data.table\")\n# install.packages(\"dtplyr\")\n# install.packages(\"tidyverse\")\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(tidyverse)\n\nWe read the data as a tibble using the (very fast) function read_csv. The call lazy_dt converts the tibble to a lazy data.table object. Lazy means that all following commands are not executed immediately but translated to data.table syntax first and then executed at a final stage. As a result, there should be almost no difference in execution time of the code in tidyverse syntax versus the data.table implementation.\n\ntv_quotes &lt;- read_csv(quotes_url, col_types = list(`Exch Time` = col_character()))\n\ntv_quotes &lt;- lazy_dt(tv_quotes)\n\nThe raw data looks as follows.\n\ntv_quotes\n\nSource: local data table [263,973 x 10]\nCall:   `_DT1`\n\n  `#RIC` Domain    `Date-Time`         `GMT Offset` Type  `Bid Price` `Bid Size`\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dttm&gt;                     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 PRU.L  Market P… 2021-06-07 04:00:03            1 Quote        1450        300\n2 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450       1028\n3 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450       1028\n4 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450       1028\n5 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450       1028\n6 PRU.L  Market P… 2021-06-07 06:50:05            1 Quote        1450       1035\n# ℹ 263,967 more rows\n# ℹ 3 more variables: `Ask Price` &lt;dbl&gt;, `Ask Size` &lt;dbl&gt;, `Exch Time` &lt;chr&gt;\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\n\n\n\nNote that we use the suffix tv_ for every object generated using tidyverse syntax. This way you can execute the entire code of this guide without overwriting data.table objects with the dtplyr equivalent and vice versa.\nThe data contain four variables that describe the state of the order book (Bid Price, Bid Size, Ask Price, and Ask Size). The LOB holds orders at numerous different prices, but we only see the best price level on each side. This is sufficient for most quote-based market quality measures. We also get three variables conveying time stamps and time zone information (Date-Time, GMT Offset, and Exch time; discussed in detail below), and three character variables (#RIC, Domain, and Type).\nBefore processing the data, we rename the variables. Names containing spaces, hashes, and dashes may make sense for data vendors, but they are impractical to work with in R. Furthermore, for the order book variables, we prefer to use the term depth to refer to the number of shares quoted, reserving the term size to the number of shares changing hands in a trade.\n\ndata.tabledtplyr\n\n\nThe command setnames replaces the existing variables with our desired column names.\n\nraw_quote_variables &lt;- c(\"#RIC\", \"Date-Time\", \"GMT Offset\", \"Domain\", \"Exch Time\",\n                         \"Type\", \"Bid Price\", \"Bid Size\", \"Ask Price\", \"Ask Size\")\n\nnew_quote_variables &lt;- c(\"ticker\", \"date_time\", \"gmt_offset\", \"domain\", \"exchange_time\", \n                         \"type\", \"bid_price\", \"bid_depth\", \"ask_price\", \"ask_depth\")\n\nsetnames(quotes, raw_quote_variables, new_quote_variables)\n\n\n\n\ntv_quotes &lt;- tv_quotes|&gt; \n  rename(ticker = `#RIC`,\n         domain = Domain,\n         date_time = `Date-Time`,\n         gmt_offset = `GMT Offset`,\n         type = Type,\n         bid_price = `Bid Price`, \n         bid_depth = `Bid Size`, \n         ask_price = `Ask Price`,\n         ask_depth = `Ask Size`,\n         exchange_time = `Exch Time`\n  ) |&gt;\n  lazy_dt()\n\n\n\n\nFrom the output above, the character columns might look like they are the same for all rows, hence occupying more memory than necessary. The data.table::table() or dtplyr::count() functions are great to gauge the variation in categorical variables. In this case, it shows us that there is variation in the ticker variable. The two tickers are for the same stock, PRU, traded at two different exchanges, LSE and TQE. The former is more active, with 165,261 quote updates.\nThe other two character variables, domain and type, are indeed constants. We delete them to save memory.\n\ndata.tabledtplyr\n\n\n\n# Output a table of sample tickers and values of `domain` and `type`\ntable(quotes$ticker, quotes$type, quotes$domain)\n\n, ,  = Market Price\n\n         \n           Quote\n  PRU.L   165261\n  PRUl.TQ  98712\n\n# Delete variables\nquotes[, c(\"domain\", \"type\") := NULL]\n\n\n\n\ntv_quotes |&gt; \n  count(ticker, type, domain)\n\nSource: local data table [2 x 4]\nCall:   `_DT2`[, .(n = .N), keyby = .(ticker, type, domain)]\n\n  ticker  type  domain            n\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;\n1 PRU.L   Quote Market Price 165261\n2 PRUl.TQ Quote Market Price  98712\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\ntv_quotes &lt;- tv_quotes |&gt;\n  select(-type, -domain)\n\n\n\n\n\nDates\nIn the raw data, dates and times are embedded in the same variable, date_time, but for us it is useful to have them in separate variables. Accordingly, we now define the variable date.\n\ndata.tabledtplyr\n\n\nNote here that the operator := is used to define a new variable (date) within an existing data.table, such as quotes in this example. Within a data.table, it suffices to refer to the variable name, date_time, when defining the new variable. This is different to a data.frame, for which we would have to write quotes$date_time.\n\n# Obtain dates\nquotes[, date := as.Date(date_time)]\n\n# Output a table of sample dates and tickers\ntable(quotes$ticker, quotes$date)\n\n         \n          2021-06-07 2021-06-08 2021-06-09 2021-06-10 2021-06-11\n  PRU.L        27415      38836      30962      39639      28409\n  PRUl.TQ      16424      26040      18207      21170      16871\n\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt; \n  mutate(date = as.Date(date_time)) |&gt;\n  lazy_dt()\n\ntv_quotes |&gt; \n  count(ticker, date) |&gt; \n  pivot_wider(names_from = ticker, \n              values_from = n)\n\nSource: local data table [5 x 3]\nCall:   dcast(`_DT3`[, .(n = .N), keyby = .(ticker, date)], formula = date ~ \n    ticker, value.var = \"n\")\n\n  date       PRU.L PRUl.TQ\n  &lt;date&gt;     &lt;int&gt;   &lt;int&gt;\n1 2021-06-07 27415   16424\n2 2021-06-08 38836   26040\n3 2021-06-09 30962   18207\n4 2021-06-10 39639   21170\n5 2021-06-11 28409   16871\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\n\n\n\nThe table output shows that there are five trading days in the sample. The number of quote observations per stock-date varies between roughly 16,000 and 40,000. The tendency that LSE has more quotes than TQE is consistent across trading days.\n\n\nTimestamps\nThe accuracy of timestamps is important in microstructure data. Timestamps are often matched between quote and trade data (that are not necessarily generated in the same systems), or between data from exchanges in different locations. It is thus essential to be aware of latencies that may arise due to geography or hardware, for example.\nWe have two timestamps for each observation. The exchange_time variable is assigned by the exchange at the time an event is recorded in the exchange matching engine. The date_time variable is the timestamp assigned on receipt at the data vendor, which is by definition later than the exchange_time. Exchanges that are located at different distances from the vendor are likely to have different reporting delays. It is then up to the researcher to determine which timestamp to rely on, and the choice may depend on the research question. In our setting, as we measure liquidity across venues, it is important that the time stamps across venues are comparable. Based on that each exchanges has strong incentives to assign accurate time stamps (to cater for low-latency participants), we choose to work with the exchange_time variable.\nFor US equity markets, the timestamp may reflect the matching engine time, the time when the national best bid and offer updates, or the participant timestamp. For discussions about which of these to use, see Bartlett & McCrary (2019)10, Holden, Pierson & Wu (2023)11, and Schwenk-Nebbe (2021)12.\n\ndata.tabledtplyr\n\n\nWhen working with timestamps in microstructure applications, it is useful to convert them into a numeric format. Dedicated time formats (e.g., xts) are imprecise when it comes to sub-second units (see here and here). We thus convert the timestamps to the number of seconds elapsed since midnight. For example, 8:30 am becomes 8.5 x 3,600 = 30,600, because there are 3,600 seconds per hour.\nThe code below converts exchange_time to numeric and adjusts it for daylight saving using the gmt_offset variable (which is measured in hours). Note the use of curly brackets {...} in the definition of the time variable, which allows us to temporarily define the variable time_elements within the call. Once the operation is complete, the temporary variable is automatically deleted. The variable that is retained should always be returned as a list, hence list(time).\n\n# Convert time stamps to numeric format, expressed in seconds past midnight\n# The function `strsplit` splits a character string at the point defined by `split`\n# `do.call` is a way to call a function, which in this case calls `rbind` to convert a \n# list of vectors to a matrix, where each vector forms one row\nquotes[, time := {\n    time_elements = strsplit(exchange_time, split = \":\")\n    time_elements = do.call(rbind, time_elements)\n    time = as.numeric(time_elements[, 1]) * 3600 + \n           as.numeric(time_elements[, 2]) * 60 + \n           as.numeric(time_elements[, 3]) +\n           gmt_offset * 3600\n    list(time)}]\n\nHaving made sure that dates and times are in the desired format, we can save space by dropping the raw time and date variables.\n\nquotes[, c(\"date_time\", \"exchange_time\", \"gmt_offset\") := NULL]\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt; \n  separate(exchange_time, \n           into = c(\"hour\", \"minute\", \"second\"), \n           sep=\":\", \n           convert = TRUE) |&gt; \n  mutate(time = hour * 3600 + minute * 60 + second + gmt_offset * 3600)\n\nHaving made sure that dates and times are in the desired format, we can save space by dropping the raw time and date variables.\n\ntv_quotes &lt;- tv_quotes|&gt; \n  select(-c(\"date_time\", \"gmt_offset\",\"hour\",\"minute\",\"second\"))\n\n\n\n\n\n\nPrices and depths\nAn important feature of LOB quotes is that they remain valid until cancelled, executed or modified. Whenever there is a change to the prevailing quotes, a new quote observation is added to the data. It is irrelevant if the latest quote is from the previous millisecond or from the previous minute – it remains valid until updated. It is thus economically meaningful to forward-fill quotes that prevailed in the previous period. Trades, in contrast, are agreed upon at a fixed point in time and do not convey any information about future prices or sizes. They should not be forward-filled, see Hagströmer and Menkveld (2023)13.\nWhen forward-filling quote data, it is important to restrict the procedure to the same date, stock and trading venue. For example, quotes should never be forward-filled from one day to the next, and not from one venue to another. This is ensured with the by argument (in data.table) or the group_by function (in tidyverse), which specifies that the operation is to be done within each combination of tickers and dates.\n\ndata.tabledtplyr\n\n\nWe use the nafill function to forward-fill, with the option type = \"locf\" (last observation carried forward) specifying the type of filling. The .SD inside the lapply command tells data.table to repeat the same operation for the set of variables specified by the option .SDcols.\nIn summary, whereas the .SD applies the same function across a set of variables (columns), the by applies it across categories of observations (rows). The same outcome could be achieved with for loops, but in R, that would be much slower. We discuss that further below.\nNote here how the := notation can be used to define multiple variables, using a vector of variable names on the left-hand-side and a function (in this case lapply) that returns a list of variables on the right-hand-side. Note also that when referring to multiple variable names within the data.table, they are specified as a character vector.\n\n# Forward-fill quoted prices and depths\nlob_variables &lt;- c(\"bid_price\", \"bid_depth\", \"ask_price\", \"ask_depth\")\n\nquotes[, \n  (lob_variables) := lapply(.SD, nafill, type = \"locf\"),    .SDcols = (lob_variables), \n  by = c(\"ticker\", \"date\")]\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt;\n  group_by(ticker, date) |&gt;\n  fill(matches(\"bid|ask\")) |&gt; \n  ungroup()\n\n\n\n\nWhen measuring market quality in continuous trading, it is common to filter out periods that may be influenced by call auctions. The LSE opens for trading with a call auction at 08:00 am, local time, and closes with another call at 4:30 pm. There is also an intraday call auction at noon, 12:00 pm. To avoid the impact of the auctions, we exclude quotes before 8:01 am and after 4:29 pm. We do not exclude quotes recorded around the intraday call auction, but set them as missing (NA). If they were instead deleted, it would give the false impression that the last observation before the excluded quotes was still valid.\nWhen entering the opening hours, remember to state them for the same time zone as recorded in the data. In our case, the gmt_offset adjustment above makes sure that the data is stated in local (London) time.\n\nopen_time &lt;- 8 * 3600\nclose_time &lt;- 16.5 * 3600\nintraday_auction_time &lt;- 12 * 3600\n\nFirst, we exclude quotes around the opening and closing of continuous trading. Next, we set quotes around the intraday auction to missing.\n\ndata.tabledtplyr\n\n\n\nquotes &lt;- quotes[time &gt; (open_time + 60) & time &lt; (close_time - 60)]\n\n\nquotes[time &gt; (intraday_auction_time - 60) & time &lt; (intraday_auction_time + 3 * 60), \n  (lob_variables)] &lt;-  NA\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt;\n  filter(time &gt; hms::as_hms(\"08:01:00\"), \n         time &lt; hms::as_hms(\"16:29:00\"))\n\ntv_quotes &lt;- tv_quotes |&gt;\n  mutate(across(matches(\"bid|ask\"), \n                ~if_else(time &gt; hms::as_hms(\"11:59:00\") & \n                         time &lt; hms::as_hms(\"12:03:00\"), NA_real_, .))) |&gt;\n  lazy_dt()\n\n\n\n\n\n\nScreening\nBefore turning to the market quality measurement, it is a good habit to check that the quote observations make economic sense. One way to do that is to study the variation in the bid-ask spread. The nominal bid-ask spread is defined as the difference between the ask price, \\(P^A\\), and the bid price, \\(P^B\\), \\(\\text{quoted}\\_\\text{spread}^{nom}= P^A - P^B\\). A histogram offers a quick overview of the variation (a line plot of the prices is also useful, see Section 2.1).\nIn the output below, note that the x-axis is in units of pence (0.01 British Pounds, GBP). All quoted prices in this example data follow that convention. Note also that the bid-ask spread is strictly positive, as it should be whenever the market is open. The TQE occasionally has wider spreads than the LSE, but there are no extraordinarily large spreads. The maximum spread, GBP 0.11, corresponds to around 0.7% of the stock price.\nAlso, it is clear from the histogram that the tick size, the minimum price increment that is allowed when quoting prices, is 0.5 pence (that is, GBP 0.005). Most spreads are quoted at one or two ticks.\nWe use the package ggplot2 to plot an histogram of the nominal quoted bid-ask spreads.\n\ndata.tabledtplyr\n\n\n\nlibrary(ggplot2)\nggplot(quotes, \n       aes(x = ask_price - bid_price, fill = ticker)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"Histogram of nominal bid-ask spread\",\n       x = \"Nominal bid-ask spread (pence)\",\n       y = \"Count\",\n       fill = NULL) +\n  scale_x_continuous(breaks = 1:12)\n\nWarning: Removed 1576 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\ntv_quotes |&gt; \n  as_tibble() |&gt;\n  ggplot(aes(x = ask_price - bid_price, fill = ticker)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"Histogram of nominal bid-ask spread\",\n       x = \"Nominal bid-ask spread (pence)\",\n       y = \"Count\",\n       fill = NULL) +\n  scale_x_continuous(breaks = 1:12)\n\nWarning: Removed 1576 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\nR produces a warning when plotting the nominal bid-ask spread. It mentions 1,576 rows containing “non-finite values”. The non-finite values refer either NA, Inf (infinite) or -Inf (negative infinite). In the timestamp section, we imposed NA for LOB variables during midday auction. To see if those are the cause of the warning, let’s create a histogram of the time stamps of the missing values.\nIndeed, all missing values are around ~43,150 and ~43,350 seconds of the trading day which is the time of the midday auction (noon is \\(12\\times3,600=43,200\\) seconds past midnight). Accounting for missing spreads by plotting the histogram without NA removes the warning.\n\ndata.tabledtplyr\n\n\n\n# Plot a histogram of missing quoted spreads\nggplot(quotes[is.na(ask_price - bid_price)], \n       aes(x = time, fill = ticker)) +\n  geom_histogram(bins = 100) + \n  labs(title = \"Histogram of missing spreads\",\n       x = \"Time of Day (seconds past midnight)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\ntv_quotes |&gt; \n  filter(is.na(ask_price) | is.na(bid_price)) |&gt;\n  mutate(time = hms::hms(time)) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(x = time, fill = ticker)) +\n  geom_histogram(bins = 100) + \n  labs(title = \"Histogram of missing spreads\",\n       x = \"Time of Day\",       \n       y = \"Count\")"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#liquidity-measures",
    "href": "blog/tidy-market-microstructure/index.html#liquidity-measures",
    "title": "Tidy Market Microstructure",
    "section": "Liquidity measures",
    "text": "Liquidity measures\nWith all the data preparation done, we are ready for the actual liquidity measurement. For comparisons across stocks, it is useful to relate the nominal spread to the fundamental value of the security. This is done by the relative quoted bid-ask spread, defined as \\(\\text{quoted}\\_\\text{spread}^{rel} = (P^A - P^B)/M\\), where \\(M\\) is the midpoint (also known as the midprice; defined as the average of the best bid and the best ask prices). One can argue that the midpoint is not always representative of the fundamental value, but it has the strong advantage that it is continuously available in the quote data.\n\ndata.tabledtplyr\n\n\n\n# Fundamental value\nquotes[, midpoint := (bid_price + ask_price) / 2]\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt; \n  mutate(midpoint = (bid_price + ask_price) / 2)\n\n\n\n\nThe quoted spread can also be measured relative to the tick size. In an open market, the spread can never be below one tick. A tick refers to the tick size of a security. It is the minimum price increment a security can be quoted and traded. The tick size in the example data is half a cent at both exchanges. We refer to the average number of ticks in the bid-ask spread as the tick spread, \\(quoted\\_spread^{tic} = (P^A - P^B) / tick\\_size\\).\n\ntick_size &lt;- 0.5\n\nAnother dimension of quoted liquidity is the market depth. We measure the average depth quoted at the best bid and ask prices. It is defined as \\(\\text{quoted}\\_\\text{depth} = (Q^A + Q^B)/2\\), where \\(Q^A\\) and \\(Q^B\\) are the depths available at the bid and ask prices.\nIn the code below, we store the liquidity measures in a new data.table named quotes_liquidity. This is because the new variables are averages, observed on a ticker-date frequency, as opposed to the tick-by-tick frequency of the quotes object. We multiply the quoted spread by 10,000 to express it in basis points, and divide the quoted depth by 100,000 to express it in thousand GBP.\nThe output shows that the liquidity is higher at the LSE than at the TQE. Both in nominal and relative terms, the spreads are somewhat tighter at the LSE, and there is more than three times more depth posted at the LSE.\n\ndata.tabledtplyr\n\n\n\n# Measure the average quote-based liquidity\n# This step calculates the mean of each of the market quality variables, for each \n# ticker-day (as indicated in `by = c(\"ticker\", \"date\")`)\n\nquotes_liquidity &lt;- quotes[, {\n    quoted_spread = ask_price - bid_price\n    list(quoted_spread_nom = mean(quoted_spread, na.rm = TRUE),\n         quoted_spread_relative = mean(quoted_spread / midpoint, na.rm = TRUE),\n         quoted_spread_tick = mean(quoted_spread / tick_size, na.rm = TRUE),\n         quoted_depth = mean(bid_depth * bid_price + ask_depth * ask_price, \n                             na.rm = TRUE) / 2)},\n    by = c(\"ticker\", \"date\")]\n\n# Output the liquidity measures, averaged across the five trading days for each ticker. \nquotes_liquidity[, \n    list(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),\n         quoted_spread_relative = round(mean(quoted_spread_relative) * 1e4, digits = 2),\n         quoted_spread_tick = round(mean(quoted_spread_tick), digits = 2),\n         quoted_depth = round(mean(quoted_depth) * 1e-5, digits = 2)), \n    by = \"ticker\"]\n\n    ticker quoted_spread_nom quoted_spread_relative quoted_spread_tick\n1:   PRU.L              0.83                   5.65               1.67\n2: PRUl.TQ              1.02                   6.94               2.05\n   quoted_depth\n1:        24.72\n2:         6.65\n\n\n\n\n\ntv_quotes_liquidity &lt;- tv_quotes |&gt;\n  mutate(quoted_spread = ask_price - bid_price) |&gt;\n  group_by(ticker, date) |&gt; \n  summarize(quoted_spread_nom = mean(quoted_spread, na.rm = TRUE),\n            quoted_spread_relative = mean(quoted_spread / midpoint, na.rm = TRUE) * 1e4,\n            quoted_spread_tick = mean(quoted_spread / tick_size, na.rm = TRUE),\n            quoted_depth = mean(bid_depth * bid_price + ask_depth * ask_price, \n                                na.rm = TRUE) / 2 * 1e-5,\n            .groups = \"drop\")\n\ntv_quotes_liquidity |&gt; \n  group_by(ticker) |&gt;\n  summarize(across(contains(\"quoted\"), \n                   ~round(mean(.), digits = 2))) |&gt;\n  pivot_longer(-ticker) |&gt;\n  pivot_wider(names_from = name, values_from = value) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 5\n  ticker  quoted_depth quoted_spread_nom quoted_spread_relative\n  &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n1 PRU.L          24.7               0.83                   5.65\n2 PRUl.TQ         6.65              1.02                   6.94\n# ℹ 1 more variable: quoted_spread_tick &lt;dbl&gt;\n\n\n\n\n\n\nDuration-weighted liquidity\nThe output above are straight averages, implying an assumption that all quote observations are equally important. But whereas some quotes remain valid for several minutes, many don’t last longer than a split-second. For this reason, it is common to either sample the quote data in fixed time intervals (such as at the end of each second), or to weight the observations by their duration. The duration is the time that a quote observation is in force. That is, the time elapsed until the next quote update arrives. We show the duration-weighted approach in the code below (for guidance on how to get the quotes at the end of each second, see Section 3.1).\nNote that the duration variable is obtained separately for each ticker and date. Even if we are interested in the average liquidity across dates, it is important to partition by each ticker and date to avoid that duration is calculated overnight (resulting in a huge weight with negative sign, because it will be roughly the opening time minus the closing time). Except for replacing the mean function with the weighted.mean, the code below is very similar to that above.\nIn the output, we note that the differences between the duration-weighted and the equal-weighted liquidity averages are small. Nevertheless, we consider the duration-weighted average more appropriate because it is not sensitive to short-lived price and depth fluctuations.\n\ndata.tabledtplyr\n\n\n\n# Calculate quote durations\nquotes[, duration := c(diff(time), 0), by = c(\"ticker\", \"date\")]\n\n# Measure the duration-weighted average quote-based liquidity\n# The specified subset excludes quotes for which no duration can be calculated\nquotes_liquidity_dw &lt;- quotes[!is.na(duration), {\n    quoted_spread = ask_price - bid_price\n    list(quoted_spread_nom = weighted.mean(quoted_spread, \n                                           w = duration, na.rm = TRUE),\n         quoted_spread_rel = weighted.mean(quoted_spread / midpoint, \n                                           w = duration, na.rm = TRUE),\n         quoted_spread_tic = weighted.mean(quoted_spread / tick_size, \n                                           w = duration, na.rm = TRUE),\n         quoted_depth = weighted.mean(bid_depth * bid_price + ask_depth * ask_price, \n                                      w = duration, na.rm = TRUE) / 2)},\n    by = c(\"ticker\", \"date\")]\n\n# Output liquidity measures, averaged across the five trading days for each ticker \nquotes_liquidity_dw[, \n    list(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),\n         quoted_spread_rel = round(mean(quoted_spread_rel) * 1e4, digits = 2),\n         quoted_spread_tic = round(mean(quoted_spread_tic), digits = 2),\n         quoted_depth = round(mean(quoted_depth) * 1e-5, digits = 2)), \n    by = \"ticker\"]\n\n    ticker quoted_spread_nom quoted_spread_rel quoted_spread_tic quoted_depth\n1:   PRU.L              0.84              5.70              1.68        25.65\n2: PRUl.TQ              0.99              6.69              1.97         7.06\n\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt;\n  group_by(ticker, date) |&gt;\n  mutate(duration = c(diff(time), 0)) |&gt;\n  ungroup()\n\ntv_quotes_liquidity &lt;- tv_quotes |&gt;\n  mutate(quoted_spread = ask_price - bid_price) |&gt;\n  group_by(ticker, date) |&gt;\n  summarize(quoted_spread_nom = weighted.mean(quoted_spread, w = duration, na.rm = TRUE),\n            quoted_spread_relative = weighted.mean(quoted_spread / midpoint, w = duration, na.rm = TRUE) * 1e4,\n            quoted_spread_tick = weighted.mean(quoted_spread / tick_size, w = duration, na.rm = TRUE),\n            quoted_depth = weighted.mean(bid_depth * bid_price + ask_depth * ask_price, w = duration, na.rm = TRUE) / 2 * 1e-5,\n            .groups = \"drop\")\n\ntv_quotes_liquidity |&gt; \n  group_by(ticker) |&gt;\n  summarize(across(contains(\"quoted\"), \n                   ~round(mean(.), digits = 2))) |&gt;\n  pivot_longer(-ticker) |&gt;\n  pivot_wider(names_from = name, values_from = value) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 5\n  ticker  quoted_depth quoted_spread_nom quoted_spread_relative\n  &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n1 PRU.L          25.6               0.84                   5.7 \n2 PRUl.TQ         7.06              0.99                   6.69\n# ℹ 1 more variable: quoted_spread_tick &lt;dbl&gt;"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#consolidated-liquidity-in-fragmented-markets",
    "href": "blog/tidy-market-microstructure/index.html#consolidated-liquidity-in-fragmented-markets",
    "title": "Tidy Market Microstructure",
    "section": "Consolidated liquidity in fragmented markets",
    "text": "Consolidated liquidity in fragmented markets\nWith the competition between exchanges, liquidity is dispersed across venues. For example, if there is a change to the market structure at the LSE, it is typically not sufficient to analyze liquidity at LSE alone. If liquidity is reduced at the LSE, it may simultaneously be boosted at the TQE. To assess the overall market quality, which may be most relevant for welfare, it is often necessary to consider the consolidated liquidity.\nIn Europe, the consolidated liquidity is sometimes referred to as the European Best Bid and Offer (EBBO). The terminology follows in the footsteps of the US market, where the National Best Bid and Offer (NBBO) is transmitted to the market on continuous basis. To obtain the EBBO, one needs to merge the LOB data from each relevant venue, and then determine the EBBO prices and depths. In the code below, we show step-by-step how to do that.\n\nRetaining only the last quote update in each interval\nQuote updates tend to cluster and it is common that several observations have identical timestamps. Multiple observations at one timestamp can be due to several investors responding to the same events, or that one market order leads to several LOB updates as it is executed against multiple limit orders. When matching quotes across venues, we need to restrict the number of observations per unit of time to one. There is no sensible way to distinguish observations with identical timestamps. In lack of a better approach, we retain the last observation in each interval.\n\ndata.tabledtplyr\n\n\n\n# Retain only the last observation per unit of time\n# The function `duplicated` returns `TRUE` if the observation is a duplicate of another \n# observation based on the columns given in the `by` option, and `FALSE` otherwise.\n# The option `fromLast = TRUE` ensures that the last rather than the first observation \n# in each millisecond that returns `FALSE`.\nquotes &lt;- quotes[!duplicated(quotes, fromLast = TRUE, by = c(\"ticker\", \"date\", \"time\"))]\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt; \n  group_by(ticker, date, time) |&gt;\n  slice(n()) |&gt; \n  ungroup() |&gt;\n  lazy_dt()\n\n\n\n\n\n\nMerging quotes from different venues\nWe are now ready to match the quotes from the two exchanges. First, we create separate quote data sets for the two exchanges. Second, we merge the two by matching on date and time. Third, we forward-fill quotes from both venues, such that for each LSE quote we know the prevailing TQE quote, and vice versa. The validity of this is ensured by the option sort = TRUE in the merge function, which returns a data.table that is sorted on the matching variables.\n\ndata.tabledtplyr\n\n\n\n# Merge quotes from two venues trading the same security\n# In the `merge` function, we add exchange suffixes to the variable names to keep track of \n# which quote comes from which exchange, using the option `suffixes`. \n# The option `all = TRUE` specifies that unmatched observations from both sets of quotes \n# should be retained (known as an outer join). \nvenues &lt;- c(\"_lse\", \"_tqe\")\n\nquotes_lse &lt;- quotes[ticker == \"PRU.L\", .SD, .SDcols = c(\"date\", \"time\", lob_variables)]\nquotes_tqe &lt;- quotes[ticker == \"PRUl.TQ\", .SD, .SDcols = c(\"date\", \"time\", lob_variables)]\n\nquotes_ebbo &lt;- merge(quotes_lse, quotes_tqe, \n                     by = c(\"date\", \"time\"), \n                     suffixes = venues, \n                     all = TRUE, sort = TRUE)\n\nNext, we forward-fill the quoted prices and depth for each exchange.\n\nlocal_lob_variables &lt;- paste0(lob_variables, rep(venues, each = 4))\n\nquotes_ebbo[, (local_lob_variables) := lapply(.SD, nafill, type = \"locf\"), \n  .SDcols = (local_lob_variables),\n    by = \"date\"]\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes |&gt; \n  select(-midpoint, -duration) |&gt;\n  mutate(ticker = case_when(ticker == \"PRUl.TQ\" ~ \"tqe\",\n                            ticker == \"PRU.L\" ~ \"lse\")) |&gt;\n  pivot_wider(names_from = ticker, \n              values_from = matches(\"bid|ask\")) |&gt;\n  arrange(date, time)\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  group_by(date) |&gt;\n  fill(matches(\"bid|ask\")) |&gt; \n  ungroup()\n\n\n\n\nThe best bid price at each point in time is the maximum of the best bid at the LSE and the best bid at the TQE. Similarly, the best ask is the minimum of the best ask prices at the two venues. We calculate the best bid using the parallel maxima function, pmax, which returns the highest value in each row. The best ask is obtained in the same way, using the parallel minima function, pmin.\nNote that it would also be possible to obtain the EBBO using a for loop, checking row-wise which is the highest bid and lowest ask. When working with large data sets in R, however, loops become extremely slow. It is strongly encouraged to run vectorised operations for the whole column at once (like we do here), or to apply functions repeatedly to blocks of data (like we have done several times above).\nWe obtain the depth at the best prices by summing the depth of the individual venues. When doing this, we should only consider both venues at times when they are both at the best price. When the two venues have the same best bid, for example, we calculate the consolidated bid depth as the sum of the two. To code this, we use the feature that a logical variable (with values FALSE or TRUE; such as bid_price_lse  == best_bid_price) works as a binary variable (with values 0 or 1) when used in multiplication.\n\ndata.tabledtplyr\n\n\n\n# Obtain the EBBO prices and depths\nquotes_ebbo[, best_bid_price := pmax(bid_price_lse, bid_price_tqe, na.rm = TRUE)]\nquotes_ebbo[, best_ask_price := pmin(ask_price_lse, ask_price_tqe, na.rm = TRUE)]\nquotes_ebbo[, best_bid_depth := bid_depth_lse * (bid_price_lse == best_bid_price) + \n              bid_depth_tqe * (bid_price_tqe == best_bid_price)]\nquotes_ebbo[, best_ask_depth := ask_depth_lse * (ask_price_lse == best_ask_price) +\n              ask_depth_tqe * (ask_price_tqe == best_ask_price)]\n\nFinally, we drop local exchange variables and objects\n\nquotes_ebbo[, (local_lob_variables) := NULL]\nrm(quotes_lse, quotes_tqe, quotes, local_lob_variables)\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  mutate(best_bid_price = pmax(bid_price_lse, bid_price_tqe, na.rm = TRUE),\n         best_ask_price = pmin(ask_price_lse, ask_price_tqe, na.rm = TRUE),\n         best_bid_depth = bid_depth_lse * (bid_price_lse == best_bid_price) + \n         bid_depth_tqe * (bid_price_tqe == best_bid_price),\n         best_ask_depth = ask_depth_lse * (ask_price_lse == best_ask_price) + \n         ask_depth_tqe * (ask_price_tqe == best_ask_price)\n  )\n\nFinally, we drop local exchange variables and objects\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  select(date, time, contains(\"best\")) |&gt;\n  lazy_dt()\n\n\n\n\n\n\nFundamental value\nWe can now obtain EBBO midpoints, as a proxy of fundamental value that factors in liquidity posted at multiple exchanges.\n\ndata.tabledtplyr\n\n\n\n# Calculate EBBO midpoints\nquotes_ebbo[, midpoint := (best_bid_price + best_ask_price) / 2]\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt; \n  mutate(midpoint = (best_bid_price + best_ask_price) / 2)\n\n\n\n\n\n\nScreening\nAs above, we check that the EBBO quotes are economically meaningful by tabulating the counts of nominal spread levels. This exercise shows us that the consolidated spread is not strictly positive. There are numerous cases of zeroes, known as locked quotes, and also many negatives, referred to crossed quotes. This is possible because orders at the LSE and the TQE are never executed against each other – it takes arbitrageurs to step in and act on crossed markets. Locked and crossed spreads are not uncommon in consolidated data. For an analysis of the incidence in US markets, see Shkilko, van Ness & van Ness, 200814.\nIt is also notable from the table that the maximum consolidated spread is 2.5, as compared to the spreads of up to 11 recorded in the single-venue analysis. By definition, the EBBO quoted spread is never wider than at the single venues.\n\ndata.tabledtplyr\n\n\n\n# Output an overview of the EBBO nominal quoted bid-ask spread \ntable(quotes_ebbo$best_ask_price - quotes_ebbo$best_bid_price)\n\n\n    -1   -0.5      0    0.5      1    1.5      2    2.5    3.5 \n     2    127   6367 104351 103265   4668    637     51      1 \n\n\n\n\n\ntv_quotes_ebbo |&gt; \n  transmute(ebbo_nominal_spread = best_ask_price - best_bid_price) |&gt;\n  count(ebbo_nominal_spread) |&gt; \n  as_tibble()\n\n# A tibble: 9 × 2\n  ebbo_nominal_spread      n\n                &lt;dbl&gt;  &lt;int&gt;\n1                -1        2\n2                -0.5    127\n3                 0     6367\n4                 0.5 104351\n5                 1   103265\n6                 1.5   4668\n7                 2      637\n8                 2.5     51\n9                 3.5      1\n\n\n\n\n\nObservations with locked or crossed quotes are usually excluded when measuring market quality. It is also common to exclude bid-ask spread observations that are unrealistically high. We have no such cases in this sample, but, for illustration, we include a filter that would capture spreads that relative to the share price are wider than 5%.\n\nthreshold &lt;- 0.05\n\nIn the procedure below, we flag the problematic quotes, but we do not exclude them. If they were deleted, it would imply that the last observation before the excluded spread was still in force, which may mislead subsequent analysis.\nThe output shows that 2.90% of the quote observations are locked, while 0.06% are crossed.\n\ndata.tabledtplyr\n\n\n\n# Flag problematic consolidated quotes\nquotes_ebbo[, c(\"crossed\", \"locked\", \"large\") := {\n    quoted_spread = (best_ask_price - best_bid_price)\n    list(quoted_spread &lt; 0, quoted_spread == 0, quoted_spread / midpoint &gt; threshold)}]\n\n# Count the incidence of the consolidated quote flags\nquotes_ebbo_filters  &lt;- quotes_ebbo[, \n    list(crossed = mean(crossed, na.rm = TRUE),\n         locked = mean(locked, na.rm = TRUE),\n         large = mean(large, na.rm = TRUE))]\n\n# Output the fraction of quotes that is flagged\nquotes_ebbo_filters[, \n    lapply(.SD * 100, round, digits = 2), .SDcols = c(\"crossed\", \"locked\", \"large\")]\n\n   crossed locked large\n1:    0.06    2.9     0\n\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  mutate(quoted_spread = best_ask_price - best_bid_price,\n         crossed = quoted_spread &lt; 0,\n         locked = quoted_spread == 0,\n         large = quoted_spread / midpoint &gt; threshold) |&gt;\n  lazy_dt()\n\ntv_quotes_ebbo |&gt;\n  summarize(across(c(crossed, locked, large), \n                   ~round(100 * mean(.),2))) |&gt;\n  as_tibble()\n\n# A tibble: 1 × 3\n  crossed locked large\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    0.06    2.9     0\n\n\n\n\n\n\n\nConsolidated liquidity measures\nWe obtain duration-weighted measures of consolidated liquidity in the same way as above. The only difference here is that we subset the quotes to filter out crossed and locked markets.\nThe consolidated relative quoted bid-ask spread is 5.59 basis points, as compared to 5.70 and 6.69 basis points at LSE and TQE locally. The consolidated depth, 3.14 million GBP, is somewhat lower than the sum of the local depths seen above. This is to be expected, as some of the local depth is posted at price levels that are inferior to the EBBO.\n\ndata.tabledtplyr\n\n\n\n# Measure the duration-weighted consolidated quotes liquidity\n# Because this is the EBBO, there is no variation across tickers, but different averages \n# across dates are considered\nquotes_ebbo[, duration := c(diff(time), 0), by = \"date\"]\n\n# Note that the subset used here excludes crossed and locked quotes\nquotes_liquidity_ebbo_dw &lt;- quotes_ebbo[!crossed & !locked & !large, {\n    quoted_spread = best_ask_price - best_bid_price\n    \n    list(quoted_spread_nom = weighted.mean(quoted_spread, \n                                           w = duration, na.rm = TRUE),\n         quoted_spread_relative = weighted.mean(quoted_spread / midpoint, \n                                           w = duration, na.rm = TRUE),\n         quoted_spread_tick = weighted.mean(quoted_spread / tick_size,\n                                           w = duration, na.rm = TRUE),\n         quoted_depth = weighted.mean(best_bid_depth * best_bid_price + \n                                      best_ask_depth * best_ask_price, \n                                      w = duration, na.rm = TRUE) / 2)}, \n    by = \"date\"]\n\n# Output the liquidity measures, averaged across the five trading days \nquotes_liquidity_ebbo_dw[, \n    list(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),\n         quoted_spread_relative = round(mean(quoted_spread_relative * 1e4), digits = 2),\n         quoted_spread_tick = round(mean(quoted_spread_tick), digits = 2),\n         quoted_depth = round(mean(quoted_depth * 1e-6), digits = 2))]\n\n   quoted_spread_nom quoted_spread_relative quoted_spread_tick quoted_depth\n1:              0.82                   5.59               1.65         3.14\n\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  group_by(date)|&gt;\n  mutate(duration = c(diff(time), 0)) |&gt;\n  ungroup()\n\ntv_quotes_liquidity_ebbo_dw &lt;- tv_quotes_ebbo |&gt;\n  group_by(date) |&gt;\n  mutate(quoted_spread = best_ask_price - best_bid_price) |&gt;\n  filter(!crossed, !locked, !large) |&gt;\n  summarize(quoted_spread_nom = weighted.mean(quoted_spread, w = duration, na.rm = TRUE),\n            quoted_spread_relative = weighted.mean(quoted_spread / midpoint, w = duration, na.rm = TRUE) * 1e4,\n            quoted_spread_tick = weighted.mean(quoted_spread / tick_size, w = duration, na.rm = TRUE),\n            quoted_depth = weighted.mean(best_bid_depth * best_bid_price + best_ask_depth * best_ask_price, w = duration, na.rm = TRUE) / 2 * 1e-5)\n\ntv_quotes_liquidity_ebbo_dw |&gt;\n  summarize(across(contains(\"quoted\"), \n                   ~round(mean(.), digits = 2))) |&gt;\n  as_tibble()\n\n# A tibble: 1 × 4\n  quoted_spread_nom quoted_spread_relative quoted_spread_tick quoted_depth\n              &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n1              0.82                   5.59               1.65         31.4"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#data-export",
    "href": "blog/tidy-market-microstructure/index.html#data-export",
    "title": "Tidy Market Microstructure",
    "section": "Data export",
    "text": "Data export\nAs we will reuse the consolidated quotes in the applications below, we save the quotes_ebbo object to disk.\n\ndata.tabledtplyr\n\n\nWe export the consolidated quotes using fwrite.\n\nfwrite(quotes_ebbo, file = \"quotes_ebbo.csv\")\n\n\n\nWe export the consolidated quotes using write_csv.\n\nwrite_csv(as_tibble(tv_quotes_ebbo), file = \"tv_quotes_ebbo.csv\")"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#trade-data-inspection-and-preparation",
    "href": "blog/tidy-market-microstructure/index.html#trade-data-inspection-and-preparation",
    "title": "Tidy Market Microstructure",
    "section": "Trade data inspection and preparation",
    "text": "Trade data inspection and preparation\nWe load the trade data and view the data structure. The ticker, date, and time variables follow the same structure as in quotes data, so we can proceed with the same date and time transformations as above.\n\ntrades_url &lt;- \"http://tinyurl.com/prutrades\"\n\nThere are three more variables: Price, Volume, and MMT Classification. As discussed above, we refer to the number of shares executed in a trade as “size” (we reserve the term “volume” to the sum of trade sizes in a given interval). We rename the Volume variable accordingly, and also alter the other variable names to make them easier to work with in R.\n\ndata.tabledtplyr\n\n\n\n# Load the trade data\ntrades &lt;- fread(trades_url)\n\n# View the trade data\ntrades\n\n          #RIC       Domain           Date-Time GMT Offset  Type  Price Volume\n    1:   PRU.L Market Price 2021-06-07 07:00:09          1 Trade 1481.5  11379\n    2:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade 1477.5    703\n    3:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade 1477.5    327\n    4:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade 1477.5    650\n    5:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade 1478.0    327\n   ---                                                                        \n34513: PRUl.TQ Market Price 2021-06-11 15:28:56          1 Trade 1492.0    315\n34514: PRUl.TQ Market Price 2021-06-11 15:28:56          1 Trade 1491.5     50\n34515: PRUl.TQ Market Price 2021-06-11 15:28:56          1 Trade 1491.5    170\n34516: PRUl.TQ Market Price 2021-06-11 15:28:58          1 Trade 1492.0      6\n34517: PRUl.TQ Market Price 2021-06-11 15:29:06          1 Trade 1492.0      9\n                Exch Time MMT Classification\n    1: 07:00:09.478177000     1O-------P----\n    2: 07:00:12.859753000     12-------PH---\n    3: 07:00:12.859753000     12-------PH---\n    4: 07:00:12.859753000     12-------PH---\n    5: 07:00:12.860655000     12-------PH---\n   ---                                      \n34513: 15:28:56.202000000     12-------PH---\n34514: 15:28:56.218000000     12-------PH---\n34515: 15:28:56.413000000     12-------PH---\n34516: 15:28:58.062128000     32D---S--PH---\n34517: 15:29:06.141081000     32D---S--PH---\n\n# Rename the variables\nraw_trade_variables &lt;- c(\"#RIC\", \"Date-Time\", \"Exch Time\", \"GMT Offset\", \n                         \"Price\", \"Volume\", \"MMT Classification\")\nnew_trade_variables &lt;- c(\"ticker\", \"date_time\", \"exchange_time\", \"gmt_offset\",\n                         \"price\", \"size\", \"mmt\")\nsetnames(trades,\n         old = raw_trade_variables, \n         new = new_trade_variables)\n\nFinally, we remove the columns Domain and Type.\n\ntrades[, c(\"Domain\", \"Type\") := NULL]\n\n\n\n\ntv_trades &lt;- read_csv(trades_url, col_types = list(`Exch Time` = col_character()))\n\ntv_trades &lt;- lazy_dt(tv_trades)\n\nThe raw data looks as follows.\n\ntv_trades\n\nSource: local data table [34,517 x 9]\nCall:   `_DT17`\n\n  `#RIC` Domain  `Date-Time`         `GMT Offset` Type  Price Volume `Exch Time`\n  &lt;chr&gt;  &lt;chr&gt;   &lt;dttm&gt;                     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n1 PRU.L  Market… 2021-06-07 07:00:09            1 Trade 1482.  11379 07:00:09.4…\n2 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478.    703 07:00:12.8…\n3 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478.    327 07:00:12.8…\n4 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478.    650 07:00:12.8…\n5 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478     327 07:00:12.8…\n6 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478     201 07:00:12.8…\n# ℹ 34,511 more rows\n# ℹ 1 more variable: `MMT Classification` &lt;chr&gt;\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\nWe rename the variables.\n\ntv_trades &lt;- tv_trades |&gt;\n  rename(ticker = `#RIC`,\n         date_time = `Date-Time`,\n         gmt_offset = `GMT Offset`,\n         price = Price, \n         size = Volume, \n         exchange_time = `Exch Time`,\n         mmt = `MMT Classification`\n  ) |&gt;\n  select(-c(\"Domain\", \"Type\")) \n\n\n\n\n\nDates and timestamps\nAs above, we filter out the first and last minute of continuous trading, as well as the minutes surrounding the intraday auction. Note here that filtered trades are excluded, not just set as missing. This is OK, because we won’t do any forward-filling for the trade data.\n\ndata.tabledtplyr\n\n\n\n# Extract dates\ntrades[, date := as.Date(date_time)]\n\n# Convert time stamps to numeric format, expressed in seconds past midnight\ntrades[, time := {\n    time_elements = strsplit(exchange_time, split = \":\")\n    time_elements = do.call(rbind, time_elements)\n    time = as.numeric(time_elements[, 1]) * 3600 + \n      as.numeric(time_elements[, 2]) * 60 + \n      as.numeric(time_elements[, 3]) +\n      gmt_offset * 3600\n    list(time)}]\n\n# Delete raw time variables\ntrades[, c(\"date_time\", \"exchange_time\", \"gmt_offset\") := NULL]\n\n# Retain trades from the continuous trading sessions\ntrades &lt;- trades[time &gt; (open_time + 60) & time &lt; (close_time - 60) & \n                    (time &lt; (intraday_auction_time - 60) | \n                     time &gt; (intraday_auction_time + 3 * 60))]\n\n\n\n\ntv_trades &lt;- tv_trades |&gt; \n  separate(exchange_time, \n           into = c(\"hour\", \"minute\", \"second\"), \n           sep=\":\", \n           convert = TRUE) |&gt; \n  mutate(date = as.Date(date_time),\n         time = hour * 3600 + minute * 60 + second + gmt_offset * 3600)\n\nHaving made sure that dates and times are in the desired format, we can save space by dropping the raw time and date variables. Further, we only retain trades from the continuous trading sessions.\n\ntv_trades &lt;- tv_trades|&gt; \n  select(-c(\"date_time\", \"gmt_offset\",\"hour\",\"minute\",\"second\"))\n\ntv_trades &lt;- tv_trades |&gt;\n  filter(time &gt; hms::as_hms(\"08:01:00\"), \n         time &lt; hms::as_hms(\"16:29:00\"),\n         time &lt; hms::as_hms(\"11:59:00\") | time &gt; hms::as_hms(\"12:03:00\")) |&gt;\n  lazy_dt()\n\n\n\n\n\n\nTrade variables\nA line plot offers a good overview of price data. In the figure below, we see that the trade prices are plagued by outliers that seem to be close to zero. The out-of-sequence prices are recorded on all trading dates and at virtually all times of the day. These outliers need to be addressed before we can proceed with the analysis.\n\ndata.tabledtplyr\n\n\n\n# Plot the trade prices\ntrades |&gt; \n  ggplot(aes(x = time, y = price)) + \n  geom_line() + \n  facet_wrap(~date, ncol= 1) + \n  labs(title = \"Trade prices\",  y = \"Price\", x = \"Time (seconds past midnight)\")\n\n\n\n\n\n\n\n\n\n\n\ntv_trades |&gt;\n  as_tibble() |&gt; \n  ggplot(aes(x = hms::hms(time), y = price)) + \n  geom_line() + \n  facet_wrap(~date, ncol= 1) + \n  labs(title = \"Trade prices\",  y = \"Price\", x = \"Time (seconds past midnight)\")\n\n\n\n\n\n\n\n\n\n\n\nThere are various ways to handle outliers, but the best way is to understand them. In trade data sets, there is often information provided about the trade circumstances (for quote observations, such information is often sparse). In the current data set, the best piece of supporting information is the MMT code. MMT, short for Market Model Typology, is a rich set of flags reported for trades in Europe in recent years. For details, see the website of the Fix Trading Community.\nThe MMT code is a 14-character string, where each position corresponds to one flag. The first character specifies the type of market mechanism. For example, “1” tells us that the trade was recorded in an LOB market, “3” indicates dark pools, “4” is for off-book trading, and “5” is for periodic auctions. The second character indicates the trading mode, where, for example, continuous trading is indicated by “2”.\nAn overview of the populated values shows in the first column that the LOB market with continuous trading (“12”) is by far the most common combination remaining after applying the filters above, followed by dark pool continuous trading (“32”).\nThe low-priced trades are captured in the second column. All those trades are off-book, as indicated by the first digit being “4”. The second digit holds information about how the off-book trades are reported (“5” is for on-exchange, “6” is for off-exchange, and “7” indicates systematic internalisers).\n\ndata.tabledtplyr\n\n\n\n# Output an overview of the MMT codes\n# The function `substr` is used here to extract the first two characters of the MMT code\ntable(substr(trades[, mmt], start = 1, stop = 2), trades[, price] &lt; 100)\n\n    \n     FALSE  TRUE\n  12 27737     0\n  32  3383     0\n  3U   114     0\n  45  1221    28\n  46     0    41\n  47  1023   236\n  5U    88     0\n\n\n\n\n\ntv_trades |&gt;\n  mutate(message = str_extract(mmt, \"^.{2}\")) |&gt;\n  mutate(small_price = price &lt; 100) |&gt;\n  count(message, small_price)\n\nSource: local data table [10 x 3]\nCall:   copy(`_DT18`)[, `:=`(message = str_extract(mmt, \"^.{2}\"))][, \n    `:=`(small_price = price &lt; 100)][, .(n = .N), keyby = .(message, \n    small_price)]\n\n  message small_price     n\n  &lt;chr&gt;   &lt;lgl&gt;       &lt;int&gt;\n1 12      FALSE       27737\n2 32      FALSE        3383\n3 3U      FALSE         114\n4 45      NA              4\n5 45      FALSE        1221\n6 45      TRUE           28\n# ℹ 4 more rows\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\n\n\n\nIn this analysis we are focusing on liquidity at the exchanges. Accordingly, we use the MMT codes to filter out trades from other trading venues. Restricting the trades to continuous trading at the exchanges, we obtain price plots that are free from outliers.\n\ndata.tabledtplyr\n\n\n\n# Define a subset with continuous trades only\nLOB_continuous_trades &lt;- substr(trades[, mmt], start = 1, stop = 2) == \"12\"\n\n# Plot the prices of continuous trades\nggplot(trades[LOB_continuous_trades], \n       aes(x = time, y = price)) + \n  geom_line() + \n  facet_wrap(~date, ncol = 1) + \n  labs(title = \"Trade prices\", y = \"Price\", x = \"Time (seconds past midnight)\")\n\n\n\n\n\n\n\n\n\n\n\ntv_trades |&gt;\n  filter(str_extract(mmt, \"^.{2}\") == \"12\") |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(x = hms::hms(time), y = price)) + \n  geom_line() + \n  facet_wrap(~date, ncol = 1) + \n  labs(title = \"Trade prices\", y = \"Price\", x = \"Time (seconds past midnight)\")\n\n\n\n\n\n\n\n\n\n\n\nFurther detective work reveals that the trade price outliers are not erroneous, they are just stated in pounds rather than in pence. This is clear because the outliers are priced 100 times lower than the other trades. Apparently, some off-exchange trades follow a different price reporting convention.\n\n# View trades with low prices\ntrades[price &lt; 100]\n\n     ticker    price size            mmt       date     time\n  1:  PRU.L 14.80000  635 47------MP---- 2021-06-07 29102.70\n  2:  PRU.L 14.86625  400 45------MP---- 2021-06-07 31868.09\n  3:  PRU.L 14.86062  204 45------MP---- 2021-06-07 32331.08\n  4:  PRU.L 14.85125  540 45------MP---- 2021-06-07 32466.05\n  5:  PRU.L 14.83875 1042 45------MP---- 2021-06-07 39062.65\n ---                                                        \n301:  PRU.L 14.88500    1 47------MP---- 2021-06-11 57966.07\n302:  PRU.L 14.87000  434 47------MP---- 2021-06-11 58540.00\n303:  PRU.L 14.91500  870 47------MP---- 2021-06-11 59286.00\n304:  PRU.L 14.92000 1463 46------MP---- 2021-06-11 59336.00\n305:  PRU.L 14.92000 1463 46------MP---- 2021-06-11 59336.00\n\n\nQuirks in the data are not unusual, and if they go unnoticed they can have strong impact on the market quality measures. The take-away from the outlier analysis is that there is often an explanation for why their prices are off. It is not always as straightforward as here, but it is worthwhile to try to find out what the cause of the deviations is. Other potential explanations are that the time stamps are off (possibly due to delayed reporting) or that the pricing is not done at the market (but in accordance to some derivative contract).\nFor the analysis below, we want to focus on exchange trades. Accordingly, we filter out all trades that are not from the on-exchange continuous trading sessions.\n\ndata.tabledtplyr\n\n\n\n# Retain continuous LOB trades only\ntrades &lt;- trades[LOB_continuous_trades]\n\n\n\n\ntv_trades &lt;- tv_trades |&gt;\n  filter(str_extract(mmt, \"^.{2}\") == \"12\")\n\n\n\n\n\n\nMatching trades to quotes\nTo evaluate the cost of trading, we want to compare the trade price to the fundamental value at the time of trade, as implied by the bid-ask quotes.\nThe objective of matching trades and quotes is to obtain the quotes that prevailed just before the trade. This is straightforward in settings where the trades and quotes are recorded at the same point, such that they are correctly sequenced. In other settings, the timestamps may need to be adjusted due to reporting latencies, or the trade size needs to be matched to changes in quoted depth (Jurkatis, 2021)15.\nFor US data, the most common approach is to match trades to the last quotes available in the millisecond or microsecond before the trade, as prescribed by Holden and Jacobsen (2014)16. There is, however, an active debate which timestamp to use. Several recent papers advocate the use of participant time stamps in trade and quote matching, see references about US timestamps above.\nIn lack of specific guidance for stocks traded in the UK, we match trades to quotes prevailing just before the trade. Based on the assumption that the combined liquidity from LSE and TQE offers the best fundamental value approximation, we match trades from all venues to the EBBO.\nThe merge function in data.table can be called as above by merge(dt1, dt2) (for two data.tables named dt1 and dt2), or simply dt1[dt2]. We use the latter approach here because it allows us to specify what to do when the timestamps do not match exactly. The option roll = TRUE specifies that each observation in trades should be matched to the quotes_ebbo observation with the latest timestamp that is equal or earlier than the trade timestamp. However, we don’t want equal matches, because the quote observation should always be before the trade. To avoid matching to contemporaneous quotes, which may be updated to reflect the impact of the trade itself, we add one microsecond to the quote timestamps before running the merge function. For further understanding and illustration of the rolling join, we refer to the blog post by R-Bloggers.\n\ndata.tabledtplyr\n\n\nFor the sake of completeness, you can load the EBBO quote data which we stored at an intermediate step above as follows.\n\nquotes_ebbo &lt;- fread(file = \"quotes_ebbo.csv\")\n\n\n# Adjust quote time stamps by one microsecond\nsetnames(quotes_ebbo, old = \"time\", new = \"quote_time\")\nquotes_ebbo[, time := quote_time + 0.000001]\n\n# Sort trades and quotes (this specifies the matching criteria for the merge function)\nsetkeyv(trades, cols = c(\"date\", \"time\"))\nsetkeyv(quotes_ebbo, cols = c(\"date\", \"time\"))\n\n# Match trades to quotes prevailing at the time of trade \n# The rolling is done only for the last of the matching variables, in this case \"time\"\n# `mult = \"last\"` specifies that if there are multiple matches with identical timestamps, \n# the last match is retained\ntrades &lt;- quotes_ebbo[trades, roll = TRUE, mult = \"last\"]\n\n\n\nFor the sake of completeness, you can load the EBBO quote data which we stored at an intermediate step above as follows.\n\ntv_quotes_ebbo &lt;- read_csv(\"tv_quotes_ebbo.csv\")\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  mutate(time = time + 0.000001) |&gt;\n  arrange(date, time) |&gt; \n  as_tibble()\n\ntv_trades &lt;- tv_trades |&gt;\n  arrange(date, time) |&gt; \n  as_tibble()\n\ntv_trades &lt;- tv_trades |&gt;\n  left_join(tv_quotes_ebbo, join_by(date, closest(time &gt;= time)), suffix = c(\"\", \"_quotes\")) |&gt;\n  lazy_dt()\n\n\n\n\n\n\nFurther screening\nAs some trades may be matched to crossed or locked quotes, another round of data screening is required. Because such quotes are not considered reliable, we do not include those trades in the liquidity measurement. Furthermore, if there are trades that could not be matched to any quotes, or that lack information on price or size, they should be excluded too.\nThe output shows that 88.5% of the trades at LSE are eligible for the liquidity analysis, and 96.8% of the trades at TQE. The criterion that drives virtually all exclusions in the sample is the locked quotes.\n\ndata.tabledtplyr\n\n\n\n# Flag trades that should be included\ntrades[, include := !crossed & !locked & !large & !is.na(size) & size &gt; 0 &  \n                    !is.na(price) & price &gt; 0 & !is.na(midpoint) & midpoint &gt; 0]\n\n# Report trade filtering stats\ntrades_filters &lt;- trades[, \n    list(crossed = mean(crossed, na.rm = TRUE),\n         locked = mean(locked, na.rm = TRUE),\n         large = mean(large, na.rm = TRUE),\n         no_price = mean(is.na(price) | price == 0),\n         no_size = mean(is.na(size) | size == 0),\n         no_quotes = mean(is.na(midpoint) | midpoint &lt;= 0),\n         included = mean(include)), \n    by = \"ticker\"]\n\ntrades_filters[,\n  lapply(.SD * 100, round, digits = 2), \n  .SDcols = c(\"crossed\", \"locked\", \"large\", \"no_price\", \"no_size\", \"no_quotes\", \"included\"),\n  by = \"ticker\"]\n\n    ticker crossed locked large no_price no_size no_quotes included\n1:   PRU.L    0.30  11.20     0        0       0      0.00    88.50\n2: PRUl.TQ    0.07   3.05     0        0       0      0.02    96.85\n\n\n\n\n\ntv_trades &lt;- tv_trades |&gt;\n  mutate(include = !crossed & !locked & !large & !is.na(size) & size &gt; 0 &\n           !is.na(price) & price &gt; 0 & !is.na(midpoint) & midpoint &gt; 0)\n\ntrades_filters &lt;- tv_trades |&gt;\n  group_by(ticker) |&gt;\n  summarize(\n    crossed = mean(crossed, na.rm = TRUE),\n    locked = mean(locked, na.rm = TRUE),\n    large = mean(large, na.rm = TRUE),\n    no_price = mean(is.na(price) | price == 0),\n    no_size = mean(is.na(size) | size == 0),\n    no_quotes = mean(is.na(midpoint) | midpoint &lt;= 0),\n    included = mean(include)\n  )\n\n# Round percentages and display\ntrades_filters |&gt;\n  mutate(across(crossed:included, ~round(. * 100, digits = 2))) |&gt;\n  select(ticker, crossed, locked, large, no_price, no_size, no_quotes, included) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 8\n  ticker  crossed locked large no_price no_size no_quotes included\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 PRU.L      0.3   11.2      0        0       0      0        88.5\n2 PRUl.TQ    0.07   3.05     0        0       0      0.02     96.8\n\n\n\n\n\n\n\nDirection of trade\nIn empirical market microstructure, we often need to determine the direction of trade. If a trade happens following a buy market order, it is said to be buyer-initiated, and vice versa.\nThe most common tool to determine the direction of trade is the algorithm prescribed by Lee and Ready (1991)17. They primarily recommend the quote rule, saying that a trade is buyer-initiated if the trade price is above the prevailing midpoint, and seller-initiated if it is below. When the price equals the midpoint, Lee and Ready propose the tick rule. It specifies that a trade is buyer-initiated (seller-initiated) if the price is higher (lower) than the closest previous trade with a different price.\nThe quote rule is straightforward to implement using the sign function, which returns +1 when the price deviation from the midpoint is positive and -1 if it is negative. The tick rule, in contrast, requires several steps of code. We create a new data.table, named price_tick, which in addition to date and time observations for each trade, indicates whether a trade is priced higher (+1), lower (-1), or the same (0) as the previous one. We then exclude all trades that don’t imply a price change. Finally, we merge the price_tick and the trades objects, such that each trade is associated with the latest previous price change.\nThe direction of trade can now be determined, using primarily the quote rule, and secondarily the tick rule. The output shows that, in this sample, seller-initiated trades are somewhat more common than buyer-initiated.\n\ndata.tabledtplyr\n\n\n\n# Quote rule (the trade price is compared to the midpoint at the time of the trade)\ntrades[, quote_diff := sign(price - midpoint)]\n\n# Tick rule (each trade is matched to the closest preceding trade price change)\nprice_tick &lt;- data.table(date = trades$date,\n                         time = trades$time,\n                         price_change = c(NA, sign(diff(trades$price))))\n\n# Retain trades that imply a trade price change\nprice_tick &lt;- price_tick[price_change != 0]\n\n# Merge trades and trade price changes\nsetkeyv(trades, c(\"date\", \"time\"))\nsetkeyv(price_tick, c(\"date\", \"time\"))\ntrades &lt;- price_tick[trades, roll = TRUE, mult = \"last\"]\n\n# Apply the Lee-Ready (1991) algorithm\ntrades[, dir := {\n  # 1st step: quote rule\n  direction = quote_diff\n  # 2nd step: tick rule\n  no_direction = is.na(direction) | direction == 0\n    direction[no_direction] = price_change[no_direction] \n    \n    list(direction)},\n    by = \"date\"]\n\ntable(trades$ticker, trades$dir)\n\n         \n             -1     1\n  PRU.L   10854  8821\n  PRUl.TQ  4222  3840\n\n\n\n\nFirst, we compute the sign of the price change based on the quote rule (the trade price is compared to the midpoint at the time of the trade).\n\ntv_trades &lt;- tv_trades |&gt;\n  mutate(quote_diff = sign(price - midpoint))\n\nSecond, each trade is matched to the closest preceding trade price change.\n\nprice_tick &lt;- tv_trades |&gt;\n  transmute(date, time, price_change = c(NA, sign(diff(price)))) |&gt;\n  filter(price_change != 0) |&gt; \n  group_by(date,time)|&gt; \n  slice(n()) |&gt; \n  ungroup() |&gt;\n  as_tibble()\n\nWe merge the price_tick and the trades objects, such that each trade is associated with the latest previous price change.\n\ntv_trades &lt;- tv_trades |&gt;\n  as_tibble() |&gt;\n  left_join(price_tick, join_by(date, closest(time&gt;=time)), suffix = c(\"\", \"_tick\")) |&gt;\n  fill(price_change, .direction = \"up\") |&gt;\n  lazy_dt()\n\nFinally, we apply the Lee-Ready (1991) algorithm\n\ntv_trades &lt;- tv_trades |&gt;\n  group_by(date) |&gt;\n  mutate(dir = case_when(!is.na(quote_diff) & quote_diff != 0 ~ quote_diff,\n                         TRUE ~ price_change)\n  ) |&gt;\n  ungroup() |&gt;\n  lazy_dt()\n\ntv_trades |&gt; \n  count(ticker, dir) |&gt; \n  as_tibble()\n\n# A tibble: 4 × 3\n  ticker    dir     n\n  &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;\n1 PRU.L      -1 10854\n2 PRU.L       1  8821\n3 PRUl.TQ    -1  4222\n4 PRUl.TQ     1  3840"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#liquidity-measures-1",
    "href": "blog/tidy-market-microstructure/index.html#liquidity-measures-1",
    "title": "Tidy Market Microstructure",
    "section": "Liquidity measures",
    "text": "Liquidity measures\n\nEffective spread\nThe relative effective spread is defined as \\(\\text{effective}\\_\\text{spread}^{rel} = 2 D(P - M)/M\\), where \\(P\\) is the trade price and \\(D\\) is the direction of trade. The multiplication by two is to make the effective spread comparable to the quoted spread. As is common in the literature, we use trade size weights when calculating the average effective spread. We multiply the effective spread by 10,000 to express it in basis points.\nWe also calculate the dollar volume, which is simply \\(\\text{dollar}\\_\\text{volume} = P \\times \\text{size}\\), with \\(\\text{size}\\) denoting the trade size. The trading volume is commonly referred to as liquidity in popular press, but in academic papers it rarely used as a liquidity measure. One reason for that is that spikes in volume are usually due to news rather than liquidity shocks. We still include trading volume here, because it often included as a control variable in microstructure event studies. To express the dollar volume in million GBP, we multiply it by \\(10^{-8}\\).\nConsistent with the quote-based measures, the effective spread indicates that PRU is more liquid at LSE than at TQE. The LSE effective spread is 4.18 bps, more than 25% lower than the LSE quoted spread at 5.70 bps. The large difference may be due to trades executed at prices better than visible in the LOB, the effective spreads benchmarked to the consolidated midpoint rather than the respective venue midpoint, or the calculation of the effective spread at times of trade rather than continuously throughout the day. If investors time their trades to reduce trading costs, it makes sense that the effective spread is on average lower than the quoted spread. The interested reader can find out which of these differences drive the wedge between the two measures, by altering the way the average quoted spread is obtained.\n\ndata.tabledtplyr\n\n\n\n# Measure average liquidity for each stock-day\n\n# First order the table\nsetorder(trades, ticker, date, time)\n\ntrades_liquidity &lt;- trades[include == TRUE, {             \n  list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n                                      w = size),\n       volume = sum(price * size))\n  },\n    by = c(\"ticker\", \"date\")]\n\n# Output liquidity measures, averaged across the five trading days for each ticker\ntrades_liquidity[, \n    list(effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n         volume = round(mean(volume * 1e-8), digits = 2)), \n    by = \"ticker\"]\n\n    ticker effective_spread volume\n1:   PRU.L             4.18  13.64\n2: PRUl.TQ             4.52   3.19\n\n\n\n\n\ntv_trades &lt;- tv_trades |&gt;\n  arrange(ticker, date, time)\n\ntv_trades_liquidity &lt;- tv_trades |&gt;\n  filter(include == TRUE) |&gt;\n  group_by(ticker, date) |&gt;\n  summarize(\n    effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, w = size),\n    volume = sum(price * size),\n    .groups = \"drop\"\n  )\n\n# Output liquidity measures, averaged across the five trading days for each ticker\ntv_trades_liquidity |&gt;\n  group_by(ticker) |&gt;\n  summarize(\n    effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n    volume = round(mean(volume * 1e-8), digits = 2)\n  ) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 3\n  ticker  effective_spread volume\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1 PRU.L               4.18  13.6 \n2 PRUl.TQ             4.52   3.19\n\n\n\n\n\nIs the midpoint really a good proxy for the fundamental value of the security? Hagströmer (2021)18 shows that the reliance on the midpoint introduces bias in the effective spread. The bias is because traders are more likely to buy the security when the fundamental value is closer to the ask price, and more inclined to sell when the true value is close to the bid price. To capture this, we also consider the weighted midpoint, which is a proxy that allows the true value to lie anywhere between the best bid and ask prices. It is defined as \\(M_w=(P^BQ^A+P^AQ^B) / (Q^A+Q^B)\\), and denoted midpoint_w in the code below. The weighted midpoint equals the midpoint when \\(Q^A=Q^B\\).\nIn line with Hagströmer (2021), we find that the weighted midpoint version of the effective spread is lower than the conventional measure. At TQE, the difference is about 15%. This is noteworthy, as it overturns the previous finding that the TQE is less liquid than the LSE. Although the quoted spread at the TQE is wider, the results indicate that the traders at TQE are better at timing their trades in accordance to the fundamental value.\nThe reason for that the choice of effective spread measure matters for PRU is that its trading is constrained by the tick size (the quoted spread is almost always one or two ticks). Stocks that are not tick-constrained tend to show smaller differences between the effective spread measures.\n\ndata.tabledtplyr\n\n\n\n# Measure average liquidity for each stock-day, including the effective spread \n# relative to the weighted midpoint\ntrades_liquidity &lt;- trades[include == TRUE, {\n  midpoint_w = (best_bid_price * best_ask_depth + best_ask_price * best_bid_depth) / \n                 (best_ask_depth + best_bid_depth)\n    list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n                                          w = size,\n                                          na.rm = TRUE),\n         effective_spread_w = weighted.mean(2 * dir * (price - midpoint_w) / midpoint, \n                                            w = size,\n                                            na.rm = TRUE))},\n    by = c(\"ticker\", \"date\")]\n\n# Output liquidity measures, averaged across the five trading days for each ticker\ntrades_liquidity[, \n    list(effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n         effective_spread_w = round(mean(effective_spread_w * 1e4), digits = 2)), \n    by = \"ticker\"]\n\n    ticker effective_spread effective_spread_w\n1:   PRU.L             4.18               4.08\n2: PRUl.TQ             4.52               3.94\n\n\n\n\n\ntv_trades_liquidity &lt;- tv_trades |&gt;\n  filter(include == TRUE) |&gt;\n  group_by(ticker, date) |&gt;\n  mutate(midpoint_w = (best_bid_price * best_ask_depth + best_ask_price * best_bid_depth) /\n           (best_ask_depth + best_bid_depth)) |&gt;\n  summarize(\n    effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, w = size, \n                                     na.rm = TRUE),\n    effective_spread_w = weighted.mean(2 * dir * (price - midpoint_w) / midpoint, w = size,\n                                       na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ntv_trades_liquidity |&gt;\n  group_by(ticker) |&gt;\n  summarize(\n    effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n    effective_spread_w = round(mean(effective_spread_w * 1e4), digits = 2)\n  ) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 3\n  ticker  effective_spread effective_spread_w\n  &lt;chr&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n1 PRU.L               4.18               4.08\n2 PRUl.TQ             4.52               3.94\n\n\n\n\n\n\n\nPrice impact and realized spreads\nThe price impact is defined as \\(\\text{price}\\_\\text{impact}^{rel} = 2 D(M_{t+\\Delta}-M)/M\\), where \\(M_{t+\\Delta}\\) is the midpoint holding \\(\\Delta\\) seconds after the trade. It thus captures the signed price change following a trade.\nThe realized spread is defined as \\(\\text{realized}\\_\\text{spread}^{rel} = 2 D(P - M_{t+\\Delta})/M\\). Note that the price impact and the realized spread may be viewed as two components of the effective spread, as \\(\\text{effective}\\_\\text{spread}^{rel}=\\text{price}\\_\\text{impact}^{rel}+\\text{realized}\\_\\text{spread}^{rel}\\).\nThe choice of horizon (\\(\\Delta\\)) for the price impact and realized spread is arbitrary but can be important. The most common choice has traditionally been five minutes, but in modern markets that may even exceed the holding period of short-term investors. For a detailed discussion of this parameter, see Conrad & Wahal (2020).19\nThe code below prepares the variables needed to calculate the realized spread and the price impact at a 60 second horizon (setting \\(\\Delta=60\\)). It loads the consolidated quotes, subtracts 60 seconds to their timestamps, and merges them with the trades. In this way, we have trades that are matched to quotes prevailing just before the trade (from the previous matching) as well as to future quotes.\n\ndata.tabledtplyr\n\n\n\n# Adjust quote time stamps by 60 seconds\nquotes_ebbo &lt;- quotes_ebbo[, time := quote_time - 60]\n\n# Rename variables to indicate that they correspond to quotes 1 minute after the trade\n# The function `paste0` adds the suffix \"_1min\" to each variable\nsetnames(quotes_ebbo, \n         old = c(\"midpoint\", \"crossed\", \"locked\", \"large\"), \n         new = paste0(c(\"midpoint\", \"crossed\", \"locked\", \"large\"), \"_1min\"))\n\n# Merge trades and quotes\nsetkeyv(quotes_ebbo, cols = c(\"date\", \"time\"))\nsetkeyv(trades, cols = c(\"date\", \"time\"))\ntrades &lt;- quotes_ebbo[trades, roll = TRUE, mult = \"last\"]\n\n# Flag valid future quotes\ntrades[, include_1min := !crossed_1min & !locked_1min & !large_1min]\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  mutate(time = time - 60)\n\n# Rename variables to indicate that they correspond to quotes 1 minute after the trade\n# The function `paste0` adds the suffix \"_1min\" to each variable\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt; \n  mutate(across(c(midpoint, crossed, locked, large), \n                ~., \n                .names=\"{col}_1min\"))\n\n# Merge trades and quotes\ntv_trades &lt;- tv_trades |&gt;\n  as_tibble() |&gt;\n  left_join(tv_quotes_ebbo |&gt; select(date, time, contains(\"_1min\")), \n            join_by(date, closest(time&gt;=time)), suffix = c(\"\", \"_quotes\")) |&gt;\n  arrange(ticker, date, time) |&gt;\n  group_by(ticker, date) |&gt;\n  fill(midpoint_1min, crossed_1min, locked_1min, large_1min, .direction = \"down\")\n\n# Flag valid future quotes\ntv_trades &lt;- tv_trades |&gt;\n  mutate(include_1min = !crossed_1min & !locked_1min & !large_1min) |&gt;\n  lazy_dt()\n\n\n\n\nThe next step calculates the liquidity measures. The results show that the price impact exceeds the effective spread. The realized spread is then negative, implying that liquidity providers on average lose money. How can that be? Wouldn’t the market makers who incur losses simply refrain from trading? It could be that the 60-second evaluation does not reflect the market makers’ trading horizon. More likely, perhaps, is that not all traders who post limit orders are in the market to earn the bid-ask spread. It could also be liquidity traders or informed investors who use limit orders to save on transaction costs. Relative to paying the effective spread, earning a negative realized spread may well be an attractive alternative.\n\ndata.tabledtplyr\n\n\n\n# Measure trade-based liquidity\neffective_spread_decomposition &lt;- trades[include & include_1min, {\n    list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n                                  w = size),\n         price_impact = weighted.mean(2 * dir * (midpoint_1min - midpoint) / midpoint, \n                                 w = size),\n         realized_spread = weighted.mean(2 * dir * (price - midpoint_1min) / midpoint, \n                                 w = size))}, \n    by = c(\"ticker\", \"date\")]\n\n# Output the average liquidity measures\neffective_spread_decomposition[, \n    list(effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n         price_impact = round(mean(price_impact * 1e4), digits = 2),\n         realized_spread = round(mean(realized_spread * 1e4), digits = 2)), \n    by = \"ticker\"]\n\n    ticker effective_spread price_impact realized_spread\n1:   PRU.L             4.18         4.75           -0.57\n2: PRUl.TQ             4.52         6.13           -1.61\n\n\n\n\n\ntv_effective_spread_decomposition &lt;- tv_trades |&gt;\n  filter(include == TRUE & include_1min == TRUE) |&gt;\n  group_by(ticker, date) |&gt;\n  summarize(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n                                             w = size),\n              price_impact = weighted.mean(2 * dir * (midpoint_1min - midpoint) / midpoint, \n                                           w = size),\n              realized_spread = weighted.mean(2 * dir * (price - midpoint_1min) / midpoint, \n                                              w = size),\n              .groups = \"drop\")\n\ntv_effective_spread_decomposition |&gt;\n  group_by(ticker) |&gt;\n  summarize(\n    effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n    price_impact = round(mean(price_impact * 1e4), digits = 2),\n    realized_spread = round(mean(realized_spread * 1e4), digits = 2)\n  ) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 4\n  ticker  effective_spread price_impact realized_spread\n  &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1 PRU.L               4.18         4.75           -0.57\n2 PRUl.TQ             4.52         6.13           -1.61\n\n\n\n\n\nThe accuracy of the effective spread decomposition may be improved by replacing the midpoint by the weighted midpoint. We leave the implementation of that to the interested reader."
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#equispaced-returns",
    "href": "blog/tidy-market-microstructure/index.html#equispaced-returns",
    "title": "Tidy Market Microstructure",
    "section": "Equispaced returns",
    "text": "Equispaced returns\nTo get started, we again rely on the consolidated quote data. For the applications below, we drop all LOB variables except the midpoint and the filtering flags.\n\ndata.tabledtplyr\n\n\n\nquotes_ebbo &lt;- fread(file = \"quotes_ebbo.csv\")\n\n\n# Delete variables\nquotes_ebbo[, c(\"best_bid_price\", \"best_bid_depth\", \"best_ask_price\", \n                \"best_ask_depth\", \"duration\") := NULL]\n\n\n\n\ntv_quotes_ebbo &lt;- read_csv(\"tv_quotes_ebbo.csv\") |&gt; \n  select(date, time, midpoint, crossed, locked , large) |&gt;\n  lazy_dt()\n\n\n\n\nTo obtain equispaced observations, we create a time grid with one observation per second. As above, we exclude the first and last minute when setting the opening and closing times. We use the expand.grid function to create a grid of second-by-second observations for each sample date, and then convert it to a data.table.\nFirst we create an equispaced time grid.\n\nsampling_freq  &lt;- 1\nopen_time &lt;- 8 * 3600\nclose_time &lt;- 16.5 * 3600\n\nThe function seq creates a sequence of discrete numbers the by option defines the increment of the sequence, which is here 1 second\n\ntime_grid &lt;- seq(from = open_time + 60, to = close_time - 60, by = sampling_freq)\n\n\ndata.tabledtplyr\n\n\n\n# Repeat the time grid for each date and sort it by date and time\ndates &lt;- unique(quotes_ebbo$date)\nquotes_1sec &lt;- expand.grid(date = dates, time = time_grid)\n\n# Make it a data.table\nquotes_1sec &lt;- data.table(quotes_1sec, key = c(\"date\", \"time\"))\n\n# View the time grid\nquotes_1sec\n\n              date  time\n     1: 2021-06-07 28860\n     2: 2021-06-07 28861\n     3: 2021-06-07 28862\n     4: 2021-06-07 28863\n     5: 2021-06-07 28864\n    ---                 \n152401: 2021-06-11 59336\n152402: 2021-06-11 59337\n152403: 2021-06-11 59338\n152404: 2021-06-11 59339\n152405: 2021-06-11 59340\n\n\n\n\n\ntv_quotes_1sec &lt;- expand_grid(\n  date = tv_quotes_ebbo |&gt; pull(date) |&gt;  unique(), \n  time = time_grid\n)\n\n\n\n\nFor each point in the grid, we want to find the prevailing quote. That is, the last quote update preceding or coinciding with the time in question. Because quotes are valid until cancelled, the same quote may be matched to several consecutive seconds. To avoid bid-ask bounce in the market efficiency measures, we use the midpoint rather than the bid or ask prices.\nIn the same way as we matched trades to quotes above, we use the rolling merge to match the grid times to quotes. Once the merge is done, it is straightforward to set prices that are flagged as problematic to NA, and to calculate returns. We use log-diff returns expressed in basis points (i.e., multiplied by 10,000).\n\ndata.tabledtplyr\n\n\n\n# Sort the quotes\nsetkeyv(quotes_ebbo, cols = c(\"date\", \"time\"))\n\n# Merge the time grid with the quotes\nquotes_1sec &lt;- quotes_ebbo[quotes_1sec, roll = TRUE]\n\n# Set problematic quotes to NA\nquotes_1sec$midpoint[quotes_1sec$crossed|quotes_1sec$locked| quotes_1sec$large] &lt;- NA\n\n# Calculate returns, expressed in basis points\n# The function `diff` returns the first difference of a time series, which in this case \n# is the log of the midpoint. \n# For each date, a leading `NA` is added to make the resulting vector fit the number of \n# observations in `quotes_1sec`. \nquotes_1sec[, return := 1e4 * c(NA, diff(log(midpoint))), by = \"date\"]\n\n\n\n\ntv_quotes_1sec &lt;- tv_quotes_1sec |&gt; \n  left_join(tv_quotes_ebbo |&gt; as_tibble(), join_by(date, closest(time&gt;=time)), suffix = c(\"\", \"_quotes\")) |&gt;\n  mutate(midpoint = na_if(midpoint, locked|crossed|large)) |&gt;\n  group_by(date) |&gt;\n  mutate(return = 1e4 * c(NA, diff(log(midpoint)))) |&gt;\n  lazy_dt()"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#efficiency-and-volatility-measures",
    "href": "blog/tidy-market-microstructure/index.html#efficiency-and-volatility-measures",
    "title": "Tidy Market Microstructure",
    "section": "Efficiency and volatility measures",
    "text": "Efficiency and volatility measures\n\nReturn autocorrelation and realized volatility\nWe obtain the return autocorrelation by applying the cor function to returns and lagged returns. The latter are generated using the shift function with one lag. We account for missing values by specifying the option use = \"complete.obs\". The return autocorrelation comes out at 0.02, a very low number. This is not surprising, as the sample stock is a large firm with high trading activity – two characteristics associated with high market efficiency.\nRealized volatility is defined as the mean of squared returns. We also obtain the return variance to be able to calculate the variance ratio below. Although their definitions differ, the realized volatility and the return variance are almost identical in this data set, 0.32. This is because the mean return is close to zero.\n\ndata.tabledtplyr\n\n\n\n# Measure market efficiency and volatility \nefficiency_1sec &lt;- quotes_1sec[order(date, time), \n  list(return_corr_1sec = cor(return, shift(return, n = 1, type = \"lag\"), \n                              use = \"complete.obs\"),\n         realized_vol_1sec = mean(return^2, na.rm = TRUE),\n         return_var_1sec = var(return, na.rm = TRUE)), \n    by = \"date\"]\n\n# Output an overview of the average efficiency and volatility \nefficiency_1sec[, \n  list(return_corr_1sec = round(mean(return_corr_1sec), digits = 2),\n       realized_vol_1sec = round(mean(realized_vol_1sec), digits = 2),\n       return_var_1sec = round(mean(return_var_1sec), digits = 2))]\n\n   return_corr_1sec realized_vol_1sec return_var_1sec\n1:             0.02              0.32            0.32\n\n\n\n\n\ntv_efficiency_1sec &lt;- tv_quotes_1sec |&gt;\n  arrange(date, time) |&gt;\n  group_by(date) |&gt;\n  summarize(return_corr_1sec = cor(return, lag(return), use = \"complete.obs\"),\n            vol_1sec = mean(return^2, na.rm = TRUE),\n            return_var_1sec = var(return, na.rm = TRUE))\n\ntv_efficiency_1sec |&gt;\n  summarize(across(everything(), \n                   ~round(mean(.), digits = 2)))\n\nSource: local data table [1 x 4]\nCall:   `_DT26`[order(date, time)][, .(return_corr_1sec = cor(return, \n    shift(return, type = \"lag\"), use = \"complete.obs\"), vol_1sec = mean(return^2, \n    na.rm = TRUE), return_var_1sec = var(return, na.rm = TRUE)), \n    keyby = .(date)][, .(date = round(mean(date), digits = 2), \n    return_corr_1sec = round(mean(return_corr_1sec), digits = 2), \n    vol_1sec = round(mean(vol_1sec), digits = 2), return_var_1sec = round(mean(return_var_1sec), \n        digits = 2))]\n\n  date       return_corr_1sec vol_1sec return_var_1sec\n  &lt;date&gt;                &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 2021-06-09             0.02     0.32            0.32\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\n\n\n\n\n\nVariance ratios\nThe variance ratio is defined as \\(var\\_ratio_{\\tau_{1}, \\tau_{2}} = (Var(R_{\\tau_{1}}) \\tau_2) / (Var(R_{\\tau_{2}})  \\tau_1)\\), where \\(\\tau_{1}\\) and \\(\\tau_{2}\\) are two return sampling frequencies, and \\(Var(R_{\\tau_{i}})\\) is the variance of returns sampled at frequency \\(\\tau_{i}\\). Under market efficiency, the variance ratio should be equal to one. Negative and positive deviations from unity are due to frictions. It is usually the absolute deviation from unity that is used as an efficiency measure.\nWe measure the variance ratio for 1-second and 10-second returns. To get the 10-second return variance, the first step is to obtain the 10-second price grid. To do so, we simply take a subset of the grid obtained for the 1-second frequency. We then proceed with the calculation of returns and the return variance in the same way as above.\n\nsampling_freq &lt;- 10 # 10 second grid\ntime_grid &lt;- seq(from = open_time + 60, to = close_time - 60, by = sampling_freq)\n\n\ndata.tabledtplyr\n\n\n\n# Subset the 1-second price grid to get the 10-second price grid \nquotes_10sec &lt;- quotes_1sec[time %in% time_grid,]\n\n# Calculate returns at the 10-second frequency, expressed in basis points\nquotes_10sec[, return := 1e4 * c(NA, diff(log(midpoint))), by = \"date\"]\n\n# Calculate the return variance at the 10-second frequency, daily\nefficiency_10sec &lt;- quotes_10sec[, \n                      list(return_var_10sec = var(return, na.rm = TRUE)), \n                      by = \"date\"]\n\n\n\n\ntv_quotes_10sec &lt;- tv_quotes_1sec |&gt;\n  inner_join(tibble(time = time_grid)) |&gt;\n  group_by(date) |&gt;\n  mutate(return = 1e4 * c(NA, diff(log(midpoint))))\n\nJoining, by = \"time\"\n\ntv_efficiency_10sec &lt;- tv_quotes_10sec |&gt;\n  group_by(date) |&gt;\n  summarize(return_var_10sec = var(return, na.rm = TRUE))\n\n\n\n\nFinally, we merge the 10-second return variance with the 1-second frequency efficiency measures and calculate the variance ratio. The output shows that the 10-second return variance is slightly higher than ten times the 1-second return variance, resulting in a variance ratio that exceeds the efficiency benchmark (unity) by 8%.\n\ndata.tabledtplyr\n\n\n\n# Merge the efficiency measures of different return sampling frequencies\nefficiency &lt;- efficiency_1sec[efficiency_10sec]\n\n# Obtain the variance ratio\nefficiency[, var_ratio := return_var_10sec / (10 * return_var_1sec)]\n\n# Output an overview of the average variance ratio\nefficiency[, \n  list(return_var_1sec = round(mean(return_var_1sec), digits = 3),\n       return_var_10sec = round(mean(return_var_10sec), digits = 3),\n       var_ratio = round(mean(var_ratio), digits = 3))]\n\n   return_var_1sec return_var_10sec var_ratio\n1:           0.318            3.442     1.081\n\n\n\n\n\ntv_efficiency &lt;- tv_efficiency_1sec |&gt;\n  inner_join(tv_efficiency_10sec) |&gt;\n  mutate(var_ratio = return_var_10sec / (10 * return_var_1sec)) |&gt;\n  lazy_dt()\n\nJoining, by = \"date\"\n\ntv_efficiency |&gt;\n  summarize(across(c(return_var_1sec, return_var_10sec, var_ratio), ~round(mean(.), digits = 3)))\n\nSource: local data table [1 x 3]\nCall:   `_DT29`[, .(return_var_1sec = round(mean(return_var_1sec), digits = 3), \n    return_var_10sec = round(mean(return_var_10sec), digits = 3), \n    var_ratio = round(mean(var_ratio), digits = 3))]\n\n  return_var_1sec return_var_10sec var_ratio\n            &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1            0.32             3.44      1.08\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#footnotes",
    "href": "blog/tidy-market-microstructure/index.html#footnotes",
    "title": "Tidy Market Microstructure",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMenkveld, A. J., et al. (342 coauthors) (2023). Non-standard errors. Forthcoming in Journal of Finance. https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=3961574↩︎\nAmihud, Y. (2002). Illiquidity and stock returns: Cross-section and time-series effects. Journal of Financial Markets, 5(1), 31-56. https://doi.org/10.1016/S1386-4181(01)00024-6↩︎\nJahan-Parvar, M. R., & Zikes, F. (2023). When Do Low-Frequency Measures Really Measure Effective Spreads? Evidence from Equity and Foreign Exchange Markets. Review of Financial Studies, 36(10), 4190-4232. https://doi.org/10.1093/rfs/hhad028↩︎\nCampbell, J. Y., Lo, A. W., & MacKinlay, A. C. (1998). The Econometrics of Financial Markets. Princeton University Press.↩︎\nFoucault, T., Pagano, M., & Röell, A. (2013). Market liquidity: Theory, evidence, and policy. Oxford University Press, USA.↩︎\nHasbrouck, J. (2007). Empirical market microstructure: The institutions, economics, and econometrics of securities trading. Oxford University Press.↩︎\nA previous guide to microstructure programming (in SAS) is provided by Boehmer, Broussard and Kallunki (2002): Boehmer, E., Broussard, J. P., & Kallunki, J. P. (2002). Using SAS in financial research. SAS Publishing.↩︎\nPart of the data.table speed advantage is that it uses parallel processing by default. Consequently, one has to be cautious to use it in parallelization, as improper specification may lead to over-parallelization and undermine the performance benefits.↩︎\nScheuch, C., Voigt, S., & Weiss P. (2023). Tidy finance with {R}. Chapman and Hall/CRC. https://doi.org/10.1201/b23237 and https://tidy-finance.org/↩︎\nBartlett, R. P., & McCrary, J. (2019). How rigged are stock markets? Evidence from microsecond timestamps. Journal of Financial Markets, 45, 37-60. https://doi.org/10.1016/j.finmar.2019.06.003↩︎\nHolden, C. W., Pierson, M., & Wu, J. (2023). In the blink of an eye: Exchange-to-SIP latency and trade classification accuracy. Working paper. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441422↩︎\nSchwenk-Nebbe, S. (2022). The participant timestamp: Get the most out of TAQ data. Working paper. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3984827↩︎\nHagströmer, B., & Menkveld, A. J. (2023). Trades, quotes, and information shares. Working paper. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4356262↩︎\nShkilko, A. V., Van Ness, B. F., & Van Ness, R. A. (2008). Locked and crossed markets on NASDAQ and the NYSE. Journal of Financial Markets, 11(3), 308-337. https://www.sciencedirect.com/science/article/pii/S1386418107000031?casa_token=GIqJQiPKyisAAAAA:ZVbaf4SPC0IzxFFLWqgI2papSw2MrEf_lPXCL9OlT_ZnKgA6-pGTl4EYisKuIUaFdx8JY1p3d7o↩︎\nJurkatis, S. (2022). Inferring trade directions in fast markets. Journal of Financial Markets, 58, 100635. https://doi.org/10.1016/j.finmar.2021.100635↩︎\nHolden, C. W., & Jacobsen, S. (2014). Liquidity measurement problems in fast, competitive markets: Expensive and cheap solutions. Journal of Finance, 69(4), 1747-1785. https://doi.org/10.1111/jofi.12127↩︎\nLee, C. M., & Ready, M. J. (1991). Inferring trade direction from intraday data. Journal of Finance, 46(2), 733-746. https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.1991.tb02683.x↩︎\nHagströmer, B. (2021). Bias in the effective bid-ask spread. Journal of Financial Economics, 142(1), 314-337. https://doi.org/10.1016/j.jfineco.2021.04.018↩︎\nConrad, J., & Wahal, S. (2020). The term structure of liquidity provision. Journal of Financial Economics, 136(1), 239-259. https://doi.org/10.1016/j.jfineco.2019.09.008↩︎"
  },
  {
    "objectID": "blog/user-conference-2022/index.html",
    "href": "blog/user-conference-2022/index.html",
    "title": "Tidy Finance at the useR!2022 Conference",
    "section": "",
    "text": "We had the pleasure of presenting our book at the virtual useR!2022 conference with 1,227 registered participants from 96 countries. It was great fun! Since it was recorded, you can find the video of my presentation below."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html",
    "href": "blog/tidy-collaborative-filtering/index.html",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "",
    "text": "Recommender systems are a key component of our digital lifes, ranging from e-commerce, online advertisements, movie recommendations, or more generally all kinds of product recommendations. A recommender system aims to efficiently deliver personalized content to users based on a large pool of potentially relevant information. In this blog post, I illustrate the concept of recommender systems by building a simple stock recommendation tool that relies on publicly available portfolios from the social trading platform wikifolio.com. The resulting recommender proposes stocks to investors who already have their own portfolios and look for new investment opportunities. The underlying assumption is that the wikifolio traders hold stock portfolios that can provide meaningful inspiration for other investors. The resulting stock recommendations of course do not constitute any investment advice and rather serve an illustrative purpose.\nwikifolio.com is the leading social trading platform in Europe, where anyone can publish and monetize their trading strategies through virtual portfolios, which are called wikifolios. The community of wikifolio traders includes full time investors and successful entrepreneurs, as well as experts from different sectors, portfolio managers, and editors of financial magazines. All traders share their trading ideas through fully transparent wikifolios. The wikifolios are easy to track and replicate, by investing in the corresponding, collateralized index certificate. As of writing, there are more than 30k published wikifolios of more than 9k traders, indicating a large diversity of available portfolios for training our recommender.\nThere are essentially three types of recommender models: recommenders via collaborative filtering, recommenders via content-based filtering, and hybrid recommenders (that mix the first two). In this blog post, I focus on the collaborative filtering approach as it requires no information other than portfolios and can provide fairly high precision with little complexity. Nonetheless, I first briefly describe the recommender approaches and refer to Ricci et al. (2011)1 for a comprehensive exposition."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#collaborative-filtering",
    "href": "blog/tidy-collaborative-filtering/index.html#collaborative-filtering",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Collaborative Filtering",
    "text": "Collaborative Filtering\nIn collaborative filtering, recommendations are based on past user interactions and items to produce new recommendations. The central notion is that past user-item interactions are sufficient to detect similar users or items. Broadly speaking, there are two sub-classes of collaborative filtering: the memory-based approach, which essentially searches nearest neighbors based on recorded transactions and is hence model-free, and the model-based approach, where new representations of users and items are built based on some generative pre-estimated model. Theoretically, the memory-based approach has a low bias (since no latent model is assumed) but a high variance (since the recommendations change a lot in the nearest neighbor search). The model-based approach relies on a trained interaction model. It has a relatively higher bias but a lower variance, i.e., recommendations are more stable since they come from a model.\nAdvantages of collaborative filtering include: (i) no information about users or items is required; (ii) a high precision can be achieved with little data; (iii) the more interaction between users and items is available, the more recommendations become accurate. However, the disadvantages of collaborative filtering are: (i) it is impossible to make recommendations to a new user or recommend a new item (cold start problem); (ii) calculating recommendations for millions of users or items consumes a lot of computational power (scalability problem); (iii) if the number of items is large relative to the users and most users only have interacted with a small subset of all items, then the resulting representation has many zero interactions and might hence lead to computational difficulties (sparsity problem)."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#content-based-filtering",
    "href": "blog/tidy-collaborative-filtering/index.html#content-based-filtering",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Content-Based Filtering",
    "text": "Content-Based Filtering\nContent-based filtering methods exploit information about users or items to create recommendations by building a model that relates available characteristics of users or items to each other. The recommendation problem is hence cast into a classification problem (the user will like the item or not) or more generally a regression problem (which rating will the user give an item). The classification problem can be item-centered by focusing on available user information and estimating a model for each item. If there are a lot of user-item interactions available, the resulting model is fairly robust, but it is less personalized (as it ignores user characteristics apart from interactions). The classification problem can also be user-centered by working with item features and estimating a model for each user. However, if a user only has a few interactions then the resulting model becomes easily unstable. Content-based filtering can also be neither user nor item-centered by stacking the two feature vectors, hence considering both input simultaneously, and putting them into a neural network.\nThe main advantage of content-based filtering is that it can make recommendations for new users without any interaction history or recommend new items to users. The disadvantages include: (i) training needs a lot of users and item examples for reliable results; (ii) tuning might be much harder in practice than collaborative filtering; (iii) missing information might be a problem since there is no clear solution how to treat missingness in user or item characteristics."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#hybrid-recommenders",
    "href": "blog/tidy-collaborative-filtering/index.html#hybrid-recommenders",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Hybrid Recommenders",
    "text": "Hybrid Recommenders\nHybrid recommender systems combine both collaborative and content-based filtering to overcome the challenges of each approach. There are different hybridization techniques available, e.g., combining the scores of different components (weighted), chosing among different component (switching), following strict priority rules (cascading), presenting outputs from different components at the same time (mixed), etc."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#roc-curves",
    "href": "blog/tidy-collaborative-filtering/index.html#roc-curves",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "ROC curves",
    "text": "ROC curves\nThe first visualization approach comes from signal-detection and is called “Receiver Operating Characteristic” (ROC). The ROC-curve plots the algorithm’s probability of detection (TPR) against the probability of false alarm (FPR).\n\nTPR = TP / (TP + FN) (i.e., share of true positive recommendations relative to all known portfolios)\nFPR = FP / (FP + TN) (i.e., share of incorrect recommendations relative to )\n\nIntuitively, the bigger the area under the ROC curve, the better is the corresponding algorithm.\n\nresults_tbl |&gt;\n  ggplot(aes(FPR, TPR, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"False Positive Rate (FPR)\",\n    y = \"True Positive Rate (TPR)\",\n    title = \"ROC curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\n\nThe figure shows that recommending random items exhibits the lowest TPR for any FPR, so it is the worst among all algorithms (which is not surprising). Association rules, on the other hand, constitute the best algorithm among the current selection. This result is neat because association rule mining is a computationally cheap algorithm, so we could potentially fine-tune or reestimate the model easily."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#precision-recall-curves",
    "href": "blog/tidy-collaborative-filtering/index.html#precision-recall-curves",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Precision-Recall Curves",
    "text": "Precision-Recall Curves\nThe second popular approach is to plot Precision-Recall curves. The two measures are often used in information retrieval problems:\n\nPrecision = TP / (TP + FP) (i.e., correctly recommended items relative to total recommended items)\nRecall = TP / (TP + FN) (i.e., correctly recommended items relative to total number of known useful recommendations)\n\nThe goal is to have a higher precision for any level of recall. In fact, there is trade-off between the two measures since high precision means low recall and vice-versa.\n\nresults_tbl |&gt;\n  ggplot(aes(x = recall, y = precision, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"Recall\", y = \"Precision\",\n    title = \"Precision-Recall curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\n\nAgain, proposing random items exhibits the worst performance, as for any given level of recall, this approach has the lowest precision. Association rules are also the best algorithm with this visualization approach."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#footnotes",
    "href": "blog/tidy-collaborative-filtering/index.html#footnotes",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRicci, F, Rokach, L., Shapira, B. and Kantor, P. (2011). “Recommender Systems Handbook”. https://link.springer.com/book/10.1007/978-0-387-85820-3.↩︎\nHahsler, M. (2022). “recommenderlab: An R Framework for Developing and Testing Recommendation Algorithms”, R package version 1.0.3. https://CRAN.R-project.org/package=recommenderlab.↩︎\nBreese, J.S., Heckerman, D. and Kadie, C. (1998). “Empirical Analysis of Predictive Algorithms for Collaborative Filtering”, Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, Madison, 43-52. https://arxiv.org/pdf/1301.7363.pdf.↩︎"
  },
  {
    "objectID": "blog/historical-sp-500-total-return/index.html",
    "href": "blog/historical-sp-500-total-return/index.html",
    "title": "Construction of a Historical S&P 500 Total Return Index",
    "section": "",
    "text": "I wanted to simulate simple equity savings plans over long time horizons and many different initiation periods for a story with the German news portal t-online. The good thing is that the S&P 500 index provides a great starting point as it is easily available since 1928 via Yahoo Finance. However, I wanted my savings plans to be accumulating, i.e., all cash distributions are reinvested in the savings plan. The S&P index is inadequate for this situation as it is a price index that only tracks its components’ price movements. The S&P 500 Total Return Index tracks the overall performance of the S&P 500 and would be the solution to my problem, but it is only available since 1988.\nFortunately, I came up with a solution using data provided by Robert Shiller and provide the complete code below for future reference. If you spot any errors or have better suggestions, please feel free to create an issue.\nThis is the set of packages I use throughout this post.\n\nlibrary(tidyverse) # for overall grammar\nlibrary(tidyquant) # to download data from yahoo finance\nlibrary(glue)      # to automatically construct figure captions\nlibrary(scales)    # for nicer axis labels \nlibrary(readxl)    # to read Shiller's data \n\nFirst, let us download the S&P 500 Total Return Index from Yahoo Finance. I only consider the closing prices of the last day of each month because my savings plans only transfer funds once a month. In principle, you could also approximate the daily time series, but I believe it will be noiser because Shiller only provides monthly data.\n\nsp500_recent &lt;-  tq_get(\"^SP500TR\", get = \"stock.prices\",\n                        from = \"1988-01-04\", to = \"2023-01-31\") |&gt;\n  select(date, total_return_index = close) |&gt;\n  drop_na() |&gt;\n  group_by(month = ceiling_date(date, \"month\")-1) |&gt;\n  arrange(date) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  select(month, total_return_index)\n\nNext, I download data from Robert Shiller’s website that he used in his great book Irrational Excuberance. I create a temporary file and read the relevant sheet. In particular, the data contains monthly S&P 500 price and dividend data. The original file has a bit of annoying date format that I have to correct before parsing.\n\ntemp &lt;- tempfile(fileext = \".xls\")\n\ndownload.file(url = \"http://www.econ.yale.edu/~shiller/data/ie_data.xls\",\n              destfile = temp, mode='wb')\n\nshiller_historical &lt;- read_excel(temp, sheet = \"Data\", skip = 7) |&gt;\n  transmute(month = ceiling_date(ymd(str_replace(str_c(Date, \".01\"), \"\\\\.1\\\\.\", \"\\\\.10\\\\.\")), \"month\")-1,\n            price = as.numeric(P),\n            dividend = as.numeric(D)) \n\nTo construct the total return index, I need a return that includes dividends. In the next code chunk, I compute monthly total returns of the S&P 500 index by incorporating the monthly dividend paid on the index in the corresponding month. Note that Shiller’s data contains the 12-month moving sum of monthly dividends, hence the division by 12. Admittedly, this is a brute force approximation, but I couldn’t come up with a better solution so far.\n\nshiller_historical &lt;- shiller_historical |&gt;\n  arrange(month) |&gt;\n  mutate(ret = (price + dividend / 12) / lag(price) - 1)\n\nBefore I go back in time, let us check whether the total return computed above is able to match the actual total return since 1988. I start with the first total return index number that is available and use the cumulative product of returns from above to construct the check time series.\n\ncheck &lt;- shiller_historical |&gt;\n  full_join(sp500_recent, by = \"month\") |&gt;\n  filter(!is.na(total_return_index)) |&gt;\n  arrange(month) |&gt;\n  mutate(ret = if_else(row_number() == 1, 0, ret), # ignore first month return\n         total_return_check = total_return_index[1] * cumprod(1 + ret)) |&gt;\n  drop_na()\n\nThe correlation between the actual time series and the check is remarkably high which gives me confidence in the method I propose here.\n\ncheck |&gt; \n  select(total_return_index, total_return_check) |&gt;  \n  cor()\n\n                   total_return_index total_return_check\ntotal_return_index              1.000              0.999\ntotal_return_check              0.999              1.000\n\n\nIn addition, the visual inspection of the two time series in Figure 1 corroborates my confidence. Note that both the actual and the simulated total return indexes start at the same index value.\n\ncheck |&gt;\n  select(month, Actual = total_return_index, Simulated = total_return_check) |&gt;\n  pivot_longer(cols = -month) |&gt;\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = comma)+ \n  labs(x = NULL, y = NULL, color = NULL,\n       title = \"Actual and simulated S&P 500 Total Return index\",\n       subtitle = glue(\"Both indexes start at {min(check$month)}\"))\n\n\n\n\n\n\n\nFigure 1: Simluated and actual S&P 500 Total Return index data move closely together.\n\n\n\n\n\nNow, let us use the same logic to construct the total return index for the time before 1988. Note that I just sort the months in descending order and divide by the cumulative product of the total return from Shiller’s data.\n\nsp500_historical &lt;- sp500_recent |&gt; \n  filter(month == min(month)) |&gt;\n  full_join(shiller_historical |&gt;\n              filter(month &lt;= min(sp500_recent$month)), by = \"month\") |&gt;\n  arrange(desc(month)) |&gt;\n  mutate(ret = if_else(row_number() == 1, 0, ret),\n         total_return_index = total_return_index[1] / cumprod(1 + ret))\n\nBefore we take a look at the results, I also add the S&P price index from Yahoo Finance for comparison.\n\nsp500_price_index &lt;- tq_get(\"^GSPC\", get = \"stock.prices\",\n                            from = \"1928-01-01\", to = \"2023-01-31\") |&gt;\n  select(date, price_index = close) |&gt;\n  drop_na() |&gt;\n  group_by(month = ceiling_date(date, \"month\") - 1) |&gt;\n  arrange(date) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  select(month, price_index)\n\nFinally, let us combine (i) the actual S&P 500 Total Return Index from 1988 until 2023, (ii) the simulated S&P 500 total return index before 1988, and (iii) the S&P 500 price index from 1928 until 2023.\n\nsp500_monthly &lt;- sp500_recent|&gt;\n  bind_rows(sp500_historical |&gt;\n              filter(month &lt; min(sp500_recent$month))  |&gt;\n              select(month, total_return_index)) |&gt;\n  full_join(sp500_price_index |&gt; \n              select(month, price_index), by = \"month\") |&gt;\n  filter(month &gt;= \"1928-01-01\")  |&gt;\n  arrange(month)\nsp500_monthly\n\n# A tibble: 1,141 × 3\n  month      total_return_index price_index\n  &lt;date&gt;                  &lt;dbl&gt;       &lt;dbl&gt;\n1 1928-01-31               1.20        17.6\n2 1928-02-29               1.21        17.3\n3 1928-03-31               1.20        19.3\n4 1928-04-30               1.26        19.8\n5 1928-05-31               1.35        20  \n# ℹ 1,136 more rows\n\n\nFigure 2 shows the dramatic differences in cumulative returns if you only consider price changes, as the S&P 500 Index does, versus total returns with reinvested capital gains. Note that I plot the indexes in log scale, otherwise everything until the last couple of decades would look like a flat line. I believe it is also important to keep the differences between price and performance indexes in mind whenever you compare equity indexes across countries. For instance, the DAX is a performance index by default and should never be compared with the S&P 500 price index.\n\nsp500_monthly |&gt;\n  select(month, \n         `Price Index` = price_index, \n         `Total Return Index` = total_return_index) |&gt;\n  pivot_longer(cols = -month) |&gt;\n  group_by(name) |&gt;\n  arrange(month) |&gt;\n  mutate(value = value / value[1] * 100) |&gt;\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_log10(labels = comma) +\n  scale_x_date(expand = c(0, 0), date_breaks = \"10 years\", date_labels = \"%Y\") + \n  labs(x = NULL, y = NULL, color = NULL,\n       title = \"S&P 500 Price index and Total Return index since 1928\",\n       subtitle = glue(\"Both indexes are normalized to 100 at {min(sp500_monthly$month)}\"))\n\n\n\n\n\n\n\nFigure 2: Using total return data yields dramatically higher cumulative returns over a few decades."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html",
    "href": "blog/crsp-v2-update/index.html",
    "title": "CRSP 2.0 Update",
    "section": "",
    "text": "With commit 6acb50b, Tidy Finance has transitioned to a new version of the CRSP tables distributed by WRDS. In this blog post, we will review the changes that affect our routines."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html#overview-crsp-and-wrds",
    "href": "blog/crsp-v2-update/index.html#overview-crsp-and-wrds",
    "title": "CRSP 2.0 Update",
    "section": "Overview: CRSP and WRDS",
    "text": "Overview: CRSP and WRDS\nTidy Finance shows how to download the most used data in empirical research in finance from the Wharton Research Data Services (WRDS). A main component of this data is the stock return history provided by the Center for Research in Security Prices (CRSP). CRSP is the de-facto gold standard for stock return data covering historical returns from the 1920s until now.\nYou can read all about how to connect to WRDS and download the CRSP data in our chapter WRDS, CRSP, and Compustat in the R version and the Python version."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html#crsp-format-2.0-ciz",
    "href": "blog/crsp-v2-update/index.html#crsp-format-2.0-ciz",
    "title": "CRSP 2.0 Update",
    "section": "CRSP format 2.0 (CIZ)",
    "text": "CRSP format 2.0 (CIZ)\nIn 2022, CRSP rolled out a new format for its data, which is called CRSP’s Stock and Indexes Flat File Format 2.0 (CIZ). You can find detailed information in the official CRSP documentation. This new format is now also available via WRDS. So, what is new in the second version? The most notable items from CRSP’s announcement include:\n\nThey renamed many variables in the daily and monthly data. Not all of these changes make it easier to understand what the variables are, and some names are hard to read.\nThey added additional variables that are sometimes redundant but make querying simpler (e.g., you do not have to merge different data to perform common tasks).\nDelisting returns are now included in the main return time series.\nThey introduce issuer-level data, which ensures unique observations for each issuer (e.g., industry codes).\nThey reworked some flag items by giving them alphanumeric values instead of the previous numeric codes. Moreover, these codes are now also documented in metadata files to facilitate access.\n\nMany of these changes mean that you will not be able to simply use the new data with your existing code. Do not worry. We have you covered and will show you how to use the new data in your previous routines easily.\nYou can access the new data via WRDS using the commands below. You can find more information about the changes at WRDS in SIZ to CIZ Overview and FAQ. For the main data, WRDS basically just added a “_v2” postfix. However, you also see we need fewer tables, as we do not load information on delisting returns separately.\n\nmsf_db &lt;- tbl(wrds, in_schema(\"crsp\", \"msf_v2\"))\nstksecurityinfohist_db &lt;- tbl(wrds, in_schema(\"crsp\", \"stksecurityinfohist\"))\n\nWe do not repost the new routines for downloading the data in this blog. Instead, we ask you to check out the new code in the respective chapter on WRDS, CRSP, and Compustat in the R version and the Python version."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html#a-small-glimpse-at-the-differences",
    "href": "blog/crsp-v2-update/index.html#a-small-glimpse-at-the-differences",
    "title": "CRSP 2.0 Update",
    "section": "A small glimpse at the differences",
    "text": "A small glimpse at the differences\nTo take a look at the new table and compare it to the old one, we use the new tidyfinance R package (see our recent blog post on the initial release here) to download the data. If you do not have the package yet, please install it from CRAN using install.packages(\"tidyfinance\"). For ease of use, we also load the full tidyverse as in all our chapters of the R book.\n\nlibrary(tidyfinance)\nlibrary(tidyverse)\n\nThen, we can download the two data formats using the function download_data_wrds_crsp().\n\ncrsp_v1 &lt;- download_data_wrds_crsp(\"crsp_monthly\", \n                                    version = \"v1\",\n                                    start_date = \"1970-01-01\",\n                                    end_date = \"2022-12-01\")\n\ncrsp_v2 &lt;- download_data_wrds_crsp(\"crsp_monthly\", \n                                    version = \"v2\",\n                                    start_date = \"1970-01-01\",\n                                    end_date = \"2022-12-01\")\n\nNow, do we find any significant differences between the data? Let us look at the number of observations identified by permno and date.\n\nnrow(crsp_v2)\n\n[1] 3105707\n\nnrow(crsp_v1)\n\n[1] 3103640\n\n\nWe see that the new data contains more rows. However, we are not sure where these observations come from or where we lost them in the old data. Let us see if the observations are actually matched for the identifiers with a anti_join().\n\nanti_join(crsp_v2, crsp_v1,\n          by = join_by(\"permno\", \"date\")) |&gt; \n  nrow()\n\n[1] 2650\n\n\nWe find that roughly 4,000 observations are not matched. However, this is a very minor difference.\nNext, we check if the monthly returns are all the same. We compare the new returns to the adjusted returns from version 1.0 to account for delisting in both versions. In fact, these returns should be slightly different, as CRSP states that they have updated the way to compound distributions. They also state this change in compounding as an example, among other things. However, they do not yet provide more information about these other things.\n\ninner_join(crsp_v2 |&gt; \n             select(permno, date, ret_new = ret_excess), \n           crsp_v1 |&gt; \n             select(permno, date, ret_old = ret_excess),\n          by = join_by(\"permno\", \"date\")) |&gt; \n  mutate(ret_differences = !near(ret_new, ret_old, tol = 10^-6)) |&gt; \n  summarize(share_all_ret_differences = sum(ret_differences, na.rm = TRUE) / n())\n\n# A tibble: 1 × 1\n  share_all_ret_differences\n                      &lt;dbl&gt;\n1                     0.123\n\n\nIndeed, we see that not all returns are exactly matched."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html#can-we-keep-using-the-old-crsp-data",
    "href": "blog/crsp-v2-update/index.html#can-we-keep-using-the-old-crsp-data",
    "title": "CRSP 2.0 Update",
    "section": "Can we keep using the old CRSP data?",
    "text": "Can we keep using the old CRSP data?\nUnfortunately, the old format (also called 1.0 (SIZ)) will not receive updates after the end of the year 2024. Nevertheless, WRDS said that the old data will remain in its current place, so we can continue replicating studies with their original data (ignoring changes to the actual data points). Thus, we have included the option to download the legacy data in our tidyfinance R package."
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html",
    "href": "blog/r-tidyfinance-0-1-0/index.html",
    "title": "tidyfinance 0.1.0",
    "section": "",
    "text": "We are happy to annouce the initial release of the tidyfinance R package on CRAN. The package contains a set of helper functions for empirical research in financial economics, addressing a variety of topics covered in Tidy Finance with R (TFWR). We designed the package to provide easy shortcuts for the applications that we discuss in the book. If you want to inspect the details of the package or propose new features, feel free to visit the package repository on Github.\nIn this blog post, we demonstrate the features of the initial release. We decided to focus on functions that allow you to download the data that we use in TFWR."
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#install-the-package",
    "href": "blog/r-tidyfinance-0-1-0/index.html#install-the-package",
    "title": "tidyfinance 0.1.0",
    "section": "Install the package",
    "text": "Install the package\nYou can install the released version of tidyfinance from CRAN via:\n\ninstall.packages(\"tidyfinance\")\n\nYou can install the development version of tidyfinance from GitHub using the pak package:\n\npak::pak(\"tidy-finance/r-tidyfinance\")"
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#download-data",
    "href": "blog/r-tidyfinance-0-1-0/index.html#download-data",
    "title": "tidyfinance 0.1.0",
    "section": "Download data",
    "text": "Download data\nLet’s start by loading the package\n\nlibrary(tidyfinance)\n\nThe main function is download_data(type, start_date, end_date) with supported type:\n\nlist_supported_types()\n\n# A tibble: 20 × 3\n  type                  dataset_name                   domain  \n  &lt;chr&gt;                 &lt;chr&gt;                          &lt;chr&gt;   \n1 factors_q5_daily      q5_factors_daily_2022.csv      Global Q\n2 factors_q5_weekly     q5_factors_weekly_2022.csv     Global Q\n3 factors_q5_weekly_w2w q5_factors_weekly_w2w_2022.csv Global Q\n4 factors_q5_monthly    q5_factors_monthly_2022.csv    Global Q\n5 factors_q5_quarterly  q5_factors_quarterly_2022.csv  Global Q\n# ℹ 15 more rows\n\n\nSo, for instance, if you want to download monthly Fama-French Three-Factor data, you can call:\n\ndownload_data(\"factors_ff3_monthly\", \"2020-01-01\", \"2020-12-31\")\n\n# A tibble: 12 × 5\n  date       risk_free mkt_excess     smb     hml\n  &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 2020-01-01    0.0013    -0.0011 -0.0311 -0.0625\n2 2020-02-01    0.0012    -0.0813  0.0107 -0.0381\n3 2020-03-01    0.0013    -0.134  -0.0483 -0.139 \n4 2020-04-01    0          0.136   0.0245 -0.0133\n5 2020-05-01    0.0001     0.0558  0.0247 -0.0488\n# ℹ 7 more rows\n\n\nUnder the hood, the function uses the frenchdata package (see its documentation here) and applies some cleaning steps, as in TFWR. If you haven’t installed frenchdata yet, you’ll get prompted to install it first before you can download this specific data type.\nYou can also access q-Factor data in this way, by calling:\n\ndownload_data(\"factors_q5_daily\", \"2020-01-01\", \"2020-12-31\")\n\n# A tibble: 253 × 7\n  date       risk_free mkt_excess       me       ia      roe       eg\n  &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2020-01-02  0.000055    0.00863 -0.0112  -0.00171  6.84e-4  3.41e-3\n2 2020-01-03  0.000055   -0.00673  0.00234 -0.00193 -1.55e-3  6.83e-4\n3 2020-01-06  0.000055    0.00360 -0.00360 -0.00409 -4.78e-3  6.11e-4\n4 2020-01-07  0.000055   -0.00192 -0.00139 -0.00322 -5.12e-3 -2.74e-3\n5 2020-01-08  0.000055    0.00467 -0.00108 -0.00121  4.56e-3  6.14e-3\n# ℹ 248 more rows\n\n\nTo ensure that we can extend the functionality of the download functions for specific types, we also provide domain-specific download functions. The download_data(\"factors_ff3_monthly\") actually calls download_data_factors(\"factors_ff3_monthly\"), which in turn calls download_data_factors_ff(\"factors_ff3_monthly\"). Why did we decide to have these nested function approach?\nSuppose that the q-Factor data changes its URL path and our original function does not work anymore. In this case, you can replace the default url value in download_data_factors_q(type, start_date, end_date, url) to apply the usual cleaning steps.\nThis feature becomes more apparent for other data sources such as wrds_crsp_monthly. Note that you need to have valid WRDS credentials and need to set them correctly (check ?get_wrds_connection and WRDS, CRSP, and Compustat in TFWR). If you want to download the standard monthly CRSP data, you can call:\n\ndownload_data(\"wrds_crsp_monthly\", \"2020-01-01\", \"2020-12-31\")\n\nIf you want to add further columns, you can add them via ... to download_data_wrds_crsp(), for instance:\n\ndownload_data_wrds_crsp(\"wrds_crsp_monthly\", \"2020-01-01\", \"2020-12-31\", mthvol)\n\nNote that the function downloads CRSP v2 as default, as we do in our book since February 2024. If you want to download the old version of CRSP before the update, you can use the version = v1 parameter in download_data_wrds_crsp() .\nAs another example, you can do the same for Compustat:\n\ndownload_data_wrds_compustat(\"wrds_compustat_annual\", \"2000-01-01\", \"2020-12-31\", acoxar, amc, aldo)\n\nCheck out the list of supported types and the corresponding download functions for more information on the respective customization options. We decided to provide limited functionality for the initial release on purpose and rather respond to community request than overengineer the package from the start."
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#browse-content-from-tfwr",
    "href": "blog/r-tidyfinance-0-1-0/index.html#browse-content-from-tfwr",
    "title": "tidyfinance 0.1.0",
    "section": "Browse content from TFWR",
    "text": "Browse content from TFWR\nWe include functions to check out content from TFWR in your browser. If you want to list all available R chapters, simply call the following function:\n\nlist_tidy_finance_chapters()\n\n [1] \"setting-up-your-environment\"                \n [2] \"introduction-to-tidy-finance\"               \n [3] \"accessing-and-managing-financial-data\"      \n [4] \"wrds-crsp-and-compustat\"                    \n [5] \"trace-and-fisd\"                             \n [6] \"other-data-providers\"                       \n [7] \"beta-estimation\"                            \n [8] \"univariate-portfolio-sorts\"                 \n [9] \"size-sorts-and-p-hacking\"                   \n[10] \"value-and-bivariate-sorts\"                  \n[11] \"replicating-fama-and-french-factors\"        \n[12] \"fama-macbeth-regressions\"                   \n[13] \"fixed-effects-and-clustered-standard-errors\"\n[14] \"difference-in-differences\"                  \n[15] \"factor-selection-via-machine-learning\"      \n[16] \"option-pricing-via-machine-learning\"        \n[17] \"parametric-portfolio-policies\"              \n[18] \"constrained-optimization-and-backtesting\"   \n[19] \"wrds-dummy-data\"                            \n[20] \"cover-and-logo-design\"                      \n[21] \"clean-enhanced-trace-with-r\"                \n[22] \"proofs\"                                     \n[23] \"hex-sticker\"                                \n[24] \"changelog\"                                  \n\n\nThe function returns a character vector containing the names of the chapters available in TFWR. If you want to look at a specific chapter, you can call:\n\nopen_tidy_finance_website(\"beta-estimation\")\n\nThis opens either the specific chapter you requested or the main index page in your default web browser."
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#regression-helpers",
    "href": "blog/r-tidyfinance-0-1-0/index.html#regression-helpers",
    "title": "tidyfinance 0.1.0",
    "section": "Regression helpers",
    "text": "Regression helpers\nWe discuss winsorization in TFWR, so we figured providing this function could be useful:\n\nlibrary(tibble)\nlibrary(dplyr)\n\nset.seed(123)\ndata &lt;- tibble(x = rnorm(100)) |&gt; \n  arrange(x)\n\ndata |&gt; \n  mutate(x_winsorized = winsorize(x, 0.01))\n\n# A tibble: 100 × 2\n      x x_winsorized\n  &lt;dbl&gt;        &lt;dbl&gt;\n1 -2.31        -1.97\n2 -1.97        -1.97\n3 -1.69        -1.69\n4 -1.55        -1.55\n5 -1.27        -1.27\n# ℹ 95 more rows\n\n\nIf you rather want to replace the bottom and top quantiles of your distribution with missing values, then you can use trim()\n\ndata |&gt; \n  mutate(x_trimmed = trim(x, 0.01))\n\n# A tibble: 100 × 2\n      x x_trimmed\n  &lt;dbl&gt;     &lt;dbl&gt;\n1 -2.31     NA   \n2 -1.97     -1.97\n3 -1.69     -1.69\n4 -1.55     -1.55\n5 -1.27     -1.27\n# ℹ 95 more rows\n\n\nWe also discuss the importance of providing summary statistics of your data, so there is also a function for that:\n\ncreate_summary_statistics(data, x, detail = TRUE)\n\n# A tibble: 1 × 15\n  variable     n   mean    sd   min   q01   q05   q10    q25    q50\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 x          100 0.0904 0.913 -2.31 -1.97 -1.27 -1.07 -0.494 0.0618\n# ℹ 5 more variables: q75 &lt;dbl&gt;, q90 &lt;dbl&gt;, q95 &lt;dbl&gt;, q99 &lt;dbl&gt;,\n#   max &lt;dbl&gt;"
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#experimental-functions",
    "href": "blog/r-tidyfinance-0-1-0/index.html#experimental-functions",
    "title": "tidyfinance 0.1.0",
    "section": "Experimental functions",
    "text": "Experimental functions\nWe have two more experimental functions in the sense that it is unclear in which direction they might evolve. First you can assign portfolios based on a sorting variable using assign_portfolio():\n\ndata &lt;- tibble(\n  id = 1:100,\n  exchange = sample(c(\"NYSE\", \"NASDAQ\"), 100, replace = TRUE),\n  market_cap = runif(100, 1e6, 1e9)\n)\n\ndata |&gt; \n  mutate(\n    portfolio = assign_portfolio(\n      pick(everything()), \"market_cap\", n_portfolios = 5, exchanges = c(\"NYSE\"))\n  )\n\n# A tibble: 100 × 4\n     id exchange market_cap portfolio\n  &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;int&gt;\n1     1 NASDAQ   784790691.         4\n2     2 NASDAQ    10420475.         1\n3     3 NASDAQ   779286817.         4\n4     4 NYSE     729661261.         4\n5     5 NASDAQ   630501721.         3\n# ℹ 95 more rows\n\n\nSecond, you can estimate the coefficients of a linear model specified by one or more independent variable using estimate_model():\n\ndata &lt;- tibble(\n  ret_excess = rnorm(100),\n  mkt_excess = rnorm(100),\n  smb = rnorm(100),\n  hml = rnorm(100)\n)\n\nestimate_model(data, \"ret_excess ~ mkt_excess + smb + hml\")\n\n  mkt_excess     smb    hml\n1    -0.0399 -0.0287 0.0207"
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#concluding-remarks",
    "href": "blog/r-tidyfinance-0-1-0/index.html#concluding-remarks",
    "title": "tidyfinance 0.1.0",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nWe are curious to learn in which direction we should extend the package, so please consider opening an issue in the package repository. For instance, we could support more data sources, add more parameters to the download_* family of functions, or we could put more emphasis on the generality of portfolio assignment or other modeling functions."
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html",
    "href": "blog/using-duckdb-with-wrds/index.html",
    "title": "Using DuckDB with WRDS Data",
    "section": "",
    "text": "In this short note, I show how one can use DuckDB with WRDS data stored in the PostgreSQL database provided by WRDS. I then use some simple benchmarks to show how DuckDB offers a powerful, fast analytical engine for researchers in accounting and finance.\nTo make the analysis concrete, I focus on data used in the excellent recent book “Tidy Finance with R”. Essentially, I combine data from CRSP’s daily stock return file (crsp.dsf) with data on factor returns from Ken French’s website and then run an aggregate query."
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#fama-french-factor-returns",
    "href": "blog/using-duckdb-with-wrds/index.html#fama-french-factor-returns",
    "title": "Using DuckDB with WRDS Data",
    "section": "Fama-French factor returns",
    "text": "Fama-French factor returns\nWe use the same start_date and end_date values used in “Tidy Finance with R” and the code below also is adapted from that book. However, we use the copy_to() function from dplyr to save the table to our database.\n\nstart_date &lt;- ymd(\"1960-01-01\")\nend_date &lt;- ymd(\"2021-12-31\")\n\nfactors_ff_daily_raw &lt;- \n  download_french_data(\"Fama/French 3 Factors [Daily]\")\n\nNew names:\n• `` -&gt; `...1`\n\nfactors_ff_daily &lt;- \n  factors_ff_daily_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = ymd(date),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date) |&gt;\n  copy_to(tidy_finance,\n          df = _,\n          name = \"factors_ff_daily\",\n          temporary = FALSE,\n          overwrite = TRUE)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#getting-daily-returns-from-wrds",
    "href": "blog/using-duckdb-with-wrds/index.html#getting-daily-returns-from-wrds",
    "title": "Using DuckDB with WRDS Data",
    "section": "Getting daily returns from WRDS",
    "text": "Getting daily returns from WRDS\nNext, I specify the connection details as follows. I recommend using environment variables (e.g., set using Sys.setenv()), as this facilitates sharing code with others. You should not include this chunk of code in your code, rather run it before executing your other code. In addition to setting these environment variables, you may want to set PGPASSWORD too. (Hopefully it is obvious that your should use your WRDS ID and password, not mine.)\n\nSys.setenv(PGHOST = \"wrds-pgdata.wharton.upenn.edu\",\n           PGPORT = 9737L,\n           PGDATABASE = \"wrds\",\n           PGUSER = Sys.getenv(\"WRDS_USER\"),\n           PGPASSWORD = Sys.getenv(\"WRDS_PASSWORD\"))\n\nThird, we connect to the CRSP daily stock file in the WRDS PostgreSQL database.\n\npg &lt;- dbConnect(RPostgres::Postgres())\ndsf_db &lt;- tbl(pg, Id(schema = \"crsp\", table = \"dsf\"))\n\nAs we can see, we have access to data in crsp.dsf.\n\ndsf_db\n\n# Source:   table&lt;\"crsp\".\"dsf\"&gt; [?? x 20]\n# Database: postgres  [pweiss@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n  cusip    permno permco issuno hexcd hsiccd date       bidlo askhi\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 68391610  10000   7952  10396     3   3990 1986-01-07  2.38  2.75\n2 68391610  10000   7952  10396     3   3990 1986-01-08  2.38  2.62\n3 68391610  10000   7952  10396     3   3990 1986-01-09  2.38  2.62\n4 68391610  10000   7952  10396     3   3990 1986-01-10  2.38  2.62\n5 68391610  10000   7952  10396     3   3990 1986-01-13  2.5   2.75\n# ℹ more rows\n# ℹ 11 more variables: prc &lt;dbl&gt;, vol &lt;dbl&gt;, ret &lt;dbl&gt;, bid &lt;dbl&gt;,\n#   ask &lt;dbl&gt;, shrout &lt;dbl&gt;, cfacpr &lt;dbl&gt;, cfacshr &lt;dbl&gt;,\n#   openprc &lt;dbl&gt;, numtrd &lt;dbl&gt;, retx &lt;dbl&gt;\n\n\nBefore proceeding with our first benchmark, we will make a version of system.time() that works with assignment.2\n\nsystem_time &lt;- function(x) {\n  print(system.time(x))\n  x\n}\n\nThe following code is adapted from the Tidy Finance code here. But the original code is much more complicated and takes slightly longer to run.3\n\nrs &lt;- dbExecute(tidy_finance, \"DROP TABLE IF EXISTS crsp_daily\")\n\ncrsp_daily &lt;- \n  dsf_db |&gt;\n  filter(between(date, start_date, end_date),\n         !is.na(ret)) |&gt;\n  select(permno, date, ret) |&gt;\n  mutate(month = as.Date(floor_date(date, \"month\"))) |&gt;\n  copy_to(tidy_finance, df = _, name = \"dsf_temp\") |&gt;\n  left_join(factors_ff_daily |&gt;\n              select(date, rf), by = \"date\") |&gt;\n  mutate(\n    ret_excess = ret - rf,\n    ret_excess = pmax(ret_excess, -1, na.rm = TRUE)\n  ) |&gt;\n  select(permno, date, month, ret_excess) |&gt;\n  compute(name = \"crsp_daily\", temporary = FALSE, overwrite = TRUE) |&gt;\n  system_time()\n\n   user  system elapsed \n  56.53    2.64  256.12"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#saving-data-to-sqlite",
    "href": "blog/using-duckdb-with-wrds/index.html#saving-data-to-sqlite",
    "title": "Using DuckDB with WRDS Data",
    "section": "Saving data to SQLite",
    "text": "Saving data to SQLite\nIf you have been working through “Tidy Finance”, you may already have an SQLite database containing crsp_daily. If not, we can easily create one now and copy the table from our DuckDB database to SQLite.\n\ntidy_finance_sqlite &lt;- dbConnect(\n  RSQLite::SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\n\ncopy_to(tidy_finance_sqlite,\n        crsp_daily,\n        name = \"crsp_daily\",\n        overwrite = TRUE,\n        temporary = FALSE)\n\ndbExecute(tidy_finance_sqlite, \"VACUUM\")\n\nWe can also save the data to a parquet file.\n\ndbExecute(tidy_finance, \n          \"COPY crsp_daily TO 'data/crsp_daily.parquet' \n          (FORMAT 'PARQUET')\")\n\nHaving created our two databases, we disconnect from them. This mimics the most common “write-once, read-many” pattern for using databases.\n\ndbDisconnect(tidy_finance_sqlite)\ndbDisconnect(tidy_finance, shutdown = TRUE)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#dplyr",
    "href": "blog/using-duckdb-with-wrds/index.html#dplyr",
    "title": "Using DuckDB with WRDS Data",
    "section": "dplyr",
    "text": "dplyr\nWe first need to load the data into memory.\n\ntidy_finance &lt;- dbConnect(\n  RSQLite::SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_daily &lt;- tbl(tidy_finance, \"crsp_daily\")\n\nWhat takes most time is simply loading nearly 2GB of data into memory.\n\ncrsp_daily_local &lt;- \n  crsp_daily |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n 208.87    2.75  280.47 \n\n\nOnce the data are in memory, it is relatively quick to run a summary query.\n\ncrsp_daily_local |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt; \n  system_time()\n\n   user  system elapsed \n   4.79    1.18    8.08 \n\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1960-01-01 -0.00213 \n2 1960-02-01  0.000325\n3 1960-03-01 -0.00115 \n4 1960-04-01 -0.00106 \n5 1960-05-01  0.00114 \n# ℹ 739 more rows\n\n\n\nrm(crsp_daily_local)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-sqlite",
    "href": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-sqlite",
    "title": "Using DuckDB with WRDS Data",
    "section": "dbplyr with SQLite",
    "text": "dbplyr with SQLite\nThings are faster with SQLite, though there’s no obvious way to split the time between reading the data and performing the aggregation. Note that we have a collect() at the end. This will not take a noticeable amount of time, but seems to be a reasonable step if our plan is to analyse the aggregated data in R.\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n   38.6    11.0    63.5 \n\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1960-01-01 -0.00213 \n2 1960-02-01  0.000325\n3 1960-03-01 -0.00115 \n4 1960-04-01 -0.00106 \n5 1960-05-01  0.00114 \n# ℹ 739 more rows\n\n\n\ndbDisconnect(tidy_finance)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-duckdb",
    "href": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-duckdb",
    "title": "Using DuckDB with WRDS Data",
    "section": "dbplyr with DuckDB",
    "text": "dbplyr with DuckDB\nLet’s consider DuckDB. Note that we are only reading the data here, so we set read_only = TRUE in connecting to the database. Apart from the connection, there is no difference between the code here and the code above using SQLite.\n\ntidy_finance &lt;- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = TRUE)\n\n\ncrsp_daily &lt;- tbl(tidy_finance, \"crsp_daily\")\n\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n   0.51    0.11    0.50 \n\n\n# A tibble: 744 × 2\n  month           ret\n  &lt;date&gt;        &lt;dbl&gt;\n1 1991-12-01 0.00237 \n2 1992-05-01 0.000740\n3 1993-01-01 0.00413 \n4 1993-05-01 0.00265 \n5 2010-10-01 0.00198 \n# ℹ 739 more rows\n\n\nHaving done our benchmarks, we can take a quick peek at the data.\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  arrange(month) |&gt;\n  collect()\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1960-01-01 -0.00213 \n2 1960-02-01  0.000325\n3 1960-03-01 -0.00115 \n4 1960-04-01 -0.00106 \n5 1960-05-01  0.00114 \n# ℹ 739 more rows\n\n\nFinally, we disconnect from the database. This will happen automatically if we close R, etc., and is less important if we have read_only = TRUE (so there is no lock on the file), but we keep things tidy here.\n\ndbDisconnect(tidy_finance, shutdown = TRUE)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-duckdb-and-a-parquet-file",
    "href": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-duckdb-and-a-parquet-file",
    "title": "Using DuckDB with WRDS Data",
    "section": "dbplyr with DuckDB and a parquet file",
    "text": "dbplyr with DuckDB and a parquet file\nLet’s do the benchmark using the parquet data.\n\ndb &lt;- dbConnect(duckdb::duckdb())\ncrsp_daily &lt;- tbl(db, \"read_parquet('data/crsp_daily.parquet')\")\n\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n   1.63    0.73    0.51 \n\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1970-01-01 -0.00210 \n2 1971-05-01 -0.00262 \n3 1973-02-01 -0.00415 \n4 1973-11-01 -0.00927 \n5 1967-05-01 -0.000372\n# ℹ 739 more rows\n\n\n\ndbDisconnect(db, shutdown = TRUE)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#the-arrow-library-with-a-parquet-file",
    "href": "blog/using-duckdb-with-wrds/index.html#the-arrow-library-with-a-parquet-file",
    "title": "Using DuckDB with WRDS Data",
    "section": "The arrow library with a parquet file",
    "text": "The arrow library with a parquet file\nLet’s do one more benchmark using the parquet data with the arrow library.\n\ncrsp_daily &lt;- open_dataset(\"data/crsp_daily.parquet\")\n\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n   0.25    0.01    0.76 \n\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1989-01-01  0.00285 \n2 1986-06-01  0.000194\n3 1986-07-01 -0.00352 \n4 1986-08-01  0.00115 \n5 1986-09-01 -0.00279 \n# ℹ 739 more rows"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#footnotes",
    "href": "blog/using-duckdb-with-wrds/index.html#footnotes",
    "title": "Using DuckDB with WRDS Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is how it’s done in “R for Data Science”. I have read comments by Hadley Wickham that this is the right way to do it, but I can’t find those comments.↩︎\nIf we put system.time() at the end of this pipe, then crsp_daily would hold the value returned by that function rather than the result of the pipeline preceding it. At first, the system_time() function may seem like magic, but Hadley Wickham explained to me that this works because of lazy evaluation, which is discussed in “Advanced R” here. Essentially, x is evaluated just once—inside system.time()—and its value is returned in the next line.↩︎\nPerformance will vary according to the speed of your connection to WRDS. Note that this query does temporarily use a significant amount of RAM on my machine, it is not clear that DuckDB will use as much RAM if this is more constrained. If necessary, you can run (say) dbExecute(tidy_finance, \"SET memory_limit='1GB'\") to constrain DuckDB’s memory usage; doing so has little impact on performance for this query.↩︎"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html",
    "href": "blog/nse-portfolio-sorts/index.html",
    "title": "Non-standard errors in portfolio sorts",
    "section": "",
    "text": "Welcome to our latest blog post where we delve into the intriguing world of non-standard errors1, a crucial aspect of academic research, which also is addressed in financial economics. These non-standard errors often stem from the methodological decisions we make, adding an extra dimension of uncertainty to the estimates we report. I had the pleasure of working with Stefan Voigt on a contribution to Menkveld et al. (2023), which was an exciting opportunity to shape the first discussions on non-standard errors.\nOne of the goals of Tidy Finance has always been focused on promoting reproducibility in finance research. We began this endeavor by introducing a chapter on Size sorts and p-hacking, which initiated some analysis of portfolio sorting choices. Recently, my fellow authors, Dominik Walter and Rüdiger Weber, and I published an update to our working paper, Non-Standard Errors in Portfolio Sorts2. This paper delves into the impact of methodological choices on return differentials derived from portfolio sorts. Our conclusion? We need to accept non-standard errors in empirical asset pricing. By reporting the entire range of return differentials, we simultaneously deepen our economic understanding of return anomalies and improve trust.\nThis blog post will guide you through conducting portfolio sorts which keep non-standard errors in perspective. We explore the multitude of possible decisions that can impact return differentials, allowing us to estimate a premium’s distribution rather than a single return differential. The code is inspired by Tidy Finance with R and the replication code for Walter, Weber, and Weiss (2023), which can be found in this Github repository. By the end of this post, you will have the knowledge to sort portfolios based on asset growth in nearly 70,000 different ways.\nWhile this post is detailed, it is not overly complex. However, if you are new to R or portfolio sorts, I recommend first checking out our chapter on Size sorts and p-hacking. Due to the length constraints, we will be skipping over certain details related to implementation and the economic background (you can find these in the WWW paper). If there are particular aspects you would like to delve into further, please feel free to reach out."
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#merge-data",
    "href": "blog/nse-portfolio-sorts/index.html#merge-data",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Merge data",
    "text": "Merge data\nOne key decision node is the sorting variable lag. However, merging data is an expensive operation, and doing it repeatedly is unnecessary. Hence, we merge the data in the three possible lag configurations and store them as separate tibbles. Thereby, we can later reference the correct table instead of merging the desired output.\nFirst, let us consider the Fama-French (FF) lag. Here, we consider accounting information published in year \\(t-1\\) starting from July of year \\(t\\). That is, we use the accounting information published 6 to 18 months ago. We first match the accounting data to the stock market data before we fill in the missing observations. A few pitfalls exist when using the fill()-function. First, one might easily forget to order and group the data. Second, the function does not care how outdated the information becomes. In principle, you can end up with data that is decades old. Therefore, we ensure that these filled data points are not older than 12 months. This is achieved with the new variable sv_age_check, which serves as a filter for outdated data. Finally, notice that this code provides much flexibility. All variables with prefixes sv_ and filter_ get filled. So you can easily adapt my code to your needs.\n\ndata_FF &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month) |&gt;\n  left_join(\n    compustat |&gt;\n      mutate(\n        sorting_date = floor_date(datadate, \"year\") %m+% months(18),\n        sv_age_check = sorting_date\n      ) |&gt;\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n# Fill variables and ensure timeliness of data\ndata_FF &lt;- data_FF |&gt;\n  arrange(permno, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |&gt;\n  ungroup() |&gt;\n  filter(sv_age_check &gt; month %m-% months(12)) |&gt;\n  select(-sv_age_check, -sorting_date, -gvkey)\n\nNext, we create the basis with lags of three and six months. The process is exactly the same as above for the FF lag, but without the floor_date() as we apply a constant lag to all observations. Again, we make sure that after the call to fill() our information does not become too old.\n\n# 3 months\n## Merge data\ndata_3m &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month) |&gt;\n  left_join(\n    compustat |&gt;\n      mutate(\n        sorting_date = floor_date(datadate, \"month\") %m+% months(3),\n        sv_age_check = sorting_date\n      ) |&gt;\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n## Fill variables and ensure timeliness of data\ndata_3m &lt;- data_3m |&gt;\n  arrange(permno, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |&gt;\n  ungroup() |&gt;\n  filter(sv_age_check &gt; month %m-% months(12)) |&gt;\n  select(-sv_age_check, -sorting_date, -gvkey)\n\n# 6 months\n## Merge data\ndata_6m &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month) |&gt;\n  left_join(\n    compustat |&gt;\n      mutate(\n        sorting_date = floor_date(datadate, \"month\") %m+% months(6),\n        sv_age_check = sorting_date\n      ) |&gt;\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n## Fill variables and ensure timeliness of data\ndata_6m &lt;- data_6m |&gt;\n  arrange(permno, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |&gt;\n  ungroup() |&gt;\n  filter(sv_age_check &gt; month %m-% months(12)) |&gt;\n  select(-sv_age_check, -sorting_date, -gvkey)"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#functions",
    "href": "blog/nse-portfolio-sorts/index.html#functions",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Functions",
    "text": "Functions\nWe write functions that complete specific tasks and then combine them to generate the desired output. Breaking it up into smaller steps makes the whole process more tractable and easier to test.\n\nSelect the sample\nThe first function gets the name handle_data() because it is intended to select the sample according to the sample construction choices. The function first selects the data based on the desired sorting variable lag (specified in sv_lag). Then, we apply the various filters we discussed above. As you see, this is relatively simple, but it already covers our sample construction nodes.\n\nhandle_data &lt;- function(include_financials, include_utilities,\n                        drop_smallNYSE_at, drop_price_at, drop_stock_age_at,\n                        drop_earnings, drop_bookequity,\n                        sv_lag) {\n  # Select dataset\n  if (sv_lag == \"FF\") data_all &lt;- data_FF\n  if (sv_lag == \"3m\") data_all &lt;- data_3m\n  if (sv_lag == \"6m\") data_all &lt;- data_6m\n\n  # Size filter based on NYSE percentile\n  if (drop_smallNYSE_at &gt; 0) {\n    data_all &lt;- data_all |&gt;\n      group_by(month) |&gt;\n      mutate(NYSE_breakpoint = quantile(\n        mktcap_lag[exchange == \"NYSE\"],\n        drop_smallNYSE_at\n      )) |&gt;\n      ungroup() |&gt;\n      filter(mktcap_lag &gt;= NYSE_breakpoint) |&gt;\n      select(-NYSE_breakpoint)\n  }\n\n  # Exclude industries\n  data_all &lt;- data_all |&gt;\n    filter(if (include_financials) TRUE else !grepl(\"Finance\", industry)) |&gt;\n    filter(if (include_utilities) TRUE else !grepl(\"Utilities\", industry))\n\n  # Book equity filter\n  if (drop_bookequity) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_be &gt; 0)\n  }\n\n  # Earnings filter\n  if (drop_earnings) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_earnings &gt; 0)\n  }\n\n  # Stock age filter\n  if (drop_stock_age_at &gt; 0) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_stock_age &gt;= drop_stock_age_at)\n  }\n\n  # Price filter\n  if (drop_price_at &gt; 0) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_price &gt;= drop_price_at)\n  }\n\n  # Define ME\n  data_all &lt;- data_all |&gt;\n    mutate(me = mktcap_lag) |&gt;\n    drop_na(me) |&gt;\n    select(-starts_with(\"filter_\"), -industry)\n\n  # Return\n  return(data_all)\n}\n\n\n\nAssign portfolios\nNext, we define a function that assigns portfolios based on the specified sorting variable, the number of portfolios, and the exchanges. The function only works on a single cross-section of data, i.e., it has to be applied to individual months of data. The central part of the function is to compute the \\(n\\) breakpoints based on the exchange filter. Then, findInterval() assigns the respective portfolio number.\nThe function also features two sanity checks. First, it does not assign portfolios if there are too few stocks in the cross-section. Second, sometimes the sorting variable creates scenarios where some portfolios are overpopulated. For example, if the variable in question is bounded from below by 0. In such a case, an unexpectedly large number of firms might end up in the lowest bucket, covering multiple quantiles.\n\nassign_portfolio &lt;- function(data, sorting_variable,\n                             n_portfolios, exchanges) {\n  # Escape small sets (i.e., less than 10 firms per portfolio)\n  if (nrow(data) &lt; n_portfolios * 10) {\n    return(NA)\n  }\n\n  # Compute breakpoints\n  breakpoints &lt;- data |&gt;\n    filter(grepl(exchanges, exchange)) |&gt;\n    pull(all_of(sorting_variable)) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  # Assign portfolios\n  portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull(all_of(sorting_variable)),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n\n  # Check if breakpoints are well defined\n  if (length(unique(breakpoints)) == n_portfolios + 1) {\n    return(portfolios)\n  } else {\n    print(breakpoints)\n    cat(paste0(\n      \"\\n Breakpoint issue! Month \",\n      as.Date(as.numeric(cur_group())),\n      \"\\n\"\n    ))\n    stop()\n  }\n}\n\n\n\nSingle and double sorts\nOur goal is to construct portfolios for single sorts, independent double sorts, and dependent double sorts. Hence, our next three functions do exactly that. The double sorts considered always take a first sort on market equity (the variable me) before sorting on the actual sorting variable.\nLet us start with single sorts. As you see, we group by month as the function assign_portfolio() we wrote above handles one cross-section at a time. The rest of the function just passes the arguments to the portfolio assignment.\n\nsort_single &lt;- function(data, sorting_variable,\n                        exchanges, n_portfolios_main) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(portfolio = assign_portfolio(\n      data = pick(all_of(sorting_variable), exchange),\n      sorting_variable = sorting_variable,\n      n_portfolios = n_portfolios_main,\n      exchanges = exchanges\n    )) |&gt;\n    drop_na(portfolio) |&gt;\n    ungroup()\n}\n\nFor double sorts, things are more interesting. First, we have the issue of independent and dependent double sorts. An independent sort considers the two sorting variables (size and asset growth) independently. In contrast, dependent sorts are, in our case, first sorting on size and within these buckets on asset growth. We group by the secondary portfolio to achieve the dependent sort before generating the main portfolios. Second, we need to generate an overall portfolio of the two sorts - we will see this later.\n\nsort_double_independent &lt;- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(\n      portfolio_secondary = assign_portfolio(\n        data = pick(me, exchange),\n        sorting_variable = \"me\",\n        n_portfolios = n_portfolios_secondary,\n        exchanges = exchanges\n      ),\n      portfolio_main = assign_portfolio(\n        data = pick(all_of(sorting_variable), exchange),\n        sorting_variable = sorting_variable,\n        n_portfolios = n_portfolios_main,\n        exchanges = exchanges\n      ),\n      portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)\n    ) |&gt;\n    drop_na(portfolio_main, portfolio_secondary) |&gt;\n    ungroup()\n}\n\nsort_double_dependent &lt;- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(portfolio_secondary = assign_portfolio(\n      data = pick(me, exchange),\n      sorting_variable = \"me\",\n      n_portfolios = n_portfolios_secondary,\n      exchanges = exchanges\n    )) |&gt;\n    drop_na(portfolio_secondary) |&gt;\n    group_by(month, portfolio_secondary) |&gt;\n    mutate(\n      portfolio_main = assign_portfolio(\n        data = pick(all_of(sorting_variable), exchange),\n        sorting_variable = sorting_variable,\n        n_portfolios = n_portfolios_main,\n        exchanges = exchanges\n      ),\n      portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)\n    ) |&gt;\n    drop_na(portfolio_main) |&gt;\n    ungroup()\n}\n\n\n\nAnnual vs monthly rebalancing\nNow, we still have one decision node to cover: Rebalancing. We can either rebalance annually in July or monthly. To achieve this, we write two more functions - the last functions before finishing up. Let us start with monthly rebalancing because it is much easier. All we need to do is to use the assigned portfolio numbers to generate portfolio returns. Inside the function, we use three if() calls to decide the sorting method. Notice that the double sorts use the simple average for aggregating the extreme portfolios of the size buckets.\n\nrebalance_monthly &lt;- function(data, sorting_variable, sorting_method,\n                              n_portfolios_main, n_portfolios_secondary,\n                              exchanges, value_weighted) {\n  # Single sort\n  if (sorting_method == \"single\") {\n    data_rets &lt;- data |&gt;\n      sort_single(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main\n      ) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        .groups = \"drop\"\n      )\n  }\n\n  # Double independent sort\n  if (sorting_method == \"dbl_ind\") {\n    data_rets &lt;- data |&gt;\n      sort_double_independent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |&gt;\n      group_by(month, portfolio_main) |&gt;\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |&gt;\n      rename(portfolio = portfolio_main)\n  }\n\n  # Double dependent sort\n  if (sorting_method == \"dbl_dep\") {\n    data_rets &lt;- data |&gt;\n      sort_double_dependent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |&gt;\n      group_by(month, portfolio_main) |&gt;\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |&gt;\n      rename(portfolio = portfolio_main)\n  }\n\n  return(data_rets)\n}\n\nNow, let us move to the annual rebalancing. Here, we first assign a portfolio on the data in July based on single or independent/dependent double sorts. Then, we fill the remaining months forward before computing returns. Hence, we need one extra step for each sort.\n\nrebalance_annually &lt;- function(data, sorting_variable, sorting_method,\n                               n_portfolios_main, n_portfolios_secondary,\n                               exchanges, value_weighted) {\n  data_sorting &lt;- data |&gt;\n    filter(month(month) == 7) |&gt;\n    group_by(month)\n\n  # Single sort\n  if (sorting_method == \"single\") {\n    # Assign portfolios\n    data_sorting &lt;- data_sorting |&gt;\n      sort_single(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main\n      ) |&gt;\n      select(permno, month, portfolio) |&gt;\n      mutate(sorting_month = month)\n  }\n\n  # Double independent sort\n  if (sorting_method == \"dbl_ind\") {\n    # Assign portfolios\n    data_sorting &lt;- data_sorting |&gt;\n      sort_double_independent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |&gt;\n      mutate(sorting_month = month)\n  }\n\n  # Double dependent sort\n  if (sorting_method == \"dbl_dep\") {\n    # Assign portfolios\n    data_sorting &lt;- data_sorting |&gt;\n      sort_double_dependent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |&gt;\n      mutate(sorting_month = month)\n  }\n\n  # Compute portfolio return\n  if (sorting_method == \"single\") {\n    data |&gt;\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |&gt;\n      group_by(permno) |&gt;\n      arrange(month) |&gt;\n      fill(portfolio, sorting_month) |&gt;\n      filter(sorting_month &gt;= month %m-% months(12)) |&gt;\n      drop_na(portfolio) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        .groups = \"drop\"\n      )\n  } else {\n    data |&gt;\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |&gt;\n      group_by(permno) |&gt;\n      arrange(month) |&gt;\n      fill(portfolio_main, portfolio_secondary, portfolio, sorting_month) |&gt;\n      filter(sorting_month &gt;= month %m-% months(12)) |&gt;\n      drop_na(portfolio_main, portfolio_secondary) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |&gt;\n      group_by(month, portfolio_main) |&gt;\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |&gt;\n      rename(portfolio = portfolio_main)\n  }\n}\n\n\n\nCombining the functions\nNow, we have everything to compute our 69,120 portfolio sorts for asset growth to understand the variation our decisions induce. To achieve this, our function considers all choices as arguments and passes them to the sample selection and portfolio construction functions.\nFinally, the function computes the return differential for each month. Since we are only interested in the mean here, we simply take the mean of these time series and call it our premium estimate.\n\nexecute_sorts &lt;- function(sorting_variable, drop_smallNYSE_at,\n                          include_financials, include_utilities,\n                          drop_bookequity, drop_earnings,\n                          drop_stock_age_at, drop_price_at,\n                          sv_lag, formation_time,\n                          n_portfolios_main, sorting_method,\n                          n_portfolios_secondary, exchanges,\n                          value_weighted) {\n  # Select data\n  data_sorts &lt;- handle_data(\n    include_financials = include_financials,\n    include_utilities = include_utilities,\n    drop_smallNYSE_at = drop_smallNYSE_at,\n    drop_price_at = drop_price_at,\n    drop_stock_age_at = drop_stock_age_at,\n    drop_earnings = drop_earnings,\n    drop_bookequity = drop_bookequity,\n    sv_lag = sv_lag\n  )\n\n  # Rebalancing\n  ## Monthly\n  if (formation_time == \"monthly\") {\n    data_return &lt;- rebalance_monthly(\n      data = data_sorts,\n      sorting_variable = sorting_variable,\n      sorting_method = sorting_method,\n      n_portfolios_main = n_portfolios_main,\n      n_portfolios_secondary = n_portfolios_secondary,\n      exchanges = exchanges,\n      value_weighted = value_weighted\n    )\n  }\n\n  ## Annually\n  if (formation_time == \"FF\") {\n    data_return &lt;- rebalance_annually(\n      data = data_sorts,\n      sorting_variable = sorting_variable,\n      sorting_method = sorting_method,\n      n_portfolios_main = n_portfolios_main,\n      n_portfolios_secondary = n_portfolios_secondary,\n      exchanges = exchanges,\n      value_weighted = value_weighted\n    )\n  }\n\n  # Compute return differential\n  data_return |&gt;\n    group_by(month) |&gt;\n    summarize(\n      premium = ret[portfolio == max(portfolio)] - ret[portfolio == min(portfolio)],\n      .groups = \"drop\"\n    ) |&gt;\n    pull(premium) |&gt;\n    mean() * 100\n}"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#applying-the-functions",
    "href": "blog/nse-portfolio-sorts/index.html#applying-the-functions",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Applying the functions",
    "text": "Applying the functions\nFinally, we have data, decisions, and functions. Indeed, we are now ready to implement the portfolio sort, right? Yes! Just let me briefly discuss how the implementation works.\nWe have a grid with 69,120 specifications. For each of these specifications, we want to estimate a return differential. This is most easily achieved with a pmap() call. However, we want to parallelize the operation to leverage the multiple cores of our device. Hence, we have to use the package furrr and a future_pmap() instead. As a side note, in most cases we also have to increase the maximum memory for each worker, which can be done with options().\n\nlibrary(furrr)\n\noptions(future.globals.maxSize = 891289600)\n\nplan(multisession, workers = availableCores())\n\nWith the parallel environment set and ready to go, we map the arguments our function needs into the final function execute_sorts() from above. Then, we go and have some tea. And some lunch, breakfast, second breakfast, and so on. In short, it takes a while - depending on your device even more than a day. Each result requires roughly 13 seconds, but you must remember that you are computing 69,120 results.\n\ndata_premia &lt;- setup_grid |&gt;\n  mutate(premium_estimate = future_pmap(\n    .l = list(\n      sorting_variable, drop_smallNYSE_at, include_financials,\n      include_utilities, drop_bookequity, drop_earnings,\n      drop_stock_age_at, drop_price_at, sv_lag,\n      formation_time, n_portfolios_main, sorting_method,\n      n_portfolios_secondary, exchanges, value_weighted\n    ),\n    .f = ~ execute_sorts(\n      sorting_variable = ..1,\n      drop_smallNYSE_at = ..2,\n      include_financials = ..3,\n      include_utilities = ..4,\n      drop_bookequity = ..5,\n      drop_earnings = ..6,\n      drop_stock_age_at = ..7,\n      drop_price_at = ..8,\n      sv_lag = ..9,\n      formation_time = ..10,\n      n_portfolios_main = ..11,\n      sorting_method = ..12,\n      n_portfolios_secondary = ..13,\n      exchanges = ..14,\n      value_weighted = ..15\n    )\n  ))\n\nNow you have all the estimates for the premium. However, one last step has to be considered when you actually investigate the premium. The portfolio sorting algorithm we constructed here is always long in the firms with a high value for the sorting variable and short in firms with low characteristics. This provides a very general way of doing it. Hence, you simply correct for this effect at the end by multiplying the column by -1 if your sorting variable predicts an inverse relation between the sorting variable and expected returns. Otherwise, you can ignore the following chunk.\n\ndata_premia &lt;- data_premia |&gt;\n  mutate(premium_estimate = premium_estimate * -1)"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#footnotes",
    "href": "blog/nse-portfolio-sorts/index.html#footnotes",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMenkveld, A. J. et al. (2023). “Non-standard Errors”, Journal of Finance (forthcoming). http://dx.doi.org/10.2139/ssrn.3961574↩︎\nWalter, D., Weber, R., and Weiss, P. (2023). “Non-Standard Errors in Portfolio Sorts”. http://dx.doi.org/10.2139/ssrn.4164117↩︎\nCooper, M. J., Gulen, H., and Schill, M. J. (2008). “Asset growth and the cross‐section of stock returns”, The Journal of Finance, 63(4), 1609-1651.↩︎"
  }
]