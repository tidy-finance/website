[
  {
    "objectID": "workshops/reproducible-research-workflows.html",
    "href": "workshops/reproducible-research-workflows.html",
    "title": "Reproducible Research Workflows",
    "section": "",
    "text": "We offer a comprehensive, three-day workshop tailored specifically for doctoral students. Reproducible research workflows are essential for ensuring the credibility and longevity of scholarly endeavors. Moreover, all finance top journals adopted some form of code and data sharing policy in recent years.\nThis workshop equips participants with the tools and techniques to foster reproducibility and effective communication in their research practices, irrespective of their preferred coding language. We encourage participants to bring existing research projects to the workshop, where they can reflect upon and improve their workflows with guidance from the instructor and peers.\nWhile prior experience with programming and data analysis tools is beneficial, this workshop is designed to accommodate participants with varying proficiency levels. It provides foundational knowledge alongside advanced techniques for reproducible research that can be applied to any programming language.\nKey Topics\n\n  \n    Structuring research projects\n    How to incorporate project-oriented workflows, safe paths, best practices for file naming\n  \n  \n    Tidy data\n    How to deal with different types of messy data and work with various data storage technologies\n  \n  \n    Tidy coding principles\n    How to use principles of tidy coding for writing reproducible, readable, and maintainable code\n  \n  \n    Version control\n    How to use Git and GitHub for collaborative projects and to leverage version control to enhance transparency and accountability\n  \n  \n    Reproducible communication and collaboration\n    How to create reproducible documents using Quarto's literate programming capabilities and development environments for reproducible collaboration\n  \n\nAbout the Instructors\nThe workshop is held by the creators and maintainers of the Tidy Finance project, a transparent, open-source approach to research in financial economics. Alongside contributing to Tidy Finance, they have published in leading academic journals, including the Journal of Finance, Journal of Financial Economics, Review of Finance, and Journal of Econometrics.\n\n    \n      \n      Christoph Scheuch\n      Independent Business Intelligence & Data Science Expert\n    \n    \n      \n      Stefan Voigt\n      Assistant Professor of Finance at University of Copenhagen\n    \n    \n      \n      Patrick Weiss\n      Assistant Professor of Finance at Reykjavik University\n    \n    \n      \n      Christoph Frey\n      Quantitative Researcher\n    \n  \nGet in Touch\nContact us to schedule a workshop at your institution.\n\n  \n    Message goes here\n  \n  \n    \n    Your name\n    \n    Your email\n    \n    Your institution or company\n    \n    Your request"
  },
  {
    "objectID": "r/wrds-dummy-data.html",
    "href": "r/wrds-dummy-data.html",
    "title": "WRDS Dummy Data",
    "section": "",
    "text": "Note\n\n\n\nThis appendix chapter is based on a blog post Dummy Data for Tidy Finance Readers without Access to WRDS by Christoph Scheuch.\nIn this appendix chapter, we alleviate the constraints of readers who do not have access to WRDS and hence cannot run the code that we provide. We show how to create a dummy database that contains the WRDS tables and corresponding columns such that all code chunks in this book can be executed with this dummy database. We do not create dummy data for tables of macroeconomic variables because they can be freely downloaded from the original sources; check out Accessing and Managing Financial Data.\nWe deliberately use the dummy label because the data is not meaningful in the sense that it allows readers to actually replicate the results of the book. For legal reasons, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books.\nTo generate the dummy database, we use the following packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nLet us initialize a SQLite database (tidy_finance_r.sqlite) or connect to your existing one. Be careful, if you already downloaded the data from WRDS, then the code in this chapter will overwrite your data!\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\nSince we draw random numbers for most of the columns, we also define a seed to ensure that the generated numbers are replicable. We also initialize vectors of dates of different frequencies over ten years that we then use to create yearly, monthly, and daily data, respectively.\nset.seed(1234)\n\nstart_date &lt;- as.Date(\"2003-01-01\")\nend_date &lt;- as.Date(\"2022-12-31\")\n\ntime_series_years &lt;- seq(year(start_date), year(end_date), 1)\ntime_series_months &lt;- seq(start_date, end_date, \"1 month\")\ntime_series_days &lt;- seq(start_date, end_date, \"1 day\")",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "r/wrds-dummy-data.html#create-stock-dummy-data",
    "href": "r/wrds-dummy-data.html#create-stock-dummy-data",
    "title": "WRDS Dummy Data",
    "section": "Create Stock Dummy Data",
    "text": "Create Stock Dummy Data\nLet us start with the core data used throughout the book: stock and firm characteristics. We first generate a table with a cross-section of stock identifiers with unique permno and gvkey values, as well as associated exchcd, exchange, industry, and siccd values. The generated data is based on the characteristics of stocks in the crsp_monthly table of the original database, ensuring that the generated stocks roughly reflect the distribution of industries and exchanges in the original data, but the identifiers and corresponding exchanges or industries do not reflect actual firms. Similarly, the permno-gvkey combinations are purely nonsensical and should not be used together with actual CRSP or Compustat data.\n\nnumber_of_stocks &lt;- 100\n\nindustries &lt;- tibble(\n  industry = c(\"Agriculture\", \"Construction\", \"Finance\", \n               \"Manufacturing\", \"Mining\", \"Public\", \"Retail\", \n               \"Services\", \"Transportation\", \"Utilities\", \n               \"Wholesale\"),\n  n = c(81, 287, 4682, 8584, 1287, 1974, 1571, 4277, 1249, \n        457, 904),\n  prob = c(0.00319, 0.0113, 0.185, 0.339, 0.0508, 0.0779, \n           0.0620, 0.169, 0.0493, 0.0180, 0.0357)\n)\n\nexchanges &lt;- exchanges &lt;- tibble(\n  exchange = c(\"AMEX\", \"NASDAQ\", \"NYSE\"),\n  n = c(2893, 17236, 5553),\n  prob = c(0.113, 0.671, 0.216)\n)\n\nstock_identifiers &lt;- 1:number_of_stocks |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        permno = x,\n        gvkey = as.character(x + 10000),\n        exchange = sample(exchanges$exchange, 1, \n                          prob = exchanges$prob),\n        industry = sample(industries$industry, 1, \n                          prob = industries$prob)\n      ) |&gt; \n        mutate(\n          exchcd = case_when(\n            exchange == \"NYSE\" ~ sample(c(1, 31), n()),\n            exchange == \"AMEX\" ~ sample(c(2, 32), n()),\n            exchange == \"NASDAQ\" ~ sample(c(3, 33), n())\n          ),\n          siccd = case_when(\n            industry == \"Agriculture\" ~ sample(1:999, n()),\n            industry == \"Mining\" ~ sample(1000:1499, n()),\n            industry == \"Construction\" ~ sample(1500:1799, n()),\n            industry == \"Manufacturing\" ~ sample(1800:3999, n()),\n            industry == \"Transportation\" ~ sample(4000:4899, n()),\n            industry == \"Utilities\" ~ sample(4900:4999, n()),\n            industry == \"Wholesale\" ~ sample(5000:5199, n()),\n            industry == \"Retail\" ~ sample(5200:5999, n()),\n            industry == \"Finance\" ~ sample(6000:6799, n()),\n            industry == \"Services\" ~ sample(7000:8999, n()),\n            industry == \"Public\" ~ sample(9000:9999, n())\n          )\n        )\n    }\n  )\n\nNext, we construct three panels of stock data with varying frequencies: yearly, monthly, and daily. We begin by creating the stock_panel_yearly panel. To achieve this, we combine the stock_identifiers table with a new table containing the variable year from dummy_years. The expand_grid() function ensures that we get all possible combinations of the two tables. After combining, we select only the gvkey and year columns for our final yearly panel.\nNext, we construct the stock_panel_monthly panel. Similar to the yearly panel, we use the expand_grid() function to combine stock_identifiers with a new table that has the month variable from dummy_months. After merging, we select the columns permno, gvkey, month, siccd, industry, exchcd, and exchange to form our monthly panel.\nLastly, we create the stock_panel_daily panel. We combine stock_identifiers with a table containing the date variable from dummy_days. After merging, we retain only the permno and date columns for our daily panel.\n\nstock_panel_yearly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(year = time_series_years)\n) |&gt; \n  select(gvkey, year)\n\nstock_panel_monthly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(month = time_series_months)\n) |&gt; \n  select(permno, gvkey, month, siccd, industry, exchcd, exchange)\n\nstock_panel_daily &lt;- expand_grid(\n  stock_identifiers, \n  tibble(date = time_series_days)\n)|&gt; \n  select(permno, date)\n\n\nDummy beta table\nWe then proceed to create dummy beta values for our stock_panel_monthly table. We generate monthly beta values beta_monthly using the rnorm() function with a mean and standard deviation of 1. For daily beta values beta_daily, we take the dummy monthly beta and add a small random noise to it. This noise is generated again using the rnorm() function, but this time we divide the random values by 100 to ensure they are small deviations from the monthly beta.\n\nbeta_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    beta_monthly = rnorm(n(), mean = 1, sd = 1),\n    beta_daily = beta_monthly + rnorm(n()) / 100\n  )\n\ndbWriteTable(\n  tidy_finance,\n  \"beta\", \n  beta_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy compustat table\nTo create dummy firm characteristics, we take all columns from the compustat table and create random numbers between 0 and 1. For simplicity, we set the datadate for each firm-year observation to the last day of the year, although it is empirically not the case. \n\nrelevant_columns &lt;- c(\n  \"seq\", \"ceq\", \"at\", \"lt\", \"txditc\", \"txdb\", \"itcb\", \n  \"pstkrv\", \"pstkl\", \"pstk\", \"capx\", \"oancf\", \"sale\", \n  \"cogs\", \"xint\", \"xsga\", \"be\", \"op\", \"at_lag\", \"inv\"\n)\n\ncommands &lt;- unlist(\n  map(\n    relevant_columns, \n    ~rlang::exprs(!!..1 := runif(n()))\n  )\n)\n\ncompustat_dummy &lt;- stock_panel_yearly |&gt; \n  mutate(\n    datadate = ymd(str_c(year, \"12\", \"31\")),\n    !!!commands\n  )\n\ndbWriteTable(\n  tidy_finance, \n  \"compustat\", \n  compustat_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_monthly table\nThe crsp_monthly table only lacks a few more columns compared to stock_panel_monthly: the returns ret drawn from a normal distribution, the excess returns ret_excess with small deviations from the returns, the shares outstanding shrout and the last price per month altprc both drawn from uniform distributions, and the market capitalization mktcap as the product of shrout and altprc. \n\ncrsp_monthly_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    date = ceiling_date(month, \"month\") - 1,\n    ret = pmax(rnorm(n()), -1),\n    ret_excess = pmax(ret - runif(n(), 0, 0.0025), -1),\n    shrout = runif(n(), 1, 50) * 1000,\n    altprc = runif(n(), 0, 1000),\n    mktcap = shrout * altprc\n  ) |&gt; \n  group_by(permno) |&gt; \n  arrange(month) |&gt; \n  mutate(mktcap_lag = lag(mktcap)) |&gt; \n  ungroup()\n\ndbWriteTable(\n  tidy_finance, \n  \"crsp_monthly\",\n  crsp_monthly_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_daily table\nThe crsp_daily table only contains a month column and the daily excess returns ret_excess as additional columns to stock_panel_daily.\n\ncrsp_daily_dummy &lt;- stock_panel_daily |&gt; \n  mutate(\n    month = floor_date(date, \"month\"),\n    ret_excess = pmax(rnorm(n()), -1)\n  )\n\ndbWriteTable(\n  tidy_finance,\n  \"crsp_daily\",\n  crsp_daily_dummy, \n  overwrite = TRUE\n)",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "r/wrds-dummy-data.html#create-bond-dummy-data",
    "href": "r/wrds-dummy-data.html#create-bond-dummy-data",
    "title": "WRDS Dummy Data",
    "section": "Create Bond Dummy Data",
    "text": "Create Bond Dummy Data\nLastly, we move to the bond data that we use in our books.\n\nDummy fisd data\nTo create dummy data with the structure of Mergent FISD, we calculate the empirical probabilities of actual bonds for several variables: maturity, offering_amt, interest_frequency, coupon, and sic_code. We use these probabilities to sample a small cross-section of bonds with completely made up complete_cusip, issue_id, and issuer_id.\n\nnumber_of_bonds &lt;- 100\n\nfisd_dummy &lt;- 1:number_of_bonds |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        complete_cusip = str_to_upper(\n          str_c(\n            sample(c(letters, 0:9), 12, replace = TRUE), \n            collapse = \"\"\n          )\n        ),\n      )\n    }\n  ) |&gt; \n  mutate(\n    maturity = sample(time_series_days, n(), replace = TRUE),\n    offering_amt = sample(seq(1:100) * 100000, n(), replace = TRUE),\n    offering_date = maturity - sample(seq(1:25) * 365, n(),replace = TRUE),\n    dated_date = offering_date - sample(-10:10, n(), replace = TRUE),\n    interest_frequency = sample(c(0, 1, 2, 4, 12), n(), replace = TRUE),\n    coupon = sample(seq(0, 2, by = 0.1), n(), replace = TRUE),\n    last_interest_date = pmax(maturity, offering_date, dated_date),\n    issue_id = row_number(),\n    issuer_id = sample(1:250, n(), replace = TRUE),\n    sic_code = as.character(sample(seq(1:9)*1000, n(), replace = TRUE))\n  )\n  \ndbWriteTable(\n  tidy_finance, \n  \"fisd\", \n  fisd_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy trace_enhanced data\nFinally, we create a dummy bond transaction data for the fictional CUSIPs of the dummy fisd data. We take the date range that we also analyze in the book and ensure that we have at least five transactions per day to fulfill a filtering step in the book. \n\nstart_date &lt;- as.Date(\"2014-01-01\")\nend_date &lt;- as.Date(\"2016-11-30\")\n\nbonds_panel &lt;- expand_grid(\n  fisd_dummy |&gt; \n    select(cusip_id = complete_cusip),\n  tibble(\n    trd_exctn_dt = seq(start_date, end_date, \"1 day\")\n  )\n)\n\ntrace_enhanced_dummy &lt;- bind_rows(\n  bonds_panel, bonds_panel, \n  bonds_panel, bonds_panel, \n  bonds_panel) |&gt; \n  mutate(\n    trd_exctn_tm = str_c(\n      sample(0:24, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE)\n    ),\n    rptd_pr = runif(n(), 10, 200),\n    entrd_vol_qt = sample(1:20, n(), replace = TRUE) * 1000,\n    yld_pt = runif(n(), -10, 10),\n    rpt_side_cd = sample(c(\"B\", \"S\"), n(), replace = TRUE),\n    cntra_mp_id = sample(c(\"C\", \"D\"), n(), replace = TRUE)\n  ) \n  \ndbWriteTable(\n  tidy_finance, \n  \"trace_enhanced\", \n  trace_enhanced_dummy, \n  overwrite = TRUE\n)\n\nAs stated in the introduction, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout this book.",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html",
    "href": "r/value-and-bivariate-sorts.html",
    "title": "Value and Bivariate Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we extend univariate portfolio analysis to bivariate sorts, which means we assign stocks to portfolios based on two characteristics. Bivariate sorts are regularly used in the academic asset pricing literature and are the basis for the factors in the Fama-French three-factor model. However, some scholars also use sorts with three grouping variables. Conceptually, portfolio sorts are easily applicable in higher dimensions.\nWe form portfolios on firm size and the book-to-market ratio. To calculate book-to-market ratios, accounting data is required, which necessitates additional steps during portfolio formation. In the end, we demonstrate how to form portfolios on two sorting variables using so-called independent and dependent portfolio sorts.\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#data-preparation",
    "href": "r/value-and-bivariate-sorts.html#data-preparation",
    "title": "Value and Bivariate Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we load the necessary data from our SQLite-database introduced in Accessing and Managing Financial Data. We conduct portfolio sorts based on the CRSP sample but keep only the necessary columns in our memory. We use the same data sources for firm size as in Size Sorts and P-Hacking.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n    select(\n    permno, gvkey, month, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |&gt; \n  collect() |&gt;\n  drop_na()\n\nFurther, we utilize accounting data. The most common source of accounting data is Compustat. We only need book equity data in this application, which we select from our database. Additionally, we convert the variable datadate to its monthly value, as we only consider monthly returns here and do not need to account for the exact date. To achieve this, we use the function floor_date().\n\nbook_equity &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n  select(gvkey, datadate, be) |&gt;\n  collect() |&gt;\n  drop_na() |&gt;\n  mutate(month = floor_date(ymd(datadate), \"month\"))",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#book-to-market-ratio",
    "href": "r/value-and-bivariate-sorts.html#book-to-market-ratio",
    "title": "Value and Bivariate Sorts",
    "section": "Book-to-Market Ratio",
    "text": "Book-to-Market Ratio\nA fundamental problem in handling accounting data is the look-ahead bias; we must not include data in forming a portfolio that is not public knowledge at the time. Of course, researchers have more information when looking into the past than agents had at that moment. However, abnormal excess returns from a trading strategy should not rely on an information advantage because the differential cannot be the result of informed agents’ trades. Hence, we have to lag accounting information.\nWe continue to lag market capitalization and firm size by one month. Then, we compute the book-to-market ratio, which relates a firm’s book equity to its market equity. Firms with high (low) book-to-market ratio are called value (growth) firms. After matching the accounting and market equity information from the same month, we lag book-to-market by six months. This is a sufficiently conservative approach because accounting information is usually released well before six months pass. However, in the asset pricing literature, even longer lags are used as well.1\nHaving both variables, i.e., firm size lagged by one month and book-to-market lagged by six months, we merge these sorting variables to our returns using the sorting_date-column created for this purpose. The final step in our data preparation deals with differences in the frequency of our variables. Returns and firm size are recorded monthly. Yet the accounting information is only released on an annual basis. Hence, we only match book-to-market to one month per year and have eleven empty observations. To solve this frequency issue, we carry the latest book-to-market ratio of each firm to the subsequent months, i.e., we fill the missing observations with the most current report. This is done via the fill()-function after sorting by date and firm (which we identify by permno and gvkey) and on a firm basis (which we do by group_by() as usual). We filter out all observations with accounting data that is older than a year. As the last step, we remove all rows with missing entries because the returns cannot be matched to any annual report.\n\nme &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month %m+% months(1)) |&gt;\n  select(permno, sorting_date, me = mktcap)\n\nbm &lt;- book_equity |&gt;\n  inner_join(crsp_monthly, join_by(gvkey, month)) |&gt;\n  mutate(\n    bm = be / mktcap,\n    sorting_date = month %m+% months(6),\n    comp_date = sorting_date\n  ) |&gt;\n  select(permno, gvkey, sorting_date, comp_date, bm)\n\ndata_for_sorts &lt;- crsp_monthly |&gt;\n  left_join(\n    bm, join_by(permno, gvkey, month == sorting_date)\n  ) |&gt;\n  left_join(\n    me, join_by(permno, month == sorting_date)\n  ) |&gt;\n  select(\n    permno, gvkey, month, ret_excess,\n    mktcap_lag, me, bm, exchange, comp_date\n  )\n\ndata_for_sorts &lt;- data_for_sorts |&gt;\n  arrange(permno, gvkey, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(bm, comp_date) |&gt;\n  ungroup() |&gt; \n  filter(comp_date &gt; month %m-% months(12)) |&gt;\n  select(-comp_date) |&gt;\n  drop_na()\n\nThe last step of preparation for the portfolio sorts is the computation of breakpoints. We continue to use the same function allowing for the specification of exchanges to use for the breakpoints. Additionally, we reintroduce the argument sorting_variable into the function for defining different sorting variables.\n\nassign_portfolio &lt;- function(data, \n                             sorting_variable, \n                             n_portfolios, \n                             exchanges) {\n  breakpoints &lt;- data |&gt;\n    filter(exchange %in% exchanges) |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  return(assigned_portfolios)\n}\n\nAfter these data preparation steps, we present bivariate portfolio sorts on an independent and dependent basis.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#independent-sorts",
    "href": "r/value-and-bivariate-sorts.html#independent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Independent Sorts",
    "text": "Independent Sorts\nBivariate sorts create portfolios within a two-dimensional space spanned by two sorting variables. It is then possible to assess the return impact of either sorting variable by the return differential from a trading strategy that invests in the portfolios at either end of the respective variables spectrum. We create a five-by-five matrix using book-to-market and firm size as sorting variables in our example below. We end up with 25 portfolios. Since we are interested in the value premium (i.e., the return differential between high and low book-to-market firms), we go long the five portfolios of the highest book-to-market firms and short the five portfolios of the lowest book-to-market firms. The five portfolios at each end are due to the size splits we employed alongside the book-to-market splits.\nTo implement the independent bivariate portfolio sort, we assign monthly portfolios for each of our sorting variables separately to create the variables portfolio_bm and portfolio_me, respectively. Then, these separate portfolios are combined to the final sort stored in portfolio_combined. After assigning the portfolios, we compute the average return within each portfolio for each month. Additionally, we keep the book-to-market portfolio as it makes the computation of the value premium easier. The alternative would be to disaggregate the combined portfolio in a separate step. Notice that we weigh the stocks within each portfolio by their market capitalization, i.e., we decide to value-weight our returns.\n\nvalue_portfolios &lt;- data_for_sorts |&gt;\n  group_by(month) |&gt;\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"bm\",\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_me = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"me\",\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    )) |&gt;\n  group_by(month, portfolio_bm, portfolio_me) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )\n\nEquipped with our monthly portfolio returns, we are ready to compute the value premium. However, we still have to decide how to invest in the five high and the five low book-to-market portfolios. The most common approach is to weigh these portfolios equally, but this is yet another researcher’s choice. Then, we compute the return differential between the high and low book-to-market portfolios and show the average value premium.\n\nvalue_premium &lt;- value_portfolios |&gt;\n  group_by(month, portfolio_bm) |&gt;\n  summarize(ret = mean(ret), .groups = \"drop_last\") |&gt;\n  summarize(\n    value_premium = ret[portfolio_bm == max(portfolio_bm)] -\n      ret[portfolio_bm == min(portfolio_bm)]\n  ) |&gt; \n  summarize(\n    value_premium = mean(value_premium)\n  )\n\nThe resulting monthly value premium is 0.43 percent with an annualized return of 5.3 percent.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#dependent-sorts",
    "href": "r/value-and-bivariate-sorts.html#dependent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Dependent Sorts",
    "text": "Dependent Sorts\nIn the previous exercise, we assigned the portfolios without considering the second variable in the assignment. This protocol is called independent portfolio sorts. The alternative, i.e., dependent sorts, creates portfolios for the second sorting variable within each bucket of the first sorting variable. In our example below, we sort firms into five size buckets, and within each of those buckets, we assign firms to five book-to-market portfolios. Hence, we have monthly breakpoints that are specific to each size group. The decision between independent and dependent portfolio sorts is another choice for the researcher. Notice that dependent sorts ensure an equal amount of stocks within each portfolio.\nTo implement the dependent sorts, we first create the size portfolios by calling assign_portfolio() with sorting_variable = \"me\". Then, we group our data again by month and by the size portfolio before assigning the book-to-market portfolio. The rest of the implementation is the same as before. Finally, we compute the value premium.\n\nvalue_portfolios &lt;- data_for_sorts |&gt;\n  group_by(month) |&gt;\n  mutate(portfolio_me = assign_portfolio(\n    data = pick(everything()),\n    sorting_variable = \"me\",\n    n_portfolios = 5,\n    exchanges = c(\"NYSE\")\n  )) |&gt;\n  group_by(month, portfolio_me) |&gt;\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"bm\",\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    )) |&gt;\n  group_by(month, portfolio_me, portfolio_bm) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )\n\nvalue_premium &lt;- value_portfolios |&gt;\n  group_by(month, portfolio_bm) |&gt;\n  summarize(ret = mean(ret), .groups = \"drop_last\") |&gt;\n  summarize(\n    value_premium = ret[portfolio_bm == max(portfolio_bm)] -\n      ret[portfolio_bm == min(portfolio_bm)]\n  ) |&gt; \n  summarize(\n    value_premium = mean(value_premium)\n  )\n\nThe monthly value premium from dependent sorts is 0.38 percent, which translates to an annualized premium of 4.6 percent per year.\nOverall, we show how to conduct bivariate portfolio sorts in this chapter. In one case, we sort the portfolios independently of each other. Yet we also discuss how to create dependent portfolio sorts. Along the lines of Size Sorts and P-Hacking, we see how many choices a researcher has to make to implement portfolio sorts, and bivariate sorts increase the number of choices.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#exercises",
    "href": "r/value-and-bivariate-sorts.html#exercises",
    "title": "Value and Bivariate Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nIn Size Sorts and P-Hacking, we examine the distribution of market equity. Repeat this analysis for book equity and the book-to-market ratio (alongside a plot of the breakpoints, i.e., deciles).\nWhen we investigate the portfolios, we focus on the returns exclusively. However, it is also of interest to understand the characteristics of the portfolios. Write a function to compute the average characteristics for size and book-to-market across the 25 independently and dependently sorted portfolios.\nAs for the size premium, also the value premium constructed here does not follow Fama and French (1993). Implement a p-hacking setup as in Size Sorts and P-Hacking to find a premium that comes closest to their HML premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#footnotes",
    "href": "r/value-and-bivariate-sorts.html#footnotes",
    "title": "Value and Bivariate Sorts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe definition of a time lag is another choice a researcher has to make, similar to breakpoint choices as we describe in Size Sorts and P-Hacking.↩︎",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html",
    "href": "r/trace-and-fisd.html",
    "title": "TRACE and FISD",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we dive into the US corporate bond market. Bond markets are far more diverse than stock markets, as most issuers have multiple bonds outstanding simultaneously with potentially very different indentures. This market segment is exciting due to its size (roughly 10 trillion USD outstanding), heterogeneity of issuers (as opposed to government bonds), market structure (mostly over-the-counter trades), and data availability. We introduce how to use bond characteristics from FISD and trade reports from TRACE and provide code to download and clean TRACE in R.\nMany researchers study liquidity in the US corporate bond market O’Hara and Zhou (2021). We do not cover bond returns here, but you can compute them from TRACE data. Instead, we refer to studies on the topic such as Bessembinder et al. (2008), Bai, Bali, and Wen (2019), and Kelly, Palhares, and Pruitt (2021) and a survey by Huang and Shi (2021). Moreover, WRDS includes bond returns computed from TRACE data at a monthly frequency.\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(RSQLite)\nlibrary(RPostgres)\nlibrary(devtools)\nCompared to previous chapters, we load the devtools package (Wickham et al. 2022) to source code that we provided to the public via gist.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#bond-data-from-wrds",
    "href": "r/trace-and-fisd.html#bond-data-from-wrds",
    "title": "TRACE and FISD",
    "section": "Bond Data from WRDS",
    "text": "Bond Data from WRDS\nBoth bond databases we need are available on WRDS to which we establish the RPostgres connection described in WRDS, CRSP, and Compustat. Additionally, we connect to our local SQLite-database to store the data we download.\n\nwrds &lt;- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"WRDS_USER\"),\n  password = Sys.getenv(\"WRDS_PASSWORD\")\n)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#mergent-fisd",
    "href": "r/trace-and-fisd.html#mergent-fisd",
    "title": "TRACE and FISD",
    "section": "Mergent FISD",
    "text": "Mergent FISD\nFor research on US corporate bonds, the Mergent Fixed Income Securities Database (FISD) is the primary resource for bond characteristics. There is a detailed manual on WRDS, so we only cover the necessary subjects here. FISD data comes in two main variants, namely, centered on issuers or issues. In either case, the most useful identifiers are CUSIPs. 9-digit CUSIPs identify securities issued by issuers. The issuers can be identified from the first six digits of a security CUSIP, which is also called 6-digit CUSIP. Both stocks and bonds have CUSIPs. This connection would, in principle, allow matching them easily, but due to changing issuer details, this approach only yields small coverage.\nWe use the issue-centered version of FISD to identify the subset of US corporate bonds that meet the standard criteria (Bessembinder, Maxwell, and Venkataraman 2006). The WRDS table fisd_mergedissue contains most of the information we need on a 9-digit CUSIP level. Due to the diversity of corporate bonds, details in the indenture vary significantly. We focus on common bonds that make up the majority of trading volume in this market without diverging too much in indentures.\nThe following chunk connects to the data and selects the bond sample to remove certain bond types that are less commonly used (see, e.g., Dick-Nielsen, Feldhütter, and Lando 2012; O’Hara and Zhou 2021, among many others). In particular, we use the filters listed below. Note that we also treat missing values in these flags.\n\nKeep only senior bonds (security_level = 'SEN').\nExclude bonds which are secured lease obligations (slob = 'N' OR slob IS NULL).\nExclude secured bonds (security_pledge IS NULL).\nExclude asset-backed bonds (asset_backed = 'N' OR asset_backed IS NULL).\nExclude defeased bonds ((defeased = 'N' OR defeased IS NULL) AND defeased_date IS NULL).\nKeep only the bond types US Corporate Debentures ('CDEB'), US Corporate Medium Term Notes ('CMTN'), US Corporate Zero Coupon Notes and Bonds ('CMTZ', 'CZ'), and US Corporate Bank Note ('USBN').\nExclude bonds that are payable in kind ((pay_in_kind != 'Y' OR pay_in_kind IS NULL) AND pay_in_kind_exp_date IS NULL).\nExclude foreign (yankee == \"N\" OR is.na(yankee)) and Canadian issuers (canadian = 'N' OR canadian IS NULL).\nExclude bonds denominated in foreign currency (foreign_currency = 'N').\nKeep only fixed (F) and zero (Z) coupon bonds with additional requirements of fix_frequency IS NULL, coupon_change_indicator = 'N' and annual, semi-annual, quarterly, or monthly interest frequencies.\nExclude bonds that were issued under SEC Rule 144A (rule_144a = 'N').\nExlcude privately placed bonds (private_placement = 'N' OR private_placement IS NULL).\nExclude defaulted bonds (defaulted = 'N' AND filing_date IS NULL AND settlement IS NULL).\nExclude convertible (convertible = 'N'), putable (putable = 'N' OR putable IS NULL), exchangeable (exchangeable = 'N' OR exchangeable IS NULL), perpetual (perpetual = 'N'), or preferred bonds (preferred_security = 'N' OR preferred_security IS NULL).\nExclude unit deal bonds ((unit_deal = 'N' OR unit_deal IS NULL)).\n\n\nfisd_mergedissue_db &lt;- tbl(wrds, I(\"fisd.fisd_mergedissue\"))\n\nfisd &lt;- fisd_mergedissue_db |&gt;\n  filter(\n    security_level == \"SEN\",\n    slob == \"N\" | is.na(slob),\n    is.na(security_pledge),\n    asset_backed == \"N\" | is.na(asset_backed),\n    defeased == \"N\" | is.na(defeased),\n    is.na(defeased_date),\n    bond_type %in% c(\n      \"CDEB\",\n      \"CMTN\",\n      \"CMTZ\",\n      \"CZ\",\n      \"USBN\"\n    ), \n    pay_in_kind != \"Y\" | is.na(pay_in_kind),\n    is.na(pay_in_kind_exp_date),\n    yankee == \"N\" | is.na(yankee),\n    canadian == \"N\" | is.na(canadian),\n    foreign_currency == \"N\",\n    coupon_type %in% c(\n      \"F\",\n      \"Z\"\n    ), \n    is.na(fix_frequency),\n    coupon_change_indicator == \"N\",\n    interest_frequency %in% c(\n      \"0\",\n      \"1\",\n      \"2\",\n      \"4\",\n      \"12\"\n    ),\n    rule_144a == \"N\",\n    private_placement == \"N\" | is.na(private_placement),\n    defaulted == \"N\",\n    is.na(filing_date),\n    is.na(settlement),\n    convertible == \"N\",\n    is.na(exchange),\n    putable == \"N\" | is.na(putable),\n    unit_deal == \"N\" | is.na(unit_deal),\n    exchangeable == \"N\" | is.na(exchangeable),\n    perpetual == \"N\",\n    preferred_security == \"N\" | is.na(preferred_security)\n  ) |&gt; \n  select(\n    complete_cusip, maturity,\n    offering_amt, offering_date,\n    dated_date, \n    interest_frequency, coupon,\n    last_interest_date, \n    issue_id, issuer_id\n  ) |&gt;\n  collect()\n\nWe also pull issuer information from fisd_mergedissuer regarding the industry and country of the firm that issued a particular bond. Then, we filter to include only US-domiciled firms’ bonds. We match the data by issuer_id.\n\nfisd_mergedissuer_db &lt;- tbl(wrds, I(\"fisd.fisd_mergedissuer\")) \n\nfisd_issuer &lt;- fisd_mergedissuer_db |&gt;\n  select(issuer_id, sic_code, country_domicile) |&gt;\n  collect()\n\nfisd &lt;- fisd |&gt;\n  inner_join(fisd_issuer, join_by(issuer_id)) |&gt;\n  filter(country_domicile == \"USA\") |&gt;\n  select(-country_domicile)\n\nFinally, we save the bond characteristics to our local database. This selection of bonds also constitutes the sample for which we will collect trade reports from TRACE below.\n\ndbWriteTable(\n  conn = tidy_finance,\n  name = \"fisd\",\n  value = fisd,\n  overwrite = TRUE\n)\n\nThe FISD database also contains other data. The issue-based file contains information on covenants, i.e., restrictions included in bond indentures to limit specific actions by firms (e.g., Handler, Jankowitsch, and Weiss 2021). Moreover, FISD also provides information on bond ratings. We do not need either here.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#trace",
    "href": "r/trace-and-fisd.html#trace",
    "title": "TRACE and FISD",
    "section": "TRACE",
    "text": "TRACE\nThe Financial Industry Regulatory Authority (FINRA) provides the Trade Reporting and Compliance Engine (TRACE). In TRACE, dealers that trade corporate bonds must report such trades individually. Hence, we observe trade messages in TRACE that contain information on the bond traded, the trade time, price, and volume. TRACE comes in two variants: standard and enhanced TRACE. We show how to download and clean enhanced TRACE as it contains uncapped volume, a crucial quantity missing in the standard distribution. Moreover, enhanced TRACE also provides information on the respective parties’ roles and the direction of the trade report. These items become essential in cleaning the messages.\nWhy do we repeatedly talk about cleaning TRACE? Trade messages are submitted within a short time window after a trade is executed (less than 15 minutes). These messages can contain errors, and the reporters subsequently correct them or they cancel a trade altogether. The cleaning needs are described by Dick-Nielsen (2009) in detail, and Dick-Nielsen (2014) shows how to clean the enhanced TRACE data using SAS. We do not go into the cleaning steps here, since the code is lengthy and serves no educational purpose. However, downloading and cleaning enhanced TRACE data is straightforward with our setup.\nWe store code for cleaning enhanced TRACE with R on the following GitHub gist. as a function. The appendix also contains the code for reference. We only need to source the code from the gist, which we can do with source_gist(). Alternatively, you can also go to the gist, download it, and source() the respective R-file. The clean_enhanced_trace() function takes a vector of CUSIPs, a connection to WRDS explained in WRDS, CRSP, and Compustat, and a start and end date, respectively.\n\nsource_gist(\"3a05b3ab281563b2e94858451c2eb3a4\")\n\nThe TRACE database is considerably large. Therefore, we only download subsets of data at once. Specifying too many CUSIPs over a long time horizon will result in very long download times and a potential failure due to the size of the request to WRDS. The size limit depends on many parameters, and we cannot give you a guideline here. If we were working with the complete TRACE data for all CUSIPs above, splitting the data into 100 parts takes roughly two hours using our setup. For the applications in this book, we need data around the Paris Agreement in December 2015 and download the data in ten sets, which we define below.\n\nfisd_cusips &lt;- fisd |&gt;\n  pull(complete_cusip)\n\nfisd_parts &lt;- split(\n  fisd_cusips,\n  rep(1:10, \n      length.out = length(fisd_cusips))\n)\n\nFinally, we run a loop in the same style as in WRDS, CRSP, and Compustat where we download daily returns from CRSP. For each of the CUSIP sets defined above, we call the cleaning function and save the resulting output. We add new data to the existing table for batch two and all following batches.\n\nbatches &lt;- length(fisd_parts)\n\nfor (j in 1:batches) {\n  trace_enhanced &lt;- clean_enhanced_trace(\n    cusips = fisd_parts[[j]],\n    connection = wrds,\n    start_date = ymd(\"2014-01-01\"),\n    end_date = ymd(\"2016-11-30\")\n  )\n\n  dbWriteTable(\n    conn = tidy_finance,\n    name = \"trace_enhanced\",\n    value = trace_enhanced,\n    overwrite = ifelse(j == 1, TRUE, FALSE),\n    append = ifelse(j != 1, TRUE, FALSE)\n  )\n  \n  cat(\"Batch\", j, \"out of\", batches, \"done (\", \n      round(j / batches, 2) * 100, \"%)\\n\")\n}",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#insights-into-corporate-bonds",
    "href": "r/trace-and-fisd.html#insights-into-corporate-bonds",
    "title": "TRACE and FISD",
    "section": "Insights into Corporate Bonds",
    "text": "Insights into Corporate Bonds\nWhile many news outlets readily provide information on stocks and the underlying firms, corporate bonds are not covered frequently. Additionally, the TRACE database contains trade-level information, potentially new to students. Therefore, we provide you with some insights by showing some summary statistics.\nWe start by looking into the number of bonds outstanding over time and compare it to the number of bonds traded in our sample. First, we compute the number of bonds outstanding for each quarter around the Paris Agreement from 2014 to 2016.\n\nbonds_outstanding &lt;- expand_grid(\"date\" = seq(ymd(\"2014-01-01\"),\n                                              ymd(\"2016-11-30\"), \n                                              by = \"quarter\"), \n                                 \"complete_cusip\" = fisd$complete_cusip) |&gt; \n  left_join(fisd |&gt; select(complete_cusip, \n                              offering_date,\n                              maturity), \n            join_by(complete_cusip)) |&gt; \n  mutate(offering_date = floor_date(offering_date),\n         maturity = floor_date(maturity)) |&gt; \n  filter(date &gt;= offering_date & date &lt;= maturity) |&gt; \n  count(date) |&gt; \n  mutate(type = \"Outstanding\")\n\nNext, we look at the bonds traded each quarter in the same period. Notice that we load the complete trace table from our database, as we only have a single part of it in the environment from the download loop above.\n\ntrace_enhanced &lt;- tbl(tidy_finance, \"trace_enhanced\") |&gt;\n  collect()\n\nbonds_traded &lt;- trace_enhanced |&gt; \n  mutate(date = floor_date(trd_exctn_dt, \"quarters\")) |&gt; \n  group_by(date) |&gt; \n  summarize(n = length(unique(cusip_id)),\n            type = \"Traded\",\n            .groups = \"drop\") \n\nFinally, we plot the two time series in Figure 1.\n\nbonds_outstanding |&gt; \n  bind_rows(bonds_traded) |&gt; \n  ggplot(aes(\n    x = date, \n    y = n, \n    color = type, \n    linetype = type\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Number of bonds outstanding and traded each quarter\"\n  )\n\n\n\n\n\n\n\nFigure 1: The number of corporate bonds outstanding each quarter as reported by Mergent FISD and the number of traded bonds from enhanced TRACE between 2014 and end of 2016.\n\n\n\n\n\nWe see that the number of bonds outstanding increases steadily between 2014 and 2016. During our sample period of trade data, we see that the fraction of bonds trading each quarter is roughly 60 percent. The relatively small number of traded bonds means that many bonds do not trade through an entire quarter. This lack of trading activity illustrates the generally low level of liquidity in the corporate bond market, where it can be hard to trade specific bonds. Does this lack of liquidity mean that corporate bond markets are irrelevant in terms of their size? With over 7,500 traded bonds each quarter, it is hard to say that the market is small. However, let us also investigate the characteristics of issued corporate bonds. In particular, we consider maturity (in years), coupon, and offering amount (in million USD).\n\nfisd |&gt;\n  mutate(maturity = as.numeric(maturity - offering_date) / 365,\n         offering_amt = offering_amt / 10^3) |&gt; \n  pivot_longer(cols = c(maturity, coupon, offering_amt),\n               names_to = \"measure\") |&gt;\n  drop_na() |&gt; \n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value)\n  )\n\n# A tibble: 3 × 8\n  measure        mean     sd    min   q05    q50    q95    max\n  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 coupon         5.94   2.67  0     1.90    6.06   10      39 \n2 maturity       9.74   9.21 -6.24  1.03    7.18   30.0   101.\n3 offering_amt 379.   570.    0.001 0.669 200    1400   15000 \n\n\nWe see that the average bond in our sample period has an offering amount of over 357 million USD with a median of 200 million USD, which both cannot be considered small. The average bond has a maturity of 10 years and pays around 6 percent in coupons.\nFinally, let us compute some summary statistics for the trades in this market. To this end, we show a summary based on aggregate information daily. In particular, we consider the trade size (in million USD) and the number of trades.\n\ntrace_enhanced |&gt; \n  group_by(trd_exctn_dt) |&gt; \n  summarize(trade_size = sum(entrd_vol_qt * rptd_pr / 100) / 10^6,\n            trade_number = n(),\n            .groups = \"drop\") |&gt; \n  pivot_longer(cols = c(trade_size, trade_number),\n               names_to = \"measure\") |&gt; \n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value)\n  )\n\n# A tibble: 2 × 8\n  measure        mean    sd   min    q05    q50    q95    max\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 trade_number 25914. 5444. 439   17844. 26051  34383. 40839 \n2 trade_size   12968. 3577.  17.2  6128. 13421. 17850. 21312.\n\n\nOn average, nearly 26 billion USD of corporate bonds are traded daily in nearly 13,000 transactions. We can hence conclude that the corporate bond market is indeed significant in terms of trading volume and activity.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/trace-and-fisd.html#exercises",
    "href": "r/trace-and-fisd.html#exercises",
    "title": "TRACE and FISD",
    "section": "Exercises",
    "text": "Exercises\n\nCompute the amount outstanding across all bonds over time. Make sure to subtract all matured bonds. How would you describe the resulting plot?\nCompute the number of days each bond is traded (accounting for the bonds’ maturities and issuances). Start by looking at the number of bonds traded each day in a graph similar to the one above. How many bonds trade on more than 75 percent of trading days?\nWRDS provides more information from Mergent FISD such as ratings in the table fisd_ratings. Download the ratings table and plot the distribution of ratings for the different rating providers. How would you map the different providers to a common numeric rating scale? \n\n\n\n\nFigure 1: The number of corporate bonds outstanding each quarter as reported by Mergent FISD and the number of traded bonds from enhanced TRACE between 2014 and end of 2016.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html",
    "href": "r/setting-up-your-environment.html",
    "title": "Setting Up Your Environment",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nWe aim to lower the bar for starting empirical research in financial economics. We want that using R is easy for you. However, given that Tidy Finance is a platform that supports multiple programming languages, we also consider the possibility that you are not familiar with R at all. Hence, we provide you with a simple guide to get started with R and RStudio. If you were not using R before, you will be able to use it after reading this chapter.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#the-r-language",
    "href": "r/setting-up-your-environment.html#the-r-language",
    "title": "Setting Up Your Environment",
    "section": "The R language",
    "text": "The R language\nSome good news first: The software you need is free and easy to download. We will start with downloading and installing R and follow up with doing the same for RStudio.\nR is provided via The Comprehensive R Archive Network (or short CRAN). CRAN does not only provide the main software but also nearly all extensions that you need. We will cover these extensions or packages later, as we usually visit the CRAN website only to download the base version. Now, go ahead and visit CRAN. On the landing page, you can choose your operating systems (i.e., Linux, macOS, and Windows). Click the respective link that fits your system:\n\nR comes as a part of many Linux distributions. If it does not, CRAN provides installation guides for individual Linux distributions.\nFor macOS, the choice currently depends on some hardware specifications, but the right version for your system is clearly indicated.\nFor Windows, you want to use the base version provided.\n\nAfter downloading and installing the software to your system, you are nearly ready to go. In fact, you could just use R now. Unfortunately for many users, R is not a program but a programming language and comes with an interpreter that you would use like a command line. While using R like this might make you feel like a hacker (not that we do not endorse any criminal activity), it is in your best interest to combine R with RStudio.\nR is constantly being updated, with new versions being released multiple times a year. This means that you might want to return to CRAN in the future to fetch yourself an update. You know it is time for an update if packages remind you that you are using an outdated version of R.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#rstudio",
    "href": "r/setting-up-your-environment.html#rstudio",
    "title": "Setting Up Your Environment",
    "section": "RStudio",
    "text": "RStudio\nAssuming you are looking for a more comfortable way of using R, you will get RStudio next. You can download it for free from Posit (i.e., the company that created RStudio, which was previously called RStudio itself). When you follow the instructions, you will see that Posit asks you to install R. However, you should have done that already and can move straight to downloading and installing RStudio.\nRStudio is a program similar to other programs you most likely use, like a browser, text editor, or anything else. It comes with many advantages, including a project manager, Github integration, and much more. Unfortunately, Tidy Finance is not the right scope to elaborate more on these possibilities or introduce the basics of programming, but we point you to some excellent resources below. For the purposes of this book, you have completed your excursions to websites that provide you with the necessary software installers.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#r-packages-and-environments",
    "href": "r/setting-up-your-environment.html#r-packages-and-environments",
    "title": "Setting Up Your Environment",
    "section": "R Packages and Environments",
    "text": "R Packages and Environments\nFollowing your read of the preface to this book, you might now wonder why we did not download the tidyverse yet. Therefore, you must understand one more concept, namely packages in R. You can think of them as extensions that you use for specific purposes, whereas R itself is the core pillar upon which everything rests. Comfortably, you can install packages within R with the following code.\n\ninstall.packages(\"tidyverse\")\n\nSimply specify the package you want where we placed tidyverse. You typically only need to install packages once - except for updates or project-specific R environments. Once installed, you can then load a package with a call to library(tidyverse) to use it.\nTo keep track of the packages’ versions and make our results replicatable, we rely on the package renv. It creates a project-specific installation of R packages and you can find the full list of packages used here in the colophon below. The recorded package versions can also be shared with collaborators to ensure consistency. Our use of renv also makes it easier for you to install the exact package versions we were using (if you want that) by initializing renv with our renv.lock-file from Github. \nOne more piece of advice is the use of RStudio projects. They are a powerful tool to save you some time and make working with R more fun. Without going into more detail here, we refer you to Wickham, Çetinkaya-Rundel, and Grolemund (2023)’s chapter on Workflow: scripts and projects.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#your-first-steps-with-r",
    "href": "r/setting-up-your-environment.html#your-first-steps-with-r",
    "title": "Setting Up Your Environment",
    "section": "Your First Steps with R",
    "text": "Your First Steps with R\nWhile we believe that downloading and installing R and RStudio is sufficiently easy, you might find help from Grolemund (2014) on R and RStudio, packages, as well as updating the software.\nThis book’s scope cannot be to give you an introduction to R itself. It is not our comparative advantage. However, we can point you to a possible path that you could follow to familiarize yourself with R. Therefore, we make the following suggestion:\n\nIf you are new to R itself, a very gentle and good introduction to the workings of R can be found in Grolemund (2014). He provides a wonderful example in the form of the weighted dice project. Once you are done setting up R on your machine, try to follow the instructions in this project.\nThe main book on the tidyverse, Wickham, Çetinkaya-Rundel, and Grolemund (2023), is available online and for free: R for Data Science explains the majority of the tools we use in our book. Working through this text is an eye-opening experience and really useful.\n\nAdditional resources we can encourage you to use are the following:\n\nIf you are an instructor searching to effectively teach R and data science methods, we recommend taking a look at the excellent data science toolbox by Mine Cetinkaya-Rundel.\nRStudio provides a range of excellent cheat sheets with extensive information on how to use the tidyverse packages.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#creating-environment-variables",
    "href": "r/setting-up-your-environment.html#creating-environment-variables",
    "title": "Setting Up Your Environment",
    "section": "Creating Environment Variables",
    "text": "Creating Environment Variables\nIf you plan to share your own code with collaborators or the public, you may encounter the situation that your projects require sensitive information, such as login credentials, that you don’t want to publish. Environment variables are widely used in software development projects because they provide a flexible and secure way to configure applications and store secrets. In later chapters, we use such environment variables to store private login data for a remote database.\nYou can use .Renviron-files to store environment variables. Upon startup, R and RStudio look for .Renviron files in your home and project directory. .Renviron-files can be either at the user or project level. If there is a project-level .Renviron, the user-level file will not be sourced. A simple way to create your own .Renviron-file is the function usethis::edit_r_environ().\n\nusethis::edit_r_environ(scope = \"project\")\n\nThis command will open your .Renviron-file and you can add variables. For the purpose of this book, we create and save the following variables (where user and password are our private login credentials)\nWRDS_USER=user\nWRDS_PASSWORD=password\nAfter you have restarted your RStudio session, you can access these environment variables via Sys.getenv() for future sessions using the specific project or user.\n\nSys.getenv(\"WRDS_USER\")\nSys.getenv(\"WRDS_PASSWORD\")\n\nNote that you can also store other login credentials, API keys, or file paths in the same environment file.\nIf you use version control, then you should make sure that the .Renviron-file is included in your .gitignore with the following code line.\n\nusethis::edit_git_ignore(scope = \"project\")",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/setting-up-your-environment.html#colophon",
    "href": "r/setting-up-your-environment.html#colophon",
    "title": "Setting Up Your Environment",
    "section": "Colophon",
    "text": "Colophon\nThis book was written in RStudio using bookdown (Xie 2016). The website was rendered using quarto (Allaire et al. 2022) and it is hosted via GitHub Pages. The complete source is available from GitHub. We generated all plots in this book using ggplot2 and its classic dark-on-light theme (theme_bw()).\nThis version of the book was built with R (R Core Team 2022) version 4.3.2 (2023-10-31, Eye Holes) and the following packages: \n\n\n\n\n\n\nPackage\nVersion\n\n\n\n\nRPostgres\n1.4.5\n\n\nRSQLite\n2.3.1\n\n\nbroom\n1.0.5\n\n\nbrulee\n0.3.0\n\n\ndbplyr\n2.5.0\n\n\ndevtools\n2.4.5\n\n\ndplyr\n1.1.4\n\n\nfixest\n0.11.1\n\n\nforcats\n1.0.0\n\n\nfrenchdata\n0.2.0\n\n\nfurrr\n0.3.1\n\n\nggplot2\n3.4.3\n\n\nglmnet\n4.1-8\n\n\nhardhat\n1.3.0\n\n\nhexSticker\n0.4.9\n\n\njsonlite\n1.8.4\n\n\nkableExtra\n1.3.4\n\n\nlmtest\n0.9-40\n\n\nlubridate\n1.9.3\n\n\nnloptr\n2.0.3\n\n\npurrr\n1.0.2\n\n\nranger\n0.15.1\n\n\nreadr\n2.1.4\n\n\nreadxl\n1.4.2\n\n\nrenv\n1.0.3\n\n\nrlang\n1.1.3\n\n\nrmarkdown\n2.21\n\n\nsandwich\n3.0-2\n\n\nscales\n1.2.1\n\n\nslider\n0.3.0\n\n\nstringr\n1.5.0\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.1.0\n\n\ntidyquant\n1.0.7\n\n\ntidyr\n1.3.0\n\n\ntidyverse\n2.0.0\n\n\ntimetk\n2.8.3\n\n\ntorch\n0.11.0\n\n\nwesanderson\n0.3.6",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "r/proofs.html",
    "href": "r/proofs.html",
    "title": "Proofs",
    "section": "",
    "text": "The minimum variance portfolio weights are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega= 1,\\] where \\(\\iota\\) is an \\((N \\times 1)\\) vector of ones. The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1).\\] We can solve the first-order conditions of the Lagrangian equation: \\[\n\\begin{aligned}\n& \\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega} = 0 \\Leftrightarrow 2\\Sigma \\omega = \\lambda\\iota \\Rightarrow \\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota \\\\ \\end{aligned}\n\\] Next, the constraint that weights have to sum up to one delivers: \\(1 = \\iota'\\omega = \\frac{\\lambda}{2}\\iota'\\Sigma^{-1}\\iota \\Rightarrow \\lambda = \\frac{2}{\\iota'\\Sigma^{-1}\\iota}.\\) Finally, plug-in the derived value of \\(\\lambda\\) to get \\[\n\\begin{aligned}\n\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}.\n\\end{aligned}\n\\]\n\n\n\nConsider an investor who aims to achieve minimum variance given a desired expected return \\(\\bar{\\mu}\\), that is: \\[\\omega_\\text{eff}\\left(\\bar{\\mu}\\right) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1) - \\tilde{\\lambda}(\\omega'\\mu - \\bar{\\mu}). \\] We can solve the first-order conditions to get \\[\n\\begin{aligned}\n2\\Sigma \\omega &= \\lambda\\iota + \\tilde\\lambda \\mu\\\\\n\\Rightarrow\\omega &= \\frac{\\lambda}{2}\\Sigma^{-1}\\iota + \\frac{\\tilde\\lambda}{2}\\Sigma^{-1}\\mu.\n\\end{aligned}\n\\]\nNext, the two constraints (\\(w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}\\)) imply \\[\n\\begin{aligned}\n1 &= \\iota'\\omega = \\frac{\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\iota}_{C} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\mu}_D\\\\\n\\Rightarrow \\lambda&= \\frac{2 - \\tilde\\lambda D}{C}\\\\\n\\bar\\mu &= \\mu'\\omega = \\frac{\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\iota}_{D} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\mu}_E = \\frac{1}{2}\\left(\\frac{2 - \\tilde\\lambda D}{C}\\right)D+\\frac{\\tilde\\lambda}{2}E  \\\\&=\\frac{D}{C}+\\frac{\\tilde\\lambda}{2}\\left(E - \\frac{D^2}{C}\\right)\\\\\n\\Rightarrow \\tilde\\lambda &= 2\\frac{\\bar\\mu - D/C}{E-D^2/C}.\n\\end{aligned}\n\\] As a result, the efficient portfolio weight takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right).\\] Thus, the efficient portfolio allocates wealth in the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and a levered (self-financing) portfolio to increase the expected return.\nNote that the portfolio weights sum up to one as \\[\\iota'\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) = D - D = 0\\text{ so }\\iota'\\omega_\\text{eff} = \\iota'\\omega_\\text{mvp} = 1.\\] Finally, the expected return of the efficient portfolio is \\[\\mu'\\omega_\\text{eff} = \\frac{D}{C} + \\bar\\mu - \\frac{D}{C} = \\bar\\mu.\\]\n\n\n\nWe argue that an investor with a quadratic utility function with certainty equivalent \\[\\max_\\omega CE(\\omega) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen with the aim to minimize volatility given a pre-specified level or expected returns \\[\\min_\\omega \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1.\\] Note the difference: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines their optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return they want to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk-averse investors (higher \\(\\gamma\\)) will chose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nTo proof this equivalence, consider first the optimal portfolio weights for a certainty equivalent maximizing investor. The first-order condition reads \\[\n\\begin{aligned}\n\\mu - \\lambda \\iota &= \\gamma \\Sigma \\omega \\\\\n\\Leftrightarrow \\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\lambda\\iota\\right)\n\\end{aligned}\n\\] Next, we make use of the constraint \\(\\iota'\\omega = 1\\). \\[\n\\begin{aligned}\n\\iota'\\omega &= 1 = \\frac{1}{\\gamma}\\left(\\iota'\\Sigma^{-1}\\mu - \\lambda\\iota'\\Sigma^{-1}\\iota\\right)\\\\\n\\Rightarrow \\lambda &= \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right).\n\\end{aligned}\n\\] Plugging in the value of \\(\\lambda\\) reveals the desired portfolio for an investor with risk aversion \\(\\gamma\\). \\[\n\\begin{aligned}\n\\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right)\\right) \\\\\n\\Rightarrow \\omega &= \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\iota'\\Sigma^{-1}\\right)\\mu\\\\\n&= \\omega_\\text{mvp} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1}\\mu - \\frac{\\iota'\\Sigma^{-1}\\mu}{\\iota'\\Sigma^{-1}\\iota}\\Sigma^{-1}\\iota\\right).\n\\end{aligned}\n\\] The resulting weights correspond to the efficient portfolio with desired return \\(\\bar r\\) such that (in the notation of book) \\[\\frac{1}{\\gamma} = \\frac{\\tilde\\lambda}{2} = \\frac{\\bar\\mu - D/C}{E - D^2/C}\\] which implies that the desired return is just \\[\\bar\\mu = \\frac{D}{C} + \\frac{1}{\\gamma}\\left({E - D^2/C}\\right)\\] which is \\(\\frac{D}{C} = \\mu'\\omega_\\text{mvp}\\) for \\(\\gamma\\rightarrow \\infty\\) as expected. For instance, letting \\(\\gamma \\rightarrow \\infty\\) implies \\(\\bar\\mu = \\frac{D}{C} = \\omega_\\text{mvp}'\\mu\\).",
    "crumbs": [
      "R",
      "Appendix",
      "Proofs"
    ]
  },
  {
    "objectID": "r/proofs.html#optimal-portfolio-choice",
    "href": "r/proofs.html#optimal-portfolio-choice",
    "title": "Proofs",
    "section": "",
    "text": "The minimum variance portfolio weights are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega= 1,\\] where \\(\\iota\\) is an \\((N \\times 1)\\) vector of ones. The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1).\\] We can solve the first-order conditions of the Lagrangian equation: \\[\n\\begin{aligned}\n& \\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega} = 0 \\Leftrightarrow 2\\Sigma \\omega = \\lambda\\iota \\Rightarrow \\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota \\\\ \\end{aligned}\n\\] Next, the constraint that weights have to sum up to one delivers: \\(1 = \\iota'\\omega = \\frac{\\lambda}{2}\\iota'\\Sigma^{-1}\\iota \\Rightarrow \\lambda = \\frac{2}{\\iota'\\Sigma^{-1}\\iota}.\\) Finally, plug-in the derived value of \\(\\lambda\\) to get \\[\n\\begin{aligned}\n\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}.\n\\end{aligned}\n\\]\n\n\n\nConsider an investor who aims to achieve minimum variance given a desired expected return \\(\\bar{\\mu}\\), that is: \\[\\omega_\\text{eff}\\left(\\bar{\\mu}\\right) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1) - \\tilde{\\lambda}(\\omega'\\mu - \\bar{\\mu}). \\] We can solve the first-order conditions to get \\[\n\\begin{aligned}\n2\\Sigma \\omega &= \\lambda\\iota + \\tilde\\lambda \\mu\\\\\n\\Rightarrow\\omega &= \\frac{\\lambda}{2}\\Sigma^{-1}\\iota + \\frac{\\tilde\\lambda}{2}\\Sigma^{-1}\\mu.\n\\end{aligned}\n\\]\nNext, the two constraints (\\(w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}\\)) imply \\[\n\\begin{aligned}\n1 &= \\iota'\\omega = \\frac{\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\iota}_{C} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\mu}_D\\\\\n\\Rightarrow \\lambda&= \\frac{2 - \\tilde\\lambda D}{C}\\\\\n\\bar\\mu &= \\mu'\\omega = \\frac{\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\iota}_{D} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\mu}_E = \\frac{1}{2}\\left(\\frac{2 - \\tilde\\lambda D}{C}\\right)D+\\frac{\\tilde\\lambda}{2}E  \\\\&=\\frac{D}{C}+\\frac{\\tilde\\lambda}{2}\\left(E - \\frac{D^2}{C}\\right)\\\\\n\\Rightarrow \\tilde\\lambda &= 2\\frac{\\bar\\mu - D/C}{E-D^2/C}.\n\\end{aligned}\n\\] As a result, the efficient portfolio weight takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right).\\] Thus, the efficient portfolio allocates wealth in the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and a levered (self-financing) portfolio to increase the expected return.\nNote that the portfolio weights sum up to one as \\[\\iota'\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) = D - D = 0\\text{ so }\\iota'\\omega_\\text{eff} = \\iota'\\omega_\\text{mvp} = 1.\\] Finally, the expected return of the efficient portfolio is \\[\\mu'\\omega_\\text{eff} = \\frac{D}{C} + \\bar\\mu - \\frac{D}{C} = \\bar\\mu.\\]\n\n\n\nWe argue that an investor with a quadratic utility function with certainty equivalent \\[\\max_\\omega CE(\\omega) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen with the aim to minimize volatility given a pre-specified level or expected returns \\[\\min_\\omega \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1.\\] Note the difference: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines their optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return they want to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk-averse investors (higher \\(\\gamma\\)) will chose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nTo proof this equivalence, consider first the optimal portfolio weights for a certainty equivalent maximizing investor. The first-order condition reads \\[\n\\begin{aligned}\n\\mu - \\lambda \\iota &= \\gamma \\Sigma \\omega \\\\\n\\Leftrightarrow \\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\lambda\\iota\\right)\n\\end{aligned}\n\\] Next, we make use of the constraint \\(\\iota'\\omega = 1\\). \\[\n\\begin{aligned}\n\\iota'\\omega &= 1 = \\frac{1}{\\gamma}\\left(\\iota'\\Sigma^{-1}\\mu - \\lambda\\iota'\\Sigma^{-1}\\iota\\right)\\\\\n\\Rightarrow \\lambda &= \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right).\n\\end{aligned}\n\\] Plugging in the value of \\(\\lambda\\) reveals the desired portfolio for an investor with risk aversion \\(\\gamma\\). \\[\n\\begin{aligned}\n\\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right)\\right) \\\\\n\\Rightarrow \\omega &= \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\iota'\\Sigma^{-1}\\right)\\mu\\\\\n&= \\omega_\\text{mvp} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1}\\mu - \\frac{\\iota'\\Sigma^{-1}\\mu}{\\iota'\\Sigma^{-1}\\iota}\\Sigma^{-1}\\iota\\right).\n\\end{aligned}\n\\] The resulting weights correspond to the efficient portfolio with desired return \\(\\bar r\\) such that (in the notation of book) \\[\\frac{1}{\\gamma} = \\frac{\\tilde\\lambda}{2} = \\frac{\\bar\\mu - D/C}{E - D^2/C}\\] which implies that the desired return is just \\[\\bar\\mu = \\frac{D}{C} + \\frac{1}{\\gamma}\\left({E - D^2/C}\\right)\\] which is \\(\\frac{D}{C} = \\mu'\\omega_\\text{mvp}\\) for \\(\\gamma\\rightarrow \\infty\\) as expected. For instance, letting \\(\\gamma \\rightarrow \\infty\\) implies \\(\\bar\\mu = \\frac{D}{C} = \\omega_\\text{mvp}'\\mu\\).",
    "crumbs": [
      "R",
      "Appendix",
      "Proofs"
    ]
  },
  {
    "objectID": "r/other-data-providers.html",
    "href": "r/other-data-providers.html",
    "title": "Other Data Providers",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn the previous chapters, we introduced many ways to get financial data that researchers regularly use. We showed how to load data into R from Yahoo!Finance and commonly used file types, such as comma-separated or Excel files. Then, we introduced remotely connecting to WRDS and downloading data from there. However, this is only a subset of the vast amounts of data available these days.\nIn this short chapter, we aim to provide an overview of common alternative data providers for which direct access via R packages exists. Such a list requires constant adjustments because both data providers and access methods change. However, we want to emphasize two main insights: First, the number of R packages that provide access to (financial) data is large. Too large actually to survey here exhaustively. Instead, we can only cover the tip of the iceberg. Second, R provides the functionalities to access basically any form of files or data available online. Thus, even if a desired data source does not come with a well-established R package, chances are high that data can be retrieved by establishing your own API connection or by scrapping the content.\nIn our non-exhaustive list below, we restrict ourselves to listing data sources accessed through easy-to-use R packages. For further inspiration on potential data sources, we recommend reading the R task view empirical finance. Further inspiration (on more general social sciences) can be found here.\nApart from the list below, we want to advertise some amazing data compiled by others. First, there is Open Source Asset Pricing related to Chen and Zimmermann (2022). They provide return data for over 200 trading strategies with different time periods and specifications. The authors also provide signals and explanations of the factor construction. Moreover, in the same spirit, Global factor data provides the data related to Jensen2022b. They provide return data for characteristic-managed portfolios from around the world. The database includes factors for 153 characteristics in 13 themes, using data from 93 countries.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "r/other-data-providers.html#exercises",
    "href": "r/other-data-providers.html#exercises",
    "title": "Other Data Providers",
    "section": "Exercises",
    "text": "Exercises\n\nSelect one of the data sources in the table above and retrieve some data. Browse the homepage of the data provider or the package documentation to find inspiration on which type of data is available to you and how to download the data into your R session.\nGenerate summary statistics of the data you retrieved and provide some useful visualization. The possibilities are endless: Maybe there is some interesting economic event you want to analyze, such as stock market responses to Twitter activity.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html",
    "href": "r/introduction-to-tidy-finance.html",
    "title": "Introduction to Tidy Finance",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nThe main aim of this chapter is to familiarize yourself with the tidyverse. We start by downloading and visualizing stock data from Yahoo!Finance. Then we move to a simple portfolio choice problem and construct the efficient frontier. These examples introduce you to our approach of Tidy Finance.",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#working-with-stock-market-data",
    "href": "r/introduction-to-tidy-finance.html#working-with-stock-market-data",
    "title": "Introduction to Tidy Finance",
    "section": "Working with Stock Market Data",
    "text": "Working with Stock Market Data\nAt the start of each session, we load the required R packages. Throughout the entire book, we always use the tidyverse (Wickham et al. 2019). In this chapter, we also load the convenient tidyquant package (Dancho and Vaughan 2022) to download price data. This package provides a convenient wrapper for various quantitative functions compatible with the tidyverse. Finally, the package scales (Wickham and Seidel 2022) provides useful scale functions for visualizations.\nYou typically have to install a package once before you can load it. In case you have not done this yet, call install.packages(\"tidyquant\"). If you have trouble using tidyquant, check out the corresponding documentation.\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(scales)\n\nWe first download daily prices for one stock symbol, e.g., the Apple stock, AAPL, directly from the data provider Yahoo!Finance. To download the data, you can use the command tq_get. If you do not know how to use it, make sure you read the help file by calling ?tq_get. We especially recommend taking a look at the examples section of the documentation. We request daily data for a period of more than 20 years.\n\nprices &lt;- tq_get(\"AAPL\",\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2022-12-31\"\n)\nprices\n\n# A tibble: 5,787 × 8\n  symbol date        open  high   low close    volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AAPL   2000-01-03 0.936 1.00  0.908 0.999 535796800    0.846\n2 AAPL   2000-01-04 0.967 0.988 0.903 0.915 512377600    0.775\n3 AAPL   2000-01-05 0.926 0.987 0.920 0.929 778321600    0.786\n4 AAPL   2000-01-06 0.948 0.955 0.848 0.848 767972800    0.718\n5 AAPL   2000-01-07 0.862 0.902 0.853 0.888 460734400    0.752\n# ℹ 5,782 more rows\n\n\n tq_get downloads stock market data from Yahoo!Finance if you do not specify another data source. The function returns a tibble with eight quite self-explanatory columns: symbol, date, the market prices at the open, high, low, and close, the daily volume (in the number of traded shares), and the adjusted price in USD. The adjusted prices are corrected for anything that might affect the stock price after the market closes, e.g., stock splits and dividends. These actions affect the quoted prices, but they have no direct impact on the investors who hold the stock. Therefore, we often rely on adjusted prices when it comes to analyzing the returns an investor would have earned by holding the stock continuously.\nNext, we use the ggplot2 package (Wickham 2016) to visualize the time series of adjusted prices in Figure 1 . This package takes care of visualization tasks based on the principles of the grammar of graphics (Wilkinson 2012).\n\nprices |&gt;\n  ggplot(aes(x = date, y = adjusted)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Apple stock prices between beginning of 2000 and end of 2022\"\n  )\n\n\n\n\n\n\n\nFigure 1: Prices are in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n\n Instead of analyzing prices, we compute daily net returns defined as \\(r_t = p_t / p_{t-1} - 1\\), where \\(p_t\\) is the adjusted day \\(t\\) price. In that context, the function lag() is helpful, which returns the previous value in a vector.\n\nreturns &lt;- prices |&gt;\n  arrange(date) |&gt;\n  mutate(ret = adjusted / lag(adjusted) - 1) |&gt;\n  select(symbol, date, ret)\nreturns\n\n# A tibble: 5,787 × 3\n  symbol date           ret\n  &lt;chr&gt;  &lt;date&gt;       &lt;dbl&gt;\n1 AAPL   2000-01-03 NA     \n2 AAPL   2000-01-04 -0.0843\n3 AAPL   2000-01-05  0.0146\n4 AAPL   2000-01-06 -0.0865\n5 AAPL   2000-01-07  0.0474\n# ℹ 5,782 more rows\n\n\nThe resulting tibble contains three columns, where the last contains the daily returns (ret). Note that the first entry naturally contains a missing value (NA) because there is no previous price. Obviously, the use of lag() would be meaningless if the time series is not ordered by ascending dates. The command arrange() provides a convenient way to order observations in the correct way for our application. In case you want to order observations by descending dates, you can use arrange(desc(date)).\nFor the upcoming examples, we remove missing values as these would require separate treatment when computing, e.g., sample averages. In general, however, make sure you understand why NA values occur and carefully examine if you can simply get rid of these observations.\n\nreturns &lt;- returns |&gt;\n  drop_na(ret)\n\nNext, we visualize the distribution of daily returns in a histogram in Figure 2. Additionally, we add a dashed line that indicates the 5 percent quantile of the daily returns to the histogram, which is a (crude) proxy for the worst return of the stock with a probability of at most 5 percent. The 5 percent quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators. We refer to Tsay (2010) for a more thorough introduction to stylized facts of returns.\n\nquantile_05 &lt;- quantile(returns |&gt; pull(ret), probs = 0.05)\nreturns |&gt;\n  ggplot(aes(x = ret)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of daily Apple stock returns\"\n  ) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\nFigure 2: The dotted vertical line indicates the historical 5 percent quantile.\n\n\n\n\n\nHere, bins = 100 determines the number of bins used in the illustration and hence implicitly the width of the bins. Before proceeding, make sure you understand how to use the geom geom_vline() to add a dashed line that indicates the 5 percent quantile of the daily returns. A typical task before proceeding with any data is to compute summary statistics for the main variables of interest.\n\nreturns |&gt;\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  ))\n\n# A tibble: 1 × 4\n  ret_daily_mean ret_daily_sd ret_daily_min ret_daily_max\n           &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        0.00120       0.0251        -0.519         0.139\n\n\nWe see that the maximum daily return was 13.905 percent. Perhaps not surprisingly, the average daily return is close to but slightly above 0. In line with the illustration above, the large losses on the day with the minimum returns indicate a strong asymmetry in the distribution of returns.\nYou can also compute these summary statistics for each year individually by imposing group_by(year = year(date)), where the call year(date) returns the year. More specifically, the few lines of code below compute the summary statistics from above for individual groups of data defined by year. The summary statistics, therefore, allow an eyeball analysis of the time-series dynamics of the return distribution.\n\nreturns |&gt;\n  group_by(year = year(date)) |&gt;\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |&gt;\n  print(n = Inf)\n\n# A tibble: 23 × 5\n    year daily_mean daily_sd daily_min daily_max\n   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2000 -0.00346     0.0549   -0.519     0.137 \n 2  2001  0.00233     0.0393   -0.172     0.129 \n 3  2002 -0.00121     0.0305   -0.150     0.0846\n 4  2003  0.00186     0.0234   -0.0814    0.113 \n 5  2004  0.00470     0.0255   -0.0558    0.132 \n 6  2005  0.00349     0.0245   -0.0921    0.0912\n 7  2006  0.000949    0.0243   -0.0633    0.118 \n 8  2007  0.00366     0.0238   -0.0702    0.105 \n 9  2008 -0.00265     0.0367   -0.179     0.139 \n10  2009  0.00382     0.0214   -0.0502    0.0676\n11  2010  0.00183     0.0169   -0.0496    0.0769\n12  2011  0.00104     0.0165   -0.0559    0.0589\n13  2012  0.00130     0.0186   -0.0644    0.0887\n14  2013  0.000472    0.0180   -0.124     0.0514\n15  2014  0.00145     0.0136   -0.0799    0.0820\n16  2015  0.0000199   0.0168   -0.0612    0.0574\n17  2016  0.000575    0.0147   -0.0657    0.0650\n18  2017  0.00164     0.0111   -0.0388    0.0610\n19  2018 -0.0000573   0.0181   -0.0663    0.0704\n20  2019  0.00266     0.0165   -0.0996    0.0683\n21  2020  0.00281     0.0294   -0.129     0.120 \n22  2021  0.00131     0.0158   -0.0417    0.0539\n23  2022 -0.000970    0.0225   -0.0587    0.0890\n\n\n\nIn case you wonder: the additional argument .names = \"{.fn}\" in across() determines how to name the output columns. The specification is rather flexible and allows almost arbitrary column names, which can be useful for reporting. The print() function simply controls the output options for the R console.",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "href": "r/introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "title": "Introduction to Tidy Finance",
    "section": "Scaling Up the Analysis",
    "text": "Scaling Up the Analysis\nAs a next step, we generalize the code from before such that all the computations can handle an arbitrary vector of symbols (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nThis is where the tidyverse magic starts: tidy data makes it extremely easy to generalize the computations from before to as many assets as you like. The following code takes any vector of symbols, e.g., symbol &lt;- c(\"AAPL\", \"MMM\", \"BA\"), and automates the download as well as the plot of the price time series. In the end, we create the table of summary statistics for an arbitrary number of assets. We perform the analysis with data from all current constituents of the Dow Jones Industrial Average index. \n\nsymbols &lt;- tq_index(\"DOW\") |&gt; \n  filter(company != \"US DOLLAR\")\nsymbols\n\n# A tibble: 30 × 8\n  symbol company           identifier sedol weight sector shares_held\n  &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n1 UNH    UNITEDHEALTH GRO… 91324P102  2917… 0.0880 -          5629634\n2 MSFT   MICROSOFT CORP    594918104  2588… 0.0683 -          5629634\n3 GS     GOLDMAN SACHS GR… 38141G104  2407… 0.0654 -          5629634\n4 HD     HOME DEPOT INC    437076102  2434… 0.0622 -          5629634\n5 CAT    CATERPILLAR INC   149123101  2180… 0.0545 -          5629634\n# ℹ 25 more rows\n# ℹ 1 more variable: local_currency &lt;chr&gt;\n\n\nConveniently, tidyquant provides a function to get all stocks in a stock index with a single call (similarly, tq_exchange(\"NASDAQ\") delivers all stocks currently listed on the NASDAQ exchange). \n\nindex_prices &lt;- tq_get(symbols,\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2022-12-31\"\n)\n\nThe resulting tibble contains 165593 daily observations for 30 different corporations. Figure 3 illustrates the time series of downloaded adjusted prices for each of the constituents of the Dow Jones index. Make sure you understand every single line of code! What are the arguments of aes()? Which alternative geoms could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes.\n\nindex_prices |&gt;\n  ggplot(aes(\n    x = date,\n    y = adjusted,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Stock prices of DOW index constituents\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 3: Prices in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n\nDo you notice the small differences relative to the code we used before? tq_get(symbols) returns a tibble for several symbols as well. All we need to do to illustrate all stock symbols simultaneously is to include color = symbol in the ggplot aesthetics. In this way, we generate a separate line for each symbol. Of course, there are simply too many lines on this graph to identify the individual stocks properly, but it illustrates the point well.\nThe same holds for stock returns. Before computing the returns, we use group_by(symbol) such that the mutate() command is performed for each symbol individually. The same logic also applies to the computation of summary statistics: group_by(symbol) is the key to aggregating the time series into symbol-specific variables of interest.\n\nall_returns &lt;- index_prices |&gt;\n  group_by(symbol) |&gt;\n  mutate(ret = adjusted / lag(adjusted) - 1) |&gt;\n  select(symbol, date, ret) |&gt;\n  drop_na(ret)\n\nall_returns |&gt;\n  group_by(symbol) |&gt;\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |&gt;\n  print(n = Inf)\n\n# A tibble: 30 × 5\n   symbol daily_mean daily_sd daily_min daily_max\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 AAPL     0.00120    0.0251    -0.519     0.139\n 2 AMGN     0.000489   0.0197    -0.134     0.151\n 3 AMZN     0.00101    0.0319    -0.248     0.345\n 4 AXP      0.000518   0.0229    -0.176     0.219\n 5 BA       0.000595   0.0224    -0.238     0.243\n 6 CAT      0.000709   0.0204    -0.145     0.147\n 7 CRM      0.00110    0.0270    -0.271     0.260\n 8 CSCO     0.000317   0.0237    -0.162     0.244\n 9 CVX      0.000553   0.0176    -0.221     0.227\n10 DIS      0.000418   0.0195    -0.184     0.160\n11 DOW      0.000562   0.0260    -0.217     0.209\n12 GS       0.000550   0.0231    -0.190     0.265\n13 HD       0.000543   0.0194    -0.287     0.141\n14 HON      0.000515   0.0194    -0.174     0.282\n15 IBM      0.000273   0.0165    -0.155     0.120\n16 INTC     0.000285   0.0236    -0.220     0.201\n17 JNJ      0.000408   0.0122    -0.158     0.122\n18 JPM      0.000582   0.0242    -0.207     0.251\n19 KO       0.000337   0.0132    -0.101     0.139\n20 MCD      0.000533   0.0147    -0.159     0.181\n21 MMM      0.000378   0.0150    -0.129     0.126\n22 MRK      0.000383   0.0168    -0.268     0.130\n23 MSFT     0.000513   0.0194    -0.156     0.196\n24 NKE      0.000743   0.0194    -0.198     0.155\n25 PG       0.000377   0.0134    -0.302     0.120\n26 TRV      0.000569   0.0183    -0.208     0.256\n27 UNH      0.000984   0.0198    -0.186     0.348\n28 V        0.000929   0.0190    -0.136     0.150\n29 VZ       0.000239   0.0151    -0.118     0.146\n30 WMT      0.000314   0.0150    -0.114     0.117\n\n\n\nNote that you are now also equipped with all tools to download price data for each symbol listed in the S&P 500 index with the same number of lines of code. Just use symbol &lt;- tq_index(\"SP500\"), which provides you with a tibble that contains each symbol that is (currently) part of the S&P 500. However, don’t try this if you are not prepared to wait for a couple of minutes because this is quite some data to download!",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "href": "r/introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "title": "Introduction to Tidy Finance",
    "section": "Other Forms of Data Aggregation",
    "text": "Other Forms of Data Aggregation\nOf course, aggregation across variables other than symbol can also make sense. For instance, suppose you are interested in answering the question: Are days with high aggregate trading volume likely followed by days with high aggregate trading volume? To provide some initial analysis on this question, we take the downloaded data and compute aggregate daily trading volume for all Dow Jones constituents in USD. Recall that the column volume is denoted in the number of traded shares. Thus, we multiply the trading volume with the daily closing price to get a proxy for the aggregate trading volume in USD. Scaling by 1e9 (R can handle scientific notation) denotes daily trading volume in billion USD.\n\ntrading_volume &lt;- index_prices |&gt;\n  group_by(date) |&gt;\n  summarize(trading_volume = sum(volume * adjusted))\n\ntrading_volume |&gt;\n  ggplot(aes(x = date, y = trading_volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume of DOW index constitutens\"\n  ) +\n    scale_y_continuous(labels = unit_format(unit = \"B\", scale = 1e-9))\n\n\n\n\n\n\n\nFigure 4: Total daily trading volume in billion USD.\n\n\n\n\n\nFigure 4 indicates a clear upward trend in aggregated daily trading volume. In particular, since the outbreak of the COVID-19 pandemic, markets have processed substantial trading volumes, as analyzed, for instance, by Goldstein, Koijen, and Mueller (2021). One way to illustrate the persistence of trading volume would be to plot volume on day \\(t\\) against volume on day \\(t-1\\) as in the example below. In Figure 5, we add a dotted 45°-line to indicate a hypothetical one-to-one relation by geom_abline(), addressing potential differences in the axes’ scales.\n\ntrading_volume |&gt;\n  ggplot(aes(x = lag(trading_volume), y = trading_volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = \"Previous day aggregate trading volume\",\n    y = \"Aggregate trading volume\",\n    title = \"Persistence in daily trading volume of DOW index constituents\"\n  ) + \n  scale_x_continuous(labels = unit_format(unit = \"B\", scale = 1e-9)) +\n  scale_y_continuous(labels = unit_format(unit = \"B\", scale = 1e-9))\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 5: Total daily trading volume in billion USD.\n\n\n\n\n\nDo you understand where the warning ## Warning: Removed 1 rows containing missing values (geom_point). comes from and what it means? Purely eye-balling reveals that days with high trading volume are often followed by similarly high trading volume days.",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#portfolio-choice-problems",
    "href": "r/introduction-to-tidy-finance.html#portfolio-choice-problems",
    "title": "Introduction to Tidy Finance",
    "section": "Portfolio Choice Problems",
    "text": "Portfolio Choice Problems\nIn the previous part, we show how to download stock market data and inspect it with graphs and summary statistics. Now, we move to a typical question in Finance: how to allocate wealth across different assets optimally. The standard framework for optimal portfolio selection considers investors that prefer higher future returns but dislike future return volatility (defined as the square root of the return variance): the mean-variance investor (Markowitz 1952).\n An essential tool to evaluate portfolios in the mean-variance context is the efficient frontier, the set of portfolios which satisfies the condition that no other portfolio exists with a higher expected return but with the same volatility (the square root of the variance, i.e., the risk), see, e.g., Merton (1972). We compute and visualize the efficient frontier for several stocks. First, we extract each asset’s monthly returns. In order to keep things simple, we work with a balanced panel and exclude DOW constituents for which we do not observe a price on every single trading day since the year 2000.\n\nindex_prices &lt;- index_prices |&gt;\n  group_by(symbol) |&gt;\n  mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  filter(n == max(n)) |&gt;\n  select(-n)\n\nreturns &lt;- index_prices |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  group_by(symbol, month) |&gt;\n  summarize(price = last(adjusted), .groups = \"drop_last\") |&gt;\n  mutate(ret = price / lag(price) - 1) |&gt;\n  drop_na(ret) |&gt;\n  select(-price)\n\nHere, floor_date() is a function from the lubridate package (Grolemund and Wickham 2011), which provides useful functions to work with dates and times.\nNext, we transform the returns from a tidy tibble into a \\((T \\times N)\\) matrix with one column for each of the \\(N\\) symbols and one row for each of the \\(T\\) trading days to compute the sample average return vector \\[\\hat\\mu = \\frac{1}{T}\\sum\\limits_{t=1}^T r_t\\] where \\(r_t\\) is the \\(N\\) vector of returns on date \\(t\\) and the sample covariance matrix \\[\\hat\\Sigma = \\frac{1}{T-1}\\sum\\limits_{t=1}^T (r_t - \\hat\\mu)(r_t - \\hat\\mu)'.\\] We achieve this by using pivot_wider() with the new column names from the column symbol and setting the values to ret. We compute the vector of sample average returns and the sample variance-covariance matrix, which we consider as proxies for the parameters of the distribution of future stock returns. Thus, for simplicity, we refer to \\(\\Sigma\\) and \\(\\mu\\) instead of explicitly highlighting that the sample moments are estimates. In later chapters, we discuss the issues that arise once we take estimation uncertainty into account.\n\nreturns_matrix &lt;- returns |&gt;\n  pivot_wider(\n    names_from = symbol,\n    values_from = ret\n  ) |&gt;\n  select(-month)\nsigma &lt;- cov(returns_matrix)\nmu &lt;- colMeans(returns_matrix)\n\nThen, we compute the minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) as well as the expected portfolio return \\(\\omega_\\text{mvp}'\\mu\\) and volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) of this portfolio. Recall that the minimum variance portfolio is the vector of portfolio weights that are the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\sum\\limits_{i=1}^N\\omega_i = 1.\\] The constraint that weights sum up to one simply implies that all funds are distributed across the available asset universe, i.e., there is no possibility to retain cash. It is easy to show analytically that \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\), where \\(\\iota\\) is a vector of ones and \\(\\Sigma^{-1}\\) is the inverse of \\(\\Sigma\\).\n\nN &lt;- ncol(returns_matrix)\niota &lt;- rep(1, N)\nsigma_inv &lt;- solve(sigma)\nmvp_weights &lt;- sigma_inv %*% iota\nmvp_weights &lt;- mvp_weights / sum(mvp_weights)\ntibble(\n  average_ret = as.numeric(t(mvp_weights) %*% mu),\n  volatility = as.numeric(sqrt(t(mvp_weights) %*% sigma %*% mvp_weights))\n)\n\n# A tibble: 1 × 2\n  average_ret volatility\n        &lt;dbl&gt;      &lt;dbl&gt;\n1     0.00785     0.0321\n\n\nThe command solve(A, b) returns the solution of a system of equations \\(Ax = b\\). If b is not provided, as in the example above, it defaults to the identity matrix such that solve(sigma) delivers \\(\\Sigma^{-1}\\) (if a unique solution exists).\nNote that the monthly volatility of the minimum variance portfolio is of the same order of magnitude as the daily standard deviation of the individual components. Thus, the diversification benefits in terms of risk reduction are tremendous!\nNext, we set out to find the weights for a portfolio that achieves, as an example, three times the expected return of the minimum variance portfolio. However, mean-variance investors are not interested in any portfolio that achieves the required return but rather in the efficient portfolio, i.e., the portfolio with the lowest standard deviation. If you wonder where the solution \\(\\omega_\\text{eff}\\) comes from: The efficient portfolio is chosen by an investor who aims to achieve minimum variance given a minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, their objective function is to choose \\(\\omega_\\text{eff}\\) as the solution to \\[\\omega_\\text{eff}(\\bar{\\mu}) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\]\nThe code below implements the analytic solution to this optimization problem for a benchmark return \\(\\bar\\mu\\), which we set to 3 times the expected return of the minimum variance portfolio. We encourage you to verify that it is correct.\n\nbenchmark_multiple &lt;- 3\nmu_bar &lt;- benchmark_multiple * t(mvp_weights) %*% mu\nC &lt;- as.numeric(t(iota) %*% sigma_inv %*% iota)\nD &lt;- as.numeric(t(iota) %*% sigma_inv %*% mu)\nE &lt;- as.numeric(t(mu) %*% sigma_inv %*% mu)\nlambda_tilde &lt;- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nefp_weights &lt;- mvp_weights +\n  lambda_tilde / 2 * (sigma_inv %*% mu - D * mvp_weights)",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#the-efficient-frontier",
    "href": "r/introduction-to-tidy-finance.html#the-efficient-frontier",
    "title": "Introduction to Tidy Finance",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n The mutual fund separation theorem states that as soon as we have two efficient portfolios (such as the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and the efficient portfolio for a higher required level of expected returns \\(\\omega_\\text{eff}(\\bar{\\mu})\\), we can characterize the entire efficient frontier by combining these two portfolios. That is, any linear combination of the two portfolio weights will again represent an efficient portfolio. The code below implements the construction of the efficient frontier, which characterizes the highest expected return achievable at each level of risk. To understand the code better, make sure to familiarize yourself with the inner workings of the for loop.\n\nlength_year &lt;- 12\na &lt;- seq(from = -0.4, to = 1.9, by = 0.01)\nres &lt;- tibble(\n  a = a,\n  mu = NA,\n  sd = NA\n)\nfor (i in seq_along(a)) {\n  w &lt;- (1 - a[i]) * mvp_weights + (a[i]) * efp_weights\n  res$mu[i] &lt;- length_year * t(w) %*% mu   \n  res$sd[i] &lt;- sqrt(length_year) * sqrt(t(w) %*% sigma %*% w)\n}\n\nThe code above proceeds in two steps: First, we compute a vector of combination weights \\(a\\) and then we evaluate the resulting linear combination with \\(a\\in\\mathbb{R}\\):\n\\[\\omega^* = a\\omega_\\text{eff}(\\bar\\mu) + (1-a)\\omega_\\text{mvp} = \\omega_\\text{mvp} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right)\\] with \\(\\lambda^* = 2\\frac{a\\bar\\mu + (1-a)\\tilde\\mu - D/C}{E-D^2/C}\\) where \\(C = \\iota'\\Sigma^{-1}\\iota\\), \\(D=\\iota'\\Sigma^{-1}\\mu\\), and \\(E=\\mu'\\Sigma^{-1}\\mu\\). Finally, it is simple to visualize the efficient frontier alongside the two efficient portfolios within one powerful figure using ggplot (see Figure 6). We also add the individual stocks in the same call. We compute annualized returns based on the simple assumption that monthly returns are independent and identically distributed. Thus, the average annualized return is just 12 times the expected monthly return.\n\nres |&gt;\n  ggplot(aes(x = sd, y = mu)) +\n  geom_point() +\n  geom_point(\n    data = res |&gt; filter(a %in% c(0, 1)),\n    size = 4\n  ) +\n  geom_point(\n    data = tibble(\n      mu = length_year * mu,       \n      sd = sqrt(length_year) * sqrt(diag(sigma))\n    ),\n    aes(y = mu, x = sd), size = 1\n  ) +\n  labs(\n    x = \"Annualized standard deviation\",\n    y = \"Annualized expected return\",\n    title = \"Efficient frontier for DOW index constituents\"\n  ) +\n  scale_x_continuous(labels = percent) +\n  scale_y_continuous(labels = percent)\n\n\n\n\n\n\n\nFigure 6: The big dots indicate the location of the minimum variance and the efficient portfolio that delivers 3 times the expected return of the minimum variance portfolio, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe line in Figure 6 indicates the efficient frontier: the set of portfolios a mean-variance efficient investor would choose from. Compare the performance relative to the individual assets (the dots) - it should become clear that diversifying yields massive performance gains (at least as long as we take the parameters \\(\\Sigma\\) and \\(\\mu\\) as given).",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#exercises",
    "href": "r/introduction-to-tidy-finance.html#exercises",
    "title": "Introduction to Tidy Finance",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily prices for another stock market symbol of your choice from Yahoo!Finance with tq_get() from the tidyquant package. Plot two time series of the ticker’s un-adjusted and adjusted closing prices. Explain the differences.\nCompute daily net returns for an asset of your choice and visualize the distribution of daily returns in a histogram using 100 bins. Also, use geom_vline() to add a dashed red vertical line that indicates the 5 percent quantile of the daily returns. Compute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns.\nTake your code from before and generalize it such that you can perform all the computations for an arbitrary vector of tickers (e.g., ticker &lt;- c(\"AAPL\", \"MMM\", \"BA\")). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nAre days with high aggregate trading volume often also days with large absolute returns? Find an appropriate visualization to analyze the question using the ticker AAPL. 1.Compute monthly returns from the downloaded stock market prices. Compute the vector of historical average returns and the sample variance-covariance matrix. Compute the minimum variance portfolio weights and the portfolio volatility and average returns. Visualize the mean-variance efficient frontier. Choose one of your assets and identify the portfolio which yields the same historical volatility but achieves the highest possible average return.\nIn the portfolio choice analysis, we restricted our sample to all assets trading every day since 2000. How is such a decision a problem when you want to infer future expected portfolio performance from the results?\nThe efficient frontier characterizes the portfolios with the highest expected return for different levels of risk. Identify the portfolio with the highest expected return per standard deviation. Which famous performance measure is close to the ratio of average returns to the standard deviation of returns?\n\n\n\n\nFigure 1: Prices are in USD, adjusted for dividend payments and stock splits.\nFigure 2: The dotted vertical line indicates the historical 5 percent quantile.\nFigure 3: Prices in USD, adjusted for dividend payments and stock splits.\nFigure 4: Total daily trading volume in billion USD.\nFigure 5: Total daily trading volume in billion USD.\nFigure 6: The big dots indicate the location of the minimum variance and the efficient portfolio that delivers 3 times the expected return of the minimum variance portfolio, respectively. The small dots indicate the location of the individual constituents.",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "r/hex-sticker.html",
    "href": "r/hex-sticker.html",
    "title": "Hex Sticker",
    "section": "",
    "text": "library(hexSticker)\n\nsticker(\"images/logo-website.png\", \n        package = \"Tidy Finance\", \n        p_size = 20, p_color = \"black\",\n        s_x = 1, s_y = 0.75, s_width = 0.7, s_height = 0.7, asp = 0.9,\n        h_color = \"#3b9ab2\",\n        h_fill = \"white\",\n        url = \"tidy-finance.org\",\n        filename = \"images/hex-sticker.png\")\n\n\n\n\nTidy Finance HEX Sticker\n\n\n\n\n\nTidy Finance HEX Sticker",
    "crumbs": [
      "R",
      "Appendix",
      "Hex Sticker"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html",
    "href": "r/fama-macbeth-regressions.html",
    "title": "Fama-MacBeth Regressions",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we present a simple implementation of Fama and MacBeth (1973), a regression approach commonly called Fama-MacBeth regressions. Fama-MacBeth regressions are widely used in empirical asset pricing studies. We use individual stocks as test assets to estimate the risk premium associated with the three factors included in Fama and French (1993).\nResearchers use the two-stage regression approach to estimate risk premiums in various markets, but predominately in the stock market. Essentially, the two-step Fama-MacBeth regressions exploit a linear relationship between expected returns and exposure to (priced) risk factors. The basic idea of the regression approach is to project asset returns on factor exposures or characteristics that resemble exposure to a risk factor in the cross-section in each time period. Then, in the second step, the estimates are aggregated across time to test if a risk factor is priced. In principle, Fama-MacBeth regressions can be used in the same way as portfolio sorts introduced in previous chapters.\nThe Fama-MacBeth procedure is a simple two-step approach: The first step uses the exposures (characteristics) as explanatory variables in \\(T\\) cross-sectional regressions. For example, if \\(r_{i,t+1}\\) denote the excess returns of asset \\(i\\) in month \\(t+1\\), then the famous Fama-French three-factor model implies the following return generating process (see also Campbell et al. 1998): \\[\\begin{aligned}r_{i,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{i,t}  + \\lambda^{SMB}_t \\beta^{SMB}_{i,t} + \\lambda^{HML}_t \\beta^{HML}_{i,t} + \\epsilon_{i,t}.\\end{aligned}\\] Here, we are interested in the compensation \\(\\lambda^{f}_t\\) for the exposure to each risk factor \\(\\beta^{f}_{i,t}\\) at each time point, i.e., the risk premium. Note the terminology: \\(\\beta^{f}_{i,t}\\) is a asset-specific characteristic, e.g., a factor exposure or an accounting variable. If there is a linear relationship between expected returns and the characteristic in a given month, we expect the regression coefficient to reflect the relationship, i.e., \\(\\lambda_t^{f}\\neq0\\).\nIn the second step, the time-series average \\(\\frac{1}{T}\\sum_{t=1}^T \\hat\\lambda^{f}_t\\) of the estimates \\(\\hat\\lambda^{f}_t\\) can then be interpreted as the risk premium for the specific risk factor \\(f\\). We follow Zaffaroni and Zhou (2022) and consider the standard cross-sectional regression to predict future returns. If the characteristics are replaced with time \\(t+1\\) variables, then the regression approach captures risk attributes rather than risk premiums.\nBefore we move to the implementation, we want to highlight that the characteristics, e.g., \\(\\hat\\beta^{f}_{i}\\), are often estimated in a separate step before applying the actual Fama-MacBeth methodology. You can think of this as a step 0. You might thus worry that the errors of \\(\\hat\\beta^{f}_{i}\\) impact the risk premiums’ standard errors. Measurement error in \\(\\hat\\beta^{f}_{i}\\) indeed affects the risk premium estimates, i.e., they lead to biased estimates. The literature provides adjustments for this bias (see, e.g., Shanken 1992; Kim 1995; Chen, Lee, and Lee 2015, among others) but also shows that the bias goes to zero as \\(T \\to \\infty\\). We refer to Gagliardini, Ossola, and Scaillet (2016) for an in-depth discussion also covering the case of time-varying betas. Moreover, if you plan to use Fama-MacBeth regressions with individual stocks, Hou, Xue, and Zhang (2020) advocates using weighted-least squares to estimate the coefficients such that they are not biased toward small firms. Without this adjustment, the high number of small firms would drive the coefficient estimates.\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(sandwich)\nlibrary(broom)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#data-preparation",
    "href": "r/fama-macbeth-regressions.html#data-preparation",
    "title": "Fama-MacBeth Regressions",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe illustrate Fama and MacBeth (1973) with the monthly CRSP sample and use three characteristics to explain the cross-section of returns: market capitalization, the book-to-market ratio, and the CAPM beta (i.e., the covariance of the excess stock returns with the market excess returns). We collect the data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, gvkey, month, ret_excess, mktcap) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n  select(datadate, gvkey, be) |&gt;\n  collect()\n\nbeta &lt;- tbl(tidy_finance, \"beta\") |&gt;\n  select(month, permno, beta_monthly) |&gt;\n  collect()\n\nWe use the Compustat and CRSP data to compute the book-to-market ratio and the (log) market capitalization. Furthermore, we also use the CAPM betas based on monthly returns we computed in the previous chapters.\n\ncharacteristics &lt;- compustat |&gt;\n  mutate(month = floor_date(ymd(datadate), \"month\")) |&gt;\n  left_join(crsp_monthly, by = c(\"gvkey\", \"month\")) |&gt;\n  left_join(beta, by = c(\"permno\", \"month\")) |&gt;\n  transmute(gvkey,\n    bm = be / mktcap,\n    log_mktcap = log(mktcap),\n    beta = beta_monthly,\n    sorting_date = month %m+% months(6)\n  )\n\ndata_fama_macbeth &lt;- crsp_monthly |&gt;\n  left_join(characteristics, by = c(\"gvkey\", \"month\" = \"sorting_date\")) |&gt;\n  group_by(permno) |&gt;\n  arrange(month) |&gt;\n  fill(c(beta, bm, log_mktcap), .direction = \"down\") |&gt;\n  ungroup() |&gt;\n  left_join(crsp_monthly |&gt;\n    select(permno, month, ret_excess_lead = ret_excess) |&gt;\n    mutate(month = month %m-% months(1)),\n  by = c(\"permno\", \"month\")\n  ) |&gt;\n  select(permno, month, ret_excess_lead, beta, log_mktcap, bm) |&gt;\n  drop_na()",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#cross-sectional-regression",
    "href": "r/fama-macbeth-regressions.html#cross-sectional-regression",
    "title": "Fama-MacBeth Regressions",
    "section": "Cross-Sectional Regression",
    "text": "Cross-Sectional Regression\nNext, we run the cross-sectional regressions with the characteristics as explanatory variables for each month. We regress the returns of the test assets at a particular time point on the characteristics of each asset. By doing so, we get an estimate of the risk premiums \\(\\hat\\lambda^{f}_t\\) for each point in time. \n\nrisk_premiums &lt;- data_fama_macbeth |&gt;\n  nest(data = c(ret_excess_lead, beta, log_mktcap, bm, permno)) |&gt;\n  mutate(estimates = map(\n    data,\n    ~ tidy(lm(ret_excess_lead ~ beta + log_mktcap + bm, data = .x))\n  )) |&gt;\n  unnest(estimates)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#time-series-aggregation",
    "href": "r/fama-macbeth-regressions.html#time-series-aggregation",
    "title": "Fama-MacBeth Regressions",
    "section": "Time-Series Aggregation",
    "text": "Time-Series Aggregation\nNow that we have the risk premiums’ estimates for each period, we can average across the time-series dimension to get the expected risk premium for each characteristic. Similarly, we manually create the \\(t\\)-test statistics for each regressor, which we can then compare to usual critical values of 1.96 or 2.576 for two-tailed significance tests.\n\nprice_of_risk &lt;- risk_premiums |&gt;\n  group_by(factor = term) |&gt;\n  summarize(\n    risk_premium = mean(estimate) * 100,\n    t_statistic = mean(estimate) / sd(estimate) * sqrt(n())\n  )\n\nIt is common to adjust for autocorrelation when reporting standard errors of risk premiums. As in Univariate Portfolio Sorts, the typical procedure for this is computing Whitney K. Newey and West (1987) standard errors. We again recommend the data-driven approach of Whitney K. Newey and West (1994) using the NeweyWest() function, but note that you can enforce the typical 6 lag settings via NeweyWest(., lag = 6, prewhite = FALSE).\n\nregressions_for_newey_west &lt;- risk_premiums |&gt;\n  select(month, factor = term, estimate) |&gt;\n  nest(data = c(month, estimate)) |&gt;\n  mutate(\n    model = map(data, ~ lm(estimate ~ 1, .)),\n    mean = map(model, tidy)\n  )\n\nprice_of_risk_newey_west &lt;- regressions_for_newey_west |&gt;\n  mutate(newey_west_se = map_dbl(model, ~ sqrt(NeweyWest(.)))) |&gt;\n  unnest(mean) |&gt;\n  mutate(t_statistic_newey_west = estimate / newey_west_se) |&gt;\n  select(factor,\n    risk_premium = estimate,\n    t_statistic_newey_west\n  )\n\nleft_join(price_of_risk,\n  price_of_risk_newey_west |&gt;\n    select(factor, t_statistic_newey_west),\n  by = \"factor\"\n)\n\n# A tibble: 4 × 4\n  factor      risk_premium t_statistic t_statistic_newey_west\n  &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;                  &lt;dbl&gt;\n1 (Intercept)       1.24        4.87                   4.04  \n2 beta             -0.0101     -0.0973                -0.0877\n3 bm                0.150       3.29                   2.78  \n4 log_mktcap       -0.106      -2.99                  -2.64  \n\n\nFinally, let us interpret the results. Stocks with higher book-to-market ratios earn higher expected future returns, which is in line with the value premium. The negative value for log market capitalization reflects the size premium for smaller stocks. Consistent with results from earlier chapters, we detect no relation between beta and future stock returns.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#exercises",
    "href": "r/fama-macbeth-regressions.html#exercises",
    "title": "Fama-MacBeth Regressions",
    "section": "Exercises",
    "text": "Exercises\n\nDownload a sample of test assets from Kenneth French’s homepage and reevaluate the risk premiums for industry portfolios instead of individual stocks.\nUse individual stocks with weighted-least squares based on a firm’s size as suggested by Hou, Xue, and Zhang (2020). Then, repeat the Fama-MacBeth regressions without the weighting scheme adjustment but drop the smallest 20 percent of firms each month. Compare the results of the three approaches.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html",
    "href": "r/difference-in-differences.html",
    "title": "Difference in Differences",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we illustrate the concept of difference in differences (DD) estimators by evaluating the effects of climate change regulation on the pricing of bonds across firms. DD estimators are typically used to recover the treatment effects of natural or quasi-natural experiments that trigger sharp changes in the environment of a specific group. Instead of looking at differences in just one group (e.g., the effect in the treated group), DD investigates the treatment effects by looking at the difference between differences in two groups. Such experiments are usually exploited to address endogeneity concerns (e.g., Roberts and Whited 2013). The identifying assumption is that the outcome variable would change equally in both groups without the treatment. This assumption is also often referred to as the assumption of parallel trends. Moreover, we would ideally also want a random assignment to the treatment and control groups. Due to lobbying or other activities, this randomness is often violated in (financial) economics.\nIn the context of our setting, we investigate the impact of the Paris Agreement (PA), signed on December 12, 2015, on the bond yields of polluting firms. We first estimate the treatment effect of the agreement using panel regression techniques that we discuss in Fixed Effects and Clustered Standard Errors. We then present two methods to illustrate the treatment effect over time graphically. Although we demonstrate that the treatment effect of the agreement is anticipated by bond market participants well in advance, the techniques we present below can also be applied to many other settings.\nThe approach we use here replicates the results of Seltzer, Starks, and Zhu (2022) partly. Specifically, we borrow their industry definitions for grouping firms into green and brown types. Overall, the literature on environmental, social, and governance (ESG) effects in corporate bond markets is already large but continues to grow (for recent examples, see, e.g., Halling, Yu, and Zechner (2021), Handler, Jankowitsch, and Pasler (2022), Huynh and Xia (2021), among many others).\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(fixest)\nlibrary(broom)",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html#data-preparation",
    "href": "r/difference-in-differences.html#data-preparation",
    "title": "Difference in Differences",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use TRACE and Mergent FISD as data sources from our SQLite-database introduced in Accessing and Managing Financial Data and TRACE and FISD. \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfisd &lt;- tbl(tidy_finance, \"fisd\") |&gt;\n  select(complete_cusip, maturity, offering_amt, sic_code) |&gt;\n  collect() |&gt; \n  drop_na()\n\ntrace_enhanced &lt;- tbl(tidy_finance, \"trace_enhanced\") |&gt;\n  select(cusip_id, trd_exctn_dt, rptd_pr, entrd_vol_qt, yld_pt)|&gt;\n  collect() |&gt; \n  drop_na()\n\nWe start our analysis by preparing the sample of bonds. We only consider bonds with a time to maturity of more than one year to the signing of the PA, so that we have sufficient data to analyze the yield behavior after the treatment date. This restriction also excludes all bonds issued after the agreement. We also consider only the first two digits of the SIC industry code to identify the polluting industries (in line with Seltzer, Starks, and Zhu 2022).\n\ntreatment_date &lt;- ymd(\"2015-12-12\")\n\npolluting_industries &lt;- c(\n  49, 13, 45, 29, 28, 33, 40, 20,\n  26, 42, 10, 53, 32, 99, 37\n)\n\nbonds &lt;- fisd |&gt;\n  filter(offering_amt &gt; 0) |&gt; \n  mutate(\n    time_to_maturity = as.numeric(maturity - treatment_date) / 365,\n    sic_code = as.integer(substr(sic_code, 1, 2)),\n    log_offering_amt = log(offering_amt)\n  ) |&gt;\n  filter(time_to_maturity &gt;= 1) |&gt;\n  select(\n    cusip_id = complete_cusip,\n    time_to_maturity, log_offering_amt, sic_code\n  ) |&gt;\n  mutate(polluter = sic_code %in% polluting_industries)\n\nNext, we aggregate the individual transactions as reported in TRACE to a monthly panel of bond yields. We consider bond yields for a bond’s last trading day in a month. Therefore, we first aggregate bond data to daily frequency and apply common restrictions from the literature (see, e.g., Bessembinder et al. 2008). We weigh each transaction by volume to reflect a trade’s relative importance and avoid emphasizing small trades. Moreover, we only consider transactions with reported prices rptd_pr larger than 25 (to exclude bonds that are close to default) and only bond-day observations with more than five trades on a corresponding day (to exclude prices based on too few, potentially non-representative transactions). \n\ntrace_aggregated &lt;- trace_enhanced |&gt;\n  filter(rptd_pr &gt; 25) |&gt;\n  group_by(cusip_id, trd_exctn_dt) |&gt;\n  summarize(\n    avg_yield = weighted.mean(yld_pt, entrd_vol_qt * rptd_pr),\n    trades = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  drop_na(avg_yield) |&gt;\n  filter(trades &gt;= 5) |&gt;\n  mutate(month = floor_date(trd_exctn_dt, \"months\")) |&gt;\n  group_by(cusip_id, month) |&gt;\n  slice_max(trd_exctn_dt) |&gt;\n  ungroup() |&gt;\n  select(cusip_id, month, avg_yield)\n\nBy combining the bond-specific information from Mergent FISD for our bond sample with the aggregated TRACE data, we arrive at the main sample for our analysis.\n\nbonds_panel &lt;- bonds |&gt;\n  inner_join(trace_aggregated, join_by(cusip_id), multiple = \"all\") |&gt;\n  drop_na()\n\nBefore we can run the first regression, we need to define the treated indicator, which is the product of the post_period (i.e., all months after the signing of the PA) and the polluter indicator defined above.\n\nbonds_panel &lt;- bonds_panel |&gt;\n  mutate(post_period = month &gt;= floor_date(treatment_date, \"months\")) |&gt;\n  mutate(treated = polluter & post_period)\n\nAs usual, we tabulate summary statistics of the variables that enter the regression to check the validity of our variable definitions.\n\nbonds_panel |&gt;\n  pivot_longer(\n    cols = c(avg_yield, time_to_maturity, log_offering_amt),\n    names_to = \"measure\"\n  ) |&gt;\n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 9\n  measure           mean    sd    min   q05   q50   q95   max      n\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;\n1 avg_yield         4.08 4.21  0.0595  1.27  3.38  8.10 128.  127530\n2 log_offering_amt 13.3  0.823 4.64   12.2  13.2  14.5   16.5 127530\n3 time_to_maturity  8.55 8.41  1.01    1.50  5.81 27.4  101.  127530",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html#panel-regressions",
    "href": "r/difference-in-differences.html#panel-regressions",
    "title": "Difference in Differences",
    "section": "Panel Regressions",
    "text": "Panel Regressions\nThe PA is a legally binding international treaty on climate change. It was adopted by 196 parties at COP 21 in Paris on December 12, 2015 and entered into force on November 4, 2016. The PA obliges developed countries to support efforts to build clean, climate-resilient futures. One may thus hypothesize that adopting climate-related policies may affect financial markets. To measure the magnitude of this effect, we first run an ordinary least square (OLS) regression without fixed effects where we include the treated, post_period, and polluter dummies, as well as the bond-specific characteristics log_offering_amt and time_to_maturity. This simple model assumes that there are essentially two periods (before and after the PA) and two groups (polluters and non-polluters). Nonetheless, it should indicate whether polluters have higher yields following the PA compared to non-polluters.\nThe second model follows the typical DD regression approach by including individual (cusip_id) and time (month) fixed effects. In this model, we do not include any other variables from the simple model because the fixed effects subsume them, and we observe the coefficient of our main variable of interest: treated.\n\nmodel_without_fe &lt;- feols(\n  fml = avg_yield ~ treated + post_period + polluter +\n    log_offering_amt + time_to_maturity,\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\nmodel_with_fe &lt;- feols(\n  fml = avg_yield ~ treated | cusip_id + month,\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\netable(\n  model_without_fe, model_with_fe, \n  coefstat = \"tstat\", digits = 3, digits.stats = 3\n)\n\n                  model_without_fe   model_with_fe\nDependent Var.:          avg_yield       avg_yield\n                                                  \nConstant            10.7*** (57.0)                \ntreatedTRUE        0.462*** (9.31) 0.983*** (29.5)\npost_periodTRUE  -0.174*** (-5.92)                \npolluterTRUE       0.481*** (15.3)                \nlog_offering_amt -0.551*** (-39.0)                \ntime_to_maturity   0.058*** (41.6)                \nFixed-Effects:   ----------------- ---------------\ncusip_id                        No             Yes\nmonth                           No             Yes\n________________ _________________ _______________\nVCOV type                      IID             IID\nObservations               127,530         127,530\nR2                           0.032           0.647\nWithin R2                       --           0.007\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth models indicate that polluters have significantly higher yields after the PA than non-polluting firms. Note that the magnitude of the treated coefficient varies considerably across models.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html#visualizing-parallel-trends",
    "href": "r/difference-in-differences.html#visualizing-parallel-trends",
    "title": "Difference in Differences",
    "section": "Visualizing Parallel Trends",
    "text": "Visualizing Parallel Trends\nEven though the regressions above indicate that there is an impact of the PA on bond yields of polluters, the tables do not tell us anything about the dynamics of the treatment effect. In particular, the models provide no indication about whether the crucial parallel trends assumption is valid. This assumption requires that in the absence of treatment, the difference between the two groups is constant over time. Although there is no well-defined statistical test for this assumption, visual inspection typically provides a good indication.\nTo provide such visual evidence, we revisit the simple OLS model and replace the treated and post_period indicators with month dummies for each group. This approach estimates the average yield change of both groups for each period and provides corresponding confidence intervals. Plotting the coefficient estimates for both groups around the treatment date shows us the dynamics of our panel data.\n\nmodel_without_fe_time &lt;- feols(\n  fml = avg_yield ~ polluter + month:polluter +\n    time_to_maturity + log_offering_amt,\n  vcov = \"iid\",\n  data = bonds_panel |&gt;\n    mutate(month = factor(month))\n)\n\nmodel_without_fe_coefs &lt;- tidy(model_without_fe_time) |&gt;\n  filter(str_detect(term, \"month\")) |&gt;\n  mutate(\n    month = ymd(substr(term, nchar(term) - 9, nchar(term))),\n    treatment = str_detect(term, \"TRUE\"),\n    ci_up = estimate + qnorm(0.975) * std.error,\n    ci_low = estimate + qnorm(0.025) * std.error\n  )\n\nmodel_without_fe_coefs |&gt;\n  ggplot(aes(\n    month, \n    color = treatment,\n    linetype = treatment,\n    shape = treatment\n    )) +\n  geom_vline(aes(xintercept = floor_date(treatment_date, \"month\")),\n    linetype = \"dashed\"\n  ) +\n  geom_hline(aes(yintercept = 0),\n    linetype = \"dashed\"\n  ) +\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_up),\n    alpha = 0.5\n  ) +\n  guides(linetype = \"none\") + \n  geom_point(aes(y = estimate)) +\n  labs(\n    x = NULL,\n    y = \"Yield\",\n    shape = \"Polluter?\",\n    color = \"Polluter?\",\n    title = \"Polluters respond stronger to Paris Agreement than green firms\"\n  )\n\n\n\n\n\n\n\nFigure 1: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters and non-polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\n\nFigure 1 shows that throughout most of 2014, the yields of the two groups changed in unison. However, starting at the end of 2014, the yields start to diverge, reaching the highest difference around the signing of the PA. Afterward, the yields for both groups fall again, and the polluters arrive at the same level as at the beginning of 2014. The non-polluters, on the other hand, even experience significantly lower yields than polluters after the signing of the agreement.\nInstead of plotting both groups using the simple model approach, we can also use the fixed-effects model and focus on the polluter’s yield response to the signing relative to the non-polluters. To perform this estimation, we need to replace the treated indicator with separate time dummies for the polluters, each marking a one-month period relative to the treatment date. We then regress the monthly yields on the set of time dummies and cusip_id and month fixed effects.\n\nbonds_panel_alt &lt;- bonds_panel |&gt;\n  mutate(\n    diff_to_treatment = interval(\n      floor_date(treatment_date, \"month\"), month\n    ) %/% months(1)\n  )\n\nvariables &lt;- bonds_panel_alt |&gt;\n  distinct(diff_to_treatment, month) |&gt;\n  arrange(month) |&gt;\n  mutate(variable_name = as.character(NA))\n\nformula &lt;- \"avg_yield ~ \"\n\nfor (j in 1:nrow(variables)) {\n  if (variables$diff_to_treatment[j] != 0) {\n    old_names &lt;- names(bonds_panel_alt)\n    bonds_panel_alt &lt;- bonds_panel_alt |&gt;\n      mutate(new_var = diff_to_treatment == variables$diff_to_treatment[j] & \n               polluter)\n    new_var_name &lt;- ifelse(variables$diff_to_treatment[j] &lt; 0,\n      str_c(\"lag\", abs(variables$diff_to_treatment[j])),\n      str_c(\"lead\", variables$diff_to_treatment[j])\n    )\n    variables$variable_name[j] &lt;- new_var_name\n    names(bonds_panel_alt) &lt;- c(old_names, new_var_name)\n    formula &lt;- str_c(\n      formula,\n      ifelse(j == 1,\n        new_var_name,\n        str_c(\"+\", new_var_name)\n      )\n    )\n  }\n}\nformula &lt;- str_c(formula, \"| cusip_id + month\")\n\nmodel_with_fe_time &lt;- feols(\n  fml = as.formula(formula),\n  vcov = \"iid\",\n  data = bonds_panel_alt\n)\n\nmodel_with_fe_time_coefs &lt;- tidy(model_with_fe_time) |&gt;\n  mutate(\n    term = str_remove(term, \"TRUE\"),\n    ci_up = estimate + qnorm(0.975) * std.error,\n    ci_low = estimate + qnorm(0.025) * std.error\n  ) |&gt;\n  left_join(\n    variables,\n    join_by(term == variable_name)\n  ) |&gt;\n  bind_rows(tibble(\n    term = \"lag0\",\n    estimate = 0,\n    ci_up = 0,\n    ci_low = 0,\n    month = floor_date(treatment_date, \"month\")\n  ))\n\nmodel_with_fe_time_coefs |&gt;\n  ggplot(aes(x = month, y = estimate)) +\n  geom_vline(aes(xintercept = floor_date(treatment_date, \"month\")),\n    linetype = \"dashed\"\n  ) +\n  geom_hline(aes(yintercept = 0),\n    linetype = \"dashed\"\n  ) +\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_up),\n    alpha = 0.5\n  ) +\n  geom_point(aes(y = estimate)) +\n  labs(\n    x = NULL,\n    y = \"Yield\",\n    title = \"Polluters' yield patterns around Paris Agreement signing\"\n  )\n\n\n\n\n\n\n\nFigure 2: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\n\n The resulting graph shown in Figure 2 confirms the main conclusion of the previous image: polluters’ yield patterns show a considerable anticipation effect starting toward the end of 2014. Yields only marginally increase after the signing of the agreement. However, as opposed to the simple model, we do not see a complete reversal back to the pre-agreement level. Yields of polluters stay at a significantly higher level even one year after the signing.\nNotice that during the year after the PA was signed, Donald Trump, the 45th president of the United States, was elected on November 8, 2016. During his campaign there were some indications of intentions to withdraw the US from the PA, which ultimately happened on November 4, 2020. Hence, reversal effects are potentially driven by these actions.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/difference-in-differences.html#exercises",
    "href": "r/difference-in-differences.html#exercises",
    "title": "Difference in Differences",
    "section": "Exercises",
    "text": "Exercises\n\nThe 46th President of the US, Joe Biden, rejoined the Paris Agreement on February 19, 2021. Repeat the difference in differences analysis for the day of his election victory. Note that you will also have to download new TRACE data. How did polluters’ yields react to this action?\nBased on the exercise on ratings in TRACE and FISD, include ratings as a control variable in the analysis above. Do the results change?\n\n\n\n\nFigure 1: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters and non-polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\nFigure 2: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html",
    "href": "r/constrained-optimization-and-backtesting.html",
    "title": "Constrained Optimization and Backtesting",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we conduct portfolio backtesting in a realistic setting by including transaction costs and investment constraints such as no-short-selling rules. We start with standard mean-variance efficient portfolios and introduce constraints in a step-by-step manner. To do so, we rely on numerical optimization procedures in R. We conclude the chapter by providing an out-of-sample backtesting procedure for the different strategies that we introduce in this chapter.\nThroughout this chapter, we use the following R packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(nloptr)\nCompared to previous chapters, we introduce the nloptr package (Johnson 2007) to perform numerical constrained optimization for portfolio choice problems.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#data-preparation",
    "href": "r/constrained-optimization-and-backtesting.html#data-preparation",
    "title": "Constrained Optimization and Backtesting",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start by loading the required data from our SQLite-database introduced in Accessing and Managing Financial Data. For simplicity, we restrict our investment universe to the monthly Fama-French industry portfolio returns in the following application. \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nindustry_returns &lt;- tbl(tidy_finance, \"industries_ff_monthly\") |&gt;\n  select(-month) |&gt;\n  collect() |&gt; \n  drop_na()",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "href": "r/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Recap of Portfolio Choice",
    "text": "Recap of Portfolio Choice\nA common objective for portfolio optimization is to find mean-variance efficient portfolio weights, i.e., the allocation which delivers the lowest possible return variance for a given minimum level of expected returns. In the most extreme case, where the investor is only concerned about portfolio variance, they may choose to implement the minimum variance portfolio (MVP) weights which are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1\\] where \\(\\Sigma\\) is the \\((N \\times N)\\) covariance matrix of the returns. The optimal weights \\(\\omega_\\text{mvp}\\) can be found analytically and are \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). In terms of code, the math is equivalent to the following chunk. \n\nn_industries &lt;- ncol(industry_returns)\n\nSigma &lt;- cov(industry_returns)\nw_mvp &lt;- solve(Sigma) %*% rep(1, n_industries)\nw_mvp &lt;- as.vector(w_mvp / sum(w_mvp))\n\nmu &lt;- colMeans(industry_returns)\n\nNext, consider an investor who aims to achieve minimum variance given a required expected portfolio return \\(\\bar{\\mu}\\) such that she chooses \\[\\omega_\\text{eff}({\\bar{\\mu}}) =\\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] We leave it as an exercise below to show that the portfolio choice problem can equivalently be formulated for an investor with mean-variance preferences and risk aversion factor \\(\\gamma\\). That means the investor aims to choose portfolio weights as the solution to \\[ \\omega^*_\\gamma = \\arg\\max \\omega' \\mu - \\frac{\\gamma}{2}\\omega'\\Sigma \\omega\\quad \\text{ s.t. } \\omega'\\iota = 1.\\] The solution to the optimal portfolio choice problem is: \\[\\omega^*_{\\gamma}  = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu  + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota.\\] Empirically, this classical solution imposes many problems. In particular, the estimates of \\(\\mu\\) are noisy over short horizons, the (\\(N \\times N\\)) matrix \\(\\Sigma\\) contains \\(N(N-1)/2\\) distinct elements and thus, estimation error is huge. Seminal papers on the effect of ignoring estimation uncertainty, among others, are Brown (1976), Jobson and Korkie (1980), Jorion (1986), and Chopra and Ziemba (1993).\nEven worse, if the asset universe contains more assets than available time periods \\((N &gt; T)\\), the sample covariance matrix is no longer positive definite such that the inverse \\(\\Sigma^{-1}\\) does not exist anymore. To address estimation issues for vast-dimensional covariance matrices, regularization techniques are a popular tool (see, e.g., Ledoit and Wolf 2003, 2004, 2012; Fan, Fan, and Lv 2008).\nWhile the uncertainty associated with estimated parameters is challenging, the data-generating process is also unknown to the investor. In other words, model uncertainty reflects that it is ex-ante not even clear which parameters require estimation (for instance, if returns are driven by a factor model, selecting the universe of relevant factors imposes model uncertainty). Wang (2005) and Garlappi, Uppal, and Wang (2007) provide theoretical analysis on optimal portfolio choice under model and estimation uncertainty. In the most extreme case, Pflug, Pichler, and Wozabal (2012) shows that the naive portfolio which allocates equal wealth to all assets is the optimal choice for an investor averse to model uncertainty.\nOn top of the estimation uncertainty, transaction costs are a major concern. Rebalancing portfolios is costly, and, therefore, the optimal choice should depend on the investor’s current holdings. In the presence of transaction costs, the benefits of reallocating wealth may be smaller than the costs associated with turnover. This aspect has been investigated theoretically, among others, for one risky asset by Magill and Constantinides (1976) and Davis and Norman (1990). Subsequent extensions to the case with multiple assets have been proposed by Balduzzi and Lynch (1999) and Balduzzi and Lynch (2000). More recent papers on empirical approaches which explicitly account for transaction costs include Gârleanu and Pedersen (2013), and DeMiguel, Nogales, and Uppal (2014), and DeMiguel, Martín-Utrera, and Nogales (2015).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "href": "r/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "title": "Constrained Optimization and Backtesting",
    "section": "Estimation Uncertainty and Transaction Costs",
    "text": "Estimation Uncertainty and Transaction Costs\nThe empirical evidence regarding the performance of a mean-variance optimization procedure in which you simply plug in some sample estimates \\(\\hat \\mu\\) and \\(\\hat \\Sigma\\) can be summarized rather briefly: mean-variance optimization performs poorly! The literature discusses many proposals to overcome these empirical issues. For instance, one may impose some form of regularization of \\(\\Sigma\\), rely on Bayesian priors inspired by theoretical asset pricing models (Kan and Zhou 2007) or use high-frequency data to improve forecasting (Hautsch, Kyj, and Malec 2015). One unifying framework that works easily, effectively (even for large dimensions), and is purely inspired by economic arguments is an ex-ante adjustment for transaction costs (Hautsch and Voigt 2019).\nAssume that returns are from a multivariate normal distribution with mean \\(\\mu\\) and variance-covariance matrix \\(\\Sigma\\), \\(N(\\mu,\\Sigma)\\). Additionally, we assume quadratic transaction costs which penalize rebalancing such that \\[\n\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) = \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\\end{aligned}\\] with cost parameter \\(\\beta&gt;0\\) and \\(\\omega_{t^+} = {\\omega_t \\circ  (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\). \\(\\omega_{t^+}\\) denotes the portfolio weights just before rebalancing. Note that \\(\\omega_{t^+}\\) differs mechanically from \\(\\omega_t\\) due to the returns in the past period. Intuitively, transaction costs penalize portfolio performance when the portfolio is shifted from the current holdings \\(\\omega_{t^+}\\) to a new allocation \\(\\omega_{t+1}\\). In this setup, transaction costs do not increase linearly. Instead, larger rebalancing is penalized more heavily than small adjustments. Then, the optimal portfolio choice for an investor with mean variance preferences is \\[\\begin{aligned}\\omega_{t+1} ^* &=  \\arg\\max \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\text{ s.t. } \\iota'\\omega = 1\\\\\n&=\\arg\\max\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t.} \\iota'\\omega=1,\\end{aligned}\\] where \\[\\mu^*=\\mu+\\beta \\omega_{t^+} \\quad  \\text{and} \\quad \\Sigma^*=\\Sigma + \\frac{\\beta}{\\gamma} I_N.\\] As a result, adjusting for transaction costs implies a standard mean-variance optimal portfolio choice with adjusted return parameters \\(\\Sigma^*\\) and \\(\\mu^*\\): \\[\\omega^*_{t+1} = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^*  + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota.\\]\nAn alternative formulation of the optimal portfolio can be derived as follows: \\[\\omega_{t+1} ^*=\\arg\\max\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t. } \\iota'\\omega=1.\\] The optimal weights correspond to a mean-variance portfolio, where the vector of expected returns is such that assets that currently exhibit a higher weight are considered as delivering a higher expected return.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "href": "r/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Optimal Portfolio Choice",
    "text": "Optimal Portfolio Choice\nThe function below implements the efficient portfolio weight in its general form, allowing for transaction costs (conditional on the holdings before reallocation). For \\(\\beta=0\\), the computation resembles the standard mean-variance efficient framework. gamma denotes the coefficient of risk aversion \\(\\gamma\\), beta is the transaction cost parameter \\(\\beta\\) and w_prev are the weights before rebalancing \\(\\omega_{t^+}\\).\n\ncompute_efficient_weight &lt;- function(Sigma,\n                                     mu,\n                                     gamma = 2,\n                                     beta = 0, # transaction costs\n                                     w_prev = rep(\n                                       1 / ncol(Sigma),\n                                       ncol(Sigma)\n                                     )) {\n  iota &lt;- rep(1, ncol(Sigma))\n  Sigma_processed &lt;- Sigma + beta / gamma * diag(ncol(Sigma))\n  mu_processed &lt;- mu + beta * w_prev\n\n  Sigma_inverse &lt;- solve(Sigma_processed)\n\n  w_mvp &lt;- Sigma_inverse %*% iota\n  w_mvp &lt;- as.vector(w_mvp / sum(w_mvp))\n  w_opt &lt;- w_mvp + 1 / gamma *\n    (Sigma_inverse - w_mvp %*% t(iota) %*% Sigma_inverse) %*%\n      mu_processed\n  \n  return(as.vector(w_opt))\n}\n\nw_efficient &lt;- compute_efficient_weight(Sigma, mu)\nround(w_efficient, 3)\n\n [1]  1.630  0.090 -1.356  0.687  0.333 -0.412  0.504  0.402 -0.219\n[10] -0.659\n\n\nThe portfolio weights above indicate the efficient portfolio for an investor with risk aversion coefficient \\(\\gamma=2\\) in absence of transaction costs. Some of the positions are negative which implies short-selling, most of the positions are rather extreme. For instance, a position of \\(-1\\) implies that the investor takes a short position worth their entire wealth to lever long positions in other assets. What is the effect of transaction costs or different levels of risk aversion on the optimal portfolio choice? The following few lines of code analyze the distance between the minimum variance portfolio and the portfolio implemented by the investor for different values of the transaction cost parameter \\(\\beta\\) and risk aversion \\(\\gamma\\).\n\ngammas &lt;- c(2, 4, 8, 20)\nbetas &lt;- 20 * qexp((1:99) / 100)\n                   \ntransaction_costs &lt;- expand_grid(\n  gamma = gammas,\n  beta = betas\n) |&gt;\n  mutate(\n    weights = map2(\n      .x = gamma,\n      .y = beta,\n      ~ compute_efficient_weight(Sigma,\n        mu,\n        gamma = .x,\n        beta = .y / 10000,\n        w_prev = w_mvp\n      )\n    ),\n    concentration = map_dbl(weights, ~ sum(abs(. - w_mvp)))\n  )\n\nThe code chunk above computes the optimal weight in presence of transaction cost for different values of \\(\\beta\\) and \\(\\gamma\\) but with the same initial allocation, the theoretical optimal minimum variance portfolio. Starting from the initial allocation, the investor chooses their optimal allocation along the efficient frontier to reflect their own risk preferences. If transaction costs would be absent, the investor would simply implement the mean-variance efficient allocation. If transaction costs make it costly to rebalance, their optimal portfolio choice reflects a shift toward the efficient portfolio, whereas their current portfolio anchors their investment.\n\ntransaction_costs |&gt;\n  mutate(risk_aversion = as_factor(gamma)) |&gt;\n  ggplot(aes(\n    x = beta,\n    y = concentration,\n    color = risk_aversion,\n    linetype = risk_aversion\n  )) +\n  geom_line() +\n  guides(linetype = \"none\") + \n  labs(\n    x = \"Transaction cost parameter\",\n    y = \"Distance from MVP\",\n    color = \"Risk aversion\",\n    title = \"Portfolio weights for different risk aversion and transaction cost\"\n  )\n\n\n\n\n\n\n\nFigure 1: The horizontal axis indicates the distance from the empirical minimum variance portfolio weight, measured by the sum of the absolute deviations of the chosen portfolio from the benchmark.\n\n\n\n\n\nFigure 1 shows rebalancing from the initial portfolio (which we always set to the minimum variance portfolio weights in this example). The higher the transaction costs parameter \\(\\beta\\), the smaller is the rebalancing from the initial portfolio. In addition, if risk aversion \\(\\gamma\\) increases, the efficient portfolio is closer to the minimum variance portfolio weights such that the investor desires less rebalancing from the initial holdings.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#constrained-optimization",
    "href": "r/constrained-optimization-and-backtesting.html#constrained-optimization",
    "title": "Constrained Optimization and Backtesting",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\nNext, we introduce constraints to the above optimization procedure. Very often, typical constraints such as short-selling restrictions prevent analytical solutions for optimal portfolio weights (short-selling restrictions simply imply that negative weights are not allowed such that we require that \\(w_i \\geq 0\\quad \\forall i\\)). However, numerical optimization allows computing the solutions to such constrained problems.\nWe rely on the powerful nloptr package, which provides a common interface to a number of different optimization routines. In particular, we employ the Sequential Least-Squares Quadratic Programming (SLSQP) algorithm of Kraft (1994) because it is able to hand multiple equality and inequality constraints at the same time and typically used for problems where the objective function and the constraints are twice continuously differentiable. We hence have to provide the algorithm with the objective function and its gradient, as well as the constraints and their Jacobian.\nWe illustrate the use of the nloptr() function by replicating the analytical solutions for the minimum variance and efficient portfolio weights from above. Note that the equality constraint for both solutions is given by the requirement that the weights must sum up to one. In addition, we supply a vector of equal weights as an initial value for the algorithm in all applications. We verify that the output is equal to the above solution. Note that near() is a safe way to compare two vectors for pairwise equality. The alternative == is sensitive to small differences that may occur due to the representation of floating points on a computer, while near() has a built-in tolerance.\n\nw_initial &lt;- rep(1 / n_industries, n_industries)\n\nobjective_mvp &lt;- function(w) {\n  0.5 * t(w) %*% Sigma %*% w\n}\n\ngradient_mvp &lt;- function(w) {\n  Sigma %*% w\n}\n\nequality_constraint &lt;- function(w) {\n  sum(w) - 1\n}\n\njacobian_equality &lt;- function(w) {\n  rep(1, n_industries)\n}\n\noptions &lt;- list(\n  \"xtol_rel\"=1e-20, \n  \"algorithm\" = \"NLOPT_LD_SLSQP\", \n  \"maxeval\" = 10000\n)\n\nw_mvp_numerical &lt;- nloptr(\n  x0 = w_initial, \n  eval_f = objective_mvp, \n  eval_grad_f = gradient_mvp,\n  eval_g_eq = equality_constraint,\n  eval_jac_g_eq = jacobian_equality,\n  opts = options\n)\n\nall(near(w_mvp, w_mvp_numerical$solution))\n\n[1] TRUE\n\nobjective_efficient &lt;- function(w) {\n  2 * 0.5 * t(w) %*% Sigma %*% w - sum((1 + mu) * w)\n}\n\ngradient_efficient &lt;- function(w) {\n  2 * Sigma %*% w - (1 + mu)\n}\n\nw_efficient_numerical &lt;- nloptr(\n  x0 = w_initial, \n  eval_f = objective_efficient, \n  eval_grad_f = gradient_efficient,\n  eval_g_eq = equality_constraint, \n  eval_jac_g_eq = jacobian_equality,\n  opts = options\n)\n\nall(near(w_efficient, w_efficient_numerical$solution))\n\n[1] TRUE\n\n\nThe result above shows that indeed the numerical procedure recovered the optimal weights for a scenario, where we already know the analytic solution. For more complex optimization routines, R’s optimization task view provides an overview of the vast optimization landscape. \nNext, we approach problems where no analytical solutions exist. First, we additionally impose short-sale constraints, which implies \\(N\\) inequality constraints of the form \\(\\omega_i &gt;=0\\). We can implement the short-sale constraints by imposing a vector of lower bounds lb = rep(0, n_industries).\n\nw_no_short_sale &lt;- nloptr(\n  x0 = w_initial, \n  eval_f = objective_efficient, \n  eval_grad_f = gradient_efficient,\n  eval_g_eq = equality_constraint, \n  eval_jac_g_eq = jacobian_equality,\n  lb = rep(0, n_industries),\n  opts = options\n)\n\nround(w_no_short_sale$solution, 3)\n\n [1] 0.610 0.000 0.000 0.211 0.000 0.000 0.000 0.179 0.000 0.000\n\n\nAs expected, the resulting portfolio weights are all positive (up to numerical precision). Typically, the holdings in the presence of short-sale constraints are concentrated among way fewer assets than for the unrestricted case. You can verify that sum(w_no_short_sale$solution) returns 1. In other words, nloptr() provides the numerical solution to a portfolio choice problem for a mean-variance investor with risk aversion gamma = 2, where negative holdings are forbidden.\nnloptr() can also handle more complex problems. As an example, we show how to compute optimal weights, subject to the so-called Regulation-T constraint, which requires that the sum of all absolute portfolio weights is smaller than 1.5, that is \\(\\sum_{i=1}^N |\\omega_i| \\leq 1.5\\). The constraint enforces that a maximum of 50 percent of the allocated wealth can be allocated to short positions, thus implying an initial margin requirement of 50 percent. Imposing such a margin requirement reduces portfolio risks because extreme portfolio weights are not attainable anymore. The implementation of Regulation-T rules is numerically interesting because the margin constraints imply a non-linear constraint on the portfolio weights. \n\nreg_t &lt;- 1.5\n\ninequality_constraint &lt;- function(w) {\n  sum(abs(w)) - reg_t\n}\n\njacobian_inequality &lt;- function(w) {\n  sign(w)\n}\n\nobjective_reg_t &lt;- function(w) {\n  - t(w) %*% (1 + mu) +\n    2 * 0.5 * t(w) %*% Sigma %*% w\n}\n\ngradient_reg_t &lt;- function(w) {\n  - (1 + mu) + 2 * Sigma %*% w\n}\n\nw_reg_t &lt;- nloptr(\n  x0 = w_initial,\n  eval_f = objective_reg_t, \n  eval_grad_f = gradient_reg_t,\n  eval_g_eq = equality_constraint, \n  eval_jac_g_eq = jacobian_equality, \n  eval_g_ineq = inequality_constraint, \n  eval_jac_g_ineq = jacobian_inequality,\n  opts = options\n)\n\nround(w_reg_t$solution, 3)\n\n [1]  0.736  0.000 -0.135  0.264  0.000 -0.019  0.028  0.223  0.000\n[10] -0.096\n\n\nFigure 2 shows the optimal allocation weights across all 10 industries for the four different strategies considered so far: minimum variance, efficient portfolio with \\(\\gamma\\) = 2, efficient portfolio with short-sale constraints, and the Regulation-T constrained portfolio.\n\ntibble(\n  `No short-sale` = w_no_short_sale$solution,\n  `Minimum Variance` = w_mvp,\n  `Efficient portfolio` = compute_efficient_weight(Sigma, mu),\n  `Regulation-T` = w_reg_t$solution,\n  Industry = colnames(industry_returns)\n) |&gt;\n  pivot_longer(-Industry,\n    names_to = \"Strategy\",\n    values_to = \"weights\"\n  ) |&gt;\n  ggplot(aes(\n    fill = Strategy,\n    y = weights,\n    x = Industry\n  )) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  coord_flip() +\n  labs(\n    y = \"Allocation weight\", fill = NULL,\n    title = \"Optimal allocations for different strategies\"\n  ) +\n  scale_y_continuous(labels = percent)\n\n\n\n\n\n\n\nFigure 2: Optimal allocation weights for the 10 industry portfolios and the 4 different allocation strategies.\n\n\n\n\n\nThe results clearly indicate the effect of imposing additional constraints: the extreme holdings the investor implements if they follow the (theoretically optimal) efficient portfolio vanish under, e.g., the Regulation-T constraint. You may wonder why an investor would deviate from what is theoretically the optimal portfolio by imposing potentially arbitrary constraints. The short answer is: the efficient portfolio is only efficient if the true parameters of the data-generating process correspond to the estimated parameters \\(\\hat\\Sigma\\) and \\(\\hat\\mu\\). Estimation uncertainty may thus lead to inefficient allocations. By imposing restrictions, we implicitly shrink the set of possible weights and prevent extreme allocations, which could result from error-maximization due to estimation uncertainty (Jagannathan and Ma 2003).\nBefore we move on, we want to propose a final allocation strategy, which reflects a somewhat more realistic structure of transaction costs instead of the quadratic specification used above. The function below computes efficient portfolio weights while adjusting for transaction costs of the form \\(\\beta\\sum_{i=1}^N |(\\omega_{i, t+1} - \\omega_{i, t^+})|\\). No closed-form solution exists, and we rely on non-linear optimization procedures.\n\ncompute_efficient_weight_L1_TC &lt;- function(mu,\n                                           Sigma,\n                                           gamma,\n                                           beta,\n                                           initial_weights) {\n  objective &lt;- function(w) {\n    -t(w) %*% mu +\n      gamma / 2 * t(w) %*% Sigma %*% w +\n      (beta / 10000) / 2 * sum(abs(w - initial_weights))\n  }\n  \n  gradient &lt;- function(w) {\n    -mu + gamma * Sigma %*% w + \n      (beta / 10000) * 0.5 * sign(w - initial_weights)\n  }\n\n  w_optimal &lt;- nloptr(\n    x0 = initial_weights,\n    eval_f = objective, \n    eval_grad_f = gradient,\n    eval_g_eq = equality_constraint, \n    eval_jac_g_eq = jacobian_equality, \n    opts = options\n  )\n\n  return(w_optimal$solution)\n}",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "href": "r/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "title": "Constrained Optimization and Backtesting",
    "section": "Out-of-Sample Backtesting",
    "text": "Out-of-Sample Backtesting\nFor the sake of simplicity, we committed one fundamental error in computing portfolio weights above: we used the full sample of the data to determine the optimal allocation (Arnott, Harvey, and Markowitz 2019). To implement this strategy at the beginning of the 2000s, you will need to know how the returns will evolve until 2021. While interesting from a methodological point of view, we cannot evaluate the performance of the portfolios in a reasonable out-of-sample fashion. We do so next in a backtesting application for three strategies. For the backtest, we recompute optimal weights just based on past available data.\nThe few lines below define the general setup. We consider 120 periods from the past to update the parameter estimates before recomputing portfolio weights. Then, we update portfolio weights which is costly and affects the performance. The portfolio weights determine the portfolio return. A period later, the current portfolio weights have changed and form the foundation for transaction costs incurred in the next period. We consider three different competing strategies: the mean-variance efficient portfolio, the mean-variance efficient portfolio with ex-ante adjustment for transaction costs, and the naive portfolio, which allocates wealth equally across the different assets.\n\nwindow_length &lt;- 120\nperiods &lt;- nrow(industry_returns) - window_length\n\nbeta &lt;- 50\ngamma &lt;- 2\n\nperformance_values &lt;- matrix(NA,\n  nrow = periods,\n  ncol = 3\n)\ncolnames(performance_values) &lt;- c(\"raw_return\", \"turnover\", \"net_return\")\n\nperformance_values &lt;- list(\n  \"MV (TC)\" = performance_values,\n  \"Naive\" = performance_values,\n  \"MV\" = performance_values\n)\n\nw_prev_1 &lt;- w_prev_2 &lt;- w_prev_3 &lt;- rep(\n  1 / n_industries,\n  n_industries\n)\n\nWe also define two helper functions: one to adjust the weights due to returns and one for performance evaluation, where we compute realized returns net of transaction costs.\n\nadjust_weights &lt;- function(w, next_return) {\n  w_prev &lt;- 1 + w * next_return\n  as.numeric(w_prev / sum(as.vector(w_prev)))\n}\n\nevaluate_performance &lt;- function(w, w_previous, next_return, beta = 50) {\n  raw_return &lt;- as.matrix(next_return) %*% w\n  turnover &lt;- sum(abs(w - w_previous))\n  net_return &lt;- raw_return - beta / 10000 * turnover\n  c(raw_return, turnover, net_return)\n}\n\nThe following code chunk performs a rolling-window estimation, which we implement in a loop. In each period, the estimation window contains the returns available up to the current period. Note that we use the sample variance-covariance matrix and ignore the estimation of \\(\\hat\\mu\\) entirely, but you might use more advanced estimators in practice.\n\nfor (p in 1:periods) {\n  returns_window &lt;- industry_returns[p:(p + window_length - 1), ]\n  next_return &lt;- industry_returns[p + window_length, ] |&gt; as.matrix()\n\n  Sigma &lt;- cov(returns_window)\n  mu &lt;- 0 * colMeans(returns_window)\n\n  # Transaction-cost adjusted portfolio\n  w_1 &lt;- compute_efficient_weight_L1_TC(\n    mu = mu,\n    Sigma = Sigma,\n    beta = beta,\n    gamma = gamma,\n    initial_weights = w_prev_1\n  )\n\n  performance_values[[1]][p, ] &lt;- evaluate_performance(w_1,\n    w_prev_1,\n    next_return,\n    beta = beta\n  )\n\n  w_prev_1 &lt;- adjust_weights(w_1, next_return)\n\n  # Naive portfolio\n  w_2 &lt;- rep(1 / n_industries, n_industries)\n\n  performance_values[[2]][p, ] &lt;- evaluate_performance(\n    w_2,\n    w_prev_2,\n    next_return\n  )\n\n  w_prev_2 &lt;- adjust_weights(w_2, next_return)\n\n  # Mean-variance efficient portfolio (w/o transaction costs)\n  w_3 &lt;- compute_efficient_weight(\n    Sigma = Sigma,\n    mu = mu,\n    gamma = gamma\n  )\n\n  performance_values[[3]][p, ] &lt;- evaluate_performance(\n    w_3,\n    w_prev_3,\n    next_return\n  )\n\n  w_prev_3 &lt;- adjust_weights(w_3, next_return)\n}\n\nFinally, we get to the evaluation of the portfolio strategies net-of-transaction costs. Note that we compute annualized returns and standard deviations.\n\nperformance &lt;- lapply(\n  performance_values,\n  as_tibble\n) |&gt;\n  bind_rows(.id = \"strategy\")\n\nlength_year &lt;- 12\n\nperformance_table &lt;- performance |&gt;\n  group_by(Strategy = strategy) |&gt;\n  summarize(\n    Mean = length_year * mean(100 * net_return),\n    SD = sqrt(length_year) * sd(100 * net_return),\n    `Sharpe ratio` = if_else(Mean &gt; 0,\n      Mean / SD,\n      NA_real_\n    ),\n    Turnover = 100 * mean(turnover)\n  )\n\nperformance_table |&gt; \n  mutate(across(-Strategy, ~round(., 4)))\n\n# A tibble: 3 × 5\n  Strategy   Mean    SD `Sharpe ratio` Turnover\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1 MV       -0.899  12.6         NA      211.   \n2 MV (TC)  11.9    15.2          0.780    0    \n3 Naive    11.8    15.2          0.779    0.234\n\n\nThe results clearly speak against mean-variance optimization. Turnover is huge when the investor only considers their portfolio’s expected return and variance. Effectively, the mean-variance portfolio generates a negative annualized return after adjusting for transaction costs. At the same time, the naive portfolio turns out to perform very well. In fact, the performance gains of the transaction-cost adjusted mean-variance portfolio are small. The out-of-sample Sharpe ratio is slightly higher than for the naive portfolio. Note the extreme effect of turnover penalization on turnover: MV (TC) effectively resembles a buy-and-hold strategy which only updates the portfolio once the estimated parameters \\(\\hat\\mu_t\\) and \\(\\hat\\Sigma_t\\) indicate that the current allocation is too far away from the optimal theoretical portfolio.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#exercises",
    "href": "r/constrained-optimization-and-backtesting.html#exercises",
    "title": "Constrained Optimization and Backtesting",
    "section": "Exercises",
    "text": "Exercises\n\nConsider the portfolio choice problem for transaction-cost adjusted certainty equivalent maximization with risk aversion parameter \\(\\gamma\\) \\[\\omega_{t+1} ^* =  \\arg\\max_{\\omega \\in \\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega\\] where \\(\\Sigma\\) and \\(\\mu\\) are (estimators of) the variance-covariance matrix of the returns and the vector of expected returns. Assume for now that transaction costs are quadratic in rebalancing and proportional to stock illiquidity such that \\[\\nu_t\\left(\\omega, B\\right) = \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right)\\] where \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) is a diagonal matrix, where \\(ill_1, \\ldots, ill_N\\). Derive a closed-form solution for the mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based on the transaction cost specification above. Discuss the effect of illiquidity \\(ill_i\\) on the individual portfolio weights relative to an investor that myopically ignores transaction costs in their decision.\nUse the solution from the previous exercise to update the function compute_efficient_weight() such that you can compute optimal weights conditional on a matrix \\(B\\) with illiquidity measures.\nIllustrate the evolution of the optimal weights from the naive portfolio to the efficient portfolio in the mean-standard deviation diagram.\nIs it always optimal to choose the same \\(\\beta\\) in the optimization problem than the value used in evaluating the portfolio performance? In other words, can it be optimal to choose theoretically sub-optimal portfolios based on transaction cost considerations that do not reflect the actual incurred costs? Evaluate the out-of-sample Sharpe ratio after transaction costs for a range of different values of imposed \\(\\beta\\) values.\n\n\n\n\nFigure 1: The horizontal axis indicates the distance from the empirical minimum variance portfolio weight, measured by the sum of the absolute deviations of the chosen portfolio from the benchmark.\nFigure 2: Optimal allocation weights for the 10 industry portfolios and the 4 different allocation strategies.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "r/changelog.html",
    "href": "r/changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "You can find every single change in our commit history. We collect the most important changes for Tidy Finance with R in the list below.\n\nMay 15, 2024, Commit 2bb2e07: We added a new subsection about creating environment variables to Setting Up Your Environment.\nMay 15, 2024, Commit adccfc9: We updated the filters in CRSP download, so that correct historical information is used and daily and monthly data are aligned.\nApril 19, 2024, Commit d8c4de3: We updated to dbplyr version 2.5.0 and switched to the new I() instead of in_schema() syntax.\nMarch 4, 2024, Commit 6acb50b: We updated the download of monthly and daily CRSP data to the new official file format as distributed by WRDS, see additional information on the WRDS website.\nFeb 13, 2024, Commit 7871900: We updated the function for cleaning enhanced TRACE used in TRACE and FISD and shown in Appendix Clean Enhanced TRACE with R to reflect the correct time zone (i.e., New York, ET) and require less dependencies. We also updated the respective gist.\nFeb 13, 2024, Commit 5fce497: We removed the depedency on googledrive in Accessing and Managing Financial Data because frequently encountered failed downloads due to quota limits on the Google API.\nJan 4, 2024, Commit e9ab1a3: We updated the syntax of *_join() functions to use join_by() instead of by.\nDec 10, 2023, Commit 9814a2f: We added handling of delisting returns to daily CRSP download.\nOct 14, 2023 Commit b5a7495: We changed the download of daily CRSP data from individual stocks to batches in WRDS, CRSP, and Compustat.\nOct 12, 2023, Commit 48b6b29: We migrated from keras to torch in Option Pricing via Machine Learning for improved environment management.\nOct 4, 2023, Commit d4e0717: We added a new chapter Setting Up Your Environment.\nSep 28, 2023, Commit 290a612: We updated all data sources until 2022-12-31.\nSep 23, 2023, Commit f88f6c9: We switched from alabama and quadprog to nloptr in Constrained Optimization and Backtesting to be more consistent with the optimization in Python and to provide more flexibility with respect to constraints.\nJune 15, 2023, Commit 47dbb30: We moved the first usage of broom:tidy() from Fama-Macbeth Regressions to Univariate Portfolio Sorts to clean up the CAPM estimation.\nJune 12, 2023, Commit e008622: We fixed some inconsencies in notation of portfolio weights. Now, we refer to portfolio weights with \\(\\omega\\) throughout the complete book.\nJune 12, 2023, Commit 186ec7b2: We fixed a typo in the discussion of the elastic net in Chapter Factor Selection via Machine Learning.\nMay 23, 2023, Commit d5e355c: We update the workflow to collect() tables from tidy_finance.sqlite: To make variable selection more obvious, we now explicitly select() columns before collecting. As part of the pull request Commit 91d3077, we now select excess returns instead of net returns in the Chapter Fama-MacBeth Regressions.\nMay 20, 2023, Commit be0f0b4: We include NA-observations in the Mergent filters in Chapter TRACE and FISD.\nMay 17, 2023, Commit 2209bb1: We changed the assign_portfolio()-functions in Chapters Univariate Portfolio Sorts, Size Sorts and p-Hacking, Value and Bivariate Sorts, and Replicating Fama and French Factors. Additionally, we added a small explanation to potential issues with the function for clustered sorting variables in Chapter Univariate Portfolio Sorts.\nMay 12, 2023, Commit 54b76d7: We removed magic numbers in Chapter Introduction to Tidy Finance and introduced the scales packages already in the introduction chapter to reduce scaling issues in figures.\nMar. 30, 2023, Issue 29: We upgraded to tidyverse 2.0.0 and R 4.2.3 and removed all explicit loads of lubridate.\nFeb. 15, 2023, Commit bfda6af: We corrected an error in the calculation of the annualized average return volatility in the Chapter Introduction to Tidy Finance.\nMar. 06, 2023, Commit 857f0f5: We corrected an error in the label of Figure 6, which wrongly claimed to show the efficient tangency portfolio.\nMar. 09, 2023, Commit fae4ac3: We corrected a typo in the definition of the power utility function in Chapter Portfolio Performance. The utility function implemented in the code is now consistent with the text.",
    "crumbs": [
      "R",
      "Appendix",
      "Changelog"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html",
    "href": "r/accessing-and-managing-financial-data.html",
    "title": "Accessing and Managing Financial Data",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we suggest a way to organize your financial data. Everybody who has experience with data is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome in the case of using different data formats, both across different projects and across different programming languages. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages.\nThis chapter shows how to import different open source data sets. Specifically, our data comes from the application programming interface (API) of Yahoo!Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series. We store all the data in a single database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\nFirst, we load the global R packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nlibrary(tidyverse)\nlibrary(scales)\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on.\nstart_date &lt;- ymd(\"1960-01-01\")\nend_date &lt;- ymd(\"2022-12-31\")",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#fama-french-data",
    "href": "r/accessing-and-managing-financial-data.html#fama-french-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Fama-French Data",
    "text": "Fama-French Data\nWe start by downloading some famous Fama-French factors (e.g., Fama and French 1993) and portfolio returns commonly used in empirical asset pricing. Fortunately, there is a neat package by Nelson Areal that allows us to access the data easily: the frenchdata package provides functions to download and read data sets from Prof. Kenneth French finance data library (Areal 2021). \n\nlibrary(frenchdata)\n\nWe can use the download_french_data() function of the package to download monthly Fama-French factors. The set Fama/French 3 Factors contains the return time series of the market mkt_excess, size smb and value hml alongside the risk-free rates rf. Note that we have to do some manual work to correctly parse all the columns and scale them appropriately, as the raw Fama-French data comes in a very unpractical data format. For precise descriptions of the variables, we suggest consulting Prof. Kenneth French’s finance data library directly. If you are on the website, check the raw data files to appreciate the time you can save thanks to frenchdata.\n\nfactors_ff3_monthly_raw &lt;- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff3_monthly &lt;- factors_ff3_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) |&gt; \n  filter(month &gt;= start_date & month &lt;= end_date)\n\nWe also download the set 5 Factors (2x3), which additionally includes the return time series of the profitability rmw and investment cma factors. We demonstrate how the monthly factors are constructed in the chapter Replicating Fama and French Factors.\n\nfactors_ff5_monthly_raw &lt;- download_french_data(\"Fama/French 5 Factors (2x3)\")\n\nfactors_ff5_monthly &lt;- factors_ff5_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    across(c(RF, `Mkt-RF`, SMB, HML, RMW, CMA), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) |&gt; \n  filter(month &gt;= start_date & month &lt;= end_date)\n\nIt is straightforward to download the corresponding daily Fama-French factors with the same function.\n\nfactors_ff3_daily_raw &lt;- download_french_data(\"Fama/French 3 Factors [Daily]\")\n\nfactors_ff3_daily &lt;- factors_ff3_daily_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = ymd(date),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date)\n\nIn a subsequent chapter, we also use the 10 monthly industry portfolios, so let us fetch that data, too.\n\nindustries_ff_monthly_raw &lt;- download_french_data(\"10 Industry Portfolios\")\n\nindustries_ff_monthly &lt;- industries_ff_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(month = floor_date(ymd(str_c(date, \"01\")), \"month\")) |&gt;\n  mutate(across(where(is.numeric), ~ . / 100)) |&gt;\n  select(month, everything(), -date) |&gt;\n  filter(month &gt;= start_date & month &lt;= end_date) |&gt; \n  rename_with(str_to_lower)\n\nIt is worth taking a look at all available portfolio return time series from Kenneth French’s homepage. You should check out the other sets by calling get_french_data_list(). For an alternative to download Fama-French data, check out the FFdownload package by Sebastian Stöckl.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#q-factors",
    "href": "r/accessing-and-managing-financial-data.html#q-factors",
    "title": "Accessing and Managing Financial Data",
    "section": "q-Factors",
    "text": "q-Factors\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the Hou, Xue, and Zhang (2014) q-factor model. We refer to the extended background information provided by the original authors for further information. The q factors can be downloaded directly from the authors’ homepage from within read_csv().\nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the “R_”-prescript using regular expressions and write all column names in lowercase. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on try. You can check out style guides available online, e.g., Hadley Wickham’s tidyverse style guide.\n\nfactors_q_monthly_link &lt;-\n  \"https://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2022.csv\"\n\nfactors_q_monthly &lt;- read_csv(factors_q_monthly_link) |&gt;\n  mutate(month = ymd(str_c(year, month, \"01\", sep = \"-\"))) |&gt;\n  select(-R_F, -R_MKT, -year) |&gt;\n  rename_with(~ str_remove(., \"R_\")) |&gt;\n  rename_with(~ str_to_lower(.)) |&gt;\n  mutate(across(-month, ~ . / 100)) |&gt;\n  filter(month &gt;= start_date & month &lt;= end_date)",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "href": "r/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "title": "Accessing and Managing Financial Data",
    "section": "Macroeconomic Predictors",
    "text": "Macroeconomic Predictors\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. Welch and Goyal (2008) comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data updated to 2022 on Amit Goyal’s website. Since the data is an XLSX-file stored on a public Google drive location, we need to first download the data to access the data directly from our R session. Therefore, we load readxl to read the XLSX-file (Wickham and Bryan 2022).\n\nlibrary(readxl)\n\nWe use the basic download.file() function to download the XLSX-file.1\n\nmacro_predictors_url &lt;-\n  \"https://docs.google.com/spreadsheets/d/1g4LOaRj4TvwJr9RIaA_nwrXXWTOy46bP/export?format=xlsx\"\n\ndownload.file(\n  url = macro_predictors_url, \n  destfile = \"macro_predictors.xlsx\", \n  mode = \"wb\"\n)\n\nNext, we read in the new data and transform the columns into the variables that we later use:\n\nThe dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (Campbell and Shiller 1988; Campbell and Yogo 2006).\nDividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).\nEarnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index (Campbell and Shiller 1988).\nDividend payout ratio (de), the difference between the log of dividends and the log of earnings (Lamont 1998).\nStock variance (svar), the sum of squared daily returns on the S&P 500 index (Guo 2006).\nBook-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average (Kothari and Shanken 1997).\nNet equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks (Campbell, Hilscher, and Szilagyi 2008).\nTreasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis (Campbell 1987).\nLong-term yield (lty), the long-term government bond yield from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nLong-term rate of returns (ltr), the long-term government bond returns from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nTerm spread (tms), the difference between the long-term yield on government bonds and the Treasury bill (Campbell 1987).\nDefault yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields (Fama and French 1989).\nInflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics (Campbell and Vuolteenaho 2004).\n\nFor variable definitions and the required data transformations, you can consult the material on Amit Goyal’s website.\n\nmacro_predictors &lt;- read_xlsx(\n  \"macro_predictors.xlsx\",\n  sheet = \"Monthly\"\n) |&gt;\n  mutate(month = ym(yyyymm)) |&gt;\n  mutate(across(where(is.character), as.numeric)) |&gt;\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend yield\n    ep = log(E12) - log(Index), # Earnings price ratio\n    de = log(D12) - log(E12), # Dividend payout ratio\n    tms = lty - tbl, # Term spread\n    dfy = BAA - AAA # Default yield spread\n  ) |&gt;\n  select(month, rp_div, dp, dy, ep, de, svar,\n    bm = `b/m`, ntis, tbl, lty, ltr,\n    tms, dfy, infl\n  ) |&gt;\n  filter(month &gt;= start_date & month &lt;= end_date) |&gt;\n  drop_na()\n\nFinally, after reading in the macro predictors to our memory, we remove the raw data file from our temporary storage.\n\nfile.remove(\"macro_predictors.xlsx\")",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "href": "r/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Other Macroeconomic Data",
    "text": "Other Macroeconomic Data\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. As an illustration, we use the already familiar tidyquant package to fetch consumer price index (CPI) data that can be found under the CPIAUCNS key.\n\nlibrary(tidyquant)\n\ncpi_monthly &lt;- tq_get(\"CPIAUCNS\",\n  get = \"economic.data\",\n  from = start_date,\n  to = end_date\n) |&gt;\n  mutate(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(month)],\n    .keep = \"none\"\n  )\n\nTo download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the PCU2122212122210 key. The tidyquant package provides access to around 10,000 time series of the FRED database. If your desired time series is not included, we recommend working with the fredr package (Boysel and Vaughan 2021). Note that you need to get an API key to use its functionality. We refer to the package documentation for details.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#setting-up-a-database",
    "href": "r/accessing-and-managing-financial-data.html#setting-up-a-database",
    "title": "Accessing and Managing Financial Data",
    "section": "Setting Up a Database",
    "text": "Setting Up a Database\nNow that we have downloaded some (freely available) data from the web into the memory of our R session let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code.\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an SQLite database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that SQL (Structured Query Language) is a standard language for accessing and manipulating databases and heavily inspired the dplyr functions. We refer to this tutorial for more information on SQL.\nThere are two packages that make working with SQLite in R very simple: RSQLite (Müller et al. 2022) embeds the SQLite database engine in R, and dbplyr (Wickham, Girlich, and Ruiz 2022) is the database back-end for dplyr. These packages allow to set up a database to remotely store tables and use these remote database tables as if they are in-memory data frames by automatically converting dplyr into SQL. Check out the RSQLite and dbplyr vignettes for more information.\n\nlibrary(RSQLite)\nlibrary(dbplyr)\n\nAn SQLite database is easily created - the code below is really all there is. You do not need any external software. Note that we use the extended_types=TRUE option to enable date types when storing and fetching data. Otherwise, date columns are stored and retrieved as integers. We will use the resulting file tidy_finance_r.sqlite in the subfolder data for all subsequent chapters to retrieve our data.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the function dbWriteTable(), which copies the data to our SQLite-database.\n\ndbWriteTable(tidy_finance,\n  \"factors_ff3_monthly\",\n  value = factors_ff3_monthly,\n  overwrite = TRUE\n)\n\nWe can use the remote table as an in-memory data frame by building a connection via tbl().\n\nfactors_ff3_monthly_db &lt;- tbl(tidy_finance, \"factors_ff3_monthly\")\n\nAll dplyr calls are evaluated lazily, i.e., the data is not in our R session’s memory, and the database does most of the work. You can see that by noticing that the output below does not show the number of rows. In fact, the following code chunk only fetches the top 10 rows from the database for printing.\n\nfactors_ff3_monthly_db |&gt;\n  select(month, rf)\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.41.2 [data/tidy_finance_r.sqlite]\n  month          rf\n  &lt;date&gt;      &lt;dbl&gt;\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# ℹ more rows\n\n\nIf we want to have the whole table in memory, we need to collect() it. You will see that we regularly load the data into the memory in the next chapters.\n\nfactors_ff3_monthly_db |&gt;\n  select(month, rf) |&gt;\n  collect()\n\n# A tibble: 756 × 2\n  month          rf\n  &lt;date&gt;      &lt;dbl&gt;\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# ℹ 751 more rows\n\n\nThe last couple of code chunks is really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages.\nBefore we move on to the next data source, let us also store the other five tables in our new SQLite database.\n\ndbWriteTable(tidy_finance,\n  \"factors_ff5_monthly\",\n  value = factors_ff5_monthly,\n  overwrite = TRUE\n)\n\ndbWriteTable(tidy_finance,\n  \"factors_ff3_daily\",\n  value = factors_ff3_daily,\n  overwrite = TRUE\n)\n\ndbWriteTable(tidy_finance,\n  \"industries_ff_monthly\",\n  value = industries_ff_monthly,\n  overwrite = TRUE\n)\n\ndbWriteTable(tidy_finance,\n  \"factors_q_monthly\",\n  value = factors_q_monthly,\n  overwrite = TRUE\n)\n\ndbWriteTable(tidy_finance,\n  \"macro_predictors\",\n  value = macro_predictors,\n  overwrite = TRUE\n)\n\ndbWriteTable(tidy_finance,\n  \"cpi_monthly\",\n  value = cpi_monthly,\n  overwrite = TRUE\n)\n\nFrom now on, all you need to do to access data that is stored in the database is to follow three steps: (i) Establish the connection to the SQLite database, (ii) call the table you want to extract, and (iii) collect the data. For your convenience, the following steps show all you need in a compact fashion.\n\nlibrary(tidyverse)\nlibrary(RSQLite)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_q_monthly &lt;- tbl(tidy_finance, \"factors_q_monthly\")\nfactors_q_monthly &lt;- factors_q_monthly |&gt; collect()",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "href": "r/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "title": "Accessing and Managing Financial Data",
    "section": "Managing SQLite Databases",
    "text": "Managing SQLite Databases\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\nTo optimize the database file, you can run the VACUUM command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the dbSendQuery() function.\n\nres &lt;- dbSendQuery(tidy_finance, \"VACUUM\")\nres\n\n&lt;SQLiteResult&gt;\n  SQL  VACUUM\n  ROWS Fetched: 0 [complete]\n       Changed: 0\n\n\nThe VACUUM command actually performs a couple of additional cleaning steps, which you can read about in this tutorial. \nWe store the result of the above query in res because the database keeps the result set open. To close open results and avoid warnings going forward, we can use dbClearResult().\n\ndbClearResult(res)\n\nApart from cleaning up, you might be interested in listing all the tables that are currently in your database. You can do this via the dbListTables() function.\n\ndbListTables(tidy_finance)\n\n [1] \"beta\"                  \"compustat\"            \n [3] \"cpi_monthly\"           \"crsp_daily\"           \n [5] \"crsp_monthly\"          \"factors_ff3_daily\"    \n [7] \"factors_ff3_monthly\"   \"factors_ff5_monthly\"  \n [9] \"factors_q_monthly\"     \"fisd\"                 \n[11] \"industries_ff_monthly\" \"macro_predictors\"     \n[13] \"trace_enhanced\"       \n\n\nThis function comes in handy if you are unsure about the correct naming of the tables in your database.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#exercises",
    "href": "r/accessing-and-managing-financial-data.html#exercises",
    "title": "Accessing and Managing Financial Data",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the monthly Fama-French factors manually from Ken French’s data library and read them in via read_csv(). Validate that you get the same data as via the frenchdata package.\nDownload the daily Fama-French 5 factors using the frenchdata package. Use get_french_data_list() to find the corresponding table name. After the successful download and conversion to the column format that we used above, compare the rf, mkt_excess, smb, and hml columns of factors_ff3_daily to factors_ff5_daily. Discuss any differences you might find.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#footnotes",
    "href": "r/accessing-and-managing-financial-data.html#footnotes",
    "title": "Accessing and Managing Financial Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the mode = \"wb\" argument is used to ensure that the file is written in binary mode, which is important for non-text files like XLSX.↩︎",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html",
    "href": "python/wrds-crsp-and-compustat.html",
    "title": "WRDS, CRSP, and Compustat",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nThis chapter shows how to connect to Wharton Research Data Services (WRDS), a popular provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics, CRSP and Compustat. Unfortunately, this data is not freely available, but most students and researchers typically have access to WRDS through their university libraries. Assuming that you have access to WRDS, we show you how to prepare and merge the databases and store them in the SQLite database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the WRDS database.\nIf you don’t have access to WRDS but still want to run the code in this book, we refer to WRDS Dummy Data, where we show how to create a dummy database that contains the WRDS tables and corresponding columns. With this database at hand, all code chunks in this book can be executed with this dummy database.\nFirst, we load the Python packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them. The last two packages are used for plotting.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format, percent_format\nfrom datetime import datetime\nWe use the same date range as in the previous chapter to ensure consistency. However, we have to use the date format that the WRDS database expects.\nstart_date = \"01/01/1960\"\nend_date = \"12/31/2022\"",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#accessing-wrds",
    "href": "python/wrds-crsp-and-compustat.html#accessing-wrds",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Accessing WRDS",
    "text": "Accessing WRDS\nWRDS is the most widely used source for asset and firm-specific financial data used in academic settings. WRDS is a data platform that provides data validation, flexible delivery options, and access to many different data sources. The data at WRDS is also organized in an SQL database, although they use the PostgreSQL engine. This database engine is just as easy to handle with Python as SQL. We use the sqlalchemy package to establish a connection to the WRDS database because it already contains a suitable driver.1\n\nfrom sqlalchemy import create_engine\n\nTo establish a connection to WRDS, you use the function create_engine() with a connection string that specifies the WRDS server and your login credentials. We defined environment variables for the purpose of this book because we obviously do not want (and are not allowed) to share our credentials with the rest of the world. See Setting Up Your Environment for information about why and how to create an .env-file that can be loaded with load_dotenv(). Alternatively, you can replace os.getenv('WRDS_USER) and os.getenv('WRDS_PASSWORD') with your own credentials (but be careful not to share them with others or the public).\nAdditionally, you have to use two-factor authentication since May 2023 when establishing a remote connection to WRDS. You have two choices to provide the additional identification. First, if you have Duo Push enabled for your WRDS account, you will receive a push notification on your mobile phone when trying to establish a connection with the code below. Upon accepting the notification, you can continue your work. Second, you can log in to a WRDS website that requires two-factor authentication with your username and the same IP address. Once you have successfully identified yourself on the website, your username-IP combination will be remembered for 30 days, and you can comfortably use the remote connection below.\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nconnection_string = (\n  \"postgresql+psycopg2://\"\n f\"{os.getenv('WRDS_USER')}:{os.getenv('WRDS_PASSWORD')}\"\n  \"@wrds-pgdata.wharton.upenn.edu:9737/wrds\"\n)\n\nwrds = create_engine(connection_string, pool_pre_ping=True)\n\nThe remote connection to WRDS is very useful. Yet, the database itself contains many different tables. You can check the WRDS homepage to identify the table’s name you are looking for (if you go beyond our exposition).",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "href": "python/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Downloading and Preparing CRSP",
    "text": "Downloading and Preparing CRSP\nThe Center for Research in Security Prices (CRSP) provides the most widely used data for US stocks. We use the wrds engine object that we just created to first access monthly CRSP return data. Actually, we need two tables to get the desired data: (i) the CRSP monthly security file (msf), and (ii) the historical identifying information (stksecurityinfohist).\nWe use the two remote tables to fetch the data we want to put into our local database. Just as above, the idea is that we let the WRDS database do all the work and just download the data that we actually need. We apply common filters and data selection criteria to narrow down our data of interest: (i) we use only stock prices from NYSE, Amex, and NASDAQ (primaryexch %in% c(\"N\", \"A\", \"Q\")) when or after issuance (conditionaltype %in% c(\"RW\", \"NW\")) for actively traded stocks (tradingstatusflg == \"A\")2, (ii) we keep only data in the time windows of interest, (iii) we keep only US-listed stocks as identified via no special share types (sharetype = 'NS'), security type equity (securitytype = 'EQTY'), security sub type common stock (securitysubtype = 'COM'), issuers that are a corporation (issuertype %in% c(\"ACOR\", \"CORP\")), and (iv) we keep only months within permno-specific start dates (secinfostartdt) and end dates (secinfoenddt). As of July 2022, there is no need to additionally download delisting information since it is already contained in the most recent version of msf. Additionally, the industry information in stksecurityinfohist records the historic industry and should be used instead of the one stored under same variable name in msf_v2.\n\ncrsp_monthly_query = (\n  \"SELECT msf.permno, msf.mthcaldt AS date, \"\n         \"date_trunc('month', msf.mthcaldt)::date AS month, \"\n         \"msf.mthret AS ret, msf.shrout, msf.mthprc AS altprc, \"\n         \"ssih.primaryexch, ssih.siccd \"\n    \"FROM crsp.msf_v2 AS msf \"\n    \"INNER JOIN crsp.stksecurityinfohist AS ssih \"\n    \"ON msf.permno = ssih.permno AND \"\n       \"ssih.secinfostartdt &lt;= msf.mthcaldt AND \"\n       \"msf.mthcaldt &lt;= ssih.secinfoenddt \"\n   f\"WHERE msf.mthcaldt BETWEEN '{start_date}' AND '{end_date}' \"\n          \"AND ssih.sharetype = 'NS' \"\n          \"AND ssih.securitytype = 'EQTY' \"  \n          \"AND ssih.securitysubtype = 'COM' \" \n          \"AND ssih.usincflg = 'Y' \" \n          \"AND ssih.issuertype in ('ACOR', 'CORP') \" \n          \"AND ssih.primaryexch in ('N', 'A', 'Q') \"\n          \"AND ssih.conditionaltype in ('RW', 'NW') \"\n          \"AND ssih.tradingstatusflg = 'A'\"\n)\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=crsp_monthly_query,\n    con=wrds,\n    dtype={\"permno\": int, \"siccd\": int},\n    parse_dates={\"date\", \"month\"})\n  .assign(shrout=lambda x: x[\"shrout\"]*1000)\n)\n\nNow, we have all the relevant monthly return data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters.\nThe first additional variable we create is market capitalization (mktcap), which is the product of the number of outstanding shares (shrout) and the last traded price in a month (prc). Note that in contrast to returns (ret), these two variables are not adjusted ex-post for any corporate actions like stock splits. Therefore, if you want to use a stock’s price, you need to adjust it with a cumulative adjustment factor. We also keep the market cap in millions of USD just for convenience, as we do not want to print huge numbers in our figures and tables. In addition, we set zero market capitalization to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).\n\ncrsp_monthly = (crsp_monthly\n  .assign(mktcap=lambda x: x[\"shrout\"]*x[\"altprc\"]/1000000)\n  .assign(mktcap=lambda x: x[\"mktcap\"].replace(0, np.nan))\n)\n\nThe next variable we frequently use is the one-month lagged market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly CRSP data.\n\nmktcap_lag = (crsp_monthly\n  .assign(\n    month=lambda x: x[\"month\"]+pd.DateOffset(months=1),\n    mktcap_lag=lambda x: x[\"mktcap\"]\n  )\n  .get([\"permno\", \"month\", \"mktcap_lag\"])\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(mktcap_lag, how=\"left\", on=[\"permno\", \"month\"])\n)\n\nNext, we transform primary listing exchange codes to explicit exchange names.\n\ndef assign_exchange(primaryexch):\n    if primaryexch == \"N\":\n        return \"NYSE\"\n    elif primaryexch == \"A\":\n        return \"AMEX\"\n    elif primaryexch == \"Q\":\n        return \"NASDAQ\"\n    else:\n        return \"Other\"\n\ncrsp_monthly[\"exchange\"] = (crsp_monthly[\"primaryexch\"]\n  .apply(assign_exchange)\n)\n\nSimilarly, we transform industry codes to industry descriptions following Bali, Engle, and Murray (2016). Notice that there are also other categorizations of industries (e.g., Fama and French 1997) that are commonly used.\n\ndef assign_industry(siccd):\n    if 1 &lt;= siccd &lt;= 999:\n        return \"Agriculture\"\n    elif 1000 &lt;= siccd &lt;= 1499:\n        return \"Mining\"\n    elif 1500 &lt;= siccd &lt;= 1799:\n        return \"Construction\"\n    elif 2000 &lt;= siccd &lt;= 3999:\n        return \"Manufacturing\"\n    elif 4000 &lt;= siccd &lt;= 4899:\n        return \"Transportation\"\n    elif 4900 &lt;= siccd &lt;= 4999:\n        return \"Utilities\"\n    elif 5000 &lt;= siccd &lt;= 5199:\n        return \"Wholesale\"\n    elif 5200 &lt;= siccd &lt;= 5999:\n        return \"Retail\"\n    elif 6000 &lt;= siccd &lt;= 6799:\n        return \"Finance\"\n    elif 7000 &lt;= siccd &lt;= 8999:\n        return \"Services\"\n    elif 9000 &lt;= siccd &lt;= 9999:\n        return \"Public\"\n    else:\n        return \"Missing\"\n\ncrsp_monthly[\"industry\"] = (crsp_monthly[\"siccd\"]\n  .apply(assign_industry)\n)\n\nNext, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop the risk-free rate from our data frame. Note that we ensure excess returns are bounded by -1 from below as a return less than -100% makes no sense conceptually. Before we can adjust the returns, we have to connect to our database and load the data frame factors_ff3_monthly.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT month, rf FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n  \ncrsp_monthly = (crsp_monthly\n  .merge(factors_ff3_monthly, how=\"left\", on=\"month\")\n  .assign(ret_excess=lambda x: x[\"ret\"]-x[\"rf\"])\n  .assign(ret_excess=lambda x: x[\"ret_excess\"].clip(lower=-1))\n  .drop(columns=[\"rf\"])\n)\n\nSince excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization.\n\ncrsp_monthly = (crsp_monthly\n  .dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n)\n\nFinally, we store the monthly CRSP file in our database.\n\n(crsp_monthly\n  .to_sql(name=\"crsp_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "href": "python/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "title": "WRDS, CRSP, and Compustat",
    "section": "First Glimpse of the CRSP Sample",
    "text": "First Glimpse of the CRSP Sample\nBefore we move on to other data sources, let us look at some descriptive statistics of the CRSP sample, which is our main source for stock returns.\nFigure 1 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks. The number of stocks listed on AMEX decreased steadily over the last couple of decades. By the end of 2022, there were 2,778 stocks with a primary listing on NASDAQ, 1,358 on NYSE, 162 on AMEX, and only one belonged to the other category. Specifically, we use the size() function here to count the number of observations for each exchange-date group.\n\nsecurities_per_exchange = (crsp_monthly\n  .groupby([\"exchange\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nsecurities_per_exchange_figure = (\n  ggplot(securities_per_exchange, \n         aes(x=\"date\", y=\"n\", color=\"exchange\", linetype=\"exchange\")) +\n  geom_line() + \n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"Monthly number of securities by listing exchange\") +\n  scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\") +\n  scale_y_continuous(labels=comma_format())\n)\nsecurities_per_exchange_figure.draw()\n\n\n\n\n\n\n\nFigure 1: The figure shows the monthly number of stocks in the CRSP sample listed at each of the US exchanges.\n\n\n\n\n\nNext, we look at the aggregate market capitalization grouped by the respective listing exchanges in Figure 2. To ensure that we look at meaningful data that is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange. All values in Figure 2 are in terms of the end of 2022 USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.\n\ncpi_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM cpi_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\nmarket_cap_per_exchange = (crsp_monthly\n  .merge(cpi_monthly, how=\"left\", on=\"month\")\n  .groupby([\"month\", \"exchange\"])\n  .apply(\n    lambda group: pd.Series({\n      \"mktcap\": group[\"mktcap\"].sum()/group[\"cpi\"].mean()\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_exchange_figure = (\n  ggplot(market_cap_per_exchange, \n         aes(x=\"month\", y=\"mktcap/1000\", \n             color=\"exchange\", linetype=\"exchange\")) +\n  geom_line() +\n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=(\"Monthly market cap by listing exchange \"\n              \"in billions of Dec 2022 USD\")) + \n  scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\") +\n  scale_y_continuous(labels=comma_format())\n)\nmarket_cap_per_exchange_figure.draw()\n\n\n\n\n\n\n\nFigure 2: The figure shows the monthly market capitalization by listing exchange. Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the horizontal axis reflect the buying power of billion USD in December 2022.\n\n\n\n\n\nNext, we look at the same descriptive statistics by industry. Figure 3 plots the number of stocks in the sample for each of the SIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.\n\nsecurities_per_industry = (crsp_monthly\n  .groupby([\"industry\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nlinetypes = [\"-\", \"--\", \"-.\", \":\"]\nn_industries = securities_per_industry[\"industry\"].nunique()\n\nsecurities_per_industry_figure = (\n  ggplot(securities_per_industry, \n         aes(x=\"date\", y=\"n\", color=\"industry\", linetype=\"industry\")) + \n  geom_line() + \n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"Monthly number of securities by industry\") +\n  scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\") + \n  scale_y_continuous(labels=comma_format()) +\n  scale_linetype_manual(\n    values=[linetypes[l % len(linetypes)] for l in range(n_industries)]\n  ) \n)\nsecurities_per_industry_figure.draw()\n\n\n\n\n\n\n\nFigure 3: The figure shows the monthly number of stocks in the CRSP sample associated with different industries.\n\n\n\n\n\nWe also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in Figure 4. All values are again in terms of billions of end of 2022 USD. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.\n\nmarket_cap_per_industry = (crsp_monthly\n  .merge(cpi_monthly, how=\"left\", on=\"month\")\n  .groupby([\"month\", \"industry\"])\n  .apply(\n    lambda group: pd.Series({\n      \"mktcap\": (group[\"mktcap\"].sum()/group[\"cpi\"].mean())\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_industry_figure = (\n  ggplot(market_cap_per_industry, \n         aes(x=\"month\", y=\"mktcap/1000\", \n             color=\"industry\", linetype=\"industry\")) +\n  geom_line() + \n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"Monthly market cap by industry in billions of Dec 2022 USD\") + \n  scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\") + \n  scale_y_continuous(labels=comma_format()) +\n  scale_linetype_manual(\n    values=[linetypes[l % len(linetypes)] for l in range(n_industries)]\n  ) \n)\nmarket_cap_per_industry_figure.draw()\n\n\n\n\n\n\n\nFigure 4: The figure shows the total Market capitalization in billion USD, adjusted for consumer price index changes such that the values on the y-axis reflect the buying power of billion USD in December 2022.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#daily-crsp-data",
    "href": "python/wrds-crsp-and-compustat.html#daily-crsp-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Daily CRSP Data",
    "text": "Daily CRSP Data\nBefore we turn to accounting data, we provide a proposal for downloading daily CRSP data with the same filters used for the monthly data (i.e., using information from stksecurityinfohist). While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can exceed 20 GB. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire dataset to your local database), and in our experience, the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your Python session.\nThere is a solution to this challenge. As with many big data problems, you can split up the big task into several smaller tasks that are easier to handle. That is, instead of downloading data about all stocks at once, download the data in small batches of stocks consecutively. Such operations can be implemented in for-loops, where we download, prepare, and store the data for a small number of stocks in each iteration. This operation might nonetheless take around 5 minutes, depending on your internet connection. To keep track of the progress, we create ad-hoc progress updates using print(). Notice that we also use the method to_sql() here with the option to append the new data to an existing table, when we process the second and all following batches. As for the monthly CRSP data, there is no need to adjust for delisting returns in the daily CRSP data since July 2022.\n\nfactors_ff3_daily = pd.read_sql(\n  sql=\"SELECT * FROM factors_ff3_daily\", \n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\npermnos = pd.read_sql(\n  sql=\"SELECT DISTINCT permno FROM crsp.stksecurityinfohist\", \n  con=wrds,\n  dtype={\"permno\": int}\n)\n\npermnos = list(permnos[\"permno\"].astype(str))\n  \nbatch_size = 500\nbatches = np.ceil(len(permnos)/batch_size).astype(int)\n  \nfor j in range(1, batches+1):  \n    \n  permno_batch = permnos[\n    ((j-1)*batch_size):(min(j*batch_size, len(permnos)))\n  ]\n  \n  permno_batch_formatted = (\n    \", \".join(f\"'{permno}'\" for permno in permno_batch)\n  )\n  permno_string = f\"({permno_batch_formatted})\"\n  \n  crsp_daily_sub_query = (\n    \"SELECT dsf.permno, dlycaldt AS date, dlyret AS ret \"\n      \"FROM crsp.dsf_v2 AS dsf \"\n      \"INNER JOIN crsp.stksecurityinfohist AS ssih \"\n      \"ON dsf.permno = ssih.permno AND \"\n         \"ssih.secinfostartdt &lt;= dsf.dlycaldt AND \"\n         \"dsf.dlycaldt &lt;= ssih.secinfoenddt \"\n      f\"WHERE dsf.permno IN {permno_string} \"\n           f\"AND dlycaldt BETWEEN '{start_date}' AND '{end_date}' \"\n            \"AND ssih.sharetype = 'NS' \"\n            \"AND ssih.securitytype = 'EQTY' \"  \n            \"AND ssih.securitysubtype = 'COM' \" \n            \"AND ssih.usincflg = 'Y' \" \n            \"AND ssih.issuertype in ('ACOR', 'CORP') \" \n            \"AND ssih.primaryexch in ('N', 'A', 'Q') \"\n            \"AND ssih.conditionaltype in ('RW', 'NW') \"\n            \"AND ssih.tradingstatusflg = 'A'\"\n  )\n    \n  crsp_daily_sub = (pd.read_sql_query(\n      sql=crsp_daily_sub_query,\n      con=wrds,\n      dtype={\"permno\": int},\n      parse_dates={\"date\"}\n    )\n    .dropna()\n   )\n\n  if not crsp_daily_sub.empty:\n    \n      crsp_daily_sub = (crsp_daily_sub\n        .assign(\n          month = lambda x: \n            x[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n        )\n        .merge(factors_ff3_daily[[\"date\", \"rf\"]], \n               on=\"date\", how=\"left\")\n        .assign(\n          ret_excess = lambda x: \n            ((x[\"ret\"] - x[\"rf\"]).clip(lower=-1))\n        )\n        .get([\"permno\", \"date\", \"month\", \"ret_excess\"])\n      )\n        \n      if j == 1:\n        if_exists_string = \"replace\"\n      else:\n        if_exists_string = \"append\"\n\n      crsp_daily_sub.to_sql(\n        name=\"crsp_daily\", \n        con=tidy_finance, \n        if_exists=if_exists_string, \n        index=False\n      )\n            \n  print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")\n\nEventually, we end up with more than 71 million rows of daily return data. Note that we only store the identifying information that we actually need, namely permno, date, and month alongside the excess returns. We thus ensure that our local database contains only the data that we actually use.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "href": "python/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Preparing Compustat Data",
    "text": "Preparing Compustat Data\nFirm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters. The commonly used source for firm financial information is Compustat provided by S&P Global Market Intelligence, which is a global data vendor that provides financial, statistical, and market information on active and inactive companies throughout the world. For US and Canadian companies, annual history is available back to 1950 and quarterly as well as monthly histories date back to 1962.\nTo access Compustat data, we can again tap WRDS, which hosts the funda table that contains annual firm-level information on North American companies. We follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, which includes companies that are primarily involved in manufacturing, services, and other non-financial business activities,3, (ii) in the standard format (i.e., consolidated information in standard presentation), and (iii) only data in the desired time window.\n\ncompustat_query = (\n  \"SELECT gvkey, datadate, seq, ceq, at, lt, txditc, txdb, itcb,  pstkrv, \"\n         \"pstkl, pstk, capx, oancf, sale, cogs, xint, xsga \"\n    \"FROM comp.funda \"\n    \"WHERE indfmt = 'INDL' \"\n          \"AND datafmt = 'STD' \"\n          \"AND consol = 'C' \"\n         f\"AND datadate BETWEEN '{start_date}' AND '{end_date}'\"\n)\n\ncompustat = pd.read_sql_query(\n  sql=compustat_query,\n  con=wrds,\n  dtype={\"gvkey\": str},\n  parse_dates={\"datadate\"}\n)\n\nNext, we calculate the book value of preferred stock and equity be and the operating profitability op inspired by the variable definitions in Kenneth French’s data library. Note that we set negative or zero equity to missing, which is a common practice when working with book-to-market ratios (see Fama and French 1992 for details).\n\ncompustat = (compustat\n  .assign(\n    be=lambda x: \n      (x[\"seq\"].combine_first(x[\"ceq\"]+x[\"pstk\"])\n       .combine_first(x[\"at\"]-x[\"lt\"])+\n       x[\"txditc\"].combine_first(x[\"txdb\"]+x[\"itcb\"]).fillna(0)-\n       x[\"pstkrv\"].combine_first(x[\"pstkl\"])\n       .combine_first(x[\"pstk\"]).fillna(0))\n  )\n  .assign(\n    be=lambda x: x[\"be\"].apply(lambda y: np.nan if y &lt;= 0 else y)\n  )\n  .assign(\n    op=lambda x: \n      ((x[\"sale\"]-x[\"cogs\"].fillna(0)- \n        x[\"xsga\"].fillna(0)-x[\"xint\"].fillna(0))/x[\"be\"])\n  )\n)\n\nWe keep only the last available information for each firm-year group (by using the tail(1) pandas function for each group). Note that datadate defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2022). Therefore, datadate is not the date when data was made available to the public. Check out the Exercises for more insights into the peculiarities of datadate.\n\ncompustat = (compustat\n  .assign(year=lambda x: pd.DatetimeIndex(x[\"datadate\"]).year)\n  .sort_values(\"datadate\")\n  .groupby([\"gvkey\", \"year\"])\n  .tail(1)\n  .reset_index()\n)\n\nWe also compute the investment ratio (inv) according to Kenneth French’s variable definitions as the change in total assets from one fiscal year to another. Note that we again use the approach using joins as introduced with the CRSP data above to construct lagged assets.\n\ncompustat_lag = (compustat\n  .get([\"gvkey\", \"year\", \"at\"])\n  .assign(year=lambda x: x[\"year\"]+1)\n  .rename(columns={\"at\": \"at_lag\"})\n)\n\ncompustat = (compustat\n  .merge(compustat_lag, how=\"left\", on=[\"gvkey\", \"year\"])\n  .assign(inv=lambda x: x[\"at\"]/x[\"at_lag\"]-1)\n  .assign(inv=lambda x: np.where(x[\"at_lag\"] &lt;= 0, np.nan, x[\"inv\"]))\n)\n\nWith the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database.\n\n(compustat\n  .to_sql(name=\"compustat\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "href": "python/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Merging CRSP with Compustat",
    "text": "Merging CRSP with Compustat\nUnfortunately, CRSP and Compustat use different keys to identify stocks and firms. CRSP uses permno for stocks, while Compustat uses gvkey to identify firms. Fortunately, a curated matching table on WRDS allows us to merge CRSP and Compustat, so we create a connection to the CRSP-Compustat Merged table (provided by CRSP). The linking table contains links between CRSP and Compustat identifiers from various approaches. However, we need to make sure that we keep only relevant and correct links, again following the description outlined in Bali, Engle, and Murray (2016). Note also that currently active links have no end date, so we just enter the current date via the SQL verb CURRENT_DATE.\n\nccmxpf_linktable_query = (\n  \"SELECT lpermno AS permno, gvkey, linkdt, \"\n         \"COALESCE(linkenddt, CURRENT_DATE) AS linkenddt \"\n    \"FROM crsp.ccmxpf_linktable \"\n    \"WHERE linktype IN ('LU', 'LC') \"\n          \"AND linkprim IN ('P', 'C') \"\n          \"AND usedflag = 1\"\n)\n\nccmxpf_linktable = pd.read_sql_query(\n  sql=ccmxpf_linktable_query,\n  con=wrds,\n  dtype={\"permno\": int, \"gvkey\": str},\n  parse_dates={\"linkdt\", \"linkenddt\"}\n)\n\nWe use these links to create a new table with a mapping between stock identifier, firm identifier, and month. We then add these links to the Compustat gvkey to our monthly stock data.\n\nccm_links = (crsp_monthly\n  .merge(ccmxpf_linktable, how=\"inner\", on=\"permno\")\n  .query(\"~gvkey.isnull() & (date &gt;= linkdt) & (date &lt;= linkenddt)\")\n  .get([\"permno\", \"gvkey\", \"date\"])\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(ccm_links, how=\"left\", on=[\"permno\", \"date\"])\n)\n\nAs the last step, we update the previously prepared monthly CRSP file with the linking information in our local database.\n\n(crsp_monthly\n  .to_sql(name=\"crsp_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\nBefore we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, Figure 5 plots the share of securities with book equity values for each exchange. It turns out that the coverage is pretty bad for AMEX- and NYSE-listed stocks in the 1960s but hovers around 80 percent for all periods thereafter. We can ignore the erratic coverage of securities that belong to the other category since there is only a handful of them anyway in our sample.\n\nshare_with_be = (crsp_monthly\n  .assign(year=lambda x: pd.DatetimeIndex(x[\"month\"]).year)\n  .sort_values(\"date\")\n  .groupby([\"permno\", \"year\"])\n  .tail(1)\n  .reset_index()\n  .merge(compustat, how=\"left\", on=[\"gvkey\", \"year\"])\n  .groupby([\"exchange\", \"year\"])\n  .apply(\n    lambda x: pd.Series({\n    \"share\": x[\"permno\"][~x[\"be\"].isnull()].nunique()/x[\"permno\"].nunique()\n    })\n  )\n  .reset_index()\n)\n\nshare_with_be_figure = (\n  ggplot(share_with_be, \n         aes(x=\"year\", y=\"share\", color=\"exchange\", linetype=\"exchange\")) + \n  geom_line() + \n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"Share of securities with book equity values by exchange\") +\n  scale_y_continuous(labels=percent_format()) + \n  coord_cartesian(ylim=(0, 1))\n)\nshare_with_be_figure.draw()\n\n\n\n\n\n\n\nFigure 5: The figure shows the end-of-year share of securities with book equity values by listing exchange.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#exercises",
    "href": "python/wrds-crsp-and-compustat.html#exercises",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Exercises",
    "text": "Exercises\n\nCompute mkt_cap_lag using lag(mktcap) rather than using joins as above. Filter out all the rows where the lag-based market capitalization measure is different from the one we computed above. Why are the two measures different?\nPlot the average market capitalization of firms for each exchange and industry, respectively, over time. What do you find?\nIn the compustat table, datadate refers to the date to which the fiscal year of a corresponding firm refers. Count the number of observations in Compustat by month of this date variable. What do you find? What does the finding suggest about pooling observations with the same fiscal year?\nGo back to the original Compustat and extract rows where the same firm has multiple rows for the same fiscal year. What is the reason for these observations?\nKeep the last observation of crsp_monthly by year and join it with the compustat table. Create the following plots: (i) aggregate book equity by exchange over time and (ii) aggregate annual book equity by industry over time. Do you notice any different patterns to the corresponding plots based on market capitalization?\nRepeat the analysis of market capitalization for book equity, which we computed from the Compustat data. Then, use the matched sample to plot book equity against market capitalization. How are these two variables related?\n\n\n\n\nFigure 1: The figure shows the monthly number of stocks in the CRSP sample listed at each of the US exchanges.\nFigure 2: The figure shows the monthly market capitalization by listing exchange. Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the horizontal axis reflect the buying power of billion USD in December 2022.\nFigure 3: The figure shows the monthly number of stocks in the CRSP sample associated with different industries.\nFigure 4: The figure shows the total Market capitalization in billion USD, adjusted for consumer price index changes such that the values on the y-axis reflect the buying power of billion USD in December 2022.\nFigure 5: The figure shows the end-of-year share of securities with book equity values by listing exchange.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#footnotes",
    "href": "python/wrds-crsp-and-compustat.html#footnotes",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn alternative to establish a connection to WRDS is to use the WRDS-Py library. We chose to work with sqlalchemy (Bayer 2012) to show how to access PostgreSQL engines in general.↩︎\nThese three criteria jointly replicate the filter exchcd %in% c(1, 2, 3, 31, 32, 33) used for the legacy version of CRSP. If you do not want to include stocks at issuance, you can set the conditionaltype == \"RW\", which is equivalent to the restriction of exchcd %in% c(1, 2, 3) with the old CRSP format.↩︎\nCompanies that operate in the banking, insurance, or utilities sector typically report in different industry formats that reflect their specific regulatory requirements.↩︎",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html",
    "href": "python/univariate-portfolio-sorts.html",
    "title": "Univariate Portfolio Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we dive into portfolio sorts, one of the most widely used statistical methodologies in empirical asset pricing (e.g., Bali, Engle, and Murray 2016). The key application of portfolio sorts is to examine whether one or more variables can predict future excess returns. In general, the idea is to sort individual stocks into portfolios, where the stocks within each portfolio are similar with respect to a sorting variable, such as firm size. The different portfolios then represent well-diversified investments that differ in the level of the sorting variable. You can then attribute the differences in the return distribution to the impact of the sorting variable. We start by introducing univariate portfolio sorts (which sort based on only one characteristic) and tackle bivariate sorting in Value and Bivariate Sorts.\nA univariate portfolio sort considers only one sorting variable \\(x_{i,t-1}\\). Here, \\(i\\) denotes the stock and \\(t-1\\) indicates that the characteristic is observable by investors at time \\(t\\). The objective is to assess the cross-sectional relation between \\(x_{i,t-1}\\) and, typically, stock excess returns \\(r_{i,t}\\) at time \\(t\\) as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas from the previous chapter as our sorting variable.\nThe current chapter relies on the following set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.api as sm\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom regtabletotext import prettify_result",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#data-preparation",
    "href": "python/univariate-portfolio-sorts.html#data-preparation",
    "title": "Univariate Portfolio Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start with loading the required data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. In particular, we use the monthly CRSP sample as our asset universe. Once we form our portfolios, we use the Fama-French market factor returns to compute the risk-adjusted performance (i.e., alpha). beta is the dataframe with market betas computed in the previous chapter.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=\"SELECT permno, month, ret_excess, mktcap_lag FROM crsp_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT month, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\nbeta = (pd.read_sql_query(\n    sql=\"SELECT permno, month, beta_monthly FROM beta\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "href": "python/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "title": "Univariate Portfolio Sorts",
    "section": "Sorting by Market Beta",
    "text": "Sorting by Market Beta\nNext, we merge our sorting variable with the return data. We use the one-month lagged betas as a sorting variable to ensure that the sorts rely only on information available when we create the portfolios. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month \\(t\\) information is available in month \\(t+1\\). You may be tempted to simply use a call such as crsp_monthly['beta_lag'] = crsp_monthly.groupby('permno')['beta'].shift(1) instead. This procedure, however, does not work correctly if there are implicit missing values in the time series.\n\nbeta_lag = (beta\n  .assign(month=lambda x: x[\"month\"]+pd.DateOffset(months=1))\n  .get([\"permno\", \"month\", \"beta_monthly\"])\n  .rename(columns={\"beta_monthly\": \"beta_lag\"})\n  .dropna()\n)\n\ndata_for_sorts = (crsp_monthly\n  .merge(beta_lag, how=\"inner\", on=[\"permno\", \"month\"])\n)\n\nThe first step to conduct portfolio sorts is to calculate periodic breakpoints that you can use to group the stocks into portfolios. For simplicity, we start with the median lagged market beta as the single breakpoint. We then compute the value-weighted returns for each of the two resulting portfolios, which means that the lagged market capitalization determines the weight in np.average().\n\nbeta_portfolios = (data_for_sorts\n  .groupby(\"month\")\n  .apply(lambda x: (x.assign(\n      portfolio=pd.qcut(\n        x[\"beta_lag\"], q=[0, 0.5, 1], labels=[\"low\", \"high\"]))\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"portfolio\",\"month\"])\n  .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n  .reset_index(name=\"ret\")\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#performance-evaluation",
    "href": "python/univariate-portfolio-sorts.html#performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "Performance Evaluation",
    "text": "Performance Evaluation\nWe can construct a long-short strategy based on the two portfolios: buy the high-beta portfolio and, at the same time, short the low-beta portfolio. Thereby, the overall position in the market is net-zero, i.e., you do not need to invest money to realize this strategy in the absence of frictions.\n\nbeta_longshort = (beta_portfolios\n  .pivot_table(index=\"month\", columns=\"portfolio\", values=\"ret\")\n  .reset_index()\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n)\n\nWe compute the average return and the corresponding standard error to test whether the long-short portfolio yields on average positive or negative excess returns. In the asset pricing literature, one typically adjusts for autocorrelation by using Newey and West (1987) \\(t\\)-statistics to test the null hypothesis that average portfolio excess returns are equal to zero. One necessary input for Newey-West standard errors is a chosen bandwidth based on the number of lags employed for the estimation. Researchers often default to choosing a pre-specified lag length of six months (which is not a data-driven approach). We do so in the fit() function by indicating the cov_type as HAC and providing the maximum lag length through an additional keywords dictionary.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\",\n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept      -0.0       0.001       -0.008    0.994\n\nSummary statistics:\n- Number of observations: 695\n- R-squared: 0.000, Adjusted R-squared: 0.000\n- F-statistic not available\n\n\n\nThe results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM yields that the high-beta stocks should yield higher expected returns. Our portfolio sort implicitly mimics an investment strategy that finances high-beta stocks by shorting low-beta stocks. Therefore, one should expect that the average excess returns yield a return that is above the risk-free rate.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "href": "python/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "title": "Univariate Portfolio Sorts",
    "section": "Functional Programming for Portfolio Sorts",
    "text": "Functional Programming for Portfolio Sorts\nNow, we take portfolio sorts to the next level. We want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming is very handy: we define a function that gives us flexibility concerning which variable to use for the sorting, denoted by sorting_variable. We use np.quantile() to compute breakpoints for n_portfolios. Then, we assign portfolios to stocks using the pd.cut() function. The output of the following function is a new column that contains the number of the portfolio to which a stock belongs.\nIn some applications, the variable used for the sorting might be clustered (e.g., at a lower bound of 0). Then, multiple breakpoints may be identical, leading to empty portfolios. Similarly, some portfolios might have a very small number of stocks at the beginning of the sample. Cases where the number of portfolio constituents differs substantially due to the distribution of the characteristics require careful consideration and, depending on the application, might require customized sorting approaches.\n\ndef assign_portfolio(data, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolios to a bin between breakpoints.\"\"\"\n    \n    breakpoints = np.quantile(\n      data[sorting_variable].dropna(), \n      np.linspace(0, 1, n_portfolios + 1), \n      method=\"linear\"\n    )\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nWe can use the above function to sort stocks into ten portfolios each month using lagged betas and compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.\n\nbeta_portfolios = (data_for_sorts\n  .groupby(\"month\")\n  .apply(lambda x: x.assign(\n      portfolio=assign_portfolio(x, \"beta_lag\", 10)\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"portfolio\", \"month\"])\n  .apply(lambda x: x.assign(\n      ret=np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    )\n  )\n  .reset_index(drop=True)\n  .merge(factors_ff3_monthly, how=\"left\", on=\"month\")\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#more-performance-evaluation",
    "href": "python/univariate-portfolio-sorts.html#more-performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "More Performance Evaluation",
    "text": "More Performance Evaluation\nIn the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns.\n\nbeta_portfolios_summary = (beta_portfolios\n  .groupby(\"portfolio\")\n  .apply(lambda x: x.assign(\n      alpha=sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[0],\n      beta=sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[1],\n      ret=x[\"ret\"].mean()\n    ).tail(1)\n  )\n  .reset_index(drop=True)\n  .get([\"portfolio\", \"alpha\", \"beta\", \"ret\"])\n)\n\nFigure 1 illustrates the CAPM alphas of beta-sorted portfolios. It shows that low-beta portfolios tend to exhibit positive alphas, while high-beta portfolios exhibit negative alphas.\n\nplot_beta_portfolios_summary = (\n  ggplot(beta_portfolios_summary, \n         aes(x=\"portfolio\", y=\"alpha\", fill=\"portfolio\")) +\n  geom_bar(stat=\"identity\") +\n  labs(x=\"Portfolio\", y=\"CAPM alpha\", fill=\"Portfolio\",\n       title=\"CAPM alphas of beta-sorted portfolios\") +\n  scale_y_continuous(labels=percent_format()) +\n  theme(legend_position=\"none\")\n)\nplot_beta_portfolios_summary.draw()\n\n\n\n\n\n\n\nFigure 1: The figure shows CAPM alphas of beta-sorted portfolios. Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the entire CRSP period.\n\n\n\n\n\nThese results suggest a negative relation between beta and future stock returns, which contradicts the predictions of the CAPM. According to the CAPM, returns should increase with beta across the portfolios and risk-adjusted returns should be statistically indistinguishable from zero.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#security-market-line-and-beta-portfolios",
    "href": "python/univariate-portfolio-sorts.html#security-market-line-and-beta-portfolios",
    "title": "Univariate Portfolio Sorts",
    "section": "Security Market Line and Beta Portfolios",
    "text": "Security Market Line and Beta Portfolios\nThe CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time. Figure 2 illustrates the security market line: We see that (not surprisingly) the high-beta portfolio returns have a high correlation with the market returns. However, it seems like the average excess returns for high-beta stocks are lower than what the security market line implies would be an “appropriate” compensation for the high market risk.\n\nsml_capm = (sm.OLS.from_formula(\n    formula=\"ret ~ 1 + beta\", \n    data=beta_portfolios_summary\n  )\n  .fit()\n  .params\n)\n\nplot_sml_capm = (\n  ggplot(beta_portfolios_summary,\n         aes(x=\"beta\", y=\"ret\", color=\"portfolio\")) +\n  geom_point() + \n  geom_abline(intercept=0,\n              slope=factors_ff3_monthly[\"mkt_excess\"].mean(),\n              linetype=\"solid\") +\n  geom_abline(intercept=sml_capm[\"Intercept\"],\n              slope=sml_capm[\"beta\"],\n              linetype=\"dashed\") +\n  labs(x=\"Beta\", y=\"Excess return\", color=\"Portfolio\",\n       title=\"Average portfolio excess returns and beta estimates\") +\n  scale_x_continuous(limits=(0, 2)) +\n  scale_y_continuous(labels=percent_format(),\n                     limits=(0, factors_ff3_monthly[\"mkt_excess\"].mean()*2))\n)\nplot_sml_capm.draw()\n\n\n\n\n\n\n\nFigure 2: The figure shows average portfolio excess returns and beta estimates. Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\n\nTo provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio.\n\nbeta_longshort = (beta_portfolios\n  .assign(\n    portfolio=lambda x: (\n      x[\"portfolio\"].apply(\n        lambda y: \"high\" if y == x[\"portfolio\"].max()\n        else (\"low\" if y == x[\"portfolio\"].min()\n        else y)\n      )\n    )\n  )\n  .query(\"portfolio in ['low', 'high']\")\n  .pivot_table(index=\"month\", columns=\"portfolio\", values=\"ret\")\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n  .merge(factors_ff3_monthly, how=\"left\", on=\"month\")\n)\n\nAgain, the resulting long-short strategy does not exhibit statistically significant returns.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept     0.002       0.003        0.549    0.583\n\nSummary statistics:\n- Number of observations: 695\n- R-squared: 0.000, Adjusted R-squared: 0.000\n- F-statistic not available\n\n\n\nHowever, controlling for the effect of beta, the long-short portfolio yields a statistically significant negative CAPM-adjusted alpha, although the average excess stock returns should be zero according to the CAPM. The results thus provide no evidence in support of the CAPM. The negative value has been documented as the so-called betting-against-beta factor (Frazzini and Pedersen 2014). Betting-against-beta corresponds to a strategy that shorts high-beta stocks and takes a (levered) long position in low-beta stocks. If borrowing constraints prevent investors from taking positions on the security market line they are instead incentivized to buy high-beta stocks, which leads to a relatively higher price (and therefore lower expected returns than implied by the CAPM) for such high-beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital-constrained investors with lower risk aversion.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1 + mkt_excess\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1 + mkt_excess\n\nCoefficients:\n            Estimate  Std. Error  t-Statistic  p-Value\nIntercept     -0.004       0.002       -1.931    0.053\nmkt_excess     1.129       0.069       16.392    0.000\n\nSummary statistics:\n- Number of observations: 695\n- R-squared: 0.439, Adjusted R-squared: 0.438\n- F-statistic: 268.700 on 1 and 693 DF, p-value: 0.000\n\n\n\nFigure 3 shows the annual returns of the extreme beta portfolios we are mainly interested in. The figure illustrates no consistent striking patterns over the last years; each portfolio exhibits periods with positive and negative annual returns.\n\nbeta_longshort_year = (beta_longshort\n  .assign(year=lambda x: x[\"month\"].dt.year)\n  .groupby(\"year\")\n  .aggregate(\n    low=(\"low\", lambda x: 1-(1+x).prod()),\n    high=(\"high\", lambda x: 1-(1+x).prod()),\n    long_short=(\"long_short\", lambda x: 1-(1+x).prod())\n  )\n  .reset_index()\n  .melt(id_vars=\"year\", var_name=\"name\", value_name=\"value\")\n)\n\nplot_beta_longshort_year = (\n  ggplot(beta_longshort_year, \n         aes(x=\"year\", y=\"value\", fill=\"name\")) +\n  geom_col(position='dodge') +\n  facet_wrap(\"~name\", ncol=1) +\n  labs(x=\"\", y=\"\", title=\"Annual returns of beta portfolios\") +\n  scale_color_discrete(guide=False) +\n  scale_y_continuous(labels=percent_format()) +\n  theme(legend_position=\"none\")\n)\nplot_beta_longshort_year.draw()\n\n\n\n\n\n\n\nFigure 3: The figure shows annual returns of beta portfolios. We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.\n\n\n\n\n\nOverall, this chapter shows how functional programming can be leveraged to form an arbitrary number of portfolios using any sorting variable and how to evaluate the performance of the resulting portfolios. In the next chapter, we dive deeper into the many degrees of freedom that arise in the context of portfolio analysis.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#exercises",
    "href": "python/univariate-portfolio-sorts.html#exercises",
    "title": "Univariate Portfolio Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nTake the two long-short beta strategies based on different numbers of portfolios and compare the returns. Is there a significant difference in returns? How do the Sharpe ratios compare between the strategies? Find one additional portfolio evaluation statistic and compute it.\nWe plotted the alphas of the ten beta portfolios above. Write a function that tests these estimates for significance. Which portfolios have significant alphas?\nThe analysis here is based on betas from monthly returns. However, we also computed betas from daily returns. Re-run the analysis and point out differences in the results.\nGiven the results in this chapter, can you define a long-short strategy that yields positive abnormal returns (i.e., alphas)? Plot the cumulative excess return of your strategy and the market excess return for comparison.\n\n\n\n\nFigure 1: The figure shows CAPM alphas of beta-sorted portfolios. Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the entire CRSP period.\nFigure 2: The figure shows average portfolio excess returns and beta estimates. Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\nFigure 3: The figure shows annual returns of beta portfolios. We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html",
    "href": "python/size-sorts-and-p-hacking.html",
    "title": "Size Sorts and p-Hacking",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we continue with portfolio sorts in a univariate setting. Yet, we consider firm size as a sorting variable, which gives rise to a well-known return factor: the size premium. The size premium arises from buying small stocks and selling large stocks. Prominently, Fama and French (1993) include it as a factor in their three-factor model. Apart from that, asset managers commonly include size as a key firm characteristic when making investment decisions.\nWe also introduce new choices in the formation of portfolios. In particular, we discuss listing exchanges, industries, weighting regimes, and periods. These choices matter for the portfolio returns and result in different size premiums Walter, Weber, and Weiss (2022). Exploiting these ideas to generate favorable results is called p-hacking. There is arguably a thin line between p-hacking and conducting robustness tests. Our purpose here is to illustrate the substantial variation that can arise along the evidence-generating process.\nThe chapter relies on the following set of Python packages:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom itertools import product\nfrom joblib import Parallel, delayed, cpu_count\nCompared to previous chapters, we introduce itertools, which is a component of the Python standard library and provides fast, memory-efficient tools for working with iterators.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#data-preparation",
    "href": "python/size-sorts-and-p-hacking.html#data-preparation",
    "title": "Size Sorts and p-Hacking",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we retrieve the relevant data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. Firm size is defined as market equity in most asset pricing applications that we retrieve from CRSP. We further use the Fama-French factor returns for performance evaluation.\n\ntidy_finance = sqlite3.connect(\n  database=\"data/tidy_finance_python.sqlite\"\n)\n\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp_monthly\", \n  con=tidy_finance, \n  parse_dates={\"month\"}\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM factors_ff3_monthly\", \n  con=tidy_finance, \n  parse_dates={\"month\"}\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#size-distribution",
    "href": "python/size-sorts-and-p-hacking.html#size-distribution",
    "title": "Size Sorts and p-Hacking",
    "section": "Size Distribution",
    "text": "Size Distribution\nBefore we build our size portfolios, we investigate the distribution of the variable firm size. Visualizing the data is a valuable starting point to understand the input to the analysis. Figure 1 shows the fraction of total market capitalization concentrated in the largest firm. To produce this graph, we create monthly indicators that track whether a stock belongs to the largest x percent of the firms. Then, we aggregate the firms within each bucket and compute the buckets’ share of total market capitalization.\nFigure 1 shows that the largest 1 percent of firms cover up to 50 percent of the total market capitalization, and holding just the 25 percent largest firms in the CRSP universe essentially replicates the market portfolio. The distribution of firm size thus implies that the largest firms of the market dominate many small firms whenever we use value-weighted benchmarks.\n\nmarket_cap_concentration = (crsp_monthly\n  .groupby(\"month\")\n  .apply(lambda x: x.assign(\n    top01=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.99)),\n    top05=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.95)),\n    top10=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.90)),\n    top25=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.75)))\n  )\n  .reset_index(drop=True)\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"Largest 1%\": x[\"mktcap\"][x[\"top01\"]].sum()/x[\"mktcap\"].sum(),\n    \"Largest 5%\": x[\"mktcap\"][x[\"top05\"]].sum()/x[\"mktcap\"].sum(),\n    \"Largest 10%\": x[\"mktcap\"][x[\"top10\"]].sum()/x[\"mktcap\"].sum(),\n    \"Largest 25%\": x[\"mktcap\"][x[\"top25\"]].sum()/x[\"mktcap\"].sum()\n    })\n  )\n  .reset_index()\n  .melt(id_vars=\"month\", var_name=\"name\", value_name=\"value\")\n)\n\nplot_market_cap_concentration = (\n  ggplot(market_cap_concentration, \n         aes(x=\"month\", y=\"value\", \n         color=\"name\", linetype=\"name\")) +\n  geom_line() +\n  scale_y_continuous(labels=percent_format()) +\n  scale_x_date(name=\"\", date_labels=\"%Y\") +\n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\", \n       title=(\"Percentage of total market capitalization in \"\n              \"largest stocks\")) +\n  theme(legend_title=element_blank())\n)\nplot_market_cap_concentration.draw()\n\n\n\n\n\n\n\nFigure 1: The figure shows the percentage of total market capitalization in largest stocks. We report the aggregate market capitalization of all stocks that belong to the 1, 5, 10, and 25 percent quantile of the largest firms in the monthly cross-section relative to the market capitalization of all stocks during the month.\n\n\n\n\n\nNext, firm sizes also differ across listing exchanges. The primary listings of stocks were important in the past and are potentially still relevant today. Figure 2 shows that the New York Stock Exchange (NYSE) was and still is the largest listing exchange in terms of market capitalization. More recently, NASDAQ has gained relevance as a listing exchange. Do you know what the small peak in NASDAQ’s market cap around the year 2000 was?\n\nmarket_cap_share = (crsp_monthly\n  .groupby([\"month\", \"exchange\"])\n  .aggregate({\"mktcap\": \"sum\"})\n  .reset_index(drop=False)\n  .groupby(\"month\")\n  .apply(lambda x:\n    x.assign(total_market_cap=lambda x: x[\"mktcap\"].sum(),\n             share=lambda x: x[\"mktcap\"]/x[\"total_market_cap\"]\n             )\n    )\n  .reset_index(drop=True)\n)\n\nplot_market_cap_share = (\n  ggplot(market_cap_share, \n         aes(x=\"month\", y=\"share\", \n             fill=\"exchange\", color=\"exchange\")) +\n  geom_area(position=\"stack\", stat=\"identity\", alpha=0.5) +\n  geom_line(position=\"stack\") +\n  scale_y_continuous(labels=percent_format()) +\n  scale_x_date(name=\"\", date_labels=\"%Y\") +\n  labs(x=\"\", y=\"\", fill=\"\", color=\"\",\n       title=\"Share of total market capitalization per listing exchange\") +\n  theme(legend_title=element_blank())\n)\nplot_market_cap_share.draw()\n\n\n\n\n\n\n\nFigure 2: The figure shows the share of total market capitalization per listing exchange. Years are on the horizontal axis and the corresponding share of total market capitalization per listing exchange on the vertical axis.\n\n\n\n\n\nFinally, we consider the distribution of firm size across listing exchanges and create summary statistics. The function describe() does not include all statistics we are interested in, which is why we create the function compute_summary() that adds the standard deviation and the number of observations. Then, we apply it to the most current month of our CRSP data on each listing exchange. We also add a row with the overall summary statistics.\nThe resulting table shows that firms listed on NYSE in December 2021 are significantly larger on average than firms listed on the other exchanges. Moreover, NASDAQ lists the largest number of firms. This discrepancy between firm sizes across listing exchanges motivated researchers to form breakpoints exclusively on the NYSE sample and apply those breakpoints to all stocks. In the following, we use this distinction to update our portfolio sort procedure.\n\ndef compute_summary(data, variable, filter_variable, percentiles):\n    \"\"\"Compute summary statistics for a given variable and percentiles.\"\"\"\n    \n    summary = (data\n      .get([filter_variable, variable])\n      .groupby(filter_variable)\n      .describe(percentiles=percentiles)\n    ) \n    \n    summary.columns = summary.columns.droplevel(0)\n    \n    summary_overall = (data\n      .get(variable)\n      .describe(percentiles=percentiles)\n    )\n    \n    summary.loc[\"Overall\", :] = summary_overall\n    \n    return summary.round(0)\n\ncompute_summary(\n  crsp_monthly[crsp_monthly[\"month\"] == crsp_monthly[\"month\"].max()],\n  variable=\"mktcap\",\n  filter_variable=\"exchange\",\n  percentiles=[0.05, 0.5, 0.95]\n)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nexchange\n\n\n\n\n\n\n\n\n\n\n\n\nAMEX\n162.0\n407.0\n2954.0\n3.0\n5.0\n45.0\n947.0\n37289.0\n\n\nNASDAQ\n2777.0\n5684.0\n58811.0\n1.0\n8.0\n265.0\n13040.0\n2058404.0\n\n\nNYSE\n1355.0\n16299.0\n47361.0\n11.0\n112.0\n2818.0\n67924.0\n495373.0\n\n\nOther\n1.0\n13310.0\nNaN\n13310.0\n13310.0\n13310.0\n13310.0\n13310.0\n\n\nOverall\n4295.0\n8836.0\n54500.0\n1.0\n11.0\n487.0\n34170.0\n2058404.0",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "href": "python/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "title": "Size Sorts and p-Hacking",
    "section": "Univariate Size Portfolios with Flexible Breakpoints",
    "text": "Univariate Size Portfolios with Flexible Breakpoints\nIn Univariate Portfolio Sorts, we construct portfolios with a varying number of breakpoints and different sorting variables. Here, we extend the framework such that we compute breakpoints on a subset of the data, for instance, based on selected listing exchanges. In published asset pricing articles, many scholars compute sorting breakpoints only on NYSE-listed stocks. These NYSE-specific breakpoints are then applied to the entire universe of stocks.\nTo replicate the NYSE-centered sorting procedure, we introduce exchanges as an argument in our assign_portfolio() function from Univariate Portfolio Sorts. The exchange-specific argument then enters in the filter data[\"exchanges\"].isin(exchanges). For example, if exchanges='NYSE' is specified, only stocks listed on NYSE are used to compute the breakpoints. Alternatively, you could specify exchanges=[\"NYSE\", \"NASDAQ\", \"AMEX\"], which keeps all stocks listed on either of these exchanges.\n\ndef assign_portfolio(data, exchanges, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolio for a given sorting variable.\"\"\"\n    \n    breakpoints = (data\n      .query(f\"exchange in {exchanges}\")\n      .get(sorting_variable)\n      .quantile(np.linspace(0, 1, num=n_portfolios+1), \n                interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    breakpoints.iloc[[0, -1]] = [-np.Inf, np.Inf]\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "href": "python/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "title": "Size Sorts and p-Hacking",
    "section": "Weighting Schemes for Portfolios",
    "text": "Weighting Schemes for Portfolios\nApart from computing breakpoints on different samples, researchers often use different portfolio weighting schemes. So far, we weighted each portfolio constituent by its relative market equity of the previous period. This protocol is called value-weighting. The alternative protocol is equal-weighting, which assigns each stock’s return the same weight, i.e., a simple average of the constituents’ returns. Notice that equal-weighting is difficult in practice as the portfolio manager needs to rebalance the portfolio monthly, while value-weighting is a truly passive investment.\nWe implement the two weighting schemes in the function compute_portfolio_returns() that takes a logical argument to weight the returns by firm value. Additionally, the long-short portfolio is long in the smallest firms and short in the largest firms, consistent with research showing that small firms outperform their larger counterparts. Apart from these two changes, the function is similar to the procedure in Univariate Portfolio Sorts.\n\ndef calculate_returns(data, value_weighted):\n    \"\"\"Calculate (value-weighted) returns.\"\"\"\n    \n    if value_weighted:\n      return np.average(data[\"ret_excess\"], weights=data[\"mktcap_lag\"])\n    else:\n      return data[\"ret_excess\"].mean()\n          \ndef compute_portfolio_returns(n_portfolios=10, \n                              exchanges=[\"NYSE\", \"NASDAQ\", \"AMEX\"], \n                              value_weighted=True, \n                              data=crsp_monthly):\n    \"\"\"Compute (value-weighted) portfolio returns.\"\"\"\n    \n    returns = (data\n      .groupby(\"month\")\n      .apply(lambda x: x.assign(\n        portfolio=assign_portfolio(x, exchanges, \n                                   \"mktcap_lag\", n_portfolios))\n      )\n      .reset_index(drop=True)\n      .groupby([\"portfolio\", \"month\"])\n      .apply(lambda x: x.assign(\n        ret=calculate_returns(x, value_weighted))\n      )\n      .reset_index(drop=True)\n      .groupby(\"month\")\n      .apply(lambda x: \n        pd.Series({\"size_premium\": x.loc[x[\"portfolio\"].idxmin(), \"ret\"]-\n          x.loc[x[\"portfolio\"].idxmax(), \"ret\"]}))\n      .reset_index(drop=True)\n      .aggregate({\"size_premium\": \"mean\"})\n    )\n    \n    return returns\n\nTo see how the function compute_portfolio_returns() works, we consider a simple median breakpoint example with value-weighted returns. We are interested in the effect of restricting listing exchanges on the estimation of the size premium. In the first function call, we compute returns based on breakpoints from all listing exchanges. Then, we computed returns based on breakpoints from NYSE-listed stocks.\n\nret_all = compute_portfolio_returns(\n  n_portfolios=2,\n  exchanges=[\"NYSE\", \"NASDAQ\", \"AMEX\"],\n  value_weighted=True,\n  data=crsp_monthly\n)\n\nret_nyse = compute_portfolio_returns(\n  n_portfolios=2,\n  exchanges=[\"NYSE\"],\n  value_weighted=True,\n  data=crsp_monthly\n)\n\ndata = pd.DataFrame([ret_all*100, ret_nyse*100], \n                    index=[\"NYSE, NASDAQ & AMEX\", \"NYSE\"])\ndata.columns = [\"Premium\"]\ndata.round(2)\n\n\n\n\n\n\n\n\n\nPremium\n\n\n\n\nNYSE, NASDAQ & AMEX\n0.08\n\n\nNYSE\n0.16\n\n\n\n\n\n\n\n\nThe table shows that the size premium is more than 60 percent larger if we consider only stocks from NYSE to form the breakpoint each month. The NYSE-specific breakpoints are larger, and there are more than 50 percent of the stocks in the entire universe in the resulting small portfolio because NYSE firms are larger on average. The impact of this choice is not negligible.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "href": "python/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "title": "Size Sorts and p-Hacking",
    "section": "P-Hacking and Non-Standard Errors",
    "text": "P-Hacking and Non-Standard Errors\nSince the choice of the listing exchange has a significant impact, the next step is to investigate the effect of other data processing decisions researchers have to make along the way. In particular, any portfolio sort analysis has to decide at least on the number of portfolios, the listing exchanges to form breakpoints, and equal- or value-weighting. Further, one may exclude firms that are active in the finance industry or restrict the analysis to some parts of the time series. All of the variations of these choices that we discuss here are part of scholarly articles published in the top finance journals. We refer to Walter, Weber, and Weiss (2022) for an extensive set of other decision nodes at the discretion of researchers.\nThe intention of this application is to show that the different ways to form portfolios result in different estimated size premiums. Despite the effects of this multitude of choices, there is no correct way. It should also be noted that none of the procedures is wrong. The aim is simply to illustrate the changes that can arise due to the variation in the evidence-generating process (Menkveld et al. 2021). The term non-standard errors refers to the variation due to (suitable) choices made by researchers. Interestingly, in a large-scale study, Menkveld et al. (2021) find that the magnitude of non-standard errors are similar to the estimation uncertainty based on a chosen model. This shows how important it is to adjust for the seemingly innocent choices in the data preparation and evaluation workflow. Moreover, it seems that this methodology-related uncertainty should be embraced rather than hidden away.\nFrom a malicious perspective, these modeling choices give the researcher multiple chances to find statistically significant results. Yet this is considered p-hacking, which renders the statistical inference invalid due to multiple testing (Harvey, Liu, and Zhu 2016).\nNevertheless, the multitude of options creates a problem since there is no single correct way of sorting portfolios. How should a researcher convince a reader that their results do not come from a p-hacking exercise? To circumvent this dilemma, academics are encouraged to present evidence from different sorting schemes as robustness tests and report multiple approaches to show that a result does not depend on a single choice. Thus, the robustness of premiums is a key feature.\nBelow, we conduct a series of robustness tests, which could also be interpreted as a p-hacking exercise. To do so, we examine the size premium in different specifications presented in the table p_hacking_setup. The function itertools.product() produces all possible permutations of its arguments. Note that we use the argument data to exclude financial firms and truncate the time series.\n\nn_portfolios = [2, 5, 10]\nexchanges = [[\"NYSE\"], [\"NYSE\", \"NASDAQ\", \"AMEX\"]]\nvalue_weighted = [True, False]\ndata = [\n  crsp_monthly,\n  crsp_monthly[crsp_monthly[\"industry\"] != \"Finance\"],\n  crsp_monthly[crsp_monthly[\"month\"] &lt; \"1990-06-01\"],\n  crsp_monthly[crsp_monthly[\"month\"] &gt;= \"1990-06-01\"],\n]\np_hacking_setup = list(\n  product(n_portfolios, exchanges, value_weighted, data)\n)\n\nTo speed the computation up, we parallelize the (many) different sorting procedures, as in Beta Estimation using the joblib package. Finally, we report the resulting size premiums in descending order. There are indeed substantial size premiums possible in our data, in particular when we use equal-weighted portfolios.\n\nn_cores = cpu_count()-1\np_hacking_results = pd.concat(\n  Parallel(n_jobs=n_cores)\n  (delayed(compute_portfolio_returns)(x, y, z, w) \n   for x, y, z, w in p_hacking_setup)\n)\np_hacking_results = p_hacking_results.reset_index(name=\"size_premium\")",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#size-premium-variation",
    "href": "python/size-sorts-and-p-hacking.html#size-premium-variation",
    "title": "Size Sorts and p-Hacking",
    "section": "Size-Premium Variation",
    "text": "Size-Premium Variation\nWe provide a graph in Figure 3 that shows the different premiums. The figure also shows the relation to the average Fama-French SMB (small minus big) premium used in the literature, which we include as a dotted vertical line.\n\np_hacking_results_figure = (\n  ggplot(p_hacking_results, \n         aes(x=\"size_premium\")) +\n  geom_histogram(bins=len(p_hacking_results)) +\n  scale_x_continuous(labels=percent_format()) +\n  labs(x=\"\", y=\"\",\n       title=\"Distribution of size premiums for various sorting choices\") +\n  geom_vline(aes(xintercept=factors_ff3_monthly[\"smb\"].mean()), \n                 linetype=\"dashed\")\n)\np_hacking_results_figure.draw()\n\n\n\n\n\n\n\nFigure 3: The figure shows the distribution of size premiums for various sorting choices. The dashed vertical line indicates the average Fama-French SMB premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#exercises",
    "href": "python/size-sorts-and-p-hacking.html#exercises",
    "title": "Size Sorts and p-Hacking",
    "section": "Exercises",
    "text": "Exercises\n\nWe gained several insights on the size distribution above. However, we did not analyze the average size across listing exchanges and industries. Which listing exchanges/industries have the largest firms? Plot the average firm size for the three listing exchanges over time. What do you conclude?\nWe compute breakpoints but do not take a look at them in the exposition above. This might cover potential data errors. Plot the breakpoints for ten size portfolios over time. Then, take the difference between the two extreme portfolios and plot it. Describe your results.\nThe returns that we analyze above do not account for differences in the exposure to market risk, i.e., the CAPM beta. Change the function compute_portfolio_returns() to output the CAPM alpha or beta instead of the average excess return.\nWhile you saw the spread in returns from the p-hacking exercise, we did not show which choices led to the largest effects. Find a way to investigate which choice variable has the largest impact on the estimated size premium.\nWe computed several size premiums, but they do not follow the definition of Fama and French (1993). Which of our approaches comes closest to their SMB premium?\n\n\n\n\nFigure 1: The figure shows the percentage of total market capitalization in largest stocks. We report the aggregate market capitalization of all stocks that belong to the 1, 5, 10, and 25 percent quantile of the largest firms in the monthly cross-section relative to the market capitalization of all stocks during the month.\nFigure 2: The figure shows the share of total market capitalization per listing exchange. Years are on the horizontal axis and the corresponding share of total market capitalization per listing exchange on the vertical axis.\nFigure 3: The figure shows the distribution of size premiums for various sorting choices. The dashed vertical line indicates the average Fama-French SMB premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html",
    "href": "python/replicating-fama-and-french-factors.html",
    "title": "Replicating Fama-French Factors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we provide a replication of the famous Fama-French factor portfolios. The Fama-French factor models are a cornerstone of empirical asset pricing Fama and French (2015). On top of the market factor represented by the traditional CAPM beta, the three-factor model includes the size and value factors to explain the cross section of returns. Its successor, the five-factor model, additionally includes profitability and investment as explanatory factors.\nWe start with the three-factor model. We already introduced the size and value factors in Value and Bivariate Sorts, and their definition remains the same: size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nAfter the replication of the three-factor model, we move to the five-factors by constructing the profitability factor RMW (robust-minus-weak) as the difference between the returns of firms with high and low operating profitability and the investment factor CMA (conservative-minus-aggressive) as the difference between firms with high versus low investment rates.\nThe current chapter relies on this set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n\nfrom regtabletotext import prettify_result",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#data-preparation",
    "href": "python/replicating-fama-and-french-factors.html#data-preparation",
    "title": "Replicating Fama-French Factors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and Compustat as data sources, as we need exactly the same variables to compute the size and value factors in the way Fama and French do it. Hence, there is nothing new below, and we only load data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.1 \n\ntidy_finance = sqlite3.connect(\n  database=\"data/tidy_finance_python.sqlite\"\n)\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=(\"SELECT permno, gvkey, month, ret_excess, mktcap, \"\n         \"mktcap_lag, exchange FROM crsp_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"month\"})\n  .dropna()\n)\n\ncompustat = (pd.read_sql_query(\n    sql=\"SELECT gvkey, datadate, be, op, inv FROM compustat\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"})\n  .dropna()\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT month, smb, hml FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\nfactors_ff5_monthly = pd.read_sql_query(\n  sql=\"SELECT month, smb, hml, rmw, cma FROM factors_ff5_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\nYet when we start merging our dataset for computing the premiums, there are a few differences to Value and Bivariate Sorts. First, Fama and French form their portfolios in June of year \\(t\\), whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year \\(t+1\\).\nSecond, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year \\(t - 1\\) and the book equity reported in year \\(t-1\\), i.e., the datadate is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity.\nTo implement all these time lags, we again employ the temporary sorting_date-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year. We ensure this by a call of drop_duplicates() at the end of the chunk below.\n\nsize = (crsp_monthly\n  .query(\"month.dt.month == 6\")\n  .assign(sorting_date=lambda x: (x[\"month\"]+pd.DateOffset(months=1)))\n  .get([\"permno\", \"exchange\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"size\"})\n)\n\nmarket_equity = (crsp_monthly\n  .query(\"month.dt.month == 12\")\n  .assign(sorting_date=lambda x: (x[\"month\"]+pd.DateOffset(months=7)))\n  .get([\"permno\", \"gvkey\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"me\"})\n)\n\nbook_to_market = (compustat\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      (x[\"datadate\"].dt.year+1).astype(str)+\"0701\", format=\"%Y%m%d\")\n    )\n  )\n  .merge(market_equity, how=\"inner\", on=[\"gvkey\", \"sorting_date\"])\n  .assign(bm=lambda x: x[\"be\"]/x[\"me\"])\n  .get([\"permno\", \"sorting_date\", \"me\", \"bm\"])\n)\n\nsorting_variables = (size\n  .merge(book_to_market, how=\"inner\", on=[\"permno\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"permno\", \"sorting_date\"])\n )",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#portfolio-sorts",
    "href": "python/replicating-fama-and-french-factors.html#portfolio-sorts",
    "title": "Replicating Fama-French Factors",
    "section": "Portfolio Sorts",
    "text": "Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on NYSE-specific breakpoints, they form two portfolios in the size dimension at the median and three portfolios in the dimension of book-to-market at the 30- and 70-percentiles, and they use independent sorts. The sorts for book-to-market require an adjustment to the function in Value and Bivariate Sorts because it would not produce the right breakpoints. Instead of n_portfolios, we now specify percentiles, which take the breakpoint-sequence as an object specified in the function’s call. Specifically, we give percentiles = [0, 0.3, 0.7, 1] to the function. Additionally, we perform a join with our return data to ensure that we only use traded stocks when computing the breakpoints as a first step.\n\ndef assign_portfolio(data, sorting_variable, percentiles):\n    \"\"\"Assign portfolios to a bin according to a sorting variable.\"\"\"\n    \n    breakpoints = (data\n      .query(\"exchange == 'NYSE'\")\n      .get(sorting_variable)\n      .quantile(percentiles, interpolation=\"linear\")\n    )\n    breakpoints.iloc[0] = -np.Inf\n    breakpoints.iloc[breakpoints.size-1] = np.Inf\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=pd.Series(range(1, breakpoints.size)),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nportfolios = (sorting_variables\n  .groupby(\"sorting_date\")\n  .apply(lambda x: x\n    .assign(\n      portfolio_size=assign_portfolio(x, \"size\", [0, 0.5, 1]),\n      portfolio_bm=assign_portfolio(x, \"bm\", [0, 0.3, 0.7, 1])\n    )\n  )\n  .reset_index(drop=True)\n  .get([\"permno\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"])\n)\n\nNext, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column sorting_date in our return data by setting the date to sort on to July of \\(t-1\\) if the month is June (of year \\(t\\)) or earlier or to July of year \\(t\\) if the month is July or later.\n\nportfolios = (crsp_monthly\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      x[\"month\"].apply(lambda x: str(x.year-1)+\n        \"0701\" if x.month &lt;= 6 else str(x.year)+\"0701\")))\n  )\n  .merge(portfolios, how=\"inner\", on=[\"permno\", \"sorting_date\"])\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#fama-french-three-factor-model",
    "href": "python/replicating-fama-and-french-factors.html#fama-french-three-factor-model",
    "title": "Replicating Fama-French Factors",
    "section": "Fama-French Three-Factor Model",
    "text": "Fama-French Three-Factor Model\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama-French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally (using the mean() function).\n\nfactors_replicated = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"month\"])\n  .apply(lambda x: pd.Series({\n    \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n   )\n  .reset_index()\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"smb_replicated\": (\n      x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_size\"] == 2].mean()),\n    \"hml_replicated\": (\n      x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() -\n        x[\"ret\"][x[\"portfolio_bm\"] == 1].mean())\n    }))\n  .reset_index()\n)\n\nfactors_replicated = (factors_replicated\n  .merge(factors_ff3_monthly, how=\"inner\", on=\"month\")\n  .round(4)\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#replication-evaluation",
    "href": "python/replicating-fama-and-french-factors.html#replication-evaluation",
    "title": "Replicating Fama-French Factors",
    "section": "Replication Evaluation",
    "text": "Replication Evaluation\nIn the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using smf.ols(). If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to one (indicating a high correlation), and an adjusted R-squared close to one (indicating a high proportion of explained variance).\nTo test the success of the SMB factor, we hence run the following regression:\n\nmodel_smb = (smf.ols(\n    formula=\"smb ~ smb_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_smb)\n\nOLS Model:\nsmb ~ smb_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept         -0.000       0.000       -1.334    0.183\nsmb_replicated     0.994       0.004      231.343    0.000\n\nSummary statistics:\n- Number of observations: 726\n- R-squared: 0.987, Adjusted R-squared: 0.987\n- F-statistic: 53,519.615 on 1 and 724 DF, p-value: 0.000\n\n\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient is 0.99 and R-squared are at 99 percent.\n\nmodel_hml = (smf.ols(\n    formula=\"hml ~ hml_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_hml)\n\nOLS Model:\nhml ~ hml_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          0.000       0.000        1.633    0.103\nhml_replicated     0.963       0.007      133.678    0.000\n\nSummary statistics:\n- Number of observations: 726\n- R-squared: 0.961, Adjusted R-squared: 0.961\n- F-statistic: 17,869.727 on 1 and 724 DF, p-value: 0.000\n\n\n\nThe replication of the HML factor is also a success, although at a slightly lower level with coefficient of 0.96 and R-squared around 96 percent.\nThe evidence allows us to conclude that we did a relatively good job in replicating the original Fama-French size and value premiums, although we do not know their underlying code. From our perspective, a perfect match is only possible with additional information from the maintainers of the original data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#fama-french-five-factor-model",
    "href": "python/replicating-fama-and-french-factors.html#fama-french-five-factor-model",
    "title": "Replicating Fama-French Factors",
    "section": "Fama-French Five-Factor Model",
    "text": "Fama-French Five-Factor Model\nNow, let us move to the replication of the five-factor model. We extend the other_sorting_variables table from above with the additional characteristics operating profitability op and investment inv. Note that the dropna() statement yields different sample sizes, as some firms with be values might not have op or inv values.\n\nother_sorting_variables = (compustat\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      (x[\"datadate\"].dt.year+1).astype(str)+\"0701\", format=\"%Y%m%d\")\n    )\n  )\n  .merge(market_equity, how=\"inner\", on=[\"gvkey\", \"sorting_date\"])\n  .assign(bm=lambda x: x[\"be\"]/x[\"me\"])\n  .get([\"permno\", \"sorting_date\", \"me\", \"bm\", \"op\", \"inv\"])\n)\n\nsorting_variables = (size\n  .merge(other_sorting_variables, how=\"inner\", on=[\"permno\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"permno\", \"sorting_date\"])\n )\n\nIn each month, we independently sort all stocks into the two size portfolios. The value, profitability, and investment portfolios, on the other hand, are the results of dependent sorts based on the size portfolios. We then merge the portfolios to the return data for the rest of the year just as above.\n\nportfolios = (sorting_variables\n  .groupby(\"sorting_date\")\n  .apply(lambda x: x\n    .assign(\n      portfolio_size=assign_portfolio(x, \"size\", [0, 0.5, 1])\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"sorting_date\", \"portfolio_size\"])\n  .apply(lambda x: x\n    .assign(\n      portfolio_bm=assign_portfolio(x, \"bm\", [0, 0.3, 0.7, 1]),\n      portfolio_op=assign_portfolio(x, \"op\", [0, 0.3, 0.7, 1]),\n      portfolio_inv=assign_portfolio(x, \"inv\", [0, 0.3, 0.7, 1])\n    )\n  )\n  .reset_index(drop=True)\n  .get([\"permno\", \"sorting_date\", \n        \"portfolio_size\", \"portfolio_bm\",\n        \"portfolio_op\", \"portfolio_inv\"])\n)\n\nportfolios = (crsp_monthly\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      x[\"month\"].apply(lambda x: str(x.year-1)+\n        \"0701\" if x.month &lt;= 6 else str(x.year)+\"0701\")))\n  )\n  .merge(portfolios, how=\"inner\", on=[\"permno\", \"sorting_date\"])\n)\n\nNow, we want to construct each of the factors, but this time, the size factor actually comes last because it is the result of averaging across all other factor portfolios. This dependency is the reason why we keep the table with value-weighted portfolio returns as a separate object that we reuse later. We construct the value factor, HML, as above by going long the two portfolios with high book-to-market ratios and shorting the two portfolios with low book-to-market.\n\nportfolios_value = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"month\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_value = (portfolios_value\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"hml_replicated\": (\n      x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_bm\"] == 1].mean())})\n  )\n  .reset_index()\n)\n\nFor the profitability factor, RMW (robust-minus-weak), we take a long position in the two high profitability portfolios and a short position in the two low profitability portfolios.\n\nportfolios_profitability = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_op\", \"month\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_profitability = (portfolios_profitability\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"rmw_replicated\": (\n      x[\"ret\"][x[\"portfolio_op\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_op\"] == 1].mean())})\n  )\n  .reset_index()\n)\n\nFor the investment factor, CMA (conservative-minus-aggressive), we go long the two low investment portfolios and short the two high investment portfolios.\n\nportfolios_investment = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_inv\", \"month\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_investment = (portfolios_investment\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"cma_replicated\": (\n      x[\"ret\"][x[\"portfolio_inv\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_inv\"] == 3].mean())})\n  )\n  .reset_index()\n)\n\nFinally, the size factor, SMB, is constructed by going long the six small portfolios and short the six large portfolios.\n\nfactors_size = (\n  pd.concat(\n    [portfolios_value, portfolios_profitability, portfolios_investment], \n    ignore_index=True\n  )\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"smb_replicated\": (\n      x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_size\"] == 2].mean())})\n  )\n  .reset_index()\n)\n\nWe then join all factors together into one dataframe and construct again a suitable table to run tests for evaluating our replication.\n\nfactors_replicated = (factors_size\n  .merge(factors_value, how=\"outer\", on=\"month\")\n  .merge(factors_profitability, how=\"outer\", on=\"month\")\n  .merge(factors_investment, how=\"outer\", on=\"month\")\n)\n\nfactors_replicated = (factors_replicated\n  .merge(factors_ff5_monthly, how=\"inner\", on=\"month\")\n  .round(4)\n)\n\nLet us start the replication evaluation again with the size factor:\n\nmodel_smb = (smf.ols(\n    formula=\"smb ~ smb_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_smb)\n\nOLS Model:\nsmb ~ smb_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          -0.00       0.000       -1.493    0.136\nsmb_replicated      0.97       0.004      222.489    0.000\n\nSummary statistics:\n- Number of observations: 714\n- R-squared: 0.986, Adjusted R-squared: 0.986\n- F-statistic: 49,501.421 on 1 and 712 DF, p-value: 0.000\n\n\n\nThe results for the SMB factor are quite convincing, as all three criteria outlined above are met and the coefficient is 0.97 and the R-squared is at 99 percent.\n\nmodel_hml = (smf.ols(\n    formula=\"hml ~ hml_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_hml)\n\nOLS Model:\nhml ~ hml_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          0.000        0.00        1.588    0.113\nhml_replicated     0.991        0.01       96.694    0.000\n\nSummary statistics:\n- Number of observations: 714\n- R-squared: 0.929, Adjusted R-squared: 0.929\n- F-statistic: 9,349.714 on 1 and 712 DF, p-value: 0.000\n\n\n\nThe replication of the HML factor is also a success, although at a slightly higher coefficient of 0.99 and an R-squared around 93 percent.\n\nmodel_rmw = (smf.ols(\n    formula=\"rmw ~ rmw_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_rmw)\n\nOLS Model:\nrmw ~ rmw_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          0.000       0.000        0.268    0.789\nrmw_replicated     0.954       0.009      107.038    0.000\n\nSummary statistics:\n- Number of observations: 714\n- R-squared: 0.941, Adjusted R-squared: 0.941\n- F-statistic: 11,457.150 on 1 and 712 DF, p-value: 0.000\n\n\n\nWe are also able to replicate the RMW factor quite well with a coefficient of 0.95 and an R-squared around 94 percent.\n\nmodel_cma = (smf.ols(\n    formula=\"cma ~ cma_replicated\", \n    data=factors_replicated\n  )\n  .fit()\n)\nprettify_result(model_cma)\n\nOLS Model:\ncma ~ cma_replicated\n\nCoefficients:\n                Estimate  Std. Error  t-Statistic  p-Value\nIntercept          0.001       0.000        4.002      0.0\ncma_replicated     0.964       0.008      117.396      0.0\n\nSummary statistics:\n- Number of observations: 714\n- R-squared: 0.951, Adjusted R-squared: 0.951\n- F-statistic: 13,781.776 on 1 and 712 DF, p-value: 0.000\n\n\n\nFinally, the CMA factor also replicates well with a coefficient of 0.96 and an R-squared around 95 percent.\nOverall, our approach seems to replicate the Fama-French three and five-factor models just as well as the three-factors.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#exercises",
    "href": "python/replicating-fama-and-french-factors.html#exercises",
    "title": "Replicating Fama-French Factors",
    "section": "Exercises",
    "text": "Exercises\n\nFama and French (1993) claim that their sample excludes firms until they have appeared in Compustat for two years. Implement this additional filter and compare the improvements of your replication effort.\nOn his homepage, Kenneth French provides instructions on how to construct the most common variables used for portfolio sorts. Try to replicate the univariate portfolio sort return time series for E/P (earnings/price) provided on his homepage and evaluate your replication effort using regressions.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#footnotes",
    "href": "python/replicating-fama-and-french-factors.html#footnotes",
    "title": "Replicating Fama-French Factors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that Fama and French (1992) claim to exclude financial firms. To a large extent this happens through using industry format “INDL”, as we do in WRDS, CRSP, and Compustat. Neither the original paper, nor Ken French’s website, or the WRDS replication contains any indication that financial companies are excluded using additional filters such as industry codes.↩︎",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html",
    "href": "python/parametric-portfolio-policies.html",
    "title": "Parametric Portfolio Policies",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we apply different portfolio performance measures to evaluate and compare portfolio allocation strategies. For this purpose, we introduce a direct way to estimate optimal portfolio weights for large-scale cross-sectional applications. More precisely, the approach of Brandt, Santa-Clara, and Valkanov (2009) proposes to parametrize the optimal portfolio weights as a function of stock characteristics instead of estimating the stock’s expected return, variance, and covariances with other stocks in a prior step. We choose weights as a function of characteristics that maximize the expected utility of the investor. This approach is feasible for large portfolio dimensions (such as the entire CRSP universe) and has been proposed by Brandt, Santa-Clara, and Valkanov (2009). See the review paper by Brandt (2010) for an excellent treatment of related portfolio choice methods.\nThe current chapter relies on the following set of Python packages:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n\nfrom itertools import product, starmap\nfrom scipy.optimize import minimize\nCompared to previous chapters, we introduce the scipy.optimize module from the scipy (Virtanen et al. 2020) for solving optimization problems.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#data-preparation",
    "href": "python/parametric-portfolio-policies.html#data-preparation",
    "title": "Parametric Portfolio Policies",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the monthly CRSP file, which forms our investment universe. We load the data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=(\"SELECT permno, month, ret_excess, mktcap, mktcap_lag \"\n         \"FROM crsp_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"month\"})\n  .dropna()\n)\n\nTo evaluate the performance of portfolios, we further use monthly market returns as a benchmark to compute CAPM alphas.\n\nfactors_ff_monthly = pd.read_sql_query(\n  sql=\"SELECT month, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\nNext, we retrieve some stock characteristics that have been shown to have an effect on the expected returns or expected variances (or even higher moments) of the return distribution. In particular, we record the lagged one-year return momentum (momentum_lag), defined as the compounded return between months \\(t-13\\) and \\(t-2\\) for each firm. In finance, momentum is the empirically observed tendency for rising asset prices to rise further and falling prices to keep falling (Jegadeesh and Titman 1993). The second characteristic is the firm’s market equity (size_lag), defined as the log of the price per share times the number of shares outstanding (Banz 1981). To construct the correct lagged values, we use the approach introduced in WRDS, CRSP, and Compustat.\n\ncrsp_monthly_lags = (crsp_monthly\n  .assign(month=lambda x: x[\"month\"]+pd.DateOffset(months=13))\n  .get([\"permno\", \"month\", \"mktcap\"])\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(crsp_monthly_lags, \n         how=\"inner\", on=[\"permno\", \"month\"], suffixes=[\"\", \"_13\"])\n)\n\ndata_portfolios = (crsp_monthly\n  .assign(\n    momentum_lag=lambda x: x[\"mktcap_lag\"]/x[\"mktcap_13\"],\n    size_lag=lambda x: np.log(x[\"mktcap_lag\"])\n  )\n  .dropna(subset=[\"momentum_lag\", \"size_lag\"])\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "href": "python/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "title": "Parametric Portfolio Policies",
    "section": "Parametric Portfolio Policies",
    "text": "Parametric Portfolio Policies\nThe basic idea of parametric portfolio weights is as follows. Suppose that at each date \\(t\\), we have \\(N_t\\) stocks in the investment universe, where each stock \\(i\\) has a return of \\(r_{i, t+1}\\) and is associated with a vector of firm characteristics \\(x_{i, t}\\) such as time-series momentum or the market capitalization. The investor’s problem is to choose portfolio weights \\(w_{i,t}\\) to maximize the expected utility of the portfolio return: \\[\\begin{aligned}\n\\max_{\\omega} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{i=1}^{N_t}\\omega_{i,t}\\cdot r_{i,t+1}\\right)\\right]\n\\end{aligned} \\tag{1}\\] where \\(u(\\cdot)\\) denotes the utility function.\nWhere do the stock characteristics show up? We parameterize the optimal portfolio weights as a function of the stock characteristic \\(x_{i,t}\\) with the following linear specification for the portfolio weights: \\[\\omega_{i,t} = \\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}, \\tag{2}\\] where \\(\\bar{\\omega}_{i,t}\\) is a stock’s weight in a benchmark portfolio (we use the value-weighted or naive portfolio in the application below), \\(\\theta\\) is a vector of coefficients which we are going to estimate, and \\(\\hat{x}_{i,t}\\) are the characteristics of stock \\(i\\), cross-sectionally standardized to have zero mean and unit standard deviation.\nIntuitively, the portfolio strategy is a form of active portfolio management relative to a performance benchmark. Deviations from the benchmark portfolio are derived from the individual stock characteristics. Note that by construction, the weights sum up to one as \\(\\sum_{i=1}^{N_t}\\hat{x}_{i,t} = 0\\) due to the standardization. Moreover, the coefficients are constant across assets and over time. The implicit assumption is that the characteristics fully capture all aspects of the joint distribution of returns that are relevant for forming optimal portfolios.\nWe first implement cross-sectional standardization for the entire CRSP universe. We also keep track of (lagged) relative market capitalization relative_mktcap, which will represent the value-weighted benchmark portfolio, while n denotes the number of traded assets \\(N_t\\), which we use to construct the naive portfolio benchmark.\n\ndata_portfolios = (data_portfolios\n  .groupby(\"month\")\n  .apply(lambda x: x.assign(\n      relative_mktcap=x[\"mktcap_lag\"]/x[\"mktcap_lag\"].sum()\n    )\n  )\n  .reset_index(drop=True)\n  .set_index(\"month\")\n  .groupby(level=\"month\")\n  .transform(\n    lambda x: (x-x.mean())/x.std() if x.name.endswith(\"lag\") else x\n  )\n  .reset_index()\n  .drop([\"mktcap_lag\"], axis=1)\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#computing-portfolio-weights",
    "href": "python/parametric-portfolio-policies.html#computing-portfolio-weights",
    "title": "Parametric Portfolio Policies",
    "section": "Computing Portfolio Weights",
    "text": "Computing Portfolio Weights\nNext, we move on to identify optimal choices of \\(\\theta\\). We rewrite the optimization problem together with the weight parametrization and can then estimate \\(\\theta\\) to maximize the objective function based on our sample \\[\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{i=1}^{N_t}\\left(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\right)r_{i,t+1}\\right).\n\\end{aligned} \\tag{3}\\] The allocation strategy is straightforward because the number of parameters to estimate is small. Instead of a tedious specification of the \\(N_t\\) dimensional vector of expected returns and the \\(N_t(N_t+1)/2\\) free elements of the covariance matrix, all we need to focus on in our application is the vector \\(\\theta\\). \\(\\theta\\) contains only two elements in our application: the relative deviation from the benchmark due to size and momentum.\nTo get a feeling for the performance of such an allocation strategy, we start with an arbitrary initial vector \\(\\theta_0\\). The next step is to choose \\(\\theta\\) optimally to maximize the objective function. We automatically detect the number of parameters by counting the number of columns with lagged values. Note that the value for \\(\\theta\\) of 1.5 is an arbitrary choice.\n\nlag_columns = [i for i in data_portfolios.columns if \"lag\" in i]\nn_parameters = len(lag_columns)\ntheta = pd.DataFrame({\"theta\": [1.5]*n_parameters}, index=lag_columns)\n\nThe function compute_portfolio_weights() below computes the portfolio weights \\(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\) according to our parametrization for a given value \\(\\theta_0\\). Everything happens within a single pipeline. Hence, we provide a short walk-through.\nWe first compute characteristic_tilt, the tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{i, t}\\) which resemble the deviation from the benchmark portfolio. Next, we compute the benchmark portfolio weight_benchmark, which can be any reasonable set of portfolio weights. In our case, we choose either the value or equal-weighted allocation. weight_tilt completes the picture and contains the final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt, which deviate from the benchmark portfolio depending on the stock characteristics.\nThe final few lines go a bit further and implement a simple version of a no-short sale constraint. While it is generally not straightforward to ensure portfolio weight constraints via parameterization, we simply normalize the portfolio weights such that they are enforced to be positive. Finally, we make sure that the normalized weights sum up to one again: \\[\\omega_{i,t}^+ = \\frac{\\max(0, \\omega_{i,t})}{\\sum_{j=1}^{N_t}\\max(0, \\omega_{i,t})}. \\tag{4}\\]\nThe following function computes the optimal portfolio weights in the way just described.\n\ndef compute_portfolio_weights(theta, \n                              data,\n                              value_weighting=True,\n                              allow_short_selling=True):\n    \"\"\"Compute portfolio weights for different strategies.\"\"\"\n    \n    lag_columns = [i for i in data.columns if \"lag\" in i]\n    theta = pd.DataFrame(theta, index=lag_columns)\n\n    data = (data\n      .groupby(\"month\")\n      .apply(lambda x: x.assign(\n          characteristic_tilt=x[theta.index] @ theta / x.shape[0]\n        )\n      )\n      .reset_index(drop=True)\n      .assign(\n        weight_benchmark=lambda x: \n          x[\"relative_mktcap\"] if value_weighting else 1/x.shape[0],\n        weight_tilt=lambda x: \n          x[\"weight_benchmark\"] + x[\"characteristic_tilt\"]\n      )\n      .drop(columns=[\"characteristic_tilt\"])\n    )\n\n    if not allow_short_selling:\n        data = (data\n          .assign(weight_tilt=lambda x: np.maximum(0, x[\"weight_tilt\"]))\n        )\n\n    data = (data\n      .groupby(\"month\")\n      .apply(lambda x: x.assign(\n        weight_tilt=lambda x: x[\"weight_tilt\"]/x[\"weight_tilt\"].sum()))\n      .reset_index(drop=True)\n    )\n\n    return data\n\nIn the next step, we compute the portfolio weights for the arbitrary vector \\(\\theta_0\\). In the example below, we use the value-weighted portfolio as a benchmark and allow negative portfolio weights.\n\nweights_crsp = compute_portfolio_weights(\n  theta,\n  data_portfolios,\n  value_weighting=True,\n  allow_short_selling=True\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#portfolio-performance",
    "href": "python/parametric-portfolio-policies.html#portfolio-performance",
    "title": "Parametric Portfolio Policies",
    "section": "Portfolio Performance",
    "text": "Portfolio Performance\n Are the computed weights optimal in any way? Most likely not, as we picked \\(\\theta_0\\) arbitrarily. To evaluate the performance of an allocation strategy, one can think of many different approaches. In their original paper, Brandt, Santa-Clara, and Valkanov (2009) focus on a simple evaluation of the hypothetical utility of an agent equipped with a power utility function \\[u_\\gamma(r) = \\frac{(1 + r)^{(1-\\gamma)}}{1-\\gamma}, \\tag{5}\\] where \\(\\gamma\\) is the risk aversion factor.\n\ndef power_utility(r, gamma=5):\n    \"\"\"Calculate power utility for given risk aversion.\"\"\"\n    \n    utility = ((1+r)**(1-gamma))/(1-gamma)\n    \n    return utility\n\nWe want to note that Gehrig, Sögner, and Westerkamp (2020) warn that, in the leading case of constant relative risk aversion (CRRA), strong assumptions on the properties of the returns, the variables used to implement the parametric portfolio policy, and the parameter space are necessary to obtain a well-defined optimization problem.\nNo doubt, there are many other ways to evaluate a portfolio. The function below provides a summary of all kinds of interesting measures that can be considered relevant. Do we need all these evaluation measures? It depends: The original paper by Brandt, Santa-Clara, and Valkanov (2009) only cares about the expected utility to choose \\(\\theta\\). However, if you want to choose optimal values that achieve the highest performance while putting some constraints on your portfolio weights, it is helpful to have everything in one function.\n\ndef evaluate_portfolio(weights_data,\n                       full_evaluation=True,\n                       capm_evaluation=True,\n                       length_year=12):\n    \"\"\"Calculate portfolio evaluation measures.\"\"\"\n    evaluation = (weights_data\n        .groupby(\"month\")\n        .apply(lambda x: pd.Series(\n          np.average(x[[\"ret_excess\", \"ret_excess\"]],\n                     weights=x[[\"weight_tilt\", \"weight_benchmark\"]],\n                     axis=0),\n          [\"return_tilt\", \"return_benchmark\"])\n        )\n        .reset_index()\n        .melt(id_vars=\"month\", var_name=\"model\",\n              value_vars=[\"return_tilt\", \"return_benchmark\"],\n              value_name=\"portfolio_return\")\n        .assign(model=lambda x: x[\"model\"].str.replace(\"return_\", \"\"))\n    )\n\n    evaluation_stats = (evaluation\n        .groupby(\"model\")[\"portfolio_return\"]\n        .aggregate([\n          (\"Expected utility\", lambda x: np.mean(power_utility(x))),\n          (\"Average return\", lambda x: np.mean(length_year*x)*100),\n          (\"SD return\", lambda x: np.std(x)*np.sqrt(length_year)*100),\n          (\"Sharpe ratio\", lambda x: (np.mean(x)/np.std(x)* \n                                        np.sqrt(length_year)))\n        ])\n    )\n\n    if capm_evaluation:\n        evaluation_capm = (evaluation\n            .merge(factors_ff_monthly, how=\"left\", on=\"month\")\n            .groupby(\"model\")\n            .apply(lambda x: \n              smf.ols(formula=\"portfolio_return ~ 1 + mkt_excess\", data=x)\n              .fit().params\n            )\n            .rename(columns={\"const\": \"CAPM alpha\",\n                             \"mkt_excess\": \"Market beta\"})\n            )\n        evaluation_stats = (evaluation_stats\n          .merge(evaluation_capm, how=\"left\", on=\"model\")\n        )\n\n    if full_evaluation:\n        evaluation_weights = (weights_data\n          .melt(id_vars=\"month\", var_name=\"model\",\n                value_vars=[\"weight_benchmark\", \"weight_tilt\"],\n                value_name=\"weight\")\n          .groupby([\"model\", \"month\"])[\"weight\"]\n          .aggregate([\n            (\"Mean abs. weight\", lambda x: np.mean(abs(x))),\n            (\"Max. weight\", lambda x: max(x)),\n            (\"Min. weight\", lambda x: min(x)),\n            (\"Avg. sum of neg. weights\", lambda x: -np.sum(x[x &lt; 0])),\n            (\"Avg. share of neg. weights\", lambda x: np.mean(x &lt; 0))\n          ])\n          .reset_index()\n          .drop(columns=[\"month\"])\n          .groupby([\"model\"])\n          .aggregate(lambda x: np.average(x)*100)\n          .reset_index()\n          .assign(model=lambda x: x[\"model\"].str.replace(\"weight_\", \"\"))\n        )\n        \n        evaluation_stats = (evaluation_stats\n          .merge(evaluation_weights, how=\"left\", on=\"model\")\n          .set_index(\"model\")\n        )\n        \n    evaluation_stats = (evaluation_stats\n      .transpose()\n      .rename_axis(columns=None)\n    )\n\n    return evaluation_stats\n\n Let us take a look at the different portfolio strategies and evaluation measures.\n\nevaluate_portfolio(weights_crsp).round(2)\n\n\n\n\n\n\n\n\n\nbenchmark\ntilt\n\n\n\n\nExpected utility\n-0.25\n-0.26\n\n\nAverage return\n6.65\n0.19\n\n\nSD return\n15.46\n21.11\n\n\nSharpe ratio\n0.43\n0.01\n\n\nIntercept\n0.00\n-0.01\n\n\nMarket beta\n0.99\n0.95\n\n\nMean abs. weight\n0.03\n0.08\n\n\nMax. weight\n4.05\n4.22\n\n\nMin. weight\n0.00\n-0.17\n\n\nAvg. sum of neg. weights\n0.00\n78.03\n\n\nAvg. share of neg. weights\n0.00\n49.08\n\n\n\n\n\n\n\n\nThe value-weighted portfolio delivers an annualized return of more than six percent and clearly outperforms the tilted portfolio, irrespective of whether we evaluate expected utility, the Sharpe ratio, or the CAPM alpha. We can conclude the market beta is close to one for both strategies (naturally almost identically one for the value-weighted benchmark portfolio). When it comes to the distribution of the portfolio weights, we see that the benchmark portfolio weight takes less extreme positions (lower average absolute weights and lower maximum weight). By definition, the value-weighted benchmark does not take any negative positions, while the tilted portfolio also takes short positions.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#optimal-parameter-choice",
    "href": "python/parametric-portfolio-policies.html#optimal-parameter-choice",
    "title": "Parametric Portfolio Policies",
    "section": "Optimal Parameter Choice",
    "text": "Optimal Parameter Choice\nNext, we move to a choice of \\(\\theta\\) that actually aims to improve some (or all) of the performance measures. We first define the helper function compute_objective_function(), which we then pass to an optimizer.\n\ndef objective_function(theta,\n                       data,\n                       objective_measure=\"Expected utility\",\n                       value_weighting=True,\n                       allow_short_selling=True):\n    \"\"\"Define portfolio objective function.\"\"\"\n    \n    processed_data = compute_portfolio_weights(\n      theta, data, value_weighting, allow_short_selling\n    )\n\n    objective_function = evaluate_portfolio(\n      processed_data, \n      capm_evaluation=False, \n      full_evaluation=False\n    )\n\n    objective_function = -objective_function.loc[objective_measure, \"tilt\"]\n\n    return objective_function\n\nYou may wonder why we return the negative value of the objective function. This is simply due to the common convention for optimization procedures to search for minima as a default. By minimizing the negative value of the objective function, we get the maximum value as a result. In its most basic form, Python optimization uses the function minimize(). As main inputs, the function requires an initial guess of the parameters and the objective function to minimize. Now, we are fully equipped to compute the optimal values of \\(\\hat\\theta\\), which maximize the hypothetical expected utility of the investor.\n\noptimal_theta = minimize(\n  fun=objective_function,\n  x0=[1.5]*n_parameters,\n  args=(data_portfolios, \"Expected utility\", True, True),\n  method=\"Nelder-Mead\",\n  tol=1e-2\n)\n\n(pd.DataFrame(\n  optimal_theta.x,\n  columns=[\"Optimal theta\"],\n  index=[\"momentum_lag\", \"size_lag\"]).T.round(3)\n)\n\n\n\n\n\n\n\n\n\nmomentum_lag\nsize_lag\n\n\n\n\nOptimal theta\n0.344\n-1.827\n\n\n\n\n\n\n\n\nThe resulting values of \\(\\hat\\theta\\) are easy to interpret: intuitively, expected utility increases by tilting weights from the value-weighted portfolio toward smaller stocks (negative coefficient for size) and toward past winners (positive value for momentum). Both findings are in line with the well-documented size effect (Banz 1981) and the momentum anomaly (Jegadeesh and Titman 1993).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#more-model-specifications",
    "href": "python/parametric-portfolio-policies.html#more-model-specifications",
    "title": "Parametric Portfolio Policies",
    "section": "More Model Specifications",
    "text": "More Model Specifications\nHow does the portfolio perform for different model specifications? For this purpose, we compute the performance of a number of different modeling choices based on the entire CRSP sample. The next code chunk performs all the heavy lifting.\n\ndef evaluate_optimal_performance(data,\n                                 objective_measure=\"Expected utility\",\n                                 value_weighting=True,\n                                 allow_short_selling=True):\n    \"\"\"Calculate optimal portfolio performance.\"\"\"\n    \n    optimal_theta = minimize(\n      fun=objective_function,\n      x0=[1.5]*n_parameters,\n      args=(data, objective_measure, value_weighting, allow_short_selling),\n      method=\"Nelder-Mead\",\n      tol=10e-2\n    ).x\n\n    processed_data = compute_portfolio_weights(\n      optimal_theta, data, \n      value_weighting, allow_short_selling\n    )\n\n    portfolio_evaluation = evaluate_portfolio(processed_data)\n\n    weight_text = \"VW\" if value_weighting else \"EW\"\n    short_text = \"\" if allow_short_selling else \" (no s.)\"\n\n    strategy_name_dict = {\n      \"benchmark\": weight_text,\n      \"tilt\": f\"{weight_text} Optimal{short_text}\"\n    }\n\n    portfolio_evaluation.columns = [\n      strategy_name_dict[i] for i in portfolio_evaluation.columns\n    ]\n    \n    return(portfolio_evaluation)\n\nFinally, we can compare the results. The table below shows summary statistics for all possible combinations: equal- or value-weighted benchmark portfolio, with or without short-selling constraints, and tilted toward maximizing expected utility.\n\ndata = [data_portfolios]\nvalue_weighting = [True, False]\nallow_short_selling = [True, False]\nobjective_measure = [\"Expected utility\"]\n\npermutations = product(\n  data, objective_measure,\n  value_weighting, allow_short_selling\n)\nresults = list(starmap(\n  evaluate_optimal_performance, \n  permutations\n))\nperformance_table = (pd.concat(results, axis=1)\n  .T.drop_duplicates().T.round(3)\n)\nperformance_table.get([\"EW\", \"VW\"])\n\n\n\n\n\n\n\n\n\nEW\nVW\n\n\n\n\nExpected utility\n-0.251\n-0.250\n\n\nAverage return\n10.009\n6.649\n\n\nSD return\n20.352\n15.462\n\n\nSharpe ratio\n0.492\n0.430\n\n\nIntercept\n0.002\n0.000\n\n\nMarket beta\n1.125\n0.994\n\n\nMean abs. weight\n0.000\n0.030\n\n\nMax. weight\n0.000\n4.053\n\n\nMin. weight\n0.000\n0.000\n\n\nAvg. sum of neg. weights\n0.000\n0.000\n\n\nAvg. share of neg. weights\n0.000\n0.000\n\n\n\n\n\n\n\n\n\nperformance_table.get([\"EW Optimal\", \"VW Optimal\"])\n\n\n\n\n\n\n\n\n\nEW Optimal\nVW Optimal\n\n\n\n\nExpected utility\n-5.840\n-0.261\n\n\nAverage return\n-4948.462\n0.194\n\n\nSD return\n14729.881\n21.106\n\n\nSharpe ratio\n-0.336\n0.009\n\n\nIntercept\n-3.697\n-0.005\n\n\nMarket beta\n-78.566\n0.952\n\n\nMean abs. weight\n62.166\n0.077\n\n\nMax. weight\n1119.202\n4.216\n\n\nMin. weight\n-221.305\n-0.174\n\n\nAvg. sum of neg. weights\n77916.229\n78.025\n\n\nAvg. share of neg. weights\n51.878\n49.076\n\n\n\n\n\n\n\n\n\nperformance_table.get([\"EW Optimal (no s.)\", \"VW Optimal (no s.)\"])\n\n\n\n\n\n\n\n\n\nEW Optimal (no s.)\nVW Optimal (no s.)\n\n\n\n\nExpected utility\n-0.252\n-0.250\n\n\nAverage return\n7.962\n7.301\n\n\nSD return\n19.126\n16.700\n\n\nSharpe ratio\n0.416\n0.437\n\n\nIntercept\n0.000\n0.000\n\n\nMarket beta\n1.136\n1.055\n\n\nMean abs. weight\n0.030\n0.030\n\n\nMax. weight\n1.309\n2.332\n\n\nMin. weight\n0.000\n0.000\n\n\nAvg. sum of neg. weights\n0.000\n0.000\n\n\nAvg. share of neg. weights\n0.000\n0.000\n\n\n\n\n\n\n\n\nThe results indicate that the average annualized Sharpe ratio of the equal-weighted portfolio exceeds the Sharpe ratio of the value-weighted benchmark portfolio. Nevertheless, starting with the weighted value portfolio as a benchmark and tilting optimally with respect to momentum and small stocks yields the highest Sharpe ratio across all specifications. Finally, imposing no short-sale constraints does not improve the performance of the portfolios in our application.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/parametric-portfolio-policies.html#exercises",
    "href": "python/parametric-portfolio-policies.html#exercises",
    "title": "Parametric Portfolio Policies",
    "section": "Exercises",
    "text": "Exercises\n\nHow do the estimated parameters \\(\\hat\\theta\\) and the portfolio performance change if your objective is to maximize the Sharpe ratio instead of the hypothetical expected utility?\nThe code above is very flexible in the sense that you can easily add new firm characteristics. Construct a new characteristic of your choice and evaluate the corresponding coefficient \\(\\hat\\theta_i\\).\nTweak the function optimal_theta() such that you can impose additional performance constraints in order to determine \\(\\hat\\theta\\), which maximizes expected utility under the constraint that the market beta is below 1.\nDoes the portfolio performance resemble a realistic out-of-sample backtesting procedure? Verify the robustness of the results by first estimating \\(\\hat\\theta\\) based on past data only. Then, use more recent periods to evaluate the actual portfolio performance.\nBy formulating the portfolio problem as a statistical estimation problem, you can easily obtain standard errors for the coefficients of the weight function. Brandt, Santa-Clara, and Valkanov (2009) provide the relevant derivations in their paper in Equation (10). Implement a small function that computes standard errors for \\(\\hat\\theta\\).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html",
    "href": "python/option-pricing-via-machine-learning.html",
    "title": "Option Pricing via Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nThis chapter covers machine learning methods in option pricing. First, we briefly introduce regression trees, random forests, and neural networks; these methods are advocated as highly flexible universal approximators, capable of recovering highly non-linear structures in the data. As the focus is on implementation, we leave a thorough treatment of the statistical underpinnings to other textbooks from authors with a real comparative advantage on these issues. We show how to implement random forests and deep neural networks with tidy principles using scikit-learn.\nMachine learning (ML) is seen as a part of artificial intelligence. ML algorithms build a model based on training data in order to make predictions or decisions without being explicitly programmed to do so. While ML can be specified along a vast array of different branches, this chapter focuses on so-called supervised learning for regressions. The basic idea of supervised learning algorithms is to build a mathematical model for data that contains both the inputs and the desired outputs. In this chapter, we apply well-known methods such as random forests and neural networks to a simple application in option pricing. More specifically, we create an artificial dataset of option prices for different values based on the Black-Scholes pricing equation for call options. Then, we train different models to learn how to price call options without prior knowledge of the theoretical underpinnings of the famous option pricing equation by Black and Scholes (1973).\nThroughout this chapter, we need the following Python packages.\nimport pandas as pd \nimport numpy as np\n\nfrom plotnine import *\nfrom itertools import product\nfrom scipy.stats import norm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Lasso",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "href": "python/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "title": "Option Pricing via Machine Learning",
    "section": "Regression Trees and Random Forests",
    "text": "Regression Trees and Random Forests\nRegression trees are a popular ML approach for incorporating multiway predictor interactions. In Finance, regression trees are gaining popularity, also in the context of asset pricing (see, e.g., Bryzgalova, Pelger, and Zhu 2022). Trees possess a logic that departs markedly from traditional regressions. Trees are designed to find groups of observations that behave similarly to each other. A tree grows in a sequence of steps. At each step, a new branch sorts the data leftover from the preceding step into bins based on one of the predictor variables. This sequential branching slices the space of predictors into partitions and approximates the unknown function \\(f(x)\\) which yields the relation between the predictors \\(x\\) and the outcome variable \\(y\\) with the average value of the outcome variable within each partition. For a more thorough treatment of regression trees, we refer to Coqueret and Guida (2020).\nFormally, we partition the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). For any predictor \\(x\\) that falls within region \\(R_j\\), we estimate \\(f(x)\\) with the average of the training observations, \\(\\hat y_i\\), for which the associated predictor \\(x_i\\) is also in \\(R_j\\). Once we select a partition \\(x\\) to split in order to create the new partitions, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, called \\(R_1(j,s)\\) and \\(R_2(j,s)\\), which split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\): \\[R_1(j,s) = \\{x \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{x \\mid x_j \\geq s\\}. \\tag{1}\\] To pick \\(j\\) and \\(s\\), we find the pair that minimizes the residual sum of square (RSS): \\[\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2 \\tag{2}\\] As in Factor Selection via Machine Learning in the context of penalized regressions, the first relevant question is: What are the hyperparameter decisions? Instead of a regularization parameter, trees are fully determined by the number of branches used to generate a partition (sometimes one specifies the minimum number of observations in each final branch instead of the maximum number of branches).\nModels with a single tree may suffer from high predictive variance. Random forests address these shortcomings of decision trees. The goal is to improve the predictive performance and reduce instability by averaging multiple regression trees. A forest basically implies creating many regression trees and averaging their predictions. To assure that the individual trees are not the same, we use a bootstrap to induce randomness. More specifically, we build \\(B\\) decision trees \\(T_1, \\ldots, T_B\\) using the training sample. For that purpose, we randomly select features to be included in the building of each tree. For each observation in the test set, we then form a prediction \\(\\hat{y} = \\frac{1}{B}\\sum\\limits_{i=1}^B\\hat{y}_{T_i}\\).",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#neural-networks",
    "href": "python/option-pricing-via-machine-learning.html#neural-networks",
    "title": "Option Pricing via Machine Learning",
    "section": "Neural Networks",
    "text": "Neural Networks\nRoughly speaking, neural networks propagate information from an input layer, through one or multiple hidden layers, to an output layer. While the number of units (neurons) in the input layer is equal to the dimension of the predictors, the output layer usually consists of one neuron (for regression) or multiple neurons for classification. The output layer predicts the future data, similar to the fitted value in a regression analysis. Neural networks have theoretical underpinnings as universal approximators for any smooth predictive association (Hornik 1991). Their complexity, however, ranks neural networks among the least transparent, least interpretable, and most highly parameterized ML tools. In finance, applications of neural networks can be found in many different contexts, e.g., Avramov, Cheng, and Metzker (2022), Chen, Pelger, and Zhu (2023), and Gu, Kelly, and Xiu (2020).\nEach neuron applies a non-linear activation function \\(f\\) to its aggregated signal before sending its output to the next layer \\[x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right) \\tag{3}\\] Here, \\(\\theta\\) are the parameters to fit, \\(N^l\\) denotes the number of units (a hyperparameter to tune), and \\(z_j\\) are the input variables which can be either the raw data or, in the case of multiple chained layers, the outcome from a previous layer \\(z_j = x_k-1\\). While the easiest case with \\(f(x) = \\alpha + \\beta x\\) resembles linear regression, typical activation functions are sigmoid (i.e., \\(f(x) = (1+e^{-x})^{-1}\\)) or ReLu (i.e., \\(f(x) = max(x, 0)\\)).\nNeural networks gain their flexibility from chaining multiple layers together. Naturally, this imposes many degrees of freedom on the network architecture for which no clear theoretical guidance exists. The specification of a neural network requires, at a minimum, a stance on depth (number of hidden layers), the activation function, the number of neurons, the connection structure of the units (dense or sparse), and the application of regularization techniques to avoid overfitting. Finally, learning means to choose optimal parameters relying on numerical optimization, which often requires specifying an appropriate learning rate. Despite these computational challenges, implementation in Python is not tedious at all because we can use the API to TensorFlow.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#option-pricing",
    "href": "python/option-pricing-via-machine-learning.html#option-pricing",
    "title": "Option Pricing via Machine Learning",
    "section": "Option Pricing",
    "text": "Option Pricing\nTo apply ML methods in a relevant field of finance, we focus on option pricing. The application in its core is taken from Hull (2020). In its most basic form, call options give the owner the right but not the obligation to buy a specific stock (the underlying) at a specific price (the strike price \\(K\\)) at a specific date (the exercise date \\(T\\)). The Black-Scholes price (Black and Scholes 1973) of a call option for a non-dividend-paying underlying stock is given by \\[\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_2)Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left(\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right) \\\\\n     d_2 &= d_1 - \\sigma\\sqrt{T}\n\\end{aligned}\n\\tag{4}\\] where \\(C(S, T)\\) is the price of the option as a function of today’s stock price of the underlying, \\(S\\), with time to maturity \\(T\\), \\(r_f\\) is the risk-free interest rate, and \\(\\sigma\\) is the volatility of the underlying stock return. \\(\\Phi\\) is the cumulative distribution function of a standard normal random variable.\nThe Black-Scholes equation provides a way to compute the arbitrage-free price of a call option once the parameters \\(S, K, r_f, T\\), and \\(\\sigma\\) are specified (arguably, in a realistic context, all parameters are easy to specify except for \\(\\sigma\\) which has to be estimated). A simple R function allows computing the price as we do below.\n\ndef black_scholes_price(S, K, r, T, sigma):\n    \"\"\"Calculate Black Scholes option price.\"\"\"\n  \n    d1 = (np.log(S/K)+(r+sigma**2/2)*T)/(sigma*np.sqrt(T))\n    d2 = d1-sigma*np.sqrt(T)\n    price = S*norm.cdf(d1)-K*np.exp(-r*T)*norm.cdf(d2)\n    \n    return price",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#learning-black-scholes",
    "href": "python/option-pricing-via-machine-learning.html#learning-black-scholes",
    "title": "Option Pricing via Machine Learning",
    "section": "Learning Black-Scholes",
    "text": "Learning Black-Scholes\nWe illustrate the concept of ML by showing how ML methods learn the Black-Scholes equation after observing some different specifications and corresponding prices without us revealing the exact pricing equation.\n\nData simulation\nTo that end, we start with simulated data. We compute option prices for call options for a grid of different combinations of times to maturity (T), risk-free rates (r), volatilities (sigma), strike prices (K), and current stock prices (S). In the code below, we add an idiosyncratic error term to each observation such that the prices considered do not exactly reflect the values implied by the Black-Scholes equation.\nIn order to keep the analysis reproducible, we use np.random.seed(). A random seed specifies the start point when a computer generates a random number sequence and ensures that our simulated data is the same across different machines.\n\nrandom_state = 42\nnp.random.seed(random_state)\n\nS = np.arange(40, 61)\nK = np.arange(20, 91)\nr = np.arange(0, 0.051, 0.01)\nT = np.arange(3/12, 2.01, 1/12)\nsigma = np.arange(0.1, 0.81, 0.1)\n\noption_prices = pd.DataFrame(\n  product(S, K, r, T, sigma),\n  columns=[\"S\", \"K\", \"r\", \"T\", \"sigma\"]\n)\n\noption_prices[\"black_scholes\"] = black_scholes_price(\n  option_prices[\"S\"].values, \n  option_prices[\"K\"].values, \n  option_prices[\"r\"].values, \n  option_prices[\"T\"].values, \n  option_prices[\"sigma\"].values\n)\n\noption_prices = (option_prices\n  .assign(\n    observed_price=lambda x: (\n      x[\"black_scholes\"] + np.random.normal(scale=0.15)\n    )\n  )\n)\n\nThe code above generates more than 1.5 million random parameter constellations (in the definition of the option_prices dataframe). For each of these values, the true prices reflecting the Black-Scholes model are given and a random innovation term pollutes the observed prices. The intuition of this application is simple: the simulated data provides many observations of option prices, by using the Black-Scholes equation we can evaluate the actual predictive performance of a ML method, which would be hard in a realistic context where the actual arbitrage-free price would be unknown.\nNext, we split the data into a training set (which contains 1 percent of all the observed option prices) and a test set that will only be used for the final evaluation. Note that the entire grid of possible combinations contains python len(option_prices.columns) different specifications. Thus, the sample to learn the Black-Scholes price contains only 31,489 observations and is therefore relatively small.\n\ntrain_data, test_data = train_test_split(\n  option_prices, \n  test_size=0.01, random_state=random_state\n)\n\nWe process the training dataset further before we fit the different ML models. We define a ColumnTransformer() that defines all processing steps for that purpose. For our specific case, we want to explain the observed price by the five variables that enter the Black-Scholes equation. The true price (stored in column black_scholes) should obviously not be used to fit the model. The recipe also reflects that we standardize all predictors via StandardScaler() to ensure that each variable exhibits a sample average of zero and a sample standard deviation of one.\n\npreprocessor = ColumnTransformer(\n  transformers=[(\n    \"normalize_predictors\", \n     StandardScaler(),\n     [\"S\", \"K\", \"r\", \"T\", \"sigma\"]\n  )],\n  remainder=\"drop\"\n)\n\n\n\nSingle layer networks and random forests\nNext, we show how to fit a neural network to the data. The function MLPRegressor() from the package scikit-learn provides the functionality to initialize a single layer, feed-forward neural network. The specification below defines a single layer feed-forward neural network with ten hidden units. We set the number of training iterations to max_iter=1000.\n\nmax_iter = 1000\n\nnnet_model = MLPRegressor(\n  hidden_layer_sizes=10, \n  max_iter=max_iter, \n  random_state=random_state\n)\n\nWe can follow the straightforward workflow as in the chapter before: define a workflow, equip it with the recipe, and specify the associated model. Finally, fit the model with the training data.\n\nnnet_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", nnet_model)\n])\n\nnnet_fit = nnet_pipeline.fit(\n  train_data.drop(columns=[\"observed_price\"]), \n  train_data.get(\"observed_price\")\n)\n\nOne word of caution regarding the training of Neural networks: For illustrative purposes we sequentially update the parameters by reiterating through the data 1,000 times (max_iter=1000). Typically, however, early stopping rules are advised that aim to interrupt the process of updating parameters as soon as the predictive performance on the validation test seems to deteriorate. A detailed discussion of these details in the implementation would go beyond the scope of this book.\nOnce you are familiar with the scikit-learn workflow, it is a piece of cake to fit other models. For instance, the model below initializes a random forest with 50 trees contained in the ensemble, where we require at least 2000 observations in a node. The random forests are trained using the function RandomForestRegressor().\n\nrf_model = RandomForestRegressor(\n  n_estimators=50, \n  min_samples_leaf=2000, \n  random_state=random_state\n)\n\nFitting the model follows exactly the same convention as for the neural network before.\n\nrf_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", rf_model)\n])\n\nrf_fit = rf_pipeline.fit(\n  train_data.drop(columns=[\"observed_price\"]), \n  train_data.get(\"observed_price\")\n)\n\n\n\nDeep neural networks\nA deep neural network is a neural network with multiple layers between the input and output layers. By chaining multiple layers together, more complex structures can be represented with fewer parameters than simple shallow (one-layer) networks as the one implemented above. For instance, image or text recognition are typical tasks where deep neural networks are used (for applications of deep neural networks in finance, see, for instance, Jiang, Kelly, and Xiu 2023; Jensen et al. 2022). The following code chunk implements a deep neural network with three hidden layers of size ten each and logistic activation functions.\n\ndeepnnet_model = MLPRegressor(\n  hidden_layer_sizes=(10, 10, 10),\n  activation=\"logistic\", \n  solver=\"lbfgs\",\n  max_iter=max_iter, \n  random_state=random_state\n)\n                              \ndeepnnet_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", deepnnet_model)\n])\n\ndeepnnet_fit = deepnnet_pipeline.fit(\n  train_data.drop(columns=[\"observed_price\"]),\n  train_data.get(\"observed_price\")\n)\n\n\n\nUniversal approximation\nBefore we evaluate the results, we implement one more model. In principle, any non-linear function can also be approximated by a linear model containing the input variables’ polynomial expansions. To illustrate this, we include polynomials up to the fifth degree of each predictor and then add all possible pairwise interaction terms. We fit a Lasso regression model with a pre-specified penalty term (consult Factor Selection via Machine Learning on how to tune the model hyperparameters).\n\nlm_pipeline = Pipeline([\n  (\"polynomial\", PolynomialFeatures(degree=5, \n                                    interaction_only=False, \n                                    include_bias=True)),\n  (\"scaler\", StandardScaler()),\n  (\"regressor\", Lasso(alpha=0.01))\n])\n\nlm_fit = lm_pipeline.fit(\n  train_data.get([\"S\", \"K\", \"r\", \"T\", \"sigma\"]),\n  train_data.get(\"observed_price\")\n)",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#prediction-evaluation",
    "href": "python/option-pricing-via-machine-learning.html#prediction-evaluation",
    "title": "Option Pricing via Machine Learning",
    "section": "Prediction Evaluation",
    "text": "Prediction Evaluation\nFinally, we collect all predictions to compare the out-of-sample prediction error evaluated on 10,000 new data points.\n\ntest_X = test_data.get([\"S\", \"K\", \"r\", \"T\", \"sigma\"])\ntest_y = test_data.get(\"observed_price\")\n\npredictive_performance = (pd.concat(\n    [test_data.reset_index(drop=True), \n     pd.DataFrame({\"Random forest\": rf_fit.predict(test_X),\n                   \"Single layer\": nnet_fit.predict(test_X),\n                   \"Deep NN\": deepnnet_fit.predict(test_X),\n                   \"Lasso\": lm_fit.predict(test_X)})\n    ], axis=1)\n  .melt(\n    id_vars=[\"S\", \"K\", \"r\", \"T\", \"sigma\",\n             \"black_scholes\", \"observed_price\"],\n    var_name=\"Model\",\n    value_name=\"Predicted\"\n  )\n  .assign(\n    moneyness=lambda x: x[\"S\"]-x[\"K\"],\n    pricing_error=lambda x: np.abs(x[\"Predicted\"]-x[\"black_scholes\"])\n  )\n)\n\nIn the lines above, we use each of the fitted models to generate predictions for the entire test dataset of option prices. We evaluate the absolute pricing error as one possible measure of pricing accuracy, defined as the absolute value of the difference between predicted option price and the theoretical correct option price from the Black-Scholes model. We show the results graphically in Figure 1.\n\npredictive_performance_plot = (\n  ggplot(predictive_performance, \n         aes(x=\"moneyness\", y=\"pricing_error\")) +\n  geom_point(alpha=0.05) +\n  facet_wrap(\"Model\") + \n  labs(x=\"Moneyness (S - K)\", y=\"Absolut prediction error (USD)\",\n       title=\"Prediction errors of call options for different models\") +\n  theme(legend_position=\"\")\n)\npredictive_performance_plot.draw()\n\n\n\n\n\n\n\nFigure 1: The figure shows absolut prediction error in USD for the different fitted methods. The prediction error is evaluated on a sample of call options that were not used for training.\n\n\n\n\n\nThe results can be summarized as follows:\n\nAll ML methods seem to be able to price call options after observing the training test set.\nRandom forest and the Lasso seem to perform consistently worse in predicting option prices than the neural networks.\nFor random forest and Lasso, the average prediction errors increase for far in-the-money options.\nThe increased complexity of the deep neural network relative to the single-layer neural network results in lower prediction errors.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/option-pricing-via-machine-learning.html#exercises",
    "href": "python/option-pricing-via-machine-learning.html#exercises",
    "title": "Option Pricing via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that takes y and a matrix of predictors X as inputs and returns a characterization of the relevant parameters of a regression tree with one branch.\nCreate a function that creates predictions for a new matrix of predictors newX based on the estimated regression tree.    \n\n\n\n\nFigure 1: The figure shows absolut prediction error in USD for the different fitted methods. The prediction error is evaluated on a sample of call options that were not used for training.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "This website is the online version of Tidy Finance with Python, a book soon available via Chapman & Hall/CRC. The book is the result of a joint effort of Christoph Scheuch, Stefan Voigt, Patrick Weiss, and Christoph Frey.\nWe are grateful for any kind of feedback on every aspect of the book. So please get in touch with us via contact@tidy-finance.org if you spot typos, discover any issues that deserve more attention, or if you have suggestions for additional chapters and sections. Additionally, let us know if you found the text helpful. We look forward to hearing from you!\n\n\n\n\n\n\nSupport Tidy Finance\n\n\n\nBuy our book Tidy Finance with R via your preferred vendor or support us with coffee here.\n\n\n\n\nOver our academic careers, we are continuously surprised by the lack of publicly available code for seminal papers or even textbooks in finance. This opaqueness and secrecy is particularly costly for young, aspiring financial economists. To tackle this issue, we started working on Tidy Finance to lift the curtain on reproducible finance. These efforts resulted in the book Tidy Finance with R (Scheuch, Voigt, and Weiss 2023), which provides a fully transparent code base in R for many common financial applications.\nSince the book’s publication, we received great feedback from students and teachers alike. However, one of the most common comments was that many interested coders are constrained and have to use Python in their institutions. We really love R for data analysis tasks, but we acknowledge the flexibility and popularity of Python. Hence, we decided to increase our team of authors with a Python expert and extend Tidy Finance to another programming language following the same tidy principles.\n\n\n\nWe write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from the undergraduate to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We choose a wide range of topics, from data handling and factor replication to portfolio allocation and option pricing, to offer something for every course and study focus. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nPractitioners like portfolio managers who like to validate and implement trading ideas or data analysts or statisticians who work with financial data and who need practical tools to succeed.\n\n\n\n\nThe book is divided into five parts:\n\nThe first part helps you to set up your Python development environment and introduces you to essential programming concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common datasets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access via R packages exists.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for portfolio optimization and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet, the data chapters provide an important background necessary for data management in all other chapters.\n\n\n\nThis book is about empirical work. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. Moreover, although we enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses, we refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nIn addition, we also do not explain all the functionalities and details about the Python functions we use. We only delve into the empirical research focus and data transformation logic and want to refer attentive readers to consult the package documentations for more information. In other words, this is not a book to learn Python from scratch. It is a book on how to use Python as a tool to produce consistent and replicable empirical results.\nThat being said, our book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nHilpisch (2018) is a great introduction to the power of Python for financial applications. It does a great job explaining the basics of the Python language, its programming structure, and packages like pandas, SciPy, and numpy and uses these methods for actual applications. The book and a series of follow-up books from the same author about financial data science, artificial intelligence, and algorithmic trading primarily target practitioners and have a hands-on focus. Our book, in contrast, emphasizes reproducibility and starts with the applications right away to utilize Python as the tool to perform data transformations and statistical analysis. Hence, we clearly focus on state-of-the-art applications for academic research in finance. Thus, we fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nFurthermore, Weiming (2019) and Kelliher (2022) are comprehensive introductions to quantitative finance with a greater focus on option pricing, quantitative modeling for various markets besides equity, and algorithmic trading. Again, these books are primarily written for finance professionals to introduce Python or enhance their Python knowledge.\nCoqueret and Guida (2023) constitutes a great compendium to our book concerning applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git.\nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Kibirige (2023) provides a highly customizable yet easy-to-use system for creating data visualizations based on the Grammar of Graphics (Wilkinson 2012). Second, in our daily work and to compile this book, we used Quarto, an open-source scientific and technical publishing system described in Allaire et al. (2023). Markdown documents are fully reproducible and support static and dynamic output formats. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).\n\n\n\n\nAs you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80 percent of data analysis is spent on preparing data. By tidying data, we want to structure datasets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as possible. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions.\nEmbrace functional programming.\nDesign for humans.\n\n\n\n\nPython (Python Software Foundation 2023) is an open-source, general-purpose, high-level programming language widely used across various industries. Python is prevalent for data science according to the Python Developers Survey (Foundation and JetBrains 2022), particularly for financial applications. Similar to R, the Python community values readable and straightforward code. Thus, Python is an excellent choice for first-time programmers. At the same time, experienced researchers in financial economics and analysts benefit from the wide range of possibilities to express complex ideas with concise and understandable code. Some of the highlights of Python include:\n\nOpen-source: Python uses a source license, making it usable and distributable for academic and commercial use.\nFlexibility: Python’s extensive ecosystem of standard libraries and community-contributed modules allows for all kinds of unique projects. It seamlessly integrates various data sources and APIs, facilitating efficient data retrieval and processing.\nVersatility: Python is a cross-platform, multipurpose language that can be used to write fast low-level executable code, large applications, and even graphical user interfaces (GUI).\nSpeed: Python is fast. In addition, parallelization is straightforward to implement in order to tackle big data problems without hassle.\nRobustness: Python provides robust tools for data manipulation, analysis, and visualization, which are crucial components in finance research.\nImportance: The language’s active community support and continuous development ensure access to cutting-edge technologies and methodologies. Learning Python enhances one’s ability to conduct sophisticated financial analysis, making it a valuable skill for professionals across diverse fields.\n\nThe so-called Zen of Python by Tim Peters summarizes its major syntax guidelines for structured, tidy, and human-readable code. It is easily accessible in every Python environment through:\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\nPython comes in many flavors, and endless external packages extend the possibilities for conducting financial research. Any code we provide echoes some arguably subjective decisions we have taken to comply with our idea of what Tidy Finance comprises: Code should not simply yield the correct output but should be easy to read. Therefore, we advocate using chaining, which is the practice of calling multiple methods in a sequence, each operating on the result of the previous step.\nFurther, the entire book rests on tidy data, which we handle with a small set of powerful packages proven effective: pandas and numpy. Regarding visualization (which we deem highly relevant to provide a fundamentally human-centered experience), we follow the Grammars of Graphics’ philosophical framework (Wilkinson 2012), which has been carefully implemented using plotnine.\nArguably, neither chaining commands nor using the Grammar of Graphics can be considered mainstream within the Python ecosystem for financial research (yet). We believe in the value of the workflows we teach and practice on a daily basis. Therefore, we also believe that adopting such coding principles will dramatically increase in the near future. For more information on why Python is great, we refer to Hilpisch (2018).\n\n\n\n\nChristoph Scheuch is the Head of Artificial Intelligence at the social trading platform wikifolio.com. He is responsible for researching, designing, and prototyping of cutting-edge AI-driven products using R and Python. Before his focus on AI, he was responsible for product management and business intelligence at wikifolio.com and an external lecturer at the Vienna University of Economics and Business, where he taught finance students how to manage empirical projects.\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals and he received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on Tidy Finance.\n\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance, with his research appearing in leading journals in financial economics. Patrick is especially passionate about empirical asset pricing and strives to understand the impact of methodological uncertainty on research outcomes.\nChristoph Frey is a Quantitative Researcher and Portfolio Manager at a family office in Hamburg and Research Fellow at the Centre for Financial Econometrics, Asset Markets and Macroeconomic Policy at Lancaster University. Prior to this, he was the leading quantitative researcher for systematic multi-asset strategies at Berenberg Bank and worked as an Assistant Professor at the Erasmus Universiteit Rotterdam. Christoph published research on Bayesian Econometrics and specializes in financial econometrics and portfolio optimization problems.\n\n\n\n\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite the Python project as follows:\n\nScheuch, C., Voigt, S., Weiss, P., & Frey, C. (2024). Tidy Finance with Python (1st ed.). Chapman and Hall/CRC https://www.tidy-finance.org\n\nYou can also use the following BibTeX entry:\n@book{Frey2024,\n  title = {Tidy Finance with Python (1st ed.)},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick and Frey, Christoph},\n  year = {2024},\n  publisher = {Chapman and Hall/CRC},\n  edition = {1st},\n  url = {https://tidy-finance.org/python}\n}",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#why-does-this-book-exist",
    "href": "python/index.html#why-does-this-book-exist",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "Over our academic careers, we are continuously surprised by the lack of publicly available code for seminal papers or even textbooks in finance. This opaqueness and secrecy is particularly costly for young, aspiring financial economists. To tackle this issue, we started working on Tidy Finance to lift the curtain on reproducible finance. These efforts resulted in the book Tidy Finance with R (Scheuch, Voigt, and Weiss 2023), which provides a fully transparent code base in R for many common financial applications.\nSince the book’s publication, we received great feedback from students and teachers alike. However, one of the most common comments was that many interested coders are constrained and have to use Python in their institutions. We really love R for data analysis tasks, but we acknowledge the flexibility and popularity of Python. Hence, we decided to increase our team of authors with a Python expert and extend Tidy Finance to another programming language following the same tidy principles.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#who-should-read-this-book",
    "href": "python/index.html#who-should-read-this-book",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "We write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from the undergraduate to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We choose a wide range of topics, from data handling and factor replication to portfolio allocation and option pricing, to offer something for every course and study focus. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nPractitioners like portfolio managers who like to validate and implement trading ideas or data analysts or statisticians who work with financial data and who need practical tools to succeed.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#what-will-you-learn",
    "href": "python/index.html#what-will-you-learn",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "The book is divided into five parts:\n\nThe first part helps you to set up your Python development environment and introduces you to essential programming concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common datasets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access via R packages exists.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for portfolio optimization and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet, the data chapters provide an important background necessary for data management in all other chapters.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#what-wont-you-learn",
    "href": "python/index.html#what-wont-you-learn",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "This book is about empirical work. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. Moreover, although we enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses, we refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nIn addition, we also do not explain all the functionalities and details about the Python functions we use. We only delve into the empirical research focus and data transformation logic and want to refer attentive readers to consult the package documentations for more information. In other words, this is not a book to learn Python from scratch. It is a book on how to use Python as a tool to produce consistent and replicable empirical results.\nThat being said, our book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nHilpisch (2018) is a great introduction to the power of Python for financial applications. It does a great job explaining the basics of the Python language, its programming structure, and packages like pandas, SciPy, and numpy and uses these methods for actual applications. The book and a series of follow-up books from the same author about financial data science, artificial intelligence, and algorithmic trading primarily target practitioners and have a hands-on focus. Our book, in contrast, emphasizes reproducibility and starts with the applications right away to utilize Python as the tool to perform data transformations and statistical analysis. Hence, we clearly focus on state-of-the-art applications for academic research in finance. Thus, we fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nFurthermore, Weiming (2019) and Kelliher (2022) are comprehensive introductions to quantitative finance with a greater focus on option pricing, quantitative modeling for various markets besides equity, and algorithmic trading. Again, these books are primarily written for finance professionals to introduce Python or enhance their Python knowledge.\nCoqueret and Guida (2023) constitutes a great compendium to our book concerning applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git.\nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Kibirige (2023) provides a highly customizable yet easy-to-use system for creating data visualizations based on the Grammar of Graphics (Wilkinson 2012). Second, in our daily work and to compile this book, we used Quarto, an open-source scientific and technical publishing system described in Allaire et al. (2023). Markdown documents are fully reproducible and support static and dynamic output formats. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#why-tidy",
    "href": "python/index.html#why-tidy",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "As you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80 percent of data analysis is spent on preparing data. By tidying data, we want to structure datasets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as possible. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions.\nEmbrace functional programming.\nDesign for humans.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#why-python",
    "href": "python/index.html#why-python",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "Python (Python Software Foundation 2023) is an open-source, general-purpose, high-level programming language widely used across various industries. Python is prevalent for data science according to the Python Developers Survey (Foundation and JetBrains 2022), particularly for financial applications. Similar to R, the Python community values readable and straightforward code. Thus, Python is an excellent choice for first-time programmers. At the same time, experienced researchers in financial economics and analysts benefit from the wide range of possibilities to express complex ideas with concise and understandable code. Some of the highlights of Python include:\n\nOpen-source: Python uses a source license, making it usable and distributable for academic and commercial use.\nFlexibility: Python’s extensive ecosystem of standard libraries and community-contributed modules allows for all kinds of unique projects. It seamlessly integrates various data sources and APIs, facilitating efficient data retrieval and processing.\nVersatility: Python is a cross-platform, multipurpose language that can be used to write fast low-level executable code, large applications, and even graphical user interfaces (GUI).\nSpeed: Python is fast. In addition, parallelization is straightforward to implement in order to tackle big data problems without hassle.\nRobustness: Python provides robust tools for data manipulation, analysis, and visualization, which are crucial components in finance research.\nImportance: The language’s active community support and continuous development ensure access to cutting-edge technologies and methodologies. Learning Python enhances one’s ability to conduct sophisticated financial analysis, making it a valuable skill for professionals across diverse fields.\n\nThe so-called Zen of Python by Tim Peters summarizes its major syntax guidelines for structured, tidy, and human-readable code. It is easily accessible in every Python environment through:\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\nPython comes in many flavors, and endless external packages extend the possibilities for conducting financial research. Any code we provide echoes some arguably subjective decisions we have taken to comply with our idea of what Tidy Finance comprises: Code should not simply yield the correct output but should be easy to read. Therefore, we advocate using chaining, which is the practice of calling multiple methods in a sequence, each operating on the result of the previous step.\nFurther, the entire book rests on tidy data, which we handle with a small set of powerful packages proven effective: pandas and numpy. Regarding visualization (which we deem highly relevant to provide a fundamentally human-centered experience), we follow the Grammars of Graphics’ philosophical framework (Wilkinson 2012), which has been carefully implemented using plotnine.\nArguably, neither chaining commands nor using the Grammar of Graphics can be considered mainstream within the Python ecosystem for financial research (yet). We believe in the value of the workflows we teach and practice on a daily basis. Therefore, we also believe that adopting such coding principles will dramatically increase in the near future. For more information on why Python is great, we refer to Hilpisch (2018).",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#about-the-authors",
    "href": "python/index.html#about-the-authors",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "Christoph Scheuch is the Head of Artificial Intelligence at the social trading platform wikifolio.com. He is responsible for researching, designing, and prototyping of cutting-edge AI-driven products using R and Python. Before his focus on AI, he was responsible for product management and business intelligence at wikifolio.com and an external lecturer at the Vienna University of Economics and Business, where he taught finance students how to manage empirical projects.\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals and he received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on Tidy Finance.\n\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance, with his research appearing in leading journals in financial economics. Patrick is especially passionate about empirical asset pricing and strives to understand the impact of methodological uncertainty on research outcomes.\nChristoph Frey is a Quantitative Researcher and Portfolio Manager at a family office in Hamburg and Research Fellow at the Centre for Financial Econometrics, Asset Markets and Macroeconomic Policy at Lancaster University. Prior to this, he was the leading quantitative researcher for systematic multi-asset strategies at Berenberg Bank and worked as an Assistant Professor at the Erasmus Universiteit Rotterdam. Christoph published research on Bayesian Econometrics and specializes in financial econometrics and portfolio optimization problems.",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/index.html#license",
    "href": "python/index.html#license",
    "title": "Tidy Finance with Python",
    "section": "",
    "text": "This book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite the Python project as follows:\n\nScheuch, C., Voigt, S., Weiss, P., & Frey, C. (2024). Tidy Finance with Python (1st ed.). Chapman and Hall/CRC https://www.tidy-finance.org\n\nYou can also use the following BibTeX entry:\n@book{Frey2024,\n  title = {Tidy Finance with Python (1st ed.)},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick and Frey, Christoph},\n  year = {2024},\n  publisher = {Chapman and Hall/CRC},\n  edition = {1st},\n  url = {https://tidy-finance.org/python}\n}",
    "crumbs": [
      "R",
      "Tidy Finance with Python",
      "Tidy Finance with Python"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html",
    "href": "python/fama-macbeth-regressions.html",
    "title": "Fama-MacBeth Regressions",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we present a simple implementation of Fama and MacBeth (1973), a regression approach commonly called Fama-MacBeth regressions. Fama-MacBeth regressions are widely used in empirical asset pricing studies. We use individual stocks as test assets to estimate the risk premium associated with the three factors included in Fama and French (1993).\nResearchers use the two-stage regression approach to estimate risk premiums in various markets, but predominately in the stock market. Essentially, the two-step Fama-MacBeth regressions exploit a linear relationship between expected returns and exposure to (priced) risk factors. The basic idea of the regression approach is to project asset returns on factor exposures or characteristics that resemble exposure to a risk factor in the cross-section in each time period. Then, in the second step, the estimates are aggregated across time to test if a risk factor is priced. In principle, Fama-MacBeth regressions can be used in the same way as portfolio sorts introduced in previous chapters.\nThe Fama-MacBeth procedure is a simple two-step approach: The first step uses the exposures (characteristics) as explanatory variables in \\(T\\) cross-sectional regressions. For example, if \\(r_{i,t+1}\\) denote the excess returns of asset \\(i\\) in month \\(t+1\\), then the famous Fama-French three-factor model implies the following return generating process (see also Campbell et al. 1998): \\[\\begin{aligned}r_{i,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{i,t} + \\lambda^{SMB}_t \\beta^{SMB}_{i,t} + \\lambda^{HML}_t \\beta^{HML}_{i,t} + \\epsilon_{i,t}.\\end{aligned} \\tag{1}\\] Here, we are interested in the compensation \\(\\lambda^{f}_t\\) for the exposure to each risk factor \\(\\beta^{f}_{i,t}\\) at each time point, i.e., the risk premium. Note the terminology: \\(\\beta^{f}_{i,t}\\) is an asset-specific characteristic, e.g., a factor exposure or an accounting variable. If there is a linear relationship between expected returns and the characteristic in a given month, we expect the regression coefficient to reflect the relationship, i.e., \\(\\lambda_t^{f}\\neq0\\).\nIn the second step, the time-series average \\(\\frac{1}{T}\\sum_{t=1}^T \\hat\\lambda^{f}_t\\) of the estimates \\(\\hat\\lambda^{f}_t\\) can then be interpreted as the risk premium for the specific risk factor \\(f\\). We follow Zaffaroni and Zhou (2022) and consider the standard cross-sectional regression to predict future returns. If the characteristics are replaced with time \\(t+1\\) variables, then the regression approach captures risk attributes rather than risk premiums.\nBefore we move to the implementation, we want to highlight that the characteristics, e.g., \\(\\hat\\beta^{f}_{i}\\), are often estimated in a separate step before applying the actual Fama-MacBeth methodology. You can think of this as a step 0. You might thus worry that the errors of \\(\\hat\\beta^{f}_{i}\\) impact the risk premiums’ standard errors. Measurement error in \\(\\hat\\beta^{f}_{i}\\) indeed affects the risk premium estimates, i.e., they lead to biased estimates. The literature provides adjustments for this bias (see, e.g., Shanken 1992; Kim 1995; Chen, Lee, and Lee 2015, among others) but also shows that the bias goes to zero as \\(T \\to \\infty\\). We refer to Gagliardini, Ossola, and Scaillet (2016) for an in-depth discussion also covering the case of time-varying betas. Moreover, if you plan to use Fama-MacBeth regressions with individual stocks, Hou, Xue, and Zhang (2020) advocates using weighted-least squares to estimate the coefficients such that they are not biased toward small firms. Without this adjustment, the high number of small firms would drive the coefficient estimates.\nThe current chapter relies on this set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#data-preparation",
    "href": "python/fama-macbeth-regressions.html#data-preparation",
    "title": "Fama-MacBeth Regressions",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe illustrate Fama and MacBeth (1973) with the monthly CRSP sample and use three characteristics to explain the cross-section of returns: Market capitalization, the book-to-market ratio, and the CAPM beta (i.e., the covariance of the excess stock returns with the market excess returns). We collect the data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT permno, gvkey, month, ret_excess, mktcap FROM crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\ncompustat = pd.read_sql_query(\n  sql=\"SELECT datadate, gvkey, be FROM compustat\",\n  con=tidy_finance,\n  parse_dates={\"datadate\"}\n)\n\nbeta = pd.read_sql_query(\n  sql=\"SELECT month, permno, beta_monthly FROM beta\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\nWe use the Compustat and CRSP data to compute the book-to-market ratio and the (log) market capitalization. Furthermore, we also use the CAPM betas based on monthly returns we computed in the previous chapters.\n\ncharacteristics = (compustat\n  .assign(month=lambda x: x[\"datadate\"].dt.to_period(\"M\").dt.to_timestamp())\n  .merge(crsp_monthly, how=\"left\", on=[\"gvkey\", \"month\"], )\n  .merge(beta, how=\"left\", on=[\"permno\", \"month\"])\n  .assign(\n    bm=lambda x: x[\"be\"]/x[\"mktcap\"],\n    log_mktcap=lambda x: np.log(x[\"mktcap\"]),\n    sorting_date=lambda x: x[\"month\"]+pd.DateOffset(months=6)\n  )\n  .get([\"gvkey\", \"bm\", \"log_mktcap\", \"beta_monthly\", \"sorting_date\"])\n  .rename(columns={\"beta_monthly\": \"beta\"})\n)\n\ndata_fama_macbeth = (crsp_monthly\n  .merge(characteristics, \n         how=\"left\",\n         left_on=[\"gvkey\", \"month\"], right_on=[\"gvkey\", \"sorting_date\"])\n  .sort_values([\"month\", \"permno\"])\n  .groupby(\"permno\")\n  .apply(lambda x: x.assign(\n      beta=x[\"beta\"].fillna(method=\"ffill\"),\n      bm=x[\"bm\"].fillna(method=\"ffill\"),\n      log_mktcap=x[\"log_mktcap\"].fillna(method=\"ffill\")\n    )\n  )\n  .reset_index(drop=True)  \n)\n\ndata_fama_macbeth_lagged = (data_fama_macbeth\n  .assign(month=lambda x: x[\"month\"]-pd.DateOffset(months=1))\n  .get([\"permno\", \"month\", \"ret_excess\"])\n  .rename(columns={\"ret_excess\": \"ret_excess_lead\"})\n)\n\ndata_fama_macbeth = (data_fama_macbeth\n  .merge(data_fama_macbeth_lagged, how=\"left\", on=[\"permno\", \"month\"])\n  .get([\"permno\", \"month\", \"ret_excess_lead\", \"beta\", \"log_mktcap\", \"bm\"])\n  .dropna()\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#cross-sectional-regression",
    "href": "python/fama-macbeth-regressions.html#cross-sectional-regression",
    "title": "Fama-MacBeth Regressions",
    "section": "Cross-Sectional Regression",
    "text": "Cross-Sectional Regression\nNext, we run the cross-sectional regressions with the characteristics as explanatory variables for each month. We regress the returns of the test assets at a particular time point on the characteristics of each asset. By doing so, we get an estimate of the risk premiums \\(\\hat\\lambda^{f}_t\\) for each point in time. \n\nrisk_premiums = (data_fama_macbeth\n  .groupby(\"month\")\n  .apply(lambda x: smf.ols(\n      formula=\"ret_excess_lead ~ beta + log_mktcap + bm\", \n      data=x\n    ).fit()\n    .params\n  )\n  .reset_index()\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#time-series-aggregation",
    "href": "python/fama-macbeth-regressions.html#time-series-aggregation",
    "title": "Fama-MacBeth Regressions",
    "section": "Time-Series Aggregation",
    "text": "Time-Series Aggregation\nNow that we have the risk premiums’ estimates for each period, we can average across the time-series dimension to get the expected risk premium for each characteristic. Similarly, we manually create the \\(t\\)-test statistics for each regressor, which we can then compare to usual critical values of 1.96 or 2.576 for two-tailed significance tests at a five percent and a one percent significance level.\n\nprice_of_risk = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")[\"estimate\"]\n  .apply(lambda x: pd.Series({\n      \"risk_premium\": 100*x.mean(),\n      \"t_statistic\": x.mean()/x.std()*np.sqrt(len(x))\n    })\n  )\n  .reset_index()\n  .pivot(index=\"factor\", columns=\"level_1\", values=\"estimate\")\n  .reset_index()\n)\n\nIt is common to adjust for autocorrelation when reporting standard errors of risk premiums. As in Univariate Portfolio Sorts, the typical procedure for this is computing Newey and West (1987) standard errors.\n\nprice_of_risk_newey_west = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")\n  .apply(lambda x: (\n      x[\"estimate\"].mean()/ \n        smf.ols(\"estimate ~ 1\", x)\n        .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6}).bse\n    )\n  )\n  .reset_index()\n  .rename(columns={\"Intercept\": \"t_statistic_newey_west\"})\n)\n\n(price_of_risk\n  .merge(price_of_risk_newey_west, on=\"factor\")\n  .round(3)\n)\n\n\n\n\n\n\n\n\n\nfactor\nrisk_premium\nt_statistic\nt_statistic_newey_west\n\n\n\n\n0\nIntercept\n1.244\n4.809\n4.168\n\n\n1\nbeta\n0.001\n0.013\n0.012\n\n\n2\nbm\n0.161\n3.054\n2.860\n\n\n3\nlog_mktcap\n-0.108\n-3.005\n-2.829\n\n\n\n\n\n\n\n\nFinally, let us interpret the results. Stocks with higher book-to-market ratios earn higher expected future returns, which is in line with the value premium. The negative value for log market capitalization reflects the size premium for smaller stocks. Consistent with results from earlier chapters, we detect no relation between beta and future stock returns.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#exercises",
    "href": "python/fama-macbeth-regressions.html#exercises",
    "title": "Fama-MacBeth Regressions",
    "section": "Exercises",
    "text": "Exercises\n\nDownload a sample of test assets from Kenneth French’s homepage and reevaluate the risk premiums for industry portfolios instead of individual stocks.\nUse individual stocks with weighted-least squares based on a firm’s size as suggested by Hou, Xue, and Zhang (2020). Then, repeat the Fama-MacBeth regressions without the weighting-scheme adjustment but drop the smallest 20 percent of firms each month. Compare the results of the three approaches.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Fama-MacBeth Regressions"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html",
    "href": "python/difference-in-differences.html",
    "title": "Difference in Differences",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we illustrate the concept of difference in differences (DD) estimators by evaluating the effects of climate change regulation on the pricing of bonds across firms. DD estimators are typically used to recover the treatment effects of natural or quasi-natural experiments that trigger sharp changes in the environment of a specific group. Instead of looking at differences in just one group (e.g., the effect in the treated group), DD investigates the treatment effects by looking at the difference between differences in two groups. Such experiments are usually exploited to address endogeneity concerns (e.g., Roberts and Whited 2013). The identifying assumption is that the outcome variable would change equally in both groups without the treatment. This assumption is also often referred to as the assumption of parallel trends. Moreover, we would ideally also want a random assignment to the treatment and control groups. Due to lobbying or other activities, this randomness is often violated in (financial) economics.\nIn the context of our setting, we investigate the impact of the Paris Agreement (PA), signed on December 12, 2015, on the bond yields of polluting firms. We first estimate the treatment effect of the agreement using panel regression techniques that we discuss in Fixed Effects and Clustered Standard Errors. We then present two methods to illustrate the treatment effect over time graphically. Although we demonstrate that the treatment effect of the agreement is anticipated by bond market participants well in advance, the techniques we present below can also be applied to many other settings.\nThe approach we use here replicates the results of Seltzer, Starks, and Zhu (2022) partly. Specifically, we borrow their industry definitions for grouping firms into green and brown types. Overall, the literature on environmental, social, and governance (ESG) effects in corporate bond markets is already large but continues to grow (for recent examples, see, e.g., Halling, Yu, and Zechner (2021), Handler, Jankowitsch, and Pasler (2022), Huynh and Xia (2021), among many others).\nThe current chapter relies on this set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport linearmodels as lm\nimport statsmodels.formula.api as smf\n\nfrom plotnine import *\nfrom scipy.stats import norm\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\nfrom regtabletotext import prettify_result\nCompared to previous chapters, we introduce the scipy.stats module from the scipy (Virtanen et al. 2020) for simple retrieval of quantiles of the standard normal distribution.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#data-preparation",
    "href": "python/difference-in-differences.html#data-preparation",
    "title": "Difference in Differences",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use TRACE and Mergent FISD as data sources from our SQLite database introduced in Accessing and Managing Financial Data and TRACE and FISD. \n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfisd = (pd.read_sql_query(\n    sql=\"SELECT complete_cusip, maturity, offering_amt, sic_code FROM fisd\",\n    con=tidy_finance,\n    parse_dates={\"maturity\"})\n  .dropna()\n)\n\ntrace_enhanced = (pd.read_sql_query(\n    sql=(\"SELECT cusip_id, trd_exctn_dt, rptd_pr, entrd_vol_qt, yld_pt \" \n         \"FROM trace_enhanced\"),\n    con=tidy_finance,\n    parse_dates={\"trd_exctn_dt\"})\n  .dropna()\n)\n\nWe start our analysis by preparing the sample of bonds. We only consider bonds with a time to maturity of more than one year to the signing of the PA, so that we have sufficient data to analyze the yield behavior after the treatment date. This restriction also excludes all bonds issued after the agreement. We also consider only the first two digits of the SIC industry code to identify the polluting industries (in line with Seltzer, Starks, and Zhu 2022).\n\ntreatment_date = pd.to_datetime(\"2015-12-12\")\npolluting_industries = [\n  49, 13, 45, 29, 28, 33, 40, 20, 26, 42, 10, 53, 32, 99, 37\n]\n\nbonds = (fisd\n  .query(\"offering_amt &gt; 0 & sic_code != 'None'\")\n  .assign(\n    time_to_maturity=lambda x: (x[\"maturity\"]-treatment_date).dt.days / 365,\n    sic_code=lambda x: x[\"sic_code\"].astype(str).str[:2].astype(int),\n    log_offering_amt=lambda x: np.log(x[\"offering_amt\"])\n  )\n  .query(\"time_to_maturity &gt;= 1\")\n  .rename(columns={\"complete_cusip\": \"cusip_id\"})\n  .get([\"cusip_id\", \"time_to_maturity\", \"log_offering_amt\", \"sic_code\"])\n  .assign(polluter=lambda x: x[\"sic_code\"].isin(polluting_industries))\n  .reset_index(drop=True)\n)\n\nNext, we aggregate the individual transactions as reported in TRACE to a monthly panel of bond yields. We consider bond yields for a bond’s last trading day in a month. Therefore, we first aggregate bond data to daily frequency and apply common restrictions from the literature (see, e.g., Bessembinder et al. 2008). We weigh each transaction by volume to reflect a trade’s relative importance and avoid emphasizing small trades. Moreover, we only consider transactions with reported prices rptd_pr larger than 25 (to exclude bonds that are close to default) and only bond-day observations with more than five trades on a corresponding day (to exclude prices based on too few, potentially non-representative transactions). \n\ntrace_enhanced = (trace_enhanced\n  .query(\"rptd_pr &gt; 25\")\n  .assign(weight=lambda x: x[\"entrd_vol_qt\"]*x[\"rptd_pr\"])\n  .assign(weighted_yield=lambda x: x[\"weight\"]*x[\"yld_pt\"])\n)\n\ntrace_aggregated = (trace_enhanced\n  .groupby([\"cusip_id\", \"trd_exctn_dt\"])\n  .aggregate(\n    weighted_yield_sum=(\"weighted_yield\", \"sum\"),\n    weight_sum=(\"weight\", \"sum\"),\n    trades=(\"rptd_pr\", \"count\")\n  )\n  .reset_index()\n  .assign(avg_yield=lambda x: x[\"weighted_yield_sum\"]/x[\"weight_sum\"])\n  .dropna(subset=[\"avg_yield\"])\n  .query(\"trades &gt;= 5\")\n  .assign(trd_exctn_dt=lambda x: pd.to_datetime(x[\"trd_exctn_dt\"]))\n  .assign(month=lambda x: x[\"trd_exctn_dt\"]-pd.offsets.MonthBegin())\n)\n\ndate_index = (trace_aggregated\n  .groupby([\"cusip_id\", \"month\"])[\"trd_exctn_dt\"]\n  .idxmax()\n)\n\ntrace_aggregated = (trace_aggregated\n  .loc[date_index]\n  .get([\"cusip_id\", \"month\", \"avg_yield\"])\n)\n\nBy combining the bond-specific information from Mergent FISD for our bond sample with the aggregated TRACE data, we arrive at the main sample for our analysis.\n\nbonds_panel = (bonds\n  .merge(trace_aggregated, how=\"inner\", on=\"cusip_id\")\n  .dropna()\n)\n\nBefore we can run the first regression, we need to define the treated indicator,1 which is the product of the post_period (i.e., all months after the signing of the PA) and the polluter indicator defined above.\n\nbonds_panel = (bonds_panel\n  .assign(\n    post_period=lambda x: (\n      x[\"month\"] &gt;= (treatment_date-pd.offsets.MonthBegin())\n    )\n  )\n  .assign(treated=lambda x: x[\"polluter\"] & x[\"post_period\"])\n  .assign(month_cat=lambda x: pd.Categorical(x[\"month\"], ordered=True))\n)\n\nAs usual, we tabulate summary statistics of the variables that enter the regression to check the validity of our variable definitions.\n\nbonds_panel_summary = (bonds_panel\n  .melt(var_name=\"measure\",\n        value_vars=[\"avg_yield\", \"time_to_maturity\", \"log_offering_amt\"])\n  .groupby(\"measure\")\n  .describe(percentiles=[0.05, 0.5, 0.95])\n)\nnp.round(bonds_panel_summary, 2)\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\ncount\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nmeasure\n\n\n\n\n\n\n\n\n\n\n\n\navg_yield\n127546.0\n4.08\n4.21\n0.06\n1.27\n3.38\n8.11\n127.97\n\n\nlog_offering_amt\n127546.0\n13.27\n0.82\n4.64\n12.21\n13.22\n14.51\n16.52\n\n\ntime_to_maturity\n127546.0\n8.55\n8.41\n1.01\n1.50\n5.81\n27.41\n100.70",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#panel-regressions",
    "href": "python/difference-in-differences.html#panel-regressions",
    "title": "Difference in Differences",
    "section": "Panel Regressions",
    "text": "Panel Regressions\nThe PA is a legally binding international treaty on climate change. It was adopted by 196 parties at COP 21 in Paris on December 12, 2015 and entered into force on November 4, 2016. The PA obliges developed countries to support efforts to build clean, climate-resilient futures. One may thus hypothesize that adopting climate-related policies may affect financial markets. To measure the magnitude of this effect, we first run an ordinary least square (OLS) regression without fixed effects where we include the treated, post_period, and polluter dummies, as well as the bond-specific characteristics log_offering_amt and time_to_maturity. This simple model assumes that there are essentially two periods (before and after the PA) and two groups (polluters and non-polluters). Nonetheless, it should indicate whether polluters have higher yields following the PA compared to non-polluters.\nThe second model follows the typical DD regression approach by including individual (cusip_id) and time (month) fixed effects. In this model, we do not include any other variables from the simple model because the fixed effects subsume them, and we observe the coefficient of our main variable of interest: treated.\n\nmodel_without_fe = lm.PanelOLS.from_formula(\n  formula=(\"avg_yield ~ treated + post_period + polluter + log_offering_amt\"\n           \" + time_to_maturity + 1\"),\n  data=bonds_panel.set_index([\"cusip_id\", \"month\"]),\n).fit()\n\nmodel_with_fe = lm.PanelOLS.from_formula(\n  formula=(\"avg_yield ~ treated + EntityEffects + TimeEffects\"),\n  data=bonds_panel.set_index([\"cusip_id\", \"month\"]),\n).fit()\n\nprettify_result([model_without_fe, model_with_fe])\n\nDependent var.      avg_yield      avg_yield\n\nIntercept         10.733 (57.06)\ntreated            0.453 (9.14)   0.974 (29.3)\npost_period       -0.178 (-6.04)\npolluter          0.486 (15.43)\nlog_offering_amt  -0.55 (-38.99)\ntime_to_maturity  0.058 (41.53)\n\nFixed effects                     Entity, Time\nVCOV type           Unadjusted     Unadjusted\nObservations         127,546        127,546\nR2 (incl. FE)         0.032          0.648\nWithin R2             0.004          0.012\n\nNote: t-statistics in parentheses\n\n\nBoth models indicate that polluters have significantly higher yields after the PA than non-polluting firms. Note that the magnitude of the treated coefficient varies considerably across models.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#visualizing-parallel-trends",
    "href": "python/difference-in-differences.html#visualizing-parallel-trends",
    "title": "Difference in Differences",
    "section": "Visualizing Parallel Trends",
    "text": "Visualizing Parallel Trends\nEven though the regressions above indicate that there is an impact of the PA on bond yields of polluters, the tables do not tell us anything about the dynamics of the treatment effect. In particular, the models provide no indication about whether the crucial parallel trends assumption is valid. This assumption requires that in the absence of treatment, the difference between the two groups is constant over time. Although there is no well-defined statistical test for this assumption, visual inspection typically provides a good indication.\nTo provide such visual evidence, we revisit the simple OLS model and replace the treated and post_period indicators with month dummies for each group. This approach estimates the average yield change of both groups for each period and provides corresponding confidence intervals. Plotting the coefficient estimates for both groups around the treatment date shows us the dynamics of our panel data.\n\nmodel_without_fe_time = (smf.ols(\n    formula=(\"avg_yield ~ polluter + month_cat:polluter + time_to_maturity\"\n             \" + log_offering_amt\"),\n    data=bonds_panel)\n  .fit()\n  .summary()\n)\n\nmodel_without_fe_coefs = (\n  pd.DataFrame(model_without_fe_time.tables[1].data[1:],\n               columns=[\"term\", \"estimate\", \"std_error\",\n                        \"t_stat\", \"p_value\", \"ci_1\", \"ci_2\"])\n  .query(\"term.str.contains('month_cat')\")\n  .assign(\n    month=lambda x: \n      x[\"term\"].str.extract(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\")\n  )\n  .assign(month=lambda x: pd.to_datetime(x[\"month\"]))\n  .assign(treatment=lambda x: x[\"term\"].str.contains(\"True\"))\n  .assign(estimate=lambda x: x[\"estimate\"].astype(float),\n          std_error=lambda x: x[\"std_error\"].astype(float))\n  .assign(ci_up=lambda x: x[\"estimate\"]+norm.ppf(0.975)*x[\"std_error\"],\n          ci_low=lambda x: x[\"estimate\"]+norm.ppf(0.025)*x[\"std_error\"])\n)\n\npolluters_plot = (\n  ggplot(model_without_fe_coefs, \n         aes(x=\"month\", y=\"estimate\",\n             color=\"treatment\", linetype=\"treatment\", shape=\"treatment\")) +\n  geom_vline(xintercept=pd.to_datetime(treatment_date) -\n             pd.offsets.MonthBegin(), linetype=\"dashed\") +\n  geom_hline(yintercept=0, linetype=\"dashed\") +\n  geom_errorbar(aes(ymin=\"ci_low\", ymax=\"ci_up\"), alpha=0.5) +\n  geom_point() +\n  guides(linetype=None) + \n  labs(x=\"\", y=\"Yield\", shape=\"Polluter?\", color=\"Polluter?\",\n       title=\"Polluters respond stronger than green firms\") +\n  scale_linetype_manual(values=[\"solid\", \"dashed\"]) +\n  scale_x_datetime(breaks=date_breaks(\"1 year\"), labels=date_format(\"%Y\")) \n)\npolluters_plot.draw()\n\n\n\n\n\n\n\nFigure 1: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters and non-polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\n\nFigure 1 shows that throughout most of 2014, the yields of the two groups changed in unison. However, starting at the end of 2014, the yields start to diverge, reaching the highest difference around the signing of the PA. Afterward, the yields for both groups fall again, and the polluters arrive at the same level as at the beginning of 2014. The non-polluters, on the other hand, even experience significantly lower yields than polluters after the signing of the agreement.\nInstead of plotting both groups using the simple model approach, we can also use the fixed-effects model and focus on the polluter’s yield response to the signing relative to the non-polluters. To perform this estimation, we need to replace the treated indicator with separate time dummies for the polluters, each marking a one-month period relative to the treatment date.\n\nbonds_panel_alt = (bonds_panel\n  .assign(\n    diff_to_treatment=lambda x: (\n      np.round(\n        ((x[\"month\"]-(treatment_date- \n            pd.offsets.MonthBegin())).dt.days/365)*12, 0\n      ).astype(int)\n    )\n  )\n)\n\nvariables = (bonds_panel_alt\n  .get([\"diff_to_treatment\", \"month\"])\n  .drop_duplicates()\n  .sort_values(\"month\")\n  .copy()\n  .assign(variable_name=np.nan)\n  .reset_index(drop=True)\n)\n\nIn the next code chunk, we assemble the model formula and regress the monthly yields on the set of time dummies and cusip_id and month fixed effects.\n\nformula = \"avg_yield ~ 1 + \"\n\nfor j in range(variables.shape[0]):\n    if variables[\"diff_to_treatment\"].iloc[j] != 0:\n        old_names=list(bonds_panel_alt.columns)\n        \n        bonds_panel_alt[\"new_var\"] = (\n          bonds_panel_alt[\"diff_to_treatment\"] == \n            variables[\"diff_to_treatment\"].iloc[j]\n        ) & bonds_panel_alt[\"polluter\"]\n        \n        diff_to_treatment_value=variables[\"diff_to_treatment\"].iloc[j]\n        direction=\"lag\" if diff_to_treatment_value &lt; 0 else \"lead\"\n        abs_diff_to_treatment=int(abs(diff_to_treatment_value))\n        new_var_name=f\"{direction}{abs_diff_to_treatment}\"\n        variables.at[j, \"variable_name\"]=new_var_name\n        bonds_panel_alt[new_var_name]=bonds_panel_alt[\"new_var\"]\n        formula += (f\" + {new_var_name}\" if j &gt; 0 else new_var_name)\n\nformula = formula + \" + EntityEffects + TimeEffects\"\n\nmodel_with_fe_time = (lm.PanelOLS.from_formula(\n    formula=formula,\n    data=bonds_panel_alt.set_index([\"cusip_id\", \"month\"]))\n  .fit()\n  .summary\n)\n\nWe then collect the regression results into a dataframe that contains the estimates and corresponding 95 percent confidence intervals. Note that we also add a row with zeros for the (omitted) reference point of the time dummies.\n\nlag0_row = pd.DataFrame({\n  \"term\": [\"lag0\"],\n  \"estimate\": [0],\n  \"ci_1\": [0],\n  \"ci_2\": [0],\n  \"ci_up\": [0],\n  \"ci_low\": [0],\n  \"month\": [treatment_date - pd.offsets.MonthBegin()]\n})\n\nmodel_with_fe_time_coefs = (\n  pd.DataFrame(model_with_fe_time.tables[1].data[1:],\n               columns=[\"term\", \"estimate\", \"std_error\",\n                        \"t_stat\", \"p_value\", \"ci_1\", \"ci_2\"])\n  .assign(term=lambda x: x[\"term\"].str.replace(\"[T.True]\", \"\"))\n  .assign(estimate=lambda x: x[\"estimate\"].astype(float),\n          std_error=lambda x: x[\"std_error\"].astype(float))\n  .assign(ci_up=lambda x: x[\"estimate\"] + norm.ppf(0.975)*x[\"std_error\"],\n          ci_low=lambda x: x[\"estimate\"] + norm.ppf(0.025)*x[\"std_error\"])\n  .merge(variables, how=\"left\", left_on=\"term\", right_on=\"variable_name\")\n  .drop(columns=\"variable_name\")\n  .query(\"term != 'Intercept'\")\n)\n\nmodel_with_fe_time_coefs = pd.concat(\n  [model_with_fe_time_coefs, lag0_row], \n  ignore_index=True\n)\n\nFigure 2 shows the resulting coefficient estimates.\n\npolluter_plot = (\n  ggplot(model_with_fe_time_coefs, aes(x=\"month\", y=\"estimate\")) +\n  geom_vline(aes(xintercept=treatment_date - pd.offsets.MonthBegin()), \n                 linetype=\"dashed\") +\n  geom_hline(aes(yintercept=0), linetype=\"dashed\") +\n  geom_errorbar(aes(ymin=\"ci_low\", ymax=\"ci_up\"), alpha=0.5) +\n  geom_point(aes(y=\"estimate\")) +\n  labs(x=\"\", y=\"Yield\",\n       title=\"Polluters' yield patterns around Paris Agreement signing\") +\n  scale_x_datetime(breaks=date_breaks(\"1 year\"), labels=date_format(\"%Y\")) \n)\npolluter_plot.draw()\n\n\n\n\n\n\n\nFigure 2: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\n\n The resulting graph shown in Figure 2 confirms the main conclusion of the previous image: polluters’ yield patterns show a considerable anticipation effect starting toward the end of 2014. Yields only marginally increase after the signing of the agreement. However, as opposed to the simple model, we do not see a complete reversal back to the pre-agreement level. Yields of polluters stay at a significantly higher level even one year after the signing.\nNotice that during the year after the PA was signed, the 45th president of the United States was elected (on November 8, 2016). During his campaign there were some indications of intentions to withdraw the US from the PA, which ultimately happened on November 4, 2020. Hence, reversal effects are potentially driven by these actions.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#exercises",
    "href": "python/difference-in-differences.html#exercises",
    "title": "Difference in Differences",
    "section": "Exercises",
    "text": "Exercises\n\nThe 46th president of the US rejoined the Paris Agreement on February 19, 2021. Repeat the difference in differences analysis for the day of his election victory. Note that you will also have to download new TRACE data. How did polluters’ yields react to this action?\nBased on the exercise on ratings in TRACE and FISD, include ratings as a control variable in the analysis above. Do the results change?\n\n\n\n\nFigure 1: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters and non-polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\nFigure 2: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Agreement on bond yields (in percent) for polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/difference-in-differences.html#footnotes",
    "href": "python/difference-in-differences.html#footnotes",
    "title": "Difference in Differences",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that by using a generic name here, everybody can replace ours with their sample data and run the code to produce standard regression tables and illustrations.↩︎",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Difference in Differences"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html",
    "href": "python/constrained-optimization-and-backtesting.html",
    "title": "Constrained Optimization and Backtesting",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with Rhere.\nIn this chapter, we conduct portfolio backtesting in a realistic setting by including transaction costs and investment constraints such as no-short-selling rules. We start with standard mean-variance efficient portfolios and introduce constraints in a step-by-step manner. To do so, we rely on numerical optimization procedures in Python. We conclude the chapter by providing an out-of-sample backtesting procedure for the different strategies that we introduce in this chapter.\nThroughout this chapter, we use the following Python packages:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom itertools import product\nfrom scipy.stats import expon\nfrom scipy.optimize import minimize\nCompared to previous chapters, we introduce expon from scipy.stats to calculate exponential continuous random variables.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#data-preparation",
    "href": "python/constrained-optimization-and-backtesting.html#data-preparation",
    "title": "Constrained Optimization and Backtesting",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start by loading the required data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. For simplicity, we restrict our investment universe to the monthly Fama-French industry portfolio returns in the following application. \n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nindustry_returns = (pd.read_sql_query(\n    sql=\"SELECT * FROM industries_ff_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n  .drop(columns=[\"month\"])\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "href": "python/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Recap of Portfolio Choice",
    "text": "Recap of Portfolio Choice\nA common objective for portfolio optimization is to find mean-variance efficient portfolio weights, i.e., the allocation that delivers the lowest possible return variance for a given minimum level of expected returns. In the most extreme case, where the investor is only concerned about portfolio variance, they may choose to implement the minimum variance portfolio (MVP) weights which are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\tag{1}\\] where \\(\\Sigma\\) is the \\((N \\times N)\\) covariance matrix of the returns. The optimal weights \\(\\omega_\\text{mvp}\\) can be found analytically and are \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). In terms of code, the math is equivalent to the following chunk. \n\nn_industries = industry_returns.shape[1]\n\nmu = np.array(industry_returns.mean()).T\nsigma = np.array(industry_returns.cov())\nw_mvp = np.linalg.inv(sigma) @ np.ones(n_industries)\nw_mvp = w_mvp/w_mvp.sum()\n\nweights_mvp = pd.DataFrame({\n  \"Industry\": industry_returns.columns.tolist(),\n  \"Minimum variance\": w_mvp\n})\nweights_mvp.round(3)\n\n\n\n\n\n\n\n\n\nIndustry\nMinimum variance\n\n\n\n\n0\nnodur\n0.248\n\n\n1\ndurbl\n-0.012\n\n\n2\nmanuf\n0.078\n\n\n3\nenrgy\n0.079\n\n\n4\nhitec\n0.008\n\n\n5\ntelcm\n0.241\n\n\n6\nshops\n0.092\n\n\n7\nhlth\n0.160\n\n\n8\nutils\n0.470\n\n\n9\nother\n-0.364\n\n\n\n\n\n\n\n\nNext, consider an investor who aims to achieve minimum variance given a required expected portfolio return \\(\\bar{\\mu}\\) such that she chooses \\[\\omega_\\text{eff}({\\bar{\\mu}}) =\\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}. \\tag{2}\\] We leave it as an exercise below to show that the portfolio choice problem can equivalently be formulated for an investor with mean-variance preferences and risk aversion factor \\(\\gamma\\). That means the investor aims to choose portfolio weights as the solution to \\[ \\omega^*_\\gamma = \\arg\\max \\omega' \\mu - \\frac{\\gamma}{2}\\omega'\\Sigma \\omega\\quad \\text{ s.t. } \\omega'\\iota = 1. \\tag{3}\\] The solution to the optimal portfolio choice problem is: \\[\\omega^*_{\\gamma} = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota. \\tag{4}\\] To proof this statement, we refer to the derivations in Proofs. Empirically, this classical solution imposes many problems. In particular, the estimates of \\(\\mu\\) are noisy over short horizons, the (\\(N \\times N\\)) matrix \\(\\Sigma\\) contains \\(N(N-1)/2\\) distinct elements and thus, estimation error is huge. Seminal papers on the effect of ignoring estimation uncertainty, among others, are Brown (1976), Jobson and Korkie (1980), Jorion (1986), and Chopra and Ziemba (1993).\nEven worse, if the asset universe contains more assets than available time periods \\((N &gt; T)\\), the sample covariance matrix is no longer positive definite such that the inverse \\(\\Sigma^{-1}\\) does not exist anymore. To address estimation issues for vast-dimensional covariance matrices, regularization techniques (see, e.g., Ledoit and Wolf 2003, 2004, 2012; Fan, Fan, and Lv 2008) and the parametric approach from the previous chapter are popular tools.\nWhile the uncertainty associated with estimated parameters is challenging, the data-generating process is also unknown to the investor. In other words, model uncertainty reflects that it is ex-ante not even clear which parameters require estimation (for instance, if returns are driven by a factor model, selecting the universe of relevant factors imposes model uncertainty). Wang (2005) and Garlappi, Uppal, and Wang (2007) provide theoretical analysis on optimal portfolio choice under model and estimation uncertainty. In the most extreme case, Pflug, Pichler, and Wozabal (2012) shows that the naive portfolio, which allocates equal wealth to all assets, is the optimal choice for an investor averse to model uncertainty.\nOn top of the estimation uncertainty, transaction costs are a major concern. Rebalancing portfolios is costly, and, therefore, the optimal choice should depend on the investor’s current holdings. In the presence of transaction costs, the benefits of reallocating wealth may be smaller than the costs associated with turnover. This aspect has been investigated theoretically, among others, for one risky asset by Magill and Constantinides (1976) and Davis and Norman (1990). Subsequent extensions to the case with multiple assets have been proposed by Balduzzi and Lynch (1999) and Balduzzi and Lynch (2000). More recent papers on empirical approaches that explicitly account for transaction costs include Gârleanu and Pedersen (2013), DeMiguel, Nogales, and Uppal (2014), and DeMiguel, Martín-Utrera, and Nogales (2015).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "href": "python/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "title": "Constrained Optimization and Backtesting",
    "section": "Estimation Uncertainty and Transaction Costs",
    "text": "Estimation Uncertainty and Transaction Costs\nThe empirical evidence regarding the performance of a mean-variance optimization procedure in which you simply plug in some sample estimates \\(\\hat \\mu\\) and \\(\\hat \\Sigma\\) can be summarized rather briefly: mean-variance optimization performs poorly! The literature discusses many proposals to overcome these empirical issues. For instance, one may impose some form of regularization of \\(\\Sigma\\), rely on Bayesian priors inspired by theoretical asset pricing models (Kan and Zhou 2007), or use high-frequency data to improve forecasting (Hautsch, Kyj, and Malec 2015). One unifying framework that works easily, effectively (even for large dimensions), and is purely inspired by economic arguments is an ex-ante adjustment for transaction costs (Hautsch and Voigt 2019).\nAssume that returns are from a multivariate normal distribution with mean \\(\\mu\\) and variance-covariance matrix \\(\\Sigma\\), \\(N(\\mu,\\Sigma)\\). Additionally, we assume quadratic transaction costs which penalize rebalancing such that \\[\n\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) = \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\\end{aligned} \\tag{5}\\] with cost parameter \\(\\beta&gt;0\\) and \\(\\omega_{t^+} = {\\omega_t \\circ (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\), where \\(\\circ\\) is the element-wise Hadamard product. \\(\\omega_{t^+}\\) denotes the portfolio weights just before rebalancing. Note that \\(\\omega_{t^+}\\) differs mechanically from \\(\\omega_t\\) due to the returns in the past period. Intuitively, transaction costs penalize portfolio performance when the portfolio is shifted from the current holdings \\(\\omega_{t^+}\\) to a new allocation \\(\\omega_{t+1}\\). In this setup, transaction costs do not increase linearly. Instead, larger rebalancing is penalized more heavily than small adjustments. Then, the optimal portfolio choice for an investor with mean variance preferences is \\[\\begin{aligned}\\omega_{t+1} ^* &= \\arg\\max \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\text{ s.t. } \\iota'\\omega = 1\\\\\n&=\\arg\\max\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t.} \\iota'\\omega=1,\\end{aligned} \\tag{6}\\] where \\[\\mu^*=\\mu+\\beta \\omega_{t^+} \\quad \\text{and} \\quad \\Sigma^*=\\Sigma + \\frac{\\beta}{\\gamma} I_N. \\tag{7}\\] As a result, adjusting for transaction costs implies a standard mean-variance optimal portfolio choice with adjusted return parameters \\(\\Sigma^*\\) and \\(\\mu^*\\): \\[\\omega^*_{t+1} = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^* + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota. \\tag{8}\\]\nAn alternative formulation of the optimal portfolio can be derived as follows: \\[\\omega_{t+1} ^*=\\arg\\max\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t. } \\iota'\\omega=1. \\tag{9}\\] The optimal weights correspond to a mean-variance portfolio, where the vector of expected returns is such that assets that currently exhibit a higher weight are considered as delivering a higher expected return.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "href": "python/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Optimal Portfolio Choice",
    "text": "Optimal Portfolio Choice\nThe function below implements the efficient portfolio weights in its general form, allowing for transaction costs (conditional on the holdings before reallocation). For \\(\\beta=0\\), the computation resembles the standard mean-variance efficient framework. gamma denotes the coefficient of risk aversion \\(\\gamma\\), beta is the transaction cost parameter \\(\\beta\\) and w_prev are the weights before rebalancing \\(\\omega_{t^+}\\).\n\ndef compute_efficient_weight(sigma, \n                             mu, \n                             gamma=2, \n                             beta=0,\n                             w_prev=np.ones(sigma.shape[1])/sigma.shape[1]):\n    \"\"\"Compute efficient portfolio weights.\"\"\"\n    \n    n = sigma.shape[1]\n    iota = np.ones(n)\n    sigma_processed = sigma+(beta/gamma)*np.eye(n)\n    mu_processed = mu+beta*w_prev\n\n    sigma_inverse = np.linalg.inv(sigma_processed)\n\n    w_mvp = sigma_inverse @ iota\n    w_mvp = w_mvp/np.sum(w_mvp)\n    w_opt = w_mvp+(1/gamma)*\\\n        (sigma_inverse-np.outer(w_mvp, iota) @ sigma_inverse) @ mu_processed\n        \n    return w_opt\n\nw_efficient = compute_efficient_weight(sigma, mu)\n\nweights_efficient = pd.DataFrame({\n  \"Industry\": industry_returns.columns.tolist(),\n  \"Efficient portfolio\": w_efficient\n})\nweights_efficient.round(3)\n\n\n\n\n\n\n\n\n\nIndustry\nEfficient portfolio\n\n\n\n\n0\nnodur\n1.630\n\n\n1\ndurbl\n0.090\n\n\n2\nmanuf\n-1.356\n\n\n3\nenrgy\n0.687\n\n\n4\nhitec\n0.333\n\n\n5\ntelcm\n-0.412\n\n\n6\nshops\n0.504\n\n\n7\nhlth\n0.402\n\n\n8\nutils\n-0.219\n\n\n9\nother\n-0.659\n\n\n\n\n\n\n\n\nThe portfolio weights above indicate the efficient portfolio for an investor with risk aversion coefficient \\(\\gamma=2\\) in the absence of transaction costs. Some of the positions are negative, which implies short-selling, and most of the positions are rather extreme. For instance, a position of \\(-1\\) implies that the investor takes a short position worth their entire wealth to lever long positions in other assets. What is the effect of transaction costs or different levels of risk aversion on the optimal portfolio choice? The following few lines of code analyze the distance between the minimum variance portfolio and the portfolio implemented by the investor for different values of the transaction cost parameter \\(\\beta\\) and risk aversion \\(\\gamma\\).\n\ngammas = [2, 4, 8, 20]\nbetas = 20*expon.ppf(np.arange(1, 100)/100, scale=1)\n\ntransaction_costs = (pd.DataFrame(\n    list(product(gammas, betas)), \n    columns=[\"gamma\", \"beta\"]\n  )\n  .assign(\n    weights=lambda x: x.apply(lambda y:\n      compute_efficient_weight(\n        sigma, mu, gamma=y[\"gamma\"], beta=y[\"beta\"]/10000, w_prev=w_mvp), \n      axis=1\n    ),\n    concentration=lambda x: x[\"weights\"].apply(\n      lambda x: np.sum(np.abs(x-w_mvp))\n    )\n  )\n)\n\nThe code chunk above computes the optimal weight in the presence of transaction cost for different values of \\(\\beta\\) and \\(\\gamma\\) but with the same initial allocation, the theoretical optimal minimum variance portfolio. Starting from the initial allocation, the investor chooses their optimal allocation along the efficient frontier to reflect their own risk preferences. If transaction costs were absent, the investor would simply implement the mean-variance efficient allocation. If transaction costs make it costly to rebalance, their optimal portfolio choice reflects a shift toward the efficient portfolio, whereas their current portfolio anchors their investment.\n\nrebalancing_plot = (\n    ggplot(transaction_costs, \n           aes(x=\"beta\", y=\"concentration\",\n               color=\"factor(gamma)\", linetype=\"factor(gamma)\")) +\n    geom_line() +\n    guides(linetype=None) +\n    labs(x=\"Transaction cost parameter\", y=\"Distance from MVP\",\n         color=\"Risk aversion\",\n         title=(\"Portfolio weights for different risk aversion and \"\n                \"transaction cost\"))\n)\nrebalancing_plot.draw()\n\n\n\n\n\n\n\nFigure 1: The figure shows portfolio weights for different risk aversion and transaction cost. The horizontal axis indicates the distance from the empirical minimum variance portfolio weight, measured by the sum of the absolute deviations of the chosen portfolio from the benchmark.\n\n\n\n\n\nFigure 1 shows rebalancing from the initial portfolio (which we always set to the minimum variance portfolio weights in this example). The higher the transaction costs parameter \\(\\beta\\), the smaller is the rebalancing from the initial portfolio. In addition, if risk aversion \\(\\gamma\\) increases, the efficient portfolio is closer to the minimum variance portfolio weights such that the investor desires less rebalancing from the initial holdings.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#constrained-optimization",
    "href": "python/constrained-optimization-and-backtesting.html#constrained-optimization",
    "title": "Constrained Optimization and Backtesting",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\nNext, we introduce constraints to the above optimization procedure. Very often, typical constraints such as short-selling restrictions prevent analytical solutions for optimal portfolio weights (short-selling restrictions simply imply that negative weights are not allowed such that we require that \\(w_i \\geq 0\\,\\forall i\\)). However, numerical optimization allows computing the solutions to such constrained problems.\nWe rely on the powerful scipy.optimize package, which provides a common interface to a number of different optimization routines. In particular, we employ the Sequential Least-Squares Quadratic Programming (SLSQP) algorithm of Kraft (1994) because it is able to handle multiple equality and inequality constraints at the same time and is typically used for problems where the objective function and the constraints are twice continuously differentiable. We have to provide the algorithm with the objective function and its gradient, as well as the constraints and their Jacobian.\nWe illustrate the use of minimize() by replicating the analytical solutions for the minimum variance and efficient portfolio weights from above. Note that the equality constraint for both solutions is given by the requirement that the weights must sum up to one. In addition, we supply a vector of equal weights as an initial value for the algorithm in all applications. We verify that the output is equal to the above solution. Note that np.allclose() is a safe way to compare two vectors for pairwise equality. The alternative == is sensitive to small differences that may occur due to the representation of floating points on a computer, while np.allclose() has a built-in tolerance. It returns True if both are equal, which is the case in both applications below.\n\nw_initial = np.ones(n_industries)/n_industries\n\ndef objective_mvp(w):\n    return 0.5*w.T @ sigma @ w\n  \ndef gradient_mvp(w):\n    return sigma @ w\n\ndef equality_constraint(w):\n    return np.sum(w)-1\n\ndef jacobian_equality(w):\n    return np.ones_like(w)\n\nconstraints = (\n  {\"type\": \"eq\", \"fun\": equality_constraint, \"jac\": jacobian_equality}\n)\n\noptions = {\n  \"tol\":1e-20,\n  \"maxiter\": 10000,\n  \"method\":\"SLSQP\"\n}\n\nw_mvp_numerical = minimize(\n  x0=w_initial,\n  fun=objective_mvp,\n  jac=gradient_mvp,\n  constraints=constraints,\n  tol=options[\"tol\"],\n  options={\"maxiter\": options[\"maxiter\"]},\n  method=options[\"method\"]\n)\n\nnp.allclose(w_mvp, w_mvp_numerical.x, atol=1e-3)\n\ndef objective_efficient(w):\n    return 2*0.5*w.T @ sigma @ w-(1+mu) @ w\n\ndef gradient_efficient(w):\n    return 2*sigma @ w-(1+mu)\n\nw_efficient_numerical = minimize(\n  x0=w_initial,\n  fun=objective_efficient,\n  jac=gradient_efficient,\n  constraints=constraints,\n  tol=options[\"tol\"],\n  options={\"maxiter\": options[\"maxiter\"]},\n  method=options[\"method\"]\n)\n\nnp.allclose(w_efficient, w_efficient_numerical.x, atol = 1e-3)\n\nThe result above shows that the numerical procedure indeed recovered the optimal weights for a scenario where we already know the analytic solution.\nNext, we approach problems where no analytical solutions exist. First, we additionally impose short-sale constraints, which implies \\(N\\) inequality constraints of the form \\(\\omega_i &gt;=0\\). We can implement the short-sale constraints by imposing a vector of lower bounds lb = rep(0, n_industries).\n\nw_no_short_sale = minimize(\n  x0=w_initial,\n  fun=objective_efficient,\n  jac=gradient_efficient,\n  constraints=constraints,\n  bounds=((0, None), )*n_industries,\n  tol=options[\"tol\"],\n  options={\"maxiter\": options[\"maxiter\"]},\n  method=options[\"method\"]\n)\n\nweights_no_short_sale = pd.DataFrame({\n  \"Industry\": industry_returns.columns.tolist(),\n  \"No short-sale\": w_no_short_sale.x\n})\nweights_no_short_sale.round(3)\n\n\n\n\n\n\n\n\n\nIndustry\nNo short-sale\n\n\n\n\n0\nnodur\n0.610\n\n\n1\ndurbl\n0.000\n\n\n2\nmanuf\n0.000\n\n\n3\nenrgy\n0.211\n\n\n4\nhitec\n0.000\n\n\n5\ntelcm\n0.000\n\n\n6\nshops\n0.000\n\n\n7\nhlth\n0.179\n\n\n8\nutils\n0.000\n\n\n9\nother\n0.000\n\n\n\n\n\n\n\n\nAs expected, the resulting portfolio weights are all positive (up to numerical precision). Typically, the holdings in the presence of short-sale constraints are concentrated among way fewer assets than in the unrestricted case. You can verify that np.sum(w_no_short_sale.x) returns 1. In other words, minimize() provides the numerical solution to a portfolio choice problem for a mean-variance investor with risk aversion gamma = 2, where negative holdings are forbidden.\nminimize() can also handle more complex problems. As an example, we show how to compute optimal weights, subject to the so-called Regulation-T constraint, which requires that the sum of all absolute portfolio weights is smaller than 1.5, that is \\(\\sum_{i=1}^N |\\omega_i| \\leq 1.5\\). The constraint enforces that a maximum of 50 percent of the allocated wealth can be allocated to short positions, thus implying an initial margin requirement of 50 percent. Imposing such a margin requirement reduces portfolio risks because extreme portfolio weights are not attainable anymore. The implementation of Regulation-T rules is numerically interesting because the margin constraints imply a non-linear constraint on the portfolio weights. \n\nreg_t = 1.5\n\ndef inequality_constraint(w):\n    return reg_t-np.sum(np.abs(w))\n\ndef jacobian_inequality(w):\n    return -np.sign(w)\n\ndef objective_reg_t(w):\n    return -w @ (1+mu)+2*0.5*w.T @ sigma @ w\n\ndef gradient_reg_t(w):\n    return -(1+mu)+2*np.dot(sigma, w)\n\nconstraints = (\n  {\"type\": \"eq\", \"fun\": equality_constraint, \"jac\": jacobian_equality},\n  {\"type\": \"ineq\", \"fun\": inequality_constraint, \"jac\": jacobian_inequality}\n)\n\nw_reg_t = minimize(\n  x0=w_initial,\n  fun=objective_reg_t,\n  jac=gradient_reg_t,\n  constraints=constraints,\n  tol=options[\"tol\"],\n  options={\"maxiter\": options[\"maxiter\"]},\n  method=options[\"method\"]\n)\n\nweights_reg_t = pd.DataFrame({\n  \"Industry\": industry_returns.columns.tolist(),\n  \"Regulation-T\": w_reg_t.x\n})\nweights_reg_t.round(3)\n\n\n\n\n\n\n\n\n\nIndustry\nRegulation-T\n\n\n\n\n0\nnodur\n0.736\n\n\n1\ndurbl\n-0.000\n\n\n2\nmanuf\n-0.135\n\n\n3\nenrgy\n0.264\n\n\n4\nhitec\n0.000\n\n\n5\ntelcm\n-0.019\n\n\n6\nshops\n0.028\n\n\n7\nhlth\n0.223\n\n\n8\nutils\n-0.000\n\n\n9\nother\n-0.096\n\n\n\n\n\n\n\n\nFigure 2 shows the optimal allocation weights across all python len(industry_returns.columns) industries for the four different strategies considered so far: minimum variance, efficient portfolio with \\(\\gamma\\) = 2, efficient portfolio with short-sale constraints, and the Regulation-T constrained portfolio.\n\nweights = (weights_mvp\n  .merge(weights_efficient)\n  .merge(weights_no_short_sale)\n  .merge(weights_reg_t)\n  .melt(id_vars=\"Industry\", var_name=\"Strategy\", value_name=\"weights\")\n)\n\nweights_plot = (\n  ggplot(weights,\n         aes(x=\"Industry\", y=\"weights\", fill=\"Strategy\")) +\n  geom_bar(stat=\"identity\", position=\"dodge\", width=0.7) +\n  coord_flip() +\n  labs(y=\"Allocation weight\", fill=\"\",\n       title=\"Optimal allocations for different strategies\") +\n  scale_y_continuous(labels=percent_format())\n)\nweights_plot.draw()\n\n\n\n\n\n\n\nFigure 2: The figure shows optimal allocation weights for the ten industry portfolios and the four different allocation strategies.\n\n\n\n\n\nThe results clearly indicate the effect of imposing additional constraints: the extreme holdings the investor implements if they follow the (theoretically optimal) efficient portfolio vanish under, e.g., the Regulation-T constraint. You may wonder why an investor would deviate from what is theoretically the optimal portfolio by imposing potentially arbitrary constraints. The short answer is: the efficient portfolio is only efficient if the true parameters of the data-generating process correspond to the estimated parameters \\(\\hat\\Sigma\\) and \\(\\hat\\mu\\). Estimation uncertainty may thus lead to inefficient allocations. By imposing restrictions, we implicitly shrink the set of possible weights and prevent extreme allocations, which could result from error-maximization due to estimation uncertainty (Jagannathan and Ma 2003).\nBefore we move on, we want to propose a final allocation strategy, which reflects a somewhat more realistic structure of transaction costs instead of the quadratic specification used above. The function below computes efficient portfolio weights while adjusting for transaction costs of the form \\(\\beta\\sum_{i=1}^N |(\\omega_{i, t+1} - \\omega_{i, t^+})|\\). No closed-form solution exists, and we rely on non-linear optimization procedures.\n\ndef compute_efficient_weight_L1_TC(mu, sigma, gamma, beta, initial_weights):\n    \"\"\"Compute efficient portfolio weights with L1 constraint.\"\"\"       \n    \n    def objective(w):\n      return (gamma*0.5*w.T @ sigma @ w-(1+mu) @ w\n               +(beta/10000)/2*np.sum(np.abs(w-initial_weights)))\n\n    def gradient(w):\n      return (-mu+gamma*sigma @ w \n              +(beta/10000)*0.5*np.sign(w-initial_weights))\n      \n    constraints = (\n      {\"type\": \"eq\", \"fun\": equality_constraint, \"jac\": jacobian_equality}\n    )\n    \n    result = minimize(\n        x0=initial_weights,\n        fun=objective,\n        jac=gradient,\n        constraints=constraints,\n        tol=options[\"tol\"],\n        options={\"maxiter\": options[\"maxiter\"]},\n        method=options[\"method\"]\n    )\n    \n    return result.x",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "href": "python/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "title": "Constrained Optimization and Backtesting",
    "section": "Out-of-Sample Backtesting",
    "text": "Out-of-Sample Backtesting\nFor the sake of simplicity, we committed one fundamental error in computing portfolio weights above: we used the full sample of the data to determine the optimal allocation (Arnott, Harvey, and Markowitz 2019). To implement this strategy at the beginning of the 2000s, you will need to know how the returns will evolve until 2021. While interesting from a methodological point of view, we cannot evaluate the performance of the portfolios in a reasonable out-of-sample fashion. We do so next in a backtesting application for three strategies. For the backtest, we recompute optimal weights just based on past available data.\nThe few lines below define the general setup. We consider 120 periods from the past to update the parameter estimates before recomputing portfolio weights. Then, we update portfolio weights, which is costly and affects the performance. The portfolio weights determine the portfolio return. A period later, the current portfolio weights have changed and form the foundation for transaction costs incurred in the next period. We consider three different competing strategies: the mean-variance efficient portfolio, the mean-variance efficient portfolio with ex-ante adjustment for transaction costs, and the naive portfolio, which allocates wealth equally across the different assets.\n\nwindow_length = 120\nperiods = industry_returns.shape[0]-window_length\n\nbeta = 50\ngamma = 2\n\nperformance_values = np.empty((periods, 3))\nperformance_values[:] = np.nan\nperformance_values = {\n  \"MV (TC)\": performance_values.copy(), \n  \"Naive\": performance_values.copy(), \n  \"MV\": performance_values.copy()\n}\n\nn_industries = industry_returns.shape[1]\nw_prev_1 = w_prev_2 = w_prev_3 = np.ones(n_industries)/n_industries\n\nWe also define two helper functions: One to adjust the weights due to returns and one for performance evaluation, where we compute realized returns net of transaction costs.\n\ndef adjust_weights(w, next_return):\n    w_prev = 1+w*next_return\n    return np.array(w_prev/np.sum(np.array(w_prev)))\n\ndef evaluate_performance(w, w_previous, next_return, beta=50):\n    \"\"\"Calculate portfolio evaluation measures.\"\"\"  \n    \n    raw_return = np.dot(next_return, w)\n    turnover = np.sum(np.abs(w-w_previous))\n    net_return = raw_return-beta/10000*turnover\n    \n    return np.array([raw_return, turnover, net_return])\n\nThe following code chunk performs a rolling-window estimation, which we implement in a loop. In each period, the estimation window contains the returns available up to the current period. Note that we use the sample variance-covariance matrix and ignore the estimation of \\(\\hat\\mu\\) entirely, but you might use more advanced estimators in practice.\n\nfor p in range(periods):\n    returns_window = industry_returns.iloc[p:(p+window_length-1), :]\n    next_return = industry_returns.iloc[p+window_length, :]\n\n    sigma_window = np.array(returns_window.cov())\n    mu = 0*np.array(returns_window.mean())\n\n    # Transaction-cost adjusted portfolio\n    w_1 = compute_efficient_weight_L1_TC(\n      mu=mu, sigma=sigma_window, \n      beta=beta, \n      gamma=gamma, \n      initial_weights=w_prev_1\n    )\n\n    performance_values[\"MV (TC)\"][p, :] = evaluate_performance(\n      w_1, w_prev_1, next_return, beta=beta\n    )\n    w_prev_1 = adjust_weights(w_1, next_return)\n\n    # Naive portfolio\n    w_2 = np.ones(n_industries)/n_industries\n    performance_values[\"Naive\"][p, :] = evaluate_performance(\n      w_2, w_prev_2, next_return\n    )\n    w_prev_2 = adjust_weights(w_2, next_return)\n\n    # Mean-variance efficient portfolio (w/o transaction costs)\n    w_3 = compute_efficient_weight(sigma=sigma_window, mu=mu, gamma=gamma)\n    performance_values[\"MV\"][p, :] = evaluate_performance(\n      w_3, w_prev_3, next_return\n    )\n    w_prev_3 = adjust_weights(w_3, next_return)\n\nFinally, we get to the evaluation of the portfolio strategies net-of-transaction costs. Note that we compute annualized returns and standard deviations. \n\nperformance = pd.DataFrame()\nfor i in enumerate(performance_values.keys()):\n    tmp_data = pd.DataFrame(\n      performance_values[i[1]], \n      columns=[\"raw_return\", \"turnover\", \"net_return\"]\n    )\n    tmp_data[\"strategy\"] = i[1]\n    performance = pd.concat([performance, tmp_data], axis=0)\n\nlength_year = 12\n\nperformance_table = (performance\n  .groupby(\"strategy\")\n  .aggregate(\n    mean=(\"net_return\", lambda x: length_year*100*x.mean()),\n    sd=(\"net_return\", lambda x: np.sqrt(length_year)*100*x.std()),\n    sharpe_ratio=(\"net_return\", lambda x: (\n      (length_year*100*x.mean())/(np.sqrt(length_year)*100*x.std()) \n        if x.mean() &gt; 0 else np.nan)\n    ),\n    turnover=(\"turnover\", lambda x: 100*x.mean())\n  )\n  .reset_index()\n)\nperformance_table.round(3)\n\n\n\n\n\n\n\n\n\nstrategy\nmean\nsd\nsharpe_ratio\nturnover\n\n\n\n\n0\nMV\n-0.896\n12.571\nNaN\n211.095\n\n\n1\nMV (TC)\n11.853\n15.189\n0.780\n0.000\n\n\n2\nNaive\n11.832\n15.191\n0.779\n0.234\n\n\n\n\n\n\n\n\nThe results clearly speak against mean-variance optimization. Turnover is huge when the investor only considers their portfolio’s expected return and variance. Effectively, the mean-variance portfolio generates a negative annualized return after adjusting for transaction costs. At the same time, the naive portfolio turns out to perform very well. In fact, the performance gains of the transaction-cost adjusted mean-variance portfolio are small. The out-of-sample Sharpe ratio is slightly higher than for the naive portfolio. Note the extreme effect of turnover penalization on turnover: MV (TC) effectively resembles a buy-and-hold strategy which only updates the portfolio once the estimated parameters \\(\\hat\\mu_t\\) and \\(\\hat\\Sigma_t\\) indicate that the current allocation is too far away from the optimal theoretical portfolio.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/constrained-optimization-and-backtesting.html#exercises",
    "href": "python/constrained-optimization-and-backtesting.html#exercises",
    "title": "Constrained Optimization and Backtesting",
    "section": "Exercises",
    "text": "Exercises\n\nConsider the portfolio choice problem for transaction-cost adjusted certainty equivalent maximization with risk aversion parameter \\(\\gamma\\) \\[\\omega_{t+1} ^* = \\arg\\max_{\\omega \\in \\mathbb{R}^N, \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\tag{10}\\] where \\(\\Sigma\\) and \\(\\mu\\) are (estimators of) the variance-covariance matrix of the returns and the vector of expected returns. Assume for now that transaction costs are quadratic in rebalancing and proportional to stock illiquidity such that \\[\\nu_t\\left(\\omega, B\\right) = \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right) \\tag{11}\\] where \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) is a diagonal matrix, where \\(ill_1, \\ldots, ill_N\\). Derive a closed-form solution for the mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based on the transaction cost specification above. Discuss the effect of illiquidity \\(ill_i\\) on the individual portfolio weights relative to an investor that myopically ignores transaction costs in their decision.\nUse the solution from the previous exercise to update the function compute_efficient_weight() such that you can compute optimal weights conditional on a matrix \\(B\\) with illiquidity measures.\nIllustrate the evolution of the optimal weights from the naive portfolio to the efficient portfolio in the mean-standard deviation diagram.\nIs it always optimal to choose the same \\(\\beta\\) in the optimization problem than the value used in evaluating the portfolio performance? In other words, can it be optimal to choose theoretically sub-optimal portfolios based on transaction cost considerations that do not reflect the actual incurred costs? Evaluate the out-of-sample Sharpe ratio after transaction costs for a range of different values of imposed \\(\\beta\\) values.\n\n\n\n\nFigure 1: The figure shows portfolio weights for different risk aversion and transaction cost. The horizontal axis indicates the distance from the empirical minimum variance portfolio weight, measured by the sum of the absolute deviations of the chosen portfolio from the benchmark.\nFigure 2: The figure shows optimal allocation weights for the ten industry portfolios and the four different allocation strategies.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Constrained Optimization and Backtesting"
    ]
  },
  {
    "objectID": "python/clean-enhanced-trace-with-python.html",
    "href": "python/clean-enhanced-trace-with-python.html",
    "title": "Clean Enhanced TRACE with Python",
    "section": "",
    "text": "This appendix contains code to clean enhanced TRACE with Python. It is also available via the following GitHub gist. Hence, you could also source the file with the following chunk.\n\ngist_url = (\n  \"https://gist.githubusercontent.com/patrick-weiss/\"\n  \"86ddef6de978fbdfb22609a7840b5d8b/raw/\"\n  \"8fbcc6c6f40f537cd3cd37368be4487d73569c6b/\"\n)\n\nwith httpimport.remote_repo(gist_url):\n  from clean_enhanced_TRACE_python import clean_enhanced_trace\n\nWe need this function in TRACE and FISD to download and clean enhanced TRACE trade messages following Dick-Nielsen (2009) and Dick-Nielsen (2014) for enhanced TRACE specifically. This code is based on the resources provided by the project Open Source Bond Asset Pricing and their related publication Dickerson, Mueller, and Robotti (2023). We encourage that you acknowledge their effort. Relatedly, WRDS provides SAS code to clean enhanced TRACE data.\nThe function takes a vector of CUSIPs (in cusips), a connection to WRDS (connection) explained in Chapter 3, and a start and end date (start_date and end_date, respectively). Specifying too many CUSIPs will result in very slow downloads and a potential failure due to the size of the request to WRDS. The dates should be within the coverage of TRACE itself, i.e., starting after 2002, and the dates should be supplied as a string indicating MM/DD/YYYY. The output of the function contains all valid trade messages for the selected CUSIPs over the specified period.\n\ndef clean_enhanced_trace(cusips, \n                         connection, \n                         start_date=\"'01/01/2002'\", \n                         end_date=\"'12/31/2023'\"):\n  \"\"\"Clean enhanced TRACE data.\"\"\"\n  \n  import pandas as pd\n  import numpy as np\n  \n  # Load main file\n  trace_query = (\n    \"SELECT cusip_id, bond_sym_id, trd_exctn_dt, \"\n           \"trd_exctn_tm, days_to_sttl_ct, lckd_in_ind, \"\n           \"wis_fl, sale_cndtn_cd, msg_seq_nb, \"\n           \"trc_st, trd_rpt_dt, trd_rpt_tm, \"\n           \"entrd_vol_qt, rptd_pr, yld_pt, \" \n           \"asof_cd, orig_msg_seq_nb, rpt_side_cd, \"\n           \"cntra_mp_id, stlmnt_dt, spcl_trd_fl \" \n    \"FROM trace.trace_enhanced \" \n   f\"WHERE cusip_id IN {cusips} \" \n         f\"AND trd_exctn_dt BETWEEN {start_date} AND {end_date}‚\"\n  )\n\n  trace_all = pd.read_sql_query(\n    sql=trace_query,\n    con=connection,\n    parse_dates={\"trd_exctn_dt\",\"trd_rpt_dt\", \"stlmnt_dt\"}\n  )\n  \n  # Post 2012-06-02\n  ## Trades (trc_st = T) and correction (trc_st = R)\n  trace_post_TR = (trace_all\n    .query(\"trc_st in ['T', 'R']\")\n    .query(\"trd_rpt_dt &gt;= '2012-06-02'\")\n  )\n  \n  # Cancellations (trc_st = X) and correction cancellations (trc_st = C)\n  trace_post_XC = (trace_all\n    .query(\"trc_st in ['X', 'C']\")\n    .query(\"trd_rpt_dt &gt;= '2012-06-02'\")\n    .get([\"cusip_id\", \"msg_seq_nb\", \"entrd_vol_qt\",\n          \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\",\n          \"trd_exctn_dt\", \"trd_exctn_tm\"])\n    .assign(drop=True)\n  )\n  \n  ## Cleaning corrected and cancelled trades\n  trace_post_TR = (trace_post_TR\n    .merge(trace_post_XC, how=\"left\")\n    .query(\"drop != True\")\n    .drop(columns=\"drop\")\n  )\n  \n  # Reversals (trc_st = Y) \n  trace_post_Y = (trace_all\n    .query(\"trc_st == 'Y'\")\n    .query(\"trd_rpt_dt &gt;= '2012-06-02'\")\n    .get([\"cusip_id\", \"orig_msg_seq_nb\", \"entrd_vol_qt\",\n          \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\",\n          \"trd_exctn_dt\", \"trd_exctn_tm\"])\n    .assign(drop=True)\n    .rename(columns={\"orig_msg_seq_nb\": \"msg_seq_nb\"})\n  )\n  \n  # Clean reversals\n  ## Match the orig_msg_seq_nb of Y-message to msg_seq_nb of main message\n  trace_post = (trace_post_TR\n    .merge(trace_post_Y, how=\"left\")\n    .query(\"drop != True\")\n    .drop(columns=\"drop\")\n  )\n  \n  # Pre 06-02-12\n  ## Trades (trc_st = T)\n  trace_pre_T = (trace_all\n    .query(\"trd_rpt_dt &lt; '2012-06-02'\")\n  )\n    \n  # Cancellations (trc_st = C) \n  trace_pre_C = (trace_all\n    .query(\"trc_st == 'C'\")\n    .query(\"trd_rpt_dt &lt; '2012-06-02'\")\n    .get([\"cusip_id\", \"orig_msg_seq_nb\", \"entrd_vol_qt\",\n          \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\",\n          \"trd_exctn_dt\", \"trd_exctn_tm\"])\n    .assign(drop=True)\n    .rename(columns={\"orig_msg_seq_nb\": \"msg_seq_nb\"})\n  )\n  \n  # Remove cancellations from trades\n  ## Match orig_msg_seq_nb of C-message to msg_seq_nb of main message\n  trace_pre_T = (trace_pre_T\n    .merge(trace_pre_C, how=\"left\")\n    .query(\"drop != True\")\n    .drop(columns=\"drop\")\n  )\n  \n  # Corrections (trc_st = W)\n  trace_pre_W = (trace_all\n    .query(\"trc_st == 'W'\")\n    .query(\"trd_rpt_dt &lt; '2012-06-02'\")\n  )\n  \n  # Implement corrections in a loop\n  ## Correction control\n  correction_control = len(trace_pre_W)\n  correction_control_last = len(trace_pre_W)\n\n  ## Correction loop\n  while (correction_control &gt; 0):\n    # Create placeholder\n    ## Only identifying columns of trace_pre_T (for joins)\n    placeholder_trace_pre_T = (trace_pre_T\n      .get([\"cusip_id\", \"trd_exctn_dt\", \"msg_seq_nb\"])\n      .rename(columns={\"msg_seq_nb\": \"orig_msg_seq_nb\"})\n      .assign(matched_T=True)\n    )\n    \n    # Corrections that correct some msg\n    trace_pre_W_correcting = (trace_pre_W\n      .merge(placeholder_trace_pre_T, how=\"left\")\n      .query(\"matched_T == True\")\n      .drop(columns=\"matched_T\")\n    )\n\n    # Corrections that do not correct some msg\n    trace_pre_W = (trace_pre_W\n      .merge(placeholder_trace_pre_T, how=\"left\")\n      .query(\"matched_T != True\")\n      .drop(columns=\"matched_T\")\n    )\n    \n    # Create placeholder \n    ## Only identifying columns of trace_pre_W_correcting (for anti-joins)\n    placeholder_trace_pre_W_correcting = (trace_pre_W_correcting\n      .get([\"cusip_id\", \"trd_exctn_dt\", \"orig_msg_seq_nb\"])\n      .rename(columns={\"orig_msg_seq_nb\": \"msg_seq_nb\"})\n      .assign(corrected=True)\n    )\n    \n    # Delete msgs that are corrected \n    trace_pre_T = (trace_pre_T\n      .merge(placeholder_trace_pre_W_correcting, how=\"left\")\n      .query(\"corrected != True\")\n      .drop(columns=\"corrected\")\n    )\n    \n    # Add correction msgs\n    trace_pre_T = pd.concat([trace_pre_T, trace_pre_W_correcting])\n\n    # Escape if no corrections remain or they cannot be matched\n    correction_control = len(trace_pre_W)\n    \n    if correction_control == correction_control_last: \n      break\n    else:\n      correction_control_last = len(trace_pre_W)\n      continue\n  \n  # Reversals (asof_cd = R)\n  ## Record reversals\n  trace_pre_R = (trace_pre_T\n    .query(\"asof_cd == 'R'\")\n    .sort_values([\"cusip_id\", \"trd_exctn_dt\",\n                 \"trd_exctn_tm\", \"trd_rpt_dt\", \"trd_rpt_tm\"])\n  )\n  \n  ## Prepare final data\n  trace_pre = (trace_pre_T\n    .query(\n      \"asof_cd == None | asof_cd.isnull() | asof_cd not in ['R', 'X', 'D']\"\n    )\n    .sort_values([\"cusip_id\", \"trd_exctn_dt\",\n                 \"trd_exctn_tm\", \"trd_rpt_dt\", \"trd_rpt_tm\"])\n  )\n  \n  ## Add grouped row numbers\n  trace_pre_R[\"seq\"] = (trace_pre_R\n    .groupby([\"cusip_id\", \"trd_exctn_dt\", \"entrd_vol_qt\",\n              \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\"])\n    .cumcount()\n  )\n\n  trace_pre[\"seq\"] = (trace_pre\n    .groupby([\"cusip_id\", \"trd_exctn_dt\", \"entrd_vol_qt\",\n              \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\"])\n    .cumcount()\n  )\n  \n  ## Select columns for reversal cleaning\n  trace_pre_R = (trace_pre_R\n    .get([\"cusip_id\", \"trd_exctn_dt\", \"entrd_vol_qt\",\n         \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\", \"seq\"])\n    .assign(reversal=True)\n  )\n  \n  ## Remove reversals and the reversed trade\n  trace_pre = (trace_pre\n    .merge(trace_pre_R, how=\"left\")\n    .query(\"reversal != True\")\n    .drop(columns=[\"reversal\", \"seq\"])\n  )\n  \n  # Combine pre and post trades\n  trace_clean = pd.concat([trace_pre, trace_post])\n  \n  # Keep agency sells and unmatched agency buys\n  trace_agency_sells = (trace_clean \n    .query(\"cntra_mp_id == 'D' & rpt_side_cd == 'S'\")\n  )\n  \n  # Placeholder for trace_agency_sells with relevant columns\n  placeholder_trace_agency_sells = (trace_agency_sells\n    .get([\"cusip_id\", \"trd_exctn_dt\",\n          \"entrd_vol_qt\", \"rptd_pr\"])\n    .assign(matched=True)\n  )\n\n  # Agency buys that are unmatched\n  trace_agency_buys_filtered = (trace_clean  \n    .query(\"cntra_mp_id == 'D' & rpt_side_cd == 'B'\")\n    .merge(placeholder_trace_agency_sells, how=\"left\")\n    .query(\"matched != True\")\n    .drop(columns=\"matched\")\n  )\n  \n  # Non-agency\n  trace_nonagency = (trace_clean \n    .query(\"cntra_mp_id == 'C'\")\n  )\n  \n  # Agency cleaned\n  trace_clean = pd.concat([trace_nonagency, \n                           trace_agency_sells, \n                           trace_agency_buys_filtered])\n  \n  # Additional Filters\n  trace_add_filters = (trace_clean\n    .assign(\n      days_to_sttl_ct2 = lambda x: (\n        (x[\"stlmnt_dt\"]-x[\"trd_exctn_dt\"]).dt.days\n      )\n    )\n    .assign(\n      days_to_sttl_ct = lambda x: pd.to_numeric(\n        x[\"days_to_sttl_ct\"], errors='coerce'\n      )\n    )\n    .query(\"days_to_sttl_ct.isnull() | days_to_sttl_ct &lt;= 7\")\n    .query(\"days_to_sttl_ct2.isnull() | days_to_sttl_ct2 &lt;= 7\")\n    .query(\"wis_fl == 'N'\")\n    .query(\"spcl_trd_fl.isnull() | spcl_trd_fl == ''\")\n    .query(\"asof_cd.isnull() | asof_cd == ''\")\n  )\n  \n  # Only keep necessary columns\n  trace_final = (trace_add_filters\n    .sort_values([\"cusip_id\", \"trd_exctn_dt\", \"trd_exctn_tm\"])\n    .get([\"cusip_id\", \"trd_exctn_dt\", \"trd_exctn_tm\", \"rptd_pr\", \n          \"entrd_vol_qt\", \"yld_pt\", \"rpt_side_cd\", \"cntra_mp_id\"])\n  )\n  \n  return trace_final\n\n\n\n\n\nReferences\n\nDickerson, Alexander, Philippe Mueller, and Cesare Robotti. 2023. “Priced Risk in Corporate Bonds.” Journal of Financial Economics 150 (2): 103707. https://doi.org/10.1016/j.jfineco.2023.103707.\n\n\nDick-Nielsen, Jens. 2009. “Liquidity biases in TRACE.” The Journal of Fixed Income 19 (2): 43–55. https://doi.org/10.3905/jfi.2009.19.2.043.\n\n\n———. 2014. “How to clean enhanced TRACE data.” Working Paper. https://ssrn.com/abstract=2337908.",
    "crumbs": [
      "R",
      "Appendix",
      "Clean Enhanced TRACE with Python"
    ]
  },
  {
    "objectID": "python/beta-estimation.html",
    "href": "python/beta-estimation.html",
    "title": "Beta Estimation",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we introduce an important concept in financial economics: The exposure of an individual stock to changes in the market portfolio. According to the Capital Asset Pricing Model (CAPM) of Sharpe (1964), Lintner (1965), and Mossin (1966), cross-sectional variation in expected asset returns should be a function of the covariance between the excess return of the asset and the excess return on the market portfolio. The regression coefficient of excess market returns on excess stock returns is usually called the market beta. We show an estimation procedure for the market betas. We do not go into details about the foundations of market beta but simply refer to any treatment of the CAPM for further information. Instead, we provide details about all the functions that we use to compute the results. In particular, we leverage useful computational concepts: Rolling-window estimation and parallelization.\nWe use the following Python packages throughout this chapter:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n\nfrom regtabletotext import prettify_result\nfrom statsmodels.regression.rolling import RollingOLS\nfrom plotnine import *\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import percent_format, date_format\nfrom joblib import Parallel, delayed, cpu_count\nfrom itertools import product\nCompared to previous chapters, we introduce statsmodels (Seabold and Perktold 2010) for regression analysis and for sliding-window regressions and joblib (Team 2023) for parallelization.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#estimating-beta-using-monthly-returns",
    "href": "python/beta-estimation.html#estimating-beta-using-monthly-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta Using Monthly Returns",
    "text": "Estimating Beta Using Monthly Returns\nThe estimation procedure is based on a rolling-window estimation, where we may use either monthly or daily returns and different window lengths. First, let us start with loading the monthly CRSP data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=\"SELECT permno, month, industry, ret_excess FROM crsp_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n  .dropna()\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT month, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(factors_ff3_monthly, how=\"left\", on=\"month\")\n)\n\nTo estimate the CAPM regression coefficients \\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{i, t},\n\\tag{1}\\] we regress stock excess returns ret_excess on excess returns of the market portfolio mkt_excess.\nPython provides a simple solution to estimate (linear) models with the function smf.ols(). The function requires a formula as input that is specified in a compact symbolic form. An expression of the form y ~ model is interpreted as a specification that the response y is modeled by a linear predictor specified symbolically by model. Such a model consists of a series of terms separated by + operators. In addition to standard linear models, smf.ols() provides a lot of flexibility. You should check out the documentation for more information. To start, we restrict the data only to the time series of observations in CRSP that correspond to Apple’s stock (i.e., to permno 14593 for Apple) and compute \\(\\hat\\alpha_i\\) as well as \\(\\hat\\beta_i\\).\n\nmodel_beta = (smf.ols(\n    formula=\"ret_excess ~ mkt_excess\",\n    data=crsp_monthly.query(\"permno == 14593\"))\n  .fit()\n)\nprettify_result(model_beta)\n\nOLS Model:\nret_excess ~ mkt_excess\n\nCoefficients:\n            Estimate  Std. Error  t-Statistic  p-Value\nIntercept      0.010       0.005        2.004    0.046\nmkt_excess     1.389       0.111       12.467    0.000\n\nSummary statistics:\n- Number of observations: 504\n- R-squared: 0.236, Adjusted R-squared: 0.235\n- F-statistic: 155.435 on 1 and 502 DF, p-value: 0.000\n\n\n\nsmf.ols() returns an object of class RegressionModel, which contains all the information we usually care about with linear models. prettify_result() returns an overview of the estimated parameters. The output above indicates that Apple moves excessively with the market as the estimated \\(\\hat\\beta_i\\) is above one (\\(\\hat\\beta_i \\approx 1.4\\)).",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#rolling-window-estimation",
    "href": "python/beta-estimation.html#rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Rolling-Window Estimation",
    "text": "Rolling-Window Estimation\nAfter we estimated the regression coefficients on an example, we scale the estimation of \\(\\beta_i\\) to a whole different level and perform rolling-window estimations for the entire CRSP sample.\nWe take a total of five years of data (window_size) and require at least 48 months with return data to compute our betas (min_obs). Check out the Exercises if you want to compute beta for different time periods. We first identify firm identifiers (permno) for which CRSP contains sufficiently many records.\n\nwindow_size = 60\nmin_obs = 48\n\nvalid_permnos = (crsp_monthly\n  .dropna()\n  .groupby(\"permno\")[\"permno\"]\n  .count()\n  .reset_index(name=\"counts\")\n  .query(f\"counts &gt; {window_size}+1\")\n)\n\nBefore we proceed with the estimation, one important issue is worth emphasizing: RollingOLS returns the estimated parameters of a linear regression by incorporating a window of the last window_size rows. Whenever monthly returns are implicitly missing (which means there is simply no entry recorded, e.g., because a company was delisted and only traded publicly again later), such a fixed window size may cause outdated observations to influence the estimation results. We thus recommend making such implicit missing rows explicit.\nWe hence collect information about the first and last listing date of each permno.\n\npermno_information = (crsp_monthly\n  .merge(valid_permnos, how=\"inner\", on=\"permno\")\n  .groupby([\"permno\"])\n  .aggregate(first_month=(\"month\", \"min\"),\n             last_month=(\"month\", \"max\"))\n  .reset_index()\n)\n\nTo complete the missing observations in the CRSP sample, we obtain all possible permno-month combinations.\n\nunique_permno = crsp_monthly[\"permno\"].unique()\nunique_month = factors_ff3_monthly[\"month\"].unique()\n\nall_combinations = pd.DataFrame(\n  product(unique_permno, unique_month), \n  columns=[\"permno\", \"month\"]\n)\n\nFinally, we expand the CRSP sample and include a row (with missing excess returns) for each possible permno-month observation that falls within the start and end date where the respective permno has been publicly listed.\n\nreturns_monthly = (all_combinations\n  .merge(crsp_monthly.get([\"permno\", \"month\", \"ret_excess\"]), \n         how=\"left\", on=[\"permno\", \"month\"])\n  .merge(permno_information, how=\"left\", on=\"permno\")\n  .query(\"(month &gt;= first_month) & (month &lt;= last_month)\")\n  .drop(columns=[\"first_month\", \"last_month\"])\n  .merge(crsp_monthly.get([\"permno\", \"month\", \"industry\"]),\n         how=\"left\", on=[\"permno\", \"month\"])\n  .merge(factors_ff3_monthly, how=\"left\", on=\"month\")\n)\n\nThe following function implements the CAPM regression for a dataframe (or a part thereof) containing at least min_obs observations to avoid huge fluctuations if the time series is too short. If the condition is violated (i.e., the time series is too short) the function returns a missing value.\n\ndef roll_capm_estimation(data, window_size, min_obs):\n    \"\"\"Calculate rolling CAPM estimation.\"\"\"\n    \n    data = data.sort_values(\"month\")\n\n    result = (RollingOLS.from_formula(\n      formula=\"ret_excess ~ mkt_excess\",\n      data=data,\n      window=window_size,\n      min_nobs=min_obs,\n      missing=\"drop\")\n      .fit()\n      .params.get(\"mkt_excess\")\n    )\n    \n    result.index = data.index\n    \n    return result\n\nBefore we approach the whole CRSP sample, let us focus on a couple of examples for well-known firms.\n\nexamples = pd.DataFrame({\n  \"permno\": [14593, 10107, 93436, 17778],\n  \"company\": [\"Apple\", \"Microsoft\", \"Tesla\", \"Berkshire Hathaway\"]\n})\n\nIt is actually quite simple to perform the rolling-window estimation for an arbitrary number of stocks, which we visualize in the following code chunk and the resulting Figure 1.\n\nbeta_example = (returns_monthly\n  .merge(examples, how=\"inner\", on=\"permno\")\n  .groupby([\"permno\"])\n  .apply(lambda x: x.assign(\n    beta=roll_capm_estimation(x, window_size, min_obs))\n  )\n  .reset_index(drop=True)\n  .dropna()\n)\n\n\nplot_beta = (\n  ggplot(beta_example, \n         aes(x=\"month\", y=\"beta\", color=\"company\", linetype=\"company\")) + \n  geom_line() + \n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"Monthly beta estimates for example stocks\") +\n  scale_x_datetime(breaks=date_breaks(\"5 year\"), labels=date_format(\"%Y\")) \n)\nplot_beta.draw()\n\n\n\n\n\n\n\nFigure 1: The figure shows monthly beta estimates for example stocks using five years of data. The CAPM betas are estimated with monthly data and a rolling window of length five years based on adjusted excess returns from CRSP. We use market excess returns from Kenneth French data library.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#parallelized-rolling-window-estimation",
    "href": "python/beta-estimation.html#parallelized-rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Parallelized Rolling-Window Estimation",
    "text": "Parallelized Rolling-Window Estimation\nNext, we perform the rolling window estimation for the entire cross-section of stocks in the CRSP sample. For that purpose, we can apply the code snippet from the example above to compute rolling window regression coefficients for all stocks. This is how to do it with the joblib package to use multiple cores. Note that we use cpu_count() to determine the number of cores available for parallelization but keep one core free for other tasks. Some machines might freeze if all cores are busy with Python jobs.\n\ndef roll_capm_estimation_for_joblib(permno, group):\n    \"\"\"Calculate rolling CAPM estimation using joblib.\"\"\"\n    \n    if \"date\" in group.columns:\n      group = group.sort_values(by=\"date\")\n    else:\n      group = group.sort_values(by=\"month\")\n\n    beta_values = (RollingOLS.from_formula(\n        formula=\"ret_excess ~ mkt_excess\",\n        data=group,\n        window=window_size,\n        min_nobs=min_obs,\n        missing=\"drop\"\n      )\n      .fit()\n      .params.get(\"mkt_excess\")\n    )\n    \n    result = pd.DataFrame(beta_values)\n    result.columns = [\"beta\"]\n    result[\"month\"] = group[\"month\"].values\n    result[\"permno\"] = permno\n    \n    try:\n      result[\"date\"] = group[\"date\"].values\n      result = result[\n        (result.groupby(\"month\")[\"date\"].transform(\"max\")) == result[\"date\"]\n      ]\n    except(KeyError):\n      pass\n    \n    return result\n\npermno_groups = (returns_monthly\n  .merge(valid_permnos, how=\"inner\", on=\"permno\")\n  .groupby(\"permno\", group_keys=False)\n)\n\nn_cores = cpu_count()-1\n\nbeta_monthly = (\n  pd.concat(\n    Parallel(n_jobs=n_cores)\n    (delayed(roll_capm_estimation_for_joblib)(name, group)\n    for name, group in permno_groups)\n  )\n  .dropna()\n  .rename(columns={\"beta\": \"beta_monthly\"})\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#estimating-beta-using-daily-returns",
    "href": "python/beta-estimation.html#estimating-beta-using-daily-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta Using Daily Returns",
    "text": "Estimating Beta Using Daily Returns\nBefore we provide some descriptive statistics of our beta estimates, we implement the estimation for the daily CRSP sample as well. Depending on the application, you might either use longer horizon beta estimates based on monthly data or shorter horizon estimates based on daily returns. As loading the full daily CRSP data requires relatively large amounts of memory, we split the beta estimation into smaller chunks.\nFirst, we load the daily Fama-French market excess returns and extract the vector of dates.\n\nfactors_ff3_daily = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_daily\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nunique_date = factors_ff3_daily[\"date\"].unique()\n\nFor the daily data, we consider around three months of data (i.e., 60 trading days), require at least 50 observations, and estimate betas in batches of 500.\n\nwindow_size = 60\nmin_obs = 50\n\npermnos = list(crsp_monthly[\"permno\"].unique().astype(str))\n\nbatch_size = 500\nbatches = np.ceil(len(permnos)/batch_size).astype(int)\n\nWe then proceed to perform the same steps as with the monthly CRSP data, just in batches: Load in daily returns, transform implicit missing returns to explicit ones, keep only valid stocks with a minimum number of rows, and parallelize the beta estimation across stocks.\n\nbeta_daily = []\n\nfor j in range(1, batches+1):  \n    \n    permno_batch = permnos[\n      ((j-1)*batch_size):(min(j*batch_size, len(permnos)))\n    ]\n    \n    permno_batch_formatted = (\n      \", \".join(f\"'{permno}'\" for permno in permno_batch)\n    )\n    permno_string = f\"({permno_batch_formatted})\"\n    \n    crsp_daily_sub_query = (\n      \"SELECT permno, month, date, ret_excess \"\n        \"FROM crsp_daily \"\n       f\"WHERE permno IN {permno_string}\" \n    )\n      \n    crsp_daily_sub = pd.read_sql_query(\n      sql=crsp_daily_sub_query,\n      con=tidy_finance,\n      dtype={\"permno\": int},\n      parse_dates={\"date\", \"month\"}\n    )\n    \n    valid_permnos = (crsp_daily_sub\n      .groupby(\"permno\")[\"permno\"]\n      .count()\n      .reset_index(name=\"counts\")\n      .query(f\"counts &gt; {window_size}+1\")\n      .drop(columns=\"counts\")\n    )\n    \n    permno_information = (crsp_daily_sub\n      .merge(valid_permnos, how=\"inner\", on=\"permno\")\n      .groupby([\"permno\"])\n      .aggregate(first_date=(\"date\", \"min\"),\n                 last_date=(\"date\", \"max\"),)\n      .reset_index()\n    )\n    \n    unique_permno = permno_information[\"permno\"].unique()\n    \n    all_combinations = pd.DataFrame(\n      product(unique_permno, unique_date), \n      columns=[\"permno\", \"date\"]\n    )\n    \n    returns_daily = (crsp_daily_sub\n      .merge(all_combinations, how=\"right\", on=[\"permno\", \"date\"])\n      .merge(permno_information, how=\"left\", on=\"permno\")\n      .query(\"(date &gt;= first_date) & (date &lt;= last_date)\")\n      .drop(columns=[\"first_date\", \"last_date\"])\n      .merge(factors_ff3_daily, how=\"left\", on=\"date\")\n    )\n    \n    permno_groups = (returns_daily\n      .groupby(\"permno\", group_keys=False)\n    )\n    \n    beta_daily_sub = (\n      pd.concat(\n        Parallel(n_jobs=n_cores)\n        (delayed(roll_capm_estimation_for_joblib)(name, group)\n        for name, group in permno_groups)\n      )\n      .dropna()\n      .rename(columns={\"beta\": \"beta_daily\"})\n    )\n    \n    beta_daily.append(beta_daily_sub)\n              \n    print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")\n  \nbeta_daily = pd.concat(beta_daily)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#comparing-beta-estimates",
    "href": "python/beta-estimation.html#comparing-beta-estimates",
    "title": "Beta Estimation",
    "section": "Comparing Beta Estimates",
    "text": "Comparing Beta Estimates\nWhat is a typical value for stock betas? To get some feeling, we illustrate the dispersion of the estimated \\(\\hat\\beta_i\\) across different industries and across time below. Figure 2 shows that typical business models across industries imply different exposure to the general market economy. However, there are barely any firms that exhibit a negative exposure to the market factor.\n\nbeta_industries = (beta_monthly\n  .merge(crsp_monthly, how=\"inner\", on=[\"permno\", \"month\"])\n  .dropna(subset=\"beta_monthly\")\n  .groupby([\"industry\",\"permno\"])[\"beta_monthly\"]\n  .aggregate(\"mean\")\n  .reset_index()\n)\n\nindustry_order = (beta_industries\n  .groupby(\"industry\")[\"beta_monthly\"]\n  .aggregate(\"median\")\n  .sort_values()\n  .index.tolist()\n)\n\nplot_beta_industries = (\n  ggplot(beta_industries, \n         aes(x=\"industry\", y=\"beta_monthly\")) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(x=\"\", y=\"\", \n       title=\"Firm-specific beta distributions by industry\") +\n  scale_x_discrete(limits=industry_order)\n)\nplot_beta_industries.draw()\n\n\n\n\n\n\n\nFigure 2: The box plots show the average firm-specific beta estimates by industry.\n\n\n\n\n\nNext, we illustrate the time-variation in the cross-section of estimated betas. Figure 3 shows the monthly deciles of estimated betas (based on monthly data) and indicates an interesting pattern: First, betas seem to vary over time in the sense that during some periods, there is a clear trend across all deciles. Second, the sample exhibits periods where the dispersion across stocks increases in the sense that the lower decile decreases and the upper decile increases, which indicates that for some stocks, the correlation with the market increases, while for others it decreases. Note also here: stocks with negative betas are a rare exception.\n\nbeta_quantiles = (beta_monthly\n  .groupby(\"month\")[\"beta_monthly\"]\n  .quantile(q=np.arange(0.1, 1.0, 0.1))\n  .reset_index()\n  .rename(columns={\"level_1\": \"quantile\"})\n  .assign(quantile=lambda x: (x[\"quantile\"]*100).astype(int))\n  .dropna()\n)\n\nlinetypes = [\"-\", \"--\", \"-.\", \":\"]\nn_quantiles = beta_quantiles[\"quantile\"].nunique()\n\nplot_beta_quantiles = (\n  ggplot(beta_quantiles, \n         aes(x=\"month\", y=\"beta_monthly\", \n         color=\"factor(quantile)\", linetype=\"factor(quantile)\")) +\n  geom_line() +\n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"Monthly deciles of estimated betas\") +\n  scale_x_datetime(breaks=date_breaks(\"5 year\"), labels=date_format(\"%Y\")) +\n  scale_linetype_manual(\n    values=[linetypes[l % len(linetypes)] for l in range(n_quantiles)]\n  ) \n)\nplot_beta_quantiles.draw()\n\n\n\n\n\n\n\nFigure 3: The figure shows monthly deciles of estimated betas. Each line corresponds to the monthly cross-sectional quantile of the estimated CAPM beta.\n\n\n\n\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table. Then, we use the table to plot a comparison of beta estimates for our example stocks in Figure 4.\n\nbeta = (beta_monthly\n  .get([\"permno\", \"month\", \"beta_monthly\"])\n  .merge(beta_daily.get([\"permno\", \"month\", \"beta_daily\"]),\n         how=\"outer\", on=[\"permno\", \"month\"])\n)\n\nbeta_comparison = (beta\n  .merge(examples, on=\"permno\")\n  .melt(id_vars=[\"permno\", \"month\", \"company\"], var_name=\"name\",\n        value_vars=[\"beta_monthly\", \"beta_daily\"], value_name=\"value\")\n  .dropna()\n)\n\nplot_beta_comparison = (\n  ggplot(beta_comparison,\n         aes(x=\"month\", y=\"value\", color=\"name\")) +\n  geom_line() +\n  facet_wrap(\"~company\", ncol=1) +\n  labs(x=\"\", y=\"\", color=\"\",\n       title=\"Comparison of beta estimates using monthly and daily data\") + \n  scale_x_datetime(breaks=date_breaks(\"10 years\"), \n                   labels=date_format(\"%Y\")) +\n  theme(figure_size=(6.4, 6.4))\n)\nplot_beta_comparison.draw()\n\n\n\n\n\n\n\nFigure 4: The figure shows the comparison of beta estimates using monthly and daily data. CAPM betas are computed using five years of monthly or three months of daily data. The two lines show the monthly estimates based on a rolling window for few exemplary stocks.\n\n\n\n\n\nThe estimates in Figure 4 look as expected. As you can see, the beta estimates really depend on the estimation window and data frequency. Nevertheless, one can observe a clear connection between daily and monthly betas in this example, in magnitude and the dynamics over time.\nFinally, we write the estimates to our database so that we can use them in later chapters.\n\n(beta.to_sql(\n  name=\"beta\", \n  con=tidy_finance, \n  if_exists=\"replace\",\n  index=False\n  )\n)\n\nWhenever you perform some kind of estimation, it also makes sense to do rough plausibility tests. A possible check is to plot the share of stocks with beta estimates over time. This descriptive analysis helps us discover potential errors in our data preparation or the estimation procedure. For instance, suppose there was a gap in our output without any betas. In this case, we would have to go back and check all previous steps to find out what went wrong. Figure 5 does not indicate any troubles, so let us move on to the next check.\n\nbeta_long = (crsp_monthly\n  .merge(beta, how=\"left\", on=[\"permno\", \"month\"])\n  .melt(id_vars=[\"permno\", \"month\"], var_name=\"name\",\n        value_vars=[\"beta_monthly\", \"beta_daily\"], value_name=\"value\")\n)\n\nbeta_shares = (beta_long\n  .groupby([\"month\", \"name\"])\n  .aggregate(share=(\"value\", lambda x: sum(~x.isna())/len(x)))\n  .reset_index()\n)\n\nplot_beta_long = (\n  ggplot(beta_shares, \n         aes(x=\"month\", y=\"share\", color=\"name\", linetype=\"name\")) +\n  geom_line() +\n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"End-of-month share of securities with beta estimates\") + \n  scale_y_continuous(labels=percent_format()) +\n  scale_x_datetime(breaks=date_breaks(\"10 year\"), labels=date_format(\"%Y\")) \n)\nplot_beta_long.draw()\n\n\n\n\n\n\n\nFigure 5: The figure shows end-of-month share of securities with beta estimates. The two lines show the share of securities with beta estimates using five years of monthly or three months of daily data.\n\n\n\n\n\nWe also encourage everyone to always look at the distributional summary statistics of variables. You can easily spot outliers or weird distributions when looking at such tables.\n\nbeta_long.groupby(\"name\")[\"value\"].describe().round(2)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nname\n\n\n\n\n\n\n\n\n\n\n\n\nbeta_daily\n3279386.0\n0.75\n0.94\n-44.85\n0.20\n0.69\n1.23\n61.64\n\n\nbeta_monthly\n2073073.0\n1.10\n0.70\n-8.96\n0.64\n1.03\n1.47\n10.35\n\n\n\n\n\n\n\n\nThe summary statistics also look plausible for the two estimation procedures.\nFinally, since we have two different estimators for the same theoretical object, we expect the estimators to be at least positively correlated (although not perfectly as the estimators are based on different sample periods and frequencies).\n\nbeta.get([\"beta_monthly\", \"beta_daily\"]).corr().round(2)\n\n\n\n\n\n\n\n\n\nbeta_monthly\nbeta_daily\n\n\n\n\nbeta_monthly\n1.00\n0.32\n\n\nbeta_daily\n0.32\n1.00\n\n\n\n\n\n\n\n\nIndeed, we find a positive correlation between our beta estimates. In the subsequent chapters, we mainly use the estimates based on monthly data, as most readers should be able to replicate them and should not encounter potential memory limitations that might arise with the daily data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "python/beta-estimation.html#exercises",
    "href": "python/beta-estimation.html#exercises",
    "title": "Beta Estimation",
    "section": "Exercises",
    "text": "Exercises\n\nCompute beta estimates based on monthly data using one, three, and five years of data and impose a minimum number of observations of 10, 28, and 48 months with return data, respectively. How strongly correlated are the estimated betas?\nCompute beta estimates based on monthly data using five years of data and impose different numbers of minimum observations. How does the share of permno-month observations with successful beta estimates vary across the different requirements? Do you find a high correlation across the estimated betas?\nInstead of using joblib, perform the beta estimation in a loop (using either monthly or daily data) for a subset of 100 permnos of your choice. Verify that you get the same results as with the parallelized code from above.\nFilter out the stocks with negative betas. Do these stocks frequently exhibit negative betas, or do they resemble estimation errors?\nCompute beta estimates for multi-factor models such as the Fama-French three-factor model. For that purpose, you extend your regression to \\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\sum\\limits_{j=1}^k\\beta_{i,k}(r_{j, t}-r_{f,t})+\\varepsilon_{i, t}\n\\tag{2}\\] where \\(r_{i, t}\\) are the \\(k\\) factor returns. Thus, you estimate four parameters (\\(\\alpha_i\\) and the slope coefficients). Provide some summary statistics of the cross-section of firms and their exposure to the different factors.\n\n\n\n\nFigure 1: The figure shows monthly beta estimates for example stocks using five years of data. The CAPM betas are estimated with monthly data and a rolling window of length five years based on adjusted excess returns from CRSP. We use market excess returns from Kenneth French data library.\nFigure 2: The box plots show the average firm-specific beta estimates by industry.\nFigure 3: The figure shows monthly deciles of estimated betas. Each line corresponds to the monthly cross-sectional quantile of the estimated CAPM beta.\nFigure 4: The figure shows the comparison of beta estimates using monthly and daily data. CAPM betas are computed using five years of monthly or three months of daily data. The two lines show the monthly estimates based on a rolling window for few exemplary stocks.\nFigure 5: The figure shows end-of-month share of securities with beta estimates. The two lines show the share of securities with beta estimates using five years of monthly or three months of daily data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Tidy Finance",
    "section": "",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-NC-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution, NonCommercial, and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-NC-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-noncommercial-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-noncommercial-sharealike-4.0-international-public-license",
    "title": "Tidy Finance",
    "section": "",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-NC-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution, NonCommercial, and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-NC-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "disclaimer.html",
    "href": "disclaimer.html",
    "title": "Tidy Finance",
    "section": "",
    "text": "The providers of this website (www.tidy-finance.org) and its sites are Christoph Frey, Christoph Scheuch, Stefan Voigt, and Patrick Weiss. Direct inquiries related to its contents to contact@tidy-finance.org.\nThe use of the website is free of charge. Various open-source licenses govern parts of the distributed content (such as packages or programming languages) created by third parties, and any further use has to agree with these rules. Any links to other websites or documents are not under the influence of this website’s providers, and no responsibility can be assumed for them. We do not claim any ownership rights in any content created by third parties.\nThe content on this website is for illustrative and educational use only. While the content is well-researched, no guarantee can be given for the accuracy, completeness, or timeliness of the information. All liability for the content on this website is excluded. No statements shall be interpreted as an offer to purchase/contract or advisory service. In particular, this website does not offer any investment advice. No contractual relationship can be created from any statements on this website.\nFor data privacy concerns or inquiries, please contact us via contact@tidy-finance.org. The website does use cookies if you agree to them, which are managed via the preferences at the bottom of the website.\nThese policies may be updated at any time."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tidy Finance Blog",
    "section": "",
    "text": "Experimental and external contributions based on Tidy Finance with R. Contribute your ideas!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCIR Model Calibration using Python\n\n\n10 min\n\n\n\nInterest rates\n\n\nPython\n\n\n\nRoutine to calibrate the Cox-Ingersoll-Ross model\n\n\n\nYuri Antonelli\n\n\nApr 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCRSP 2.0 Update\n\n\n6 min\n\n\n\nData\n\n\nR\n\n\nPython\n\n\n\nThe highlights of the recent switch to CRSP 2.0 data\n\n\n\nPatrick Weiss, Christoph Scheuch, Stefan Voigt, Christoph Frey\n\n\nMar 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyfinance 0.1.0\n\n\n5 min\n\n\n\nData\n\n\nR\n\n\n\ntidyfinance 0.1.0 is now on CRAN. Discover what this release includes.\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nMar 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Market Microstructure\n\n\n76 min\n\n\n\nMarket microstructure\n\n\nR\n\n\ndata.table\n\n\n\nA beginner’s guide to market quality measurement in high-frequency data using R.\n\n\n\nBjörn Hagströmer, Niklas Landsberg\n\n\nJan 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing DuckDB with WRDS Data\n\n\n10 min\n\n\n\nData\n\n\nR\n\n\n\nDemonstrate the power of DuckDB and dbplyr with WRDS data.\n\n\n\nIan Gow\n\n\nDec 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Fama-French Three vs Five Factors\n\n\n7 min\n\n\n\nData\n\n\nReplications\n\n\nR\n\n\n\nAn explanation for the difference in the size factors of Fama and French 3 and 5 factor data\n\n\n\nChristoph Scheuch\n\n\nOct 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDummy Data for Tidy Finance Readers without Access to WRDS\n\n\n12 min\n\n\n\nData\n\n\nR\n\n\n\nR code to generate dummy data that can be used to run the code chunks in Tidy Finance with R\n\n\n\nChristoph Scheuch\n\n\nSep 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvert Raw TRACE Data to a Local SQLite Database\n\n\n37 min\n\n\n\nData\n\n\nR\n\n\n\nAn R code that converts TRACE files from FINRA into a SQLite for facilitated analysis and filtering\n\n\n\nKevin Riehl, Lukas Müller\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Collaborative Filtering: Building A Stock Recommender\n\n\n13 min\n\n\n\nRecommender System\n\n\nR\n\n\n\nA simple implementation for prototyping multiple collaborative filtering algorithms\n\n\n\nChristoph Scheuch\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Standard Errors in Portfolio Sorts\n\n\n39 min\n\n\n\nReplications\n\n\nR\n\n\n\nAn all-in-one implementation of non-standard errors in portfolio sorts\n\n\n\nPatrick Weiss\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstruction of a Historical S&P 500 Total Return Index\n\n\n8 min\n\n\n\nData\n\n\nR\n\n\n\nAn approximation of total returns using Robert Shiller’s stock market data\n\n\n\nChristoph Scheuch\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Tidy Finance?\n\n\n5 min\n\n\n\nOp-Ed\n\n\n\nAn op-ed about the motives behind Tidy Finance with R\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at Workshops for Ukraine\n\n\n2 min\n\n\n\nWorkshops\n\n\n\nYou can learn Tidy Finance and support Ukraine at the same time\n\n\n\nPatrick Weiss\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at the useR!2022 Conference\n\n\n1 min\n\n\n\nConferences\n\n\n\nTidy Finance presentation at the gathering supported by the R Foundation\n\n\n\nPatrick Weiss\n\n\nJun 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html",
    "href": "blog/using-duckdb-with-wrds/index.html",
    "title": "Using DuckDB with WRDS Data",
    "section": "",
    "text": "In this short note, I show how one can use DuckDB with WRDS data stored in the PostgreSQL database provided by WRDS. I then use some simple benchmarks to show how DuckDB offers a powerful, fast analytical engine for researchers in accounting and finance.\nTo make the analysis concrete, I focus on data used in the excellent recent book “Tidy Finance with R”. Essentially, I combine data from CRSP’s daily stock return file (crsp.dsf) with data on factor returns from Ken French’s website and then run an aggregate query."
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#fama-french-factor-returns",
    "href": "blog/using-duckdb-with-wrds/index.html#fama-french-factor-returns",
    "title": "Using DuckDB with WRDS Data",
    "section": "Fama-French factor returns",
    "text": "Fama-French factor returns\nWe use the same start_date and end_date values used in “Tidy Finance with R” and the code below also is adapted from that book. However, we use the copy_to() function from dplyr to save the table to our database.\n\nstart_date &lt;- ymd(\"1960-01-01\")\nend_date &lt;- ymd(\"2021-12-31\")\n\nfactors_ff_daily_raw &lt;- \n  download_french_data(\"Fama/French 3 Factors [Daily]\")\n\nNew names:\n• `` -&gt; `...1`\n\nfactors_ff_daily &lt;- \n  factors_ff_daily_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = ymd(date),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date) |&gt;\n  copy_to(tidy_finance,\n          df = _,\n          name = \"factors_ff_daily\",\n          temporary = FALSE,\n          overwrite = TRUE)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#getting-daily-returns-from-wrds",
    "href": "blog/using-duckdb-with-wrds/index.html#getting-daily-returns-from-wrds",
    "title": "Using DuckDB with WRDS Data",
    "section": "Getting daily returns from WRDS",
    "text": "Getting daily returns from WRDS\nNext, I specify the connection details as follows. I recommend using environment variables (e.g., set using Sys.setenv()), as this facilitates sharing code with others. You should not include this chunk of code in your code, rather run it before executing your other code. In addition to setting these environment variables, you may want to set PGPASSWORD too. (Hopefully it is obvious that your should use your WRDS ID and password, not mine.)\n\nSys.setenv(PGHOST = \"wrds-pgdata.wharton.upenn.edu\",\n           PGPORT = 9737L,\n           PGDATABASE = \"wrds\",\n           PGUSER = Sys.getenv(\"WRDS_USER\"),\n           PGPASSWORD = Sys.getenv(\"WRDS_PASSWORD\"))\n\nThird, we connect to the CRSP daily stock file in the WRDS PostgreSQL database.\n\npg &lt;- dbConnect(RPostgres::Postgres())\ndsf_db &lt;- tbl(pg, Id(schema = \"crsp\", table = \"dsf\"))\n\nAs we can see, we have access to data in crsp.dsf.\n\ndsf_db\n\n# Source:   table&lt;\"crsp\".\"dsf\"&gt; [?? x 20]\n# Database: postgres  [pweiss@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n  cusip    permno permco issuno hexcd hsiccd date       bidlo askhi\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 68391610  10000   7952  10396     3   3990 1986-01-07  2.38  2.75\n2 68391610  10000   7952  10396     3   3990 1986-01-08  2.38  2.62\n3 68391610  10000   7952  10396     3   3990 1986-01-09  2.38  2.62\n4 68391610  10000   7952  10396     3   3990 1986-01-10  2.38  2.62\n5 68391610  10000   7952  10396     3   3990 1986-01-13  2.5   2.75\n# ℹ more rows\n# ℹ 11 more variables: prc &lt;dbl&gt;, vol &lt;dbl&gt;, ret &lt;dbl&gt;, bid &lt;dbl&gt;,\n#   ask &lt;dbl&gt;, shrout &lt;dbl&gt;, cfacpr &lt;dbl&gt;, cfacshr &lt;dbl&gt;,\n#   openprc &lt;dbl&gt;, numtrd &lt;dbl&gt;, retx &lt;dbl&gt;\n\n\nBefore proceeding with our first benchmark, we will make a version of system.time() that works with assignment.2\n\nsystem_time &lt;- function(x) {\n  print(system.time(x))\n  x\n}\n\nThe following code is adapted from the Tidy Finance code here. But the original code is much more complicated and takes slightly longer to run.3\n\nrs &lt;- dbExecute(tidy_finance, \"DROP TABLE IF EXISTS crsp_daily\")\n\ncrsp_daily &lt;- \n  dsf_db |&gt;\n  filter(between(date, start_date, end_date),\n         !is.na(ret)) |&gt;\n  select(permno, date, ret) |&gt;\n  mutate(month = as.Date(floor_date(date, \"month\"))) |&gt;\n  copy_to(tidy_finance, df = _, name = \"dsf_temp\") |&gt;\n  left_join(factors_ff_daily |&gt;\n              select(date, rf), by = \"date\") |&gt;\n  mutate(\n    ret_excess = ret - rf,\n    ret_excess = pmax(ret_excess, -1, na.rm = TRUE)\n  ) |&gt;\n  select(permno, date, month, ret_excess) |&gt;\n  compute(name = \"crsp_daily\", temporary = FALSE, overwrite = TRUE) |&gt;\n  system_time()\n\n   user  system elapsed \n  56.53    2.64  256.12"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#saving-data-to-sqlite",
    "href": "blog/using-duckdb-with-wrds/index.html#saving-data-to-sqlite",
    "title": "Using DuckDB with WRDS Data",
    "section": "Saving data to SQLite",
    "text": "Saving data to SQLite\nIf you have been working through “Tidy Finance”, you may already have an SQLite database containing crsp_daily. If not, we can easily create one now and copy the table from our DuckDB database to SQLite.\n\ntidy_finance_sqlite &lt;- dbConnect(\n  RSQLite::SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\n\ncopy_to(tidy_finance_sqlite,\n        crsp_daily,\n        name = \"crsp_daily\",\n        overwrite = TRUE,\n        temporary = FALSE)\n\ndbExecute(tidy_finance_sqlite, \"VACUUM\")\n\nWe can also save the data to a parquet file.\n\ndbExecute(tidy_finance, \n          \"COPY crsp_daily TO 'data/crsp_daily.parquet' \n          (FORMAT 'PARQUET')\")\n\nHaving created our two databases, we disconnect from them. This mimics the most common “write-once, read-many” pattern for using databases.\n\ndbDisconnect(tidy_finance_sqlite)\ndbDisconnect(tidy_finance, shutdown = TRUE)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#dplyr",
    "href": "blog/using-duckdb-with-wrds/index.html#dplyr",
    "title": "Using DuckDB with WRDS Data",
    "section": "dplyr",
    "text": "dplyr\nWe first need to load the data into memory.\n\ntidy_finance &lt;- dbConnect(\n  RSQLite::SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_daily &lt;- tbl(tidy_finance, \"crsp_daily\")\n\nWhat takes most time is simply loading nearly 2GB of data into memory.\n\ncrsp_daily_local &lt;- \n  crsp_daily |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n 208.87    2.75  280.47 \n\n\nOnce the data are in memory, it is relatively quick to run a summary query.\n\ncrsp_daily_local |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt; \n  system_time()\n\n   user  system elapsed \n   4.79    1.18    8.08 \n\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1960-01-01 -0.00213 \n2 1960-02-01  0.000325\n3 1960-03-01 -0.00115 \n4 1960-04-01 -0.00106 \n5 1960-05-01  0.00114 \n# ℹ 739 more rows\n\n\n\nrm(crsp_daily_local)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-sqlite",
    "href": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-sqlite",
    "title": "Using DuckDB with WRDS Data",
    "section": "dbplyr with SQLite",
    "text": "dbplyr with SQLite\nThings are faster with SQLite, though there’s no obvious way to split the time between reading the data and performing the aggregation. Note that we have a collect() at the end. This will not take a noticeable amount of time, but seems to be a reasonable step if our plan is to analyse the aggregated data in R.\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n   38.6    11.0    63.5 \n\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1960-01-01 -0.00213 \n2 1960-02-01  0.000325\n3 1960-03-01 -0.00115 \n4 1960-04-01 -0.00106 \n5 1960-05-01  0.00114 \n# ℹ 739 more rows\n\n\n\ndbDisconnect(tidy_finance)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-duckdb",
    "href": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-duckdb",
    "title": "Using DuckDB with WRDS Data",
    "section": "dbplyr with DuckDB",
    "text": "dbplyr with DuckDB\nLet’s consider DuckDB. Note that we are only reading the data here, so we set read_only = TRUE in connecting to the database. Apart from the connection, there is no difference between the code here and the code above using SQLite.\n\ntidy_finance &lt;- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = TRUE)\n\n\ncrsp_daily &lt;- tbl(tidy_finance, \"crsp_daily\")\n\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n   0.51    0.11    0.50 \n\n\n# A tibble: 744 × 2\n  month           ret\n  &lt;date&gt;        &lt;dbl&gt;\n1 1991-12-01 0.00237 \n2 1992-05-01 0.000740\n3 1993-01-01 0.00413 \n4 1993-05-01 0.00265 \n5 2010-10-01 0.00198 \n# ℹ 739 more rows\n\n\nHaving done our benchmarks, we can take a quick peek at the data.\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  arrange(month) |&gt;\n  collect()\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1960-01-01 -0.00213 \n2 1960-02-01  0.000325\n3 1960-03-01 -0.00115 \n4 1960-04-01 -0.00106 \n5 1960-05-01  0.00114 \n# ℹ 739 more rows\n\n\nFinally, we disconnect from the database. This will happen automatically if we close R, etc., and is less important if we have read_only = TRUE (so there is no lock on the file), but we keep things tidy here.\n\ndbDisconnect(tidy_finance, shutdown = TRUE)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-duckdb-and-a-parquet-file",
    "href": "blog/using-duckdb-with-wrds/index.html#dbplyr-with-duckdb-and-a-parquet-file",
    "title": "Using DuckDB with WRDS Data",
    "section": "dbplyr with DuckDB and a parquet file",
    "text": "dbplyr with DuckDB and a parquet file\nLet’s do the benchmark using the parquet data.\n\ndb &lt;- dbConnect(duckdb::duckdb())\ncrsp_daily &lt;- tbl(db, \"read_parquet('data/crsp_daily.parquet')\")\n\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n   1.63    0.73    0.51 \n\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1970-01-01 -0.00210 \n2 1971-05-01 -0.00262 \n3 1973-02-01 -0.00415 \n4 1973-11-01 -0.00927 \n5 1967-05-01 -0.000372\n# ℹ 739 more rows\n\n\n\ndbDisconnect(db, shutdown = TRUE)"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#the-arrow-library-with-a-parquet-file",
    "href": "blog/using-duckdb-with-wrds/index.html#the-arrow-library-with-a-parquet-file",
    "title": "Using DuckDB with WRDS Data",
    "section": "The arrow library with a parquet file",
    "text": "The arrow library with a parquet file\nLet’s do one more benchmark using the parquet data with the arrow library.\n\ncrsp_daily &lt;- open_dataset(\"data/crsp_daily.parquet\")\n\n\ncrsp_daily |&gt; \n  group_by(month) |&gt; \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |&gt; \n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n   0.25    0.01    0.76 \n\n\n# A tibble: 744 × 2\n  month            ret\n  &lt;date&gt;         &lt;dbl&gt;\n1 1989-01-01  0.00285 \n2 1986-06-01  0.000194\n3 1986-07-01 -0.00352 \n4 1986-08-01  0.00115 \n5 1986-09-01 -0.00279 \n# ℹ 739 more rows"
  },
  {
    "objectID": "blog/using-duckdb-with-wrds/index.html#footnotes",
    "href": "blog/using-duckdb-with-wrds/index.html#footnotes",
    "title": "Using DuckDB with WRDS Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is how it’s done in “R for Data Science”. I have read comments by Hadley Wickham that this is the right way to do it, but I can’t find those comments.↩︎\nIf we put system.time() at the end of this pipe, then crsp_daily would hold the value returned by that function rather than the result of the pipeline preceding it. At first, the system_time() function may seem like magic, but Hadley Wickham explained to me that this works because of lazy evaluation, which is discussed in “Advanced R” here. Essentially, x is evaluated just once—inside system.time()—and its value is returned in the next line.↩︎\nPerformance will vary according to the speed of your connection to WRDS. Note that this query does temporarily use a significant amount of RAM on my machine, it is not clear that DuckDB will use as much RAM if this is more constrained. If necessary, you can run (say) dbExecute(tidy_finance, \"SET memory_limit='1GB'\") to constrain DuckDB’s memory usage; doing so has little impact on performance for this query.↩︎"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html",
    "href": "blog/tidy-market-microstructure/index.html",
    "title": "Tidy Market Microstructure",
    "section": "",
    "text": "Anyone active in market microstructure research knows that the devil is in the details. When clocks tick in microseconds and prices move in cents, a brief delay or a small fee discount can make a huge difference for traders. In fact, they can even be the business model of an exchange. But as much as such institutional detail is fascinating, the empirical implementation of even the most conventional microstructure concepts can be frustrating. Referring to the interest of brevity, many journal articles often defer the implementation details to an appendix, and even there they tend to be vague or incomplete. With the additional challenge of vast data sets, new entrants into this field face a steep challenge.\nWe provide a beginner’s guide to market quality measurement in high-frequency data, aiming to lower the barriers to entry into empirical market microstructure. We discuss economic considerations and show step-by-step how to code the most common measures of market liquidity and market efficiency. Because virtually all securities now trade at multiple venues, we also emphasize how market quality can account for market fragmentation.\nIs this guide really needed? Well, a recent paper by Menkveld et al., (2023)1 shows in full clarity that even small variations in methodology can lead to large differences in market quality measures. The authors assigned the same set of market microstructure hypotheses and the same data to 164 research teams. They found that the variation in results across teams, the non-standard error, was of a magnitude similar to the standard error. Many teams included seasoned professors, but past publication performance and seniority did not reduce the non-standard error.\nAnother question is if the cumbersome high-frequency data analysis is really worth the effort? After all, there are numerous liquidity proxies based on daily data. The answer depends on the research question. First, low-frequency proxies are designed for low-frequency applications. For example, Amihud’s (2002)2 popular proxy was originally proposed to be measured as an annual average. Most microstructure applications require liquidity measures at higher frequencies than that. Furthermore, recent evidence by Jahan-Parvar and Zikes (2023)3 show that many low-frequency proxies capture volatility rather than liquidity.\nIf we convinced you to take on the high-frequency data, here’s what we offer. Table 1 lists the market quality measures that we cover, as well as their underlying data type. For some measures, we include several versions and discuss the differences between them. We organize the text by the data type, as we think that is a natural work flow. We start with liquidity measures based on tick-by-tick quote data, followed by measures based on both trade and quote data. Finally, we look into a set of measures of efficiency and volatility that require equispaced quote data.\nMarket quality variables and data types\n\n\nVariable name\nVariable type\nData type\n\n\n\n\nQuoted bid-ask spread\nLiquidity\nQuote tick data\n\n\nQuoted depth\nLiquidity\nQuote tick data\n\n\nEffective bid-ask spread\nLiquidity\nQuote and trade tick data\n\n\nTrade volume\nVolume\nTrade tick data\n\n\nPrice impact\nLiquidity\nQuote and trade tick data\n\n\nRealized spread\nLiquidity\nQuote and trade tick data\n\n\nReturn autocorrelation\nEfficiency\nEquispaced quote data\n\n\nRealized volatility\nVolatility\nEquispaced quote data\n\n\nVariance ratio\nEfficiency\nEquispaced quote data\nOur focus is on the practice of empirical market microstructure. As such, we often motivate the coding choices with economic concepts. Other than that, however, readers interested in the economics underlying each metric are referred to introductory texts such as Campbell, Lo and MacKinlay (1997)4, Foucault, Pagano & Röell (2013)5 and Hasbrouck (2007)6."
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#quote-data-inspection-and-preparation",
    "href": "blog/tidy-market-microstructure/index.html#quote-data-inspection-and-preparation",
    "title": "Tidy Market Microstructure",
    "section": "Quote data inspection and preparation",
    "text": "Quote data inspection and preparation\nLet’s dive into it. After loading the required packages, the following code shows how to import and preview the data. We get an overview of the data by simply typing the name of the data frame in the console. It automatically abbreviates the content to show only a subset of the data.\nThe data can be downloaded directly from within R.\n\nquotes_url &lt;- \"http://tinyurl.com/pruquotes\"\n\n\ndata.tabledtplyr\n\n\nTo load the data we use the function ‘fread’ which is similar to read.csv, but much faster.\n\n# Install and load the `data.table` package\n# install.packages(\"data.table\")\nlibrary(data.table)\n\n# Load the view the quote data\nquotes &lt;- fread(quotes_url)\n\nThe raw data looks as follows.\n\nquotes\n\n           #RIC       Domain           Date-Time GMT Offset  Type Bid Price\n     1:   PRU.L Market Price 2021-06-07 04:00:03          1 Quote    1450.0\n     2:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote    1450.0\n     3:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote    1450.0\n     4:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote    1450.0\n     5:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote    1450.0\n    ---                                                                    \n263969: PRUl.TQ Market Price 2021-06-11 15:29:59          1 Quote    1488.5\n263970: PRUl.TQ Market Price 2021-06-11 15:29:59          1 Quote    1488.5\n263971: PRUl.TQ Market Price 2021-06-11 15:29:59          1 Quote        NA\n263972: PRUl.TQ Market Price 2021-06-11 15:30:00          1 Quote        NA\n263973: PRUl.TQ Market Price 2021-06-11 15:30:00          1 Quote        NA\n        Bid Size Ask Price Ask Size          Exch Time\n     1:      300    1550.0      700 04:00:02.983920000\n     2:     1028    1550.0      700 06:50:00.016246000\n     3:     1028    1510.5     1000 06:50:00.251314000\n     4:     1028    1504.5     1000 06:50:00.402760000\n     5:     1028    1504.5     1009 06:50:00.547739000\n    ---                                               \n263969:      476    1496.0      472 15:29:59.623000000\n263970:      476    1600.0      425 15:29:59.798000000\n263971:       NA    1600.0      425 15:29:59.833000000\n263972:       NA    1600.0      325 15:30:00.104000000\n263973:       NA        NA       NA 15:30:00.104000000\n\n\n\n\n\n# install.packages(\"data.table\")\n# install.packages(\"dtplyr\")\n# install.packages(\"tidyverse\")\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(tidyverse)\n\nWe read the data as a tibble using the (very fast) function read_csv. The call lazy_dt converts the tibble to a lazy data.table object. Lazy means that all following commands are not executed immediately but translated to data.table syntax first and then executed at a final stage. As a result, there should be almost no difference in execution time of the code in tidyverse syntax versus the data.table implementation.\n\ntv_quotes &lt;- read_csv(quotes_url, col_types = list(`Exch Time` = col_character()))\n\ntv_quotes &lt;- lazy_dt(tv_quotes)\n\nThe raw data looks as follows.\n\ntv_quotes\n\nSource: local data table [263,973 x 10]\nCall:   `_DT1`\n\n  `#RIC` Domain    `Date-Time`         `GMT Offset` Type  `Bid Price` `Bid Size`\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dttm&gt;                     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 PRU.L  Market P… 2021-06-07 04:00:03            1 Quote        1450        300\n2 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450       1028\n3 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450       1028\n4 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450       1028\n5 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450       1028\n6 PRU.L  Market P… 2021-06-07 06:50:05            1 Quote        1450       1035\n# ℹ 263,967 more rows\n# ℹ 3 more variables: `Ask Price` &lt;dbl&gt;, `Ask Size` &lt;dbl&gt;, `Exch Time` &lt;chr&gt;\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\n\n\n\nNote that we use the suffix tv_ for every object generated using tidyverse syntax. This way you can execute the entire code of this guide without overwriting data.table objects with the dtplyr equivalent and vice versa.\nThe data contain four variables that describe the state of the order book (Bid Price, Bid Size, Ask Price, and Ask Size). The LOB holds orders at numerous different prices, but we only see the best price level on each side. This is sufficient for most quote-based market quality measures. We also get three variables conveying time stamps and time zone information (Date-Time, GMT Offset, and Exch time; discussed in detail below), and three character variables (#RIC, Domain, and Type).\nBefore processing the data, we rename the variables. Names containing spaces, hashes, and dashes may make sense for data vendors, but they are impractical to work with in R. Furthermore, for the order book variables, we prefer to use the term depth to refer to the number of shares quoted, reserving the term size to the number of shares changing hands in a trade.\n\ndata.tabledtplyr\n\n\nThe command setnames replaces the existing variables with our desired column names.\n\nraw_quote_variables &lt;- c(\"#RIC\", \"Date-Time\", \"GMT Offset\", \"Domain\", \"Exch Time\",\n                         \"Type\", \"Bid Price\", \"Bid Size\", \"Ask Price\", \"Ask Size\")\n\nnew_quote_variables &lt;- c(\"ticker\", \"date_time\", \"gmt_offset\", \"domain\", \"exchange_time\", \n                         \"type\", \"bid_price\", \"bid_depth\", \"ask_price\", \"ask_depth\")\n\nsetnames(quotes, raw_quote_variables, new_quote_variables)\n\n\n\n\ntv_quotes &lt;- tv_quotes|&gt; \n  rename(ticker = `#RIC`,\n         domain = Domain,\n         date_time = `Date-Time`,\n         gmt_offset = `GMT Offset`,\n         type = Type,\n         bid_price = `Bid Price`, \n         bid_depth = `Bid Size`, \n         ask_price = `Ask Price`,\n         ask_depth = `Ask Size`,\n         exchange_time = `Exch Time`\n  ) |&gt;\n  lazy_dt()\n\n\n\n\nFrom the output above, the character columns might look like they are the same for all rows, hence occupying more memory than necessary. The data.table::table() or dtplyr::count() functions are great to gauge the variation in categorical variables. In this case, it shows us that there is variation in the ticker variable. The two tickers are for the same stock, PRU, traded at two different exchanges, LSE and TQE. The former is more active, with 165,261 quote updates.\nThe other two character variables, domain and type, are indeed constants. We delete them to save memory.\n\ndata.tabledtplyr\n\n\n\n# Output a table of sample tickers and values of `domain` and `type`\ntable(quotes$ticker, quotes$type, quotes$domain)\n\n, ,  = Market Price\n\n         \n           Quote\n  PRU.L   165261\n  PRUl.TQ  98712\n\n# Delete variables\nquotes[, c(\"domain\", \"type\") := NULL]\n\n\n\n\ntv_quotes |&gt; \n  count(ticker, type, domain)\n\nSource: local data table [2 x 4]\nCall:   `_DT2`[, .(n = .N), keyby = .(ticker, type, domain)]\n\n  ticker  type  domain            n\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;\n1 PRU.L   Quote Market Price 165261\n2 PRUl.TQ Quote Market Price  98712\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\ntv_quotes &lt;- tv_quotes |&gt;\n  select(-type, -domain)\n\n\n\n\n\nDates\nIn the raw data, dates and times are embedded in the same variable, date_time, but for us it is useful to have them in separate variables. Accordingly, we now define the variable date.\n\ndata.tabledtplyr\n\n\nNote here that the operator := is used to define a new variable (date) within an existing data.table, such as quotes in this example. Within a data.table, it suffices to refer to the variable name, date_time, when defining the new variable. This is different to a data.frame, for which we would have to write quotes$date_time.\n\n# Obtain dates\nquotes[, date := as.Date(date_time)]\n\n# Output a table of sample dates and tickers\ntable(quotes$ticker, quotes$date)\n\n         \n          2021-06-07 2021-06-08 2021-06-09 2021-06-10 2021-06-11\n  PRU.L        27415      38836      30962      39639      28409\n  PRUl.TQ      16424      26040      18207      21170      16871\n\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt; \n  mutate(date = as.Date(date_time)) |&gt;\n  lazy_dt()\n\ntv_quotes |&gt; \n  count(ticker, date) |&gt; \n  pivot_wider(names_from = ticker, \n              values_from = n)\n\nSource: local data table [5 x 3]\nCall:   dcast(`_DT3`[, .(n = .N), keyby = .(ticker, date)], formula = date ~ \n    ticker, value.var = \"n\")\n\n  date       PRU.L PRUl.TQ\n  &lt;date&gt;     &lt;int&gt;   &lt;int&gt;\n1 2021-06-07 27415   16424\n2 2021-06-08 38836   26040\n3 2021-06-09 30962   18207\n4 2021-06-10 39639   21170\n5 2021-06-11 28409   16871\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\n\n\n\nThe table output shows that there are five trading days in the sample. The number of quote observations per stock-date varies between roughly 16,000 and 40,000. The tendency that LSE has more quotes than TQE is consistent across trading days.\n\n\nTimestamps\nThe accuracy of timestamps is important in microstructure data. Timestamps are often matched between quote and trade data (that are not necessarily generated in the same systems), or between data from exchanges in different locations. It is thus essential to be aware of latencies that may arise due to geography or hardware, for example.\nWe have two timestamps for each observation. The exchange_time variable is assigned by the exchange at the time an event is recorded in the exchange matching engine. The date_time variable is the timestamp assigned on receipt at the data vendor, which is by definition later than the exchange_time. Exchanges that are located at different distances from the vendor are likely to have different reporting delays. It is then up to the researcher to determine which timestamp to rely on, and the choice may depend on the research question. In our setting, as we measure liquidity across venues, it is important that the time stamps across venues are comparable. Based on that each exchanges has strong incentives to assign accurate time stamps (to cater for low-latency participants), we choose to work with the exchange_time variable.\nFor US equity markets, the timestamp may reflect the matching engine time, the time when the national best bid and offer updates, or the participant timestamp. For discussions about which of these to use, see Bartlett & McCrary (2019)10, Holden, Pierson & Wu (2023)11, and Schwenk-Nebbe (2021)12.\n\ndata.tabledtplyr\n\n\nWhen working with timestamps in microstructure applications, it is useful to convert them into a numeric format. Dedicated time formats (e.g., xts) are imprecise when it comes to sub-second units (see here and here). We thus convert the timestamps to the number of seconds elapsed since midnight. For example, 8:30 am becomes 8.5 x 3,600 = 30,600, because there are 3,600 seconds per hour.\nThe code below converts exchange_time to numeric and adjusts it for daylight saving using the gmt_offset variable (which is measured in hours). Note the use of curly brackets {...} in the definition of the time variable, which allows us to temporarily define the variable time_elements within the call. Once the operation is complete, the temporary variable is automatically deleted. The variable that is retained should always be returned as a list, hence list(time).\n\n# Convert time stamps to numeric format, expressed in seconds past midnight\n# The function `strsplit` splits a character string at the point defined by `split`\n# `do.call` is a way to call a function, which in this case calls `rbind` to convert a \n# list of vectors to a matrix, where each vector forms one row\nquotes[, time := {\n    time_elements = strsplit(exchange_time, split = \":\")\n    time_elements = do.call(rbind, time_elements)\n    time = as.numeric(time_elements[, 1]) * 3600 + \n           as.numeric(time_elements[, 2]) * 60 + \n           as.numeric(time_elements[, 3]) +\n           gmt_offset * 3600\n    list(time)}]\n\nHaving made sure that dates and times are in the desired format, we can save space by dropping the raw time and date variables.\n\nquotes[, c(\"date_time\", \"exchange_time\", \"gmt_offset\") := NULL]\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt; \n  separate(exchange_time, \n           into = c(\"hour\", \"minute\", \"second\"), \n           sep=\":\", \n           convert = TRUE) |&gt; \n  mutate(time = hour * 3600 + minute * 60 + second + gmt_offset * 3600)\n\nHaving made sure that dates and times are in the desired format, we can save space by dropping the raw time and date variables.\n\ntv_quotes &lt;- tv_quotes|&gt; \n  select(-c(\"date_time\", \"gmt_offset\",\"hour\",\"minute\",\"second\"))\n\n\n\n\n\n\nPrices and depths\nAn important feature of LOB quotes is that they remain valid until cancelled, executed or modified. Whenever there is a change to the prevailing quotes, a new quote observation is added to the data. It is irrelevant if the latest quote is from the previous millisecond or from the previous minute – it remains valid until updated. It is thus economically meaningful to forward-fill quotes that prevailed in the previous period. Trades, in contrast, are agreed upon at a fixed point in time and do not convey any information about future prices or sizes. They should not be forward-filled, see Hagströmer and Menkveld (2023)13.\nWhen forward-filling quote data, it is important to restrict the procedure to the same date, stock and trading venue. For example, quotes should never be forward-filled from one day to the next, and not from one venue to another. This is ensured with the by argument (in data.table) or the group_by function (in tidyverse), which specifies that the operation is to be done within each combination of tickers and dates.\n\ndata.tabledtplyr\n\n\nWe use the nafill function to forward-fill, with the option type = \"locf\" (last observation carried forward) specifying the type of filling. The .SD inside the lapply command tells data.table to repeat the same operation for the set of variables specified by the option .SDcols.\nIn summary, whereas the .SD applies the same function across a set of variables (columns), the by applies it across categories of observations (rows). The same outcome could be achieved with for loops, but in R, that would be much slower. We discuss that further below.\nNote here how the := notation can be used to define multiple variables, using a vector of variable names on the left-hand-side and a function (in this case lapply) that returns a list of variables on the right-hand-side. Note also that when referring to multiple variable names within the data.table, they are specified as a character vector.\n\n# Forward-fill quoted prices and depths\nlob_variables &lt;- c(\"bid_price\", \"bid_depth\", \"ask_price\", \"ask_depth\")\n\nquotes[, \n  (lob_variables) := lapply(.SD, nafill, type = \"locf\"),    .SDcols = (lob_variables), \n  by = c(\"ticker\", \"date\")]\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt;\n  group_by(ticker, date) |&gt;\n  fill(matches(\"bid|ask\")) |&gt; \n  ungroup()\n\n\n\n\nWhen measuring market quality in continuous trading, it is common to filter out periods that may be influenced by call auctions. The LSE opens for trading with a call auction at 08:00 am, local time, and closes with another call at 4:30 pm. There is also an intraday call auction at noon, 12:00 pm. To avoid the impact of the auctions, we exclude quotes before 8:01 am and after 4:29 pm. We do not exclude quotes recorded around the intraday call auction, but set them as missing (NA). If they were instead deleted, it would give the false impression that the last observation before the excluded quotes was still valid.\nWhen entering the opening hours, remember to state them for the same time zone as recorded in the data. In our case, the gmt_offset adjustment above makes sure that the data is stated in local (London) time.\n\nopen_time &lt;- 8 * 3600\nclose_time &lt;- 16.5 * 3600\nintraday_auction_time &lt;- 12 * 3600\n\nFirst, we exclude quotes around the opening and closing of continuous trading. Next, we set quotes around the intraday auction to missing.\n\ndata.tabledtplyr\n\n\n\nquotes &lt;- quotes[time &gt; (open_time + 60) & time &lt; (close_time - 60)]\n\n\nquotes[time &gt; (intraday_auction_time - 60) & time &lt; (intraday_auction_time + 3 * 60), \n  (lob_variables)] &lt;-  NA\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt;\n  filter(time &gt; hms::as_hms(\"08:01:00\"), \n         time &lt; hms::as_hms(\"16:29:00\"))\n\ntv_quotes &lt;- tv_quotes |&gt;\n  mutate(across(matches(\"bid|ask\"), \n                ~if_else(time &gt; hms::as_hms(\"11:59:00\") & \n                         time &lt; hms::as_hms(\"12:03:00\"), NA_real_, .))) |&gt;\n  lazy_dt()\n\n\n\n\n\n\nScreening\nBefore turning to the market quality measurement, it is a good habit to check that the quote observations make economic sense. One way to do that is to study the variation in the bid-ask spread. The nominal bid-ask spread is defined as the difference between the ask price, \\(P^A\\), and the bid price, \\(P^B\\), \\(\\text{quoted}\\_\\text{spread}^{nom}= P^A - P^B\\). A histogram offers a quick overview of the variation (a line plot of the prices is also useful, see Section 2.1).\nIn the output below, note that the x-axis is in units of pence (0.01 British Pounds, GBP). All quoted prices in this example data follow that convention. Note also that the bid-ask spread is strictly positive, as it should be whenever the market is open. The TQE occasionally has wider spreads than the LSE, but there are no extraordinarily large spreads. The maximum spread, GBP 0.11, corresponds to around 0.7% of the stock price.\nAlso, it is clear from the histogram that the tick size, the minimum price increment that is allowed when quoting prices, is 0.5 pence (that is, GBP 0.005). Most spreads are quoted at one or two ticks.\nWe use the package ggplot2 to plot an histogram of the nominal quoted bid-ask spreads.\n\ndata.tabledtplyr\n\n\n\nlibrary(ggplot2)\nggplot(quotes, \n       aes(x = ask_price - bid_price, fill = ticker)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"Histogram of nominal bid-ask spread\",\n       x = \"Nominal bid-ask spread (pence)\",\n       y = \"Count\",\n       fill = NULL) +\n  scale_x_continuous(breaks = 1:12)\n\nWarning: Removed 1576 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\ntv_quotes |&gt; \n  as_tibble() |&gt;\n  ggplot(aes(x = ask_price - bid_price, fill = ticker)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"Histogram of nominal bid-ask spread\",\n       x = \"Nominal bid-ask spread (pence)\",\n       y = \"Count\",\n       fill = NULL) +\n  scale_x_continuous(breaks = 1:12)\n\nWarning: Removed 1576 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\nR produces a warning when plotting the nominal bid-ask spread. It mentions 1,576 rows containing “non-finite values”. The non-finite values refer either NA, Inf (infinite) or -Inf (negative infinite). In the timestamp section, we imposed NA for LOB variables during midday auction. To see if those are the cause of the warning, let’s create a histogram of the time stamps of the missing values.\nIndeed, all missing values are around ~43,150 and ~43,350 seconds of the trading day which is the time of the midday auction (noon is \\(12\\times3,600=43,200\\) seconds past midnight). Accounting for missing spreads by plotting the histogram without NA removes the warning.\n\ndata.tabledtplyr\n\n\n\n# Plot a histogram of missing quoted spreads\nggplot(quotes[is.na(ask_price - bid_price)], \n       aes(x = time, fill = ticker)) +\n  geom_histogram(bins = 100) + \n  labs(title = \"Histogram of missing spreads\",\n       x = \"Time of Day (seconds past midnight)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\ntv_quotes |&gt; \n  filter(is.na(ask_price) | is.na(bid_price)) |&gt;\n  mutate(time = hms::hms(time)) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(x = time, fill = ticker)) +\n  geom_histogram(bins = 100) + \n  labs(title = \"Histogram of missing spreads\",\n       x = \"Time of Day\",       \n       y = \"Count\")"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#liquidity-measures",
    "href": "blog/tidy-market-microstructure/index.html#liquidity-measures",
    "title": "Tidy Market Microstructure",
    "section": "Liquidity measures",
    "text": "Liquidity measures\nWith all the data preparation done, we are ready for the actual liquidity measurement. For comparisons across stocks, it is useful to relate the nominal spread to the fundamental value of the security. This is done by the relative quoted bid-ask spread, defined as \\(\\text{quoted}\\_\\text{spread}^{rel} = (P^A - P^B)/M\\), where \\(M\\) is the midpoint (also known as the midprice; defined as the average of the best bid and the best ask prices). One can argue that the midpoint is not always representative of the fundamental value, but it has the strong advantage that it is continuously available in the quote data.\n\ndata.tabledtplyr\n\n\n\n# Fundamental value\nquotes[, midpoint := (bid_price + ask_price) / 2]\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt; \n  mutate(midpoint = (bid_price + ask_price) / 2)\n\n\n\n\nThe quoted spread can also be measured relative to the tick size. In an open market, the spread can never be below one tick. A tick refers to the tick size of a security. It is the minimum price increment a security can be quoted and traded. The tick size in the example data is half a cent at both exchanges. We refer to the average number of ticks in the bid-ask spread as the tick spread, \\(quoted\\_spread^{tic} = (P^A - P^B) / tick\\_size\\).\n\ntick_size &lt;- 0.5\n\nAnother dimension of quoted liquidity is the market depth. We measure the average depth quoted at the best bid and ask prices. It is defined as \\(\\text{quoted}\\_\\text{depth} = (Q^A + Q^B)/2\\), where \\(Q^A\\) and \\(Q^B\\) are the depths available at the bid and ask prices.\nIn the code below, we store the liquidity measures in a new data.table named quotes_liquidity. This is because the new variables are averages, observed on a ticker-date frequency, as opposed to the tick-by-tick frequency of the quotes object. We multiply the quoted spread by 10,000 to express it in basis points, and divide the quoted depth by 100,000 to express it in thousand GBP.\nThe output shows that the liquidity is higher at the LSE than at the TQE. Both in nominal and relative terms, the spreads are somewhat tighter at the LSE, and there is more than three times more depth posted at the LSE.\n\ndata.tabledtplyr\n\n\n\n# Measure the average quote-based liquidity\n# This step calculates the mean of each of the market quality variables, for each \n# ticker-day (as indicated in `by = c(\"ticker\", \"date\")`)\n\nquotes_liquidity &lt;- quotes[, {\n    quoted_spread = ask_price - bid_price\n    list(quoted_spread_nom = mean(quoted_spread, na.rm = TRUE),\n         quoted_spread_relative = mean(quoted_spread / midpoint, na.rm = TRUE),\n         quoted_spread_tick = mean(quoted_spread / tick_size, na.rm = TRUE),\n         quoted_depth = mean(bid_depth * bid_price + ask_depth * ask_price, \n                             na.rm = TRUE) / 2)},\n    by = c(\"ticker\", \"date\")]\n\n# Output the liquidity measures, averaged across the five trading days for each ticker. \nquotes_liquidity[, \n    list(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),\n         quoted_spread_relative = round(mean(quoted_spread_relative) * 1e4, digits = 2),\n         quoted_spread_tick = round(mean(quoted_spread_tick), digits = 2),\n         quoted_depth = round(mean(quoted_depth) * 1e-5, digits = 2)), \n    by = \"ticker\"]\n\n    ticker quoted_spread_nom quoted_spread_relative quoted_spread_tick\n1:   PRU.L              0.83                   5.65               1.67\n2: PRUl.TQ              1.02                   6.94               2.05\n   quoted_depth\n1:        24.72\n2:         6.65\n\n\n\n\n\ntv_quotes_liquidity &lt;- tv_quotes |&gt;\n  mutate(quoted_spread = ask_price - bid_price) |&gt;\n  group_by(ticker, date) |&gt; \n  summarize(quoted_spread_nom = mean(quoted_spread, na.rm = TRUE),\n            quoted_spread_relative = mean(quoted_spread / midpoint, na.rm = TRUE) * 1e4,\n            quoted_spread_tick = mean(quoted_spread / tick_size, na.rm = TRUE),\n            quoted_depth = mean(bid_depth * bid_price + ask_depth * ask_price, \n                                na.rm = TRUE) / 2 * 1e-5,\n            .groups = \"drop\")\n\ntv_quotes_liquidity |&gt; \n  group_by(ticker) |&gt;\n  summarize(across(contains(\"quoted\"), \n                   ~round(mean(.), digits = 2))) |&gt;\n  pivot_longer(-ticker) |&gt;\n  pivot_wider(names_from = name, values_from = value) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 5\n  ticker  quoted_depth quoted_spread_nom quoted_spread_relative\n  &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n1 PRU.L          24.7               0.83                   5.65\n2 PRUl.TQ         6.65              1.02                   6.94\n# ℹ 1 more variable: quoted_spread_tick &lt;dbl&gt;\n\n\n\n\n\n\nDuration-weighted liquidity\nThe output above are straight averages, implying an assumption that all quote observations are equally important. But whereas some quotes remain valid for several minutes, many don’t last longer than a split-second. For this reason, it is common to either sample the quote data in fixed time intervals (such as at the end of each second), or to weight the observations by their duration. The duration is the time that a quote observation is in force. That is, the time elapsed until the next quote update arrives. We show the duration-weighted approach in the code below (for guidance on how to get the quotes at the end of each second, see Section 3.1).\nNote that the duration variable is obtained separately for each ticker and date. Even if we are interested in the average liquidity across dates, it is important to partition by each ticker and date to avoid that duration is calculated overnight (resulting in a huge weight with negative sign, because it will be roughly the opening time minus the closing time). Except for replacing the mean function with the weighted.mean, the code below is very similar to that above.\nIn the output, we note that the differences between the duration-weighted and the equal-weighted liquidity averages are small. Nevertheless, we consider the duration-weighted average more appropriate because it is not sensitive to short-lived price and depth fluctuations.\n\ndata.tabledtplyr\n\n\n\n# Calculate quote durations\nquotes[, duration := c(diff(time), 0), by = c(\"ticker\", \"date\")]\n\n# Measure the duration-weighted average quote-based liquidity\n# The specified subset excludes quotes for which no duration can be calculated\nquotes_liquidity_dw &lt;- quotes[!is.na(duration), {\n    quoted_spread = ask_price - bid_price\n    list(quoted_spread_nom = weighted.mean(quoted_spread, \n                                           w = duration, na.rm = TRUE),\n         quoted_spread_rel = weighted.mean(quoted_spread / midpoint, \n                                           w = duration, na.rm = TRUE),\n         quoted_spread_tic = weighted.mean(quoted_spread / tick_size, \n                                           w = duration, na.rm = TRUE),\n         quoted_depth = weighted.mean(bid_depth * bid_price + ask_depth * ask_price, \n                                      w = duration, na.rm = TRUE) / 2)},\n    by = c(\"ticker\", \"date\")]\n\n# Output liquidity measures, averaged across the five trading days for each ticker \nquotes_liquidity_dw[, \n    list(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),\n         quoted_spread_rel = round(mean(quoted_spread_rel) * 1e4, digits = 2),\n         quoted_spread_tic = round(mean(quoted_spread_tic), digits = 2),\n         quoted_depth = round(mean(quoted_depth) * 1e-5, digits = 2)), \n    by = \"ticker\"]\n\n    ticker quoted_spread_nom quoted_spread_rel quoted_spread_tic quoted_depth\n1:   PRU.L              0.84              5.70              1.68        25.65\n2: PRUl.TQ              0.99              6.69              1.97         7.06\n\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt;\n  group_by(ticker, date) |&gt;\n  mutate(duration = c(diff(time), 0)) |&gt;\n  ungroup()\n\ntv_quotes_liquidity &lt;- tv_quotes |&gt;\n  mutate(quoted_spread = ask_price - bid_price) |&gt;\n  group_by(ticker, date) |&gt;\n  summarize(quoted_spread_nom = weighted.mean(quoted_spread, w = duration, na.rm = TRUE),\n            quoted_spread_relative = weighted.mean(quoted_spread / midpoint, w = duration, na.rm = TRUE) * 1e4,\n            quoted_spread_tick = weighted.mean(quoted_spread / tick_size, w = duration, na.rm = TRUE),\n            quoted_depth = weighted.mean(bid_depth * bid_price + ask_depth * ask_price, w = duration, na.rm = TRUE) / 2 * 1e-5,\n            .groups = \"drop\")\n\ntv_quotes_liquidity |&gt; \n  group_by(ticker) |&gt;\n  summarize(across(contains(\"quoted\"), \n                   ~round(mean(.), digits = 2))) |&gt;\n  pivot_longer(-ticker) |&gt;\n  pivot_wider(names_from = name, values_from = value) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 5\n  ticker  quoted_depth quoted_spread_nom quoted_spread_relative\n  &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n1 PRU.L          25.6               0.84                   5.7 \n2 PRUl.TQ         7.06              0.99                   6.69\n# ℹ 1 more variable: quoted_spread_tick &lt;dbl&gt;"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#consolidated-liquidity-in-fragmented-markets",
    "href": "blog/tidy-market-microstructure/index.html#consolidated-liquidity-in-fragmented-markets",
    "title": "Tidy Market Microstructure",
    "section": "Consolidated liquidity in fragmented markets",
    "text": "Consolidated liquidity in fragmented markets\nWith the competition between exchanges, liquidity is dispersed across venues. For example, if there is a change to the market structure at the LSE, it is typically not sufficient to analyze liquidity at LSE alone. If liquidity is reduced at the LSE, it may simultaneously be boosted at the TQE. To assess the overall market quality, which may be most relevant for welfare, it is often necessary to consider the consolidated liquidity.\nIn Europe, the consolidated liquidity is sometimes referred to as the European Best Bid and Offer (EBBO). The terminology follows in the footsteps of the US market, where the National Best Bid and Offer (NBBO) is transmitted to the market on continuous basis. To obtain the EBBO, one needs to merge the LOB data from each relevant venue, and then determine the EBBO prices and depths. In the code below, we show step-by-step how to do that.\n\nRetaining only the last quote update in each interval\nQuote updates tend to cluster and it is common that several observations have identical timestamps. Multiple observations at one timestamp can be due to several investors responding to the same events, or that one market order leads to several LOB updates as it is executed against multiple limit orders. When matching quotes across venues, we need to restrict the number of observations per unit of time to one. There is no sensible way to distinguish observations with identical timestamps. In lack of a better approach, we retain the last observation in each interval.\n\ndata.tabledtplyr\n\n\n\n# Retain only the last observation per unit of time\n# The function `duplicated` returns `TRUE` if the observation is a duplicate of another \n# observation based on the columns given in the `by` option, and `FALSE` otherwise.\n# The option `fromLast = TRUE` ensures that the last rather than the first observation \n# in each millisecond that returns `FALSE`.\nquotes &lt;- quotes[!duplicated(quotes, fromLast = TRUE, by = c(\"ticker\", \"date\", \"time\"))]\n\n\n\n\ntv_quotes &lt;- tv_quotes |&gt; \n  group_by(ticker, date, time) |&gt;\n  slice(n()) |&gt; \n  ungroup() |&gt;\n  lazy_dt()\n\n\n\n\n\n\nMerging quotes from different venues\nWe are now ready to match the quotes from the two exchanges. First, we create separate quote data sets for the two exchanges. Second, we merge the two by matching on date and time. Third, we forward-fill quotes from both venues, such that for each LSE quote we know the prevailing TQE quote, and vice versa. The validity of this is ensured by the option sort = TRUE in the merge function, which returns a data.table that is sorted on the matching variables.\n\ndata.tabledtplyr\n\n\n\n# Merge quotes from two venues trading the same security\n# In the `merge` function, we add exchange suffixes to the variable names to keep track of \n# which quote comes from which exchange, using the option `suffixes`. \n# The option `all = TRUE` specifies that unmatched observations from both sets of quotes \n# should be retained (known as an outer join). \nvenues &lt;- c(\"_lse\", \"_tqe\")\n\nquotes_lse &lt;- quotes[ticker == \"PRU.L\", .SD, .SDcols = c(\"date\", \"time\", lob_variables)]\nquotes_tqe &lt;- quotes[ticker == \"PRUl.TQ\", .SD, .SDcols = c(\"date\", \"time\", lob_variables)]\n\nquotes_ebbo &lt;- merge(quotes_lse, quotes_tqe, \n                     by = c(\"date\", \"time\"), \n                     suffixes = venues, \n                     all = TRUE, sort = TRUE)\n\nNext, we forward-fill the quoted prices and depth for each exchange.\n\nlocal_lob_variables &lt;- paste0(lob_variables, rep(venues, each = 4))\n\nquotes_ebbo[, (local_lob_variables) := lapply(.SD, nafill, type = \"locf\"), \n  .SDcols = (local_lob_variables),\n    by = \"date\"]\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes |&gt; \n  select(-midpoint, -duration) |&gt;\n  mutate(ticker = case_when(ticker == \"PRUl.TQ\" ~ \"tqe\",\n                            ticker == \"PRU.L\" ~ \"lse\")) |&gt;\n  pivot_wider(names_from = ticker, \n              values_from = matches(\"bid|ask\")) |&gt;\n  arrange(date, time)\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  group_by(date) |&gt;\n  fill(matches(\"bid|ask\")) |&gt; \n  ungroup()\n\n\n\n\nThe best bid price at each point in time is the maximum of the best bid at the LSE and the best bid at the TQE. Similarly, the best ask is the minimum of the best ask prices at the two venues. We calculate the best bid using the parallel maxima function, pmax, which returns the highest value in each row. The best ask is obtained in the same way, using the parallel minima function, pmin.\nNote that it would also be possible to obtain the EBBO using a for loop, checking row-wise which is the highest bid and lowest ask. When working with large data sets in R, however, loops become extremely slow. It is strongly encouraged to run vectorised operations for the whole column at once (like we do here), or to apply functions repeatedly to blocks of data (like we have done several times above).\nWe obtain the depth at the best prices by summing the depth of the individual venues. When doing this, we should only consider both venues at times when they are both at the best price. When the two venues have the same best bid, for example, we calculate the consolidated bid depth as the sum of the two. To code this, we use the feature that a logical variable (with values FALSE or TRUE; such as bid_price_lse  == best_bid_price) works as a binary variable (with values 0 or 1) when used in multiplication.\n\ndata.tabledtplyr\n\n\n\n# Obtain the EBBO prices and depths\nquotes_ebbo[, best_bid_price := pmax(bid_price_lse, bid_price_tqe, na.rm = TRUE)]\nquotes_ebbo[, best_ask_price := pmin(ask_price_lse, ask_price_tqe, na.rm = TRUE)]\nquotes_ebbo[, best_bid_depth := bid_depth_lse * (bid_price_lse == best_bid_price) + \n              bid_depth_tqe * (bid_price_tqe == best_bid_price)]\nquotes_ebbo[, best_ask_depth := ask_depth_lse * (ask_price_lse == best_ask_price) +\n              ask_depth_tqe * (ask_price_tqe == best_ask_price)]\n\nFinally, we drop local exchange variables and objects\n\nquotes_ebbo[, (local_lob_variables) := NULL]\nrm(quotes_lse, quotes_tqe, quotes, local_lob_variables)\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  mutate(best_bid_price = pmax(bid_price_lse, bid_price_tqe, na.rm = TRUE),\n         best_ask_price = pmin(ask_price_lse, ask_price_tqe, na.rm = TRUE),\n         best_bid_depth = bid_depth_lse * (bid_price_lse == best_bid_price) + \n         bid_depth_tqe * (bid_price_tqe == best_bid_price),\n         best_ask_depth = ask_depth_lse * (ask_price_lse == best_ask_price) + \n         ask_depth_tqe * (ask_price_tqe == best_ask_price)\n  )\n\nFinally, we drop local exchange variables and objects\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  select(date, time, contains(\"best\")) |&gt;\n  lazy_dt()\n\n\n\n\n\n\nFundamental value\nWe can now obtain EBBO midpoints, as a proxy of fundamental value that factors in liquidity posted at multiple exchanges.\n\ndata.tabledtplyr\n\n\n\n# Calculate EBBO midpoints\nquotes_ebbo[, midpoint := (best_bid_price + best_ask_price) / 2]\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt; \n  mutate(midpoint = (best_bid_price + best_ask_price) / 2)\n\n\n\n\n\n\nScreening\nAs above, we check that the EBBO quotes are economically meaningful by tabulating the counts of nominal spread levels. This exercise shows us that the consolidated spread is not strictly positive. There are numerous cases of zeroes, known as locked quotes, and also many negatives, referred to crossed quotes. This is possible because orders at the LSE and the TQE are never executed against each other – it takes arbitrageurs to step in and act on crossed markets. Locked and crossed spreads are not uncommon in consolidated data. For an analysis of the incidence in US markets, see Shkilko, van Ness & van Ness, 200814.\nIt is also notable from the table that the maximum consolidated spread is 2.5, as compared to the spreads of up to 11 recorded in the single-venue analysis. By definition, the EBBO quoted spread is never wider than at the single venues.\n\ndata.tabledtplyr\n\n\n\n# Output an overview of the EBBO nominal quoted bid-ask spread \ntable(quotes_ebbo$best_ask_price - quotes_ebbo$best_bid_price)\n\n\n    -1   -0.5      0    0.5      1    1.5      2    2.5    3.5 \n     2    127   6367 104351 103265   4668    637     51      1 \n\n\n\n\n\ntv_quotes_ebbo |&gt; \n  transmute(ebbo_nominal_spread = best_ask_price - best_bid_price) |&gt;\n  count(ebbo_nominal_spread) |&gt; \n  as_tibble()\n\n# A tibble: 9 × 2\n  ebbo_nominal_spread      n\n                &lt;dbl&gt;  &lt;int&gt;\n1                -1        2\n2                -0.5    127\n3                 0     6367\n4                 0.5 104351\n5                 1   103265\n6                 1.5   4668\n7                 2      637\n8                 2.5     51\n9                 3.5      1\n\n\n\n\n\nObservations with locked or crossed quotes are usually excluded when measuring market quality. It is also common to exclude bid-ask spread observations that are unrealistically high. We have no such cases in this sample, but, for illustration, we include a filter that would capture spreads that relative to the share price are wider than 5%.\n\nthreshold &lt;- 0.05\n\nIn the procedure below, we flag the problematic quotes, but we do not exclude them. If they were deleted, it would imply that the last observation before the excluded spread was still in force, which may mislead subsequent analysis.\nThe output shows that 2.90% of the quote observations are locked, while 0.06% are crossed.\n\ndata.tabledtplyr\n\n\n\n# Flag problematic consolidated quotes\nquotes_ebbo[, c(\"crossed\", \"locked\", \"large\") := {\n    quoted_spread = (best_ask_price - best_bid_price)\n    list(quoted_spread &lt; 0, quoted_spread == 0, quoted_spread / midpoint &gt; threshold)}]\n\n# Count the incidence of the consolidated quote flags\nquotes_ebbo_filters  &lt;- quotes_ebbo[, \n    list(crossed = mean(crossed, na.rm = TRUE),\n         locked = mean(locked, na.rm = TRUE),\n         large = mean(large, na.rm = TRUE))]\n\n# Output the fraction of quotes that is flagged\nquotes_ebbo_filters[, \n    lapply(.SD * 100, round, digits = 2), .SDcols = c(\"crossed\", \"locked\", \"large\")]\n\n   crossed locked large\n1:    0.06    2.9     0\n\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  mutate(quoted_spread = best_ask_price - best_bid_price,\n         crossed = quoted_spread &lt; 0,\n         locked = quoted_spread == 0,\n         large = quoted_spread / midpoint &gt; threshold) |&gt;\n  lazy_dt()\n\ntv_quotes_ebbo |&gt;\n  summarize(across(c(crossed, locked, large), \n                   ~round(100 * mean(.),2))) |&gt;\n  as_tibble()\n\n# A tibble: 1 × 3\n  crossed locked large\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    0.06    2.9     0\n\n\n\n\n\n\n\nConsolidated liquidity measures\nWe obtain duration-weighted measures of consolidated liquidity in the same way as above. The only difference here is that we subset the quotes to filter out crossed and locked markets.\nThe consolidated relative quoted bid-ask spread is 5.59 basis points, as compared to 5.70 and 6.69 basis points at LSE and TQE locally. The consolidated depth, 3.14 million GBP, is somewhat lower than the sum of the local depths seen above. This is to be expected, as some of the local depth is posted at price levels that are inferior to the EBBO.\n\ndata.tabledtplyr\n\n\n\n# Measure the duration-weighted consolidated quotes liquidity\n# Because this is the EBBO, there is no variation across tickers, but different averages \n# across dates are considered\nquotes_ebbo[, duration := c(diff(time), 0), by = \"date\"]\n\n# Note that the subset used here excludes crossed and locked quotes\nquotes_liquidity_ebbo_dw &lt;- quotes_ebbo[!crossed & !locked & !large, {\n    quoted_spread = best_ask_price - best_bid_price\n    \n    list(quoted_spread_nom = weighted.mean(quoted_spread, \n                                           w = duration, na.rm = TRUE),\n         quoted_spread_relative = weighted.mean(quoted_spread / midpoint, \n                                           w = duration, na.rm = TRUE),\n         quoted_spread_tick = weighted.mean(quoted_spread / tick_size,\n                                           w = duration, na.rm = TRUE),\n         quoted_depth = weighted.mean(best_bid_depth * best_bid_price + \n                                      best_ask_depth * best_ask_price, \n                                      w = duration, na.rm = TRUE) / 2)}, \n    by = \"date\"]\n\n# Output the liquidity measures, averaged across the five trading days \nquotes_liquidity_ebbo_dw[, \n    list(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),\n         quoted_spread_relative = round(mean(quoted_spread_relative * 1e4), digits = 2),\n         quoted_spread_tick = round(mean(quoted_spread_tick), digits = 2),\n         quoted_depth = round(mean(quoted_depth * 1e-6), digits = 2))]\n\n   quoted_spread_nom quoted_spread_relative quoted_spread_tick quoted_depth\n1:              0.82                   5.59               1.65         3.14\n\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  group_by(date)|&gt;\n  mutate(duration = c(diff(time), 0)) |&gt;\n  ungroup()\n\ntv_quotes_liquidity_ebbo_dw &lt;- tv_quotes_ebbo |&gt;\n  group_by(date) |&gt;\n  mutate(quoted_spread = best_ask_price - best_bid_price) |&gt;\n  filter(!crossed, !locked, !large) |&gt;\n  summarize(quoted_spread_nom = weighted.mean(quoted_spread, w = duration, na.rm = TRUE),\n            quoted_spread_relative = weighted.mean(quoted_spread / midpoint, w = duration, na.rm = TRUE) * 1e4,\n            quoted_spread_tick = weighted.mean(quoted_spread / tick_size, w = duration, na.rm = TRUE),\n            quoted_depth = weighted.mean(best_bid_depth * best_bid_price + best_ask_depth * best_ask_price, w = duration, na.rm = TRUE) / 2 * 1e-5)\n\ntv_quotes_liquidity_ebbo_dw |&gt;\n  summarize(across(contains(\"quoted\"), \n                   ~round(mean(.), digits = 2))) |&gt;\n  as_tibble()\n\n# A tibble: 1 × 4\n  quoted_spread_nom quoted_spread_relative quoted_spread_tick quoted_depth\n              &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n1              0.82                   5.59               1.65         31.4"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#data-export",
    "href": "blog/tidy-market-microstructure/index.html#data-export",
    "title": "Tidy Market Microstructure",
    "section": "Data export",
    "text": "Data export\nAs we will reuse the consolidated quotes in the applications below, we save the quotes_ebbo object to disk.\n\ndata.tabledtplyr\n\n\nWe export the consolidated quotes using fwrite.\n\nfwrite(quotes_ebbo, file = \"quotes_ebbo.csv\")\n\n\n\nWe export the consolidated quotes using write_csv.\n\nwrite_csv(as_tibble(tv_quotes_ebbo), file = \"tv_quotes_ebbo.csv\")"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#trade-data-inspection-and-preparation",
    "href": "blog/tidy-market-microstructure/index.html#trade-data-inspection-and-preparation",
    "title": "Tidy Market Microstructure",
    "section": "Trade data inspection and preparation",
    "text": "Trade data inspection and preparation\nWe load the trade data and view the data structure. The ticker, date, and time variables follow the same structure as in quotes data, so we can proceed with the same date and time transformations as above.\n\ntrades_url &lt;- \"http://tinyurl.com/prutrades\"\n\nThere are three more variables: Price, Volume, and MMT Classification. As discussed above, we refer to the number of shares executed in a trade as “size” (we reserve the term “volume” to the sum of trade sizes in a given interval). We rename the Volume variable accordingly, and also alter the other variable names to make them easier to work with in R.\n\ndata.tabledtplyr\n\n\n\n# Load the trade data\ntrades &lt;- fread(trades_url)\n\n# View the trade data\ntrades\n\n          #RIC       Domain           Date-Time GMT Offset  Type  Price Volume\n    1:   PRU.L Market Price 2021-06-07 07:00:09          1 Trade 1481.5  11379\n    2:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade 1477.5    703\n    3:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade 1477.5    327\n    4:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade 1477.5    650\n    5:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade 1478.0    327\n   ---                                                                        \n34513: PRUl.TQ Market Price 2021-06-11 15:28:56          1 Trade 1492.0    315\n34514: PRUl.TQ Market Price 2021-06-11 15:28:56          1 Trade 1491.5     50\n34515: PRUl.TQ Market Price 2021-06-11 15:28:56          1 Trade 1491.5    170\n34516: PRUl.TQ Market Price 2021-06-11 15:28:58          1 Trade 1492.0      6\n34517: PRUl.TQ Market Price 2021-06-11 15:29:06          1 Trade 1492.0      9\n                Exch Time MMT Classification\n    1: 07:00:09.478177000     1O-------P----\n    2: 07:00:12.859753000     12-------PH---\n    3: 07:00:12.859753000     12-------PH---\n    4: 07:00:12.859753000     12-------PH---\n    5: 07:00:12.860655000     12-------PH---\n   ---                                      \n34513: 15:28:56.202000000     12-------PH---\n34514: 15:28:56.218000000     12-------PH---\n34515: 15:28:56.413000000     12-------PH---\n34516: 15:28:58.062128000     32D---S--PH---\n34517: 15:29:06.141081000     32D---S--PH---\n\n# Rename the variables\nraw_trade_variables &lt;- c(\"#RIC\", \"Date-Time\", \"Exch Time\", \"GMT Offset\", \n                         \"Price\", \"Volume\", \"MMT Classification\")\nnew_trade_variables &lt;- c(\"ticker\", \"date_time\", \"exchange_time\", \"gmt_offset\",\n                         \"price\", \"size\", \"mmt\")\nsetnames(trades,\n         old = raw_trade_variables, \n         new = new_trade_variables)\n\nFinally, we remove the columns Domain and Type.\n\ntrades[, c(\"Domain\", \"Type\") := NULL]\n\n\n\n\ntv_trades &lt;- read_csv(trades_url, col_types = list(`Exch Time` = col_character()))\n\ntv_trades &lt;- lazy_dt(tv_trades)\n\nThe raw data looks as follows.\n\ntv_trades\n\nSource: local data table [34,517 x 9]\nCall:   `_DT17`\n\n  `#RIC` Domain  `Date-Time`         `GMT Offset` Type  Price Volume `Exch Time`\n  &lt;chr&gt;  &lt;chr&gt;   &lt;dttm&gt;                     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n1 PRU.L  Market… 2021-06-07 07:00:09            1 Trade 1482.  11379 07:00:09.4…\n2 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478.    703 07:00:12.8…\n3 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478.    327 07:00:12.8…\n4 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478.    650 07:00:12.8…\n5 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478     327 07:00:12.8…\n6 PRU.L  Market… 2021-06-07 07:00:12            1 Trade 1478     201 07:00:12.8…\n# ℹ 34,511 more rows\n# ℹ 1 more variable: `MMT Classification` &lt;chr&gt;\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\nWe rename the variables.\n\ntv_trades &lt;- tv_trades |&gt;\n  rename(ticker = `#RIC`,\n         date_time = `Date-Time`,\n         gmt_offset = `GMT Offset`,\n         price = Price, \n         size = Volume, \n         exchange_time = `Exch Time`,\n         mmt = `MMT Classification`\n  ) |&gt;\n  select(-c(\"Domain\", \"Type\")) \n\n\n\n\n\nDates and timestamps\nAs above, we filter out the first and last minute of continuous trading, as well as the minutes surrounding the intraday auction. Note here that filtered trades are excluded, not just set as missing. This is OK, because we won’t do any forward-filling for the trade data.\n\ndata.tabledtplyr\n\n\n\n# Extract dates\ntrades[, date := as.Date(date_time)]\n\n# Convert time stamps to numeric format, expressed in seconds past midnight\ntrades[, time := {\n    time_elements = strsplit(exchange_time, split = \":\")\n    time_elements = do.call(rbind, time_elements)\n    time = as.numeric(time_elements[, 1]) * 3600 + \n      as.numeric(time_elements[, 2]) * 60 + \n      as.numeric(time_elements[, 3]) +\n      gmt_offset * 3600\n    list(time)}]\n\n# Delete raw time variables\ntrades[, c(\"date_time\", \"exchange_time\", \"gmt_offset\") := NULL]\n\n# Retain trades from the continuous trading sessions\ntrades &lt;- trades[time &gt; (open_time + 60) & time &lt; (close_time - 60) & \n                    (time &lt; (intraday_auction_time - 60) | \n                     time &gt; (intraday_auction_time + 3 * 60))]\n\n\n\n\ntv_trades &lt;- tv_trades |&gt; \n  separate(exchange_time, \n           into = c(\"hour\", \"minute\", \"second\"), \n           sep=\":\", \n           convert = TRUE) |&gt; \n  mutate(date = as.Date(date_time),\n         time = hour * 3600 + minute * 60 + second + gmt_offset * 3600)\n\nHaving made sure that dates and times are in the desired format, we can save space by dropping the raw time and date variables. Further, we only retain trades from the continuous trading sessions.\n\ntv_trades &lt;- tv_trades|&gt; \n  select(-c(\"date_time\", \"gmt_offset\",\"hour\",\"minute\",\"second\"))\n\ntv_trades &lt;- tv_trades |&gt;\n  filter(time &gt; hms::as_hms(\"08:01:00\"), \n         time &lt; hms::as_hms(\"16:29:00\"),\n         time &lt; hms::as_hms(\"11:59:00\") | time &gt; hms::as_hms(\"12:03:00\")) |&gt;\n  lazy_dt()\n\n\n\n\n\n\nTrade variables\nA line plot offers a good overview of price data. In the figure below, we see that the trade prices are plagued by outliers that seem to be close to zero. The out-of-sequence prices are recorded on all trading dates and at virtually all times of the day. These outliers need to be addressed before we can proceed with the analysis.\n\ndata.tabledtplyr\n\n\n\n# Plot the trade prices\ntrades |&gt; \n  ggplot(aes(x = time, y = price)) + \n  geom_line() + \n  facet_wrap(~date, ncol= 1) + \n  labs(title = \"Trade prices\",  y = \"Price\", x = \"Time (seconds past midnight)\")\n\n\n\n\n\n\n\n\n\n\n\ntv_trades |&gt;\n  as_tibble() |&gt; \n  ggplot(aes(x = hms::hms(time), y = price)) + \n  geom_line() + \n  facet_wrap(~date, ncol= 1) + \n  labs(title = \"Trade prices\",  y = \"Price\", x = \"Time (seconds past midnight)\")\n\n\n\n\n\n\n\n\n\n\n\nThere are various ways to handle outliers, but the best way is to understand them. In trade data sets, there is often information provided about the trade circumstances (for quote observations, such information is often sparse). In the current data set, the best piece of supporting information is the MMT code. MMT, short for Market Model Typology, is a rich set of flags reported for trades in Europe in recent years. For details, see the website of the Fix Trading Community.\nThe MMT code is a 14-character string, where each position corresponds to one flag. The first character specifies the type of market mechanism. For example, “1” tells us that the trade was recorded in an LOB market, “3” indicates dark pools, “4” is for off-book trading, and “5” is for periodic auctions. The second character indicates the trading mode, where, for example, continuous trading is indicated by “2”.\nAn overview of the populated values shows in the first column that the LOB market with continuous trading (“12”) is by far the most common combination remaining after applying the filters above, followed by dark pool continuous trading (“32”).\nThe low-priced trades are captured in the second column. All those trades are off-book, as indicated by the first digit being “4”. The second digit holds information about how the off-book trades are reported (“5” is for on-exchange, “6” is for off-exchange, and “7” indicates systematic internalisers).\n\ndata.tabledtplyr\n\n\n\n# Output an overview of the MMT codes\n# The function `substr` is used here to extract the first two characters of the MMT code\ntable(substr(trades[, mmt], start = 1, stop = 2), trades[, price] &lt; 100)\n\n    \n     FALSE  TRUE\n  12 27737     0\n  32  3383     0\n  3U   114     0\n  45  1221    28\n  46     0    41\n  47  1023   236\n  5U    88     0\n\n\n\n\n\ntv_trades |&gt;\n  mutate(message = str_extract(mmt, \"^.{2}\")) |&gt;\n  mutate(small_price = price &lt; 100) |&gt;\n  count(message, small_price)\n\nSource: local data table [10 x 3]\nCall:   copy(`_DT18`)[, `:=`(message = str_extract(mmt, \"^.{2}\"))][, \n    `:=`(small_price = price &lt; 100)][, .(n = .N), keyby = .(message, \n    small_price)]\n\n  message small_price     n\n  &lt;chr&gt;   &lt;lgl&gt;       &lt;int&gt;\n1 12      FALSE       27737\n2 32      FALSE        3383\n3 3U      FALSE         114\n4 45      NA              4\n5 45      FALSE        1221\n6 45      TRUE           28\n# ℹ 4 more rows\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\n\n\n\nIn this analysis we are focusing on liquidity at the exchanges. Accordingly, we use the MMT codes to filter out trades from other trading venues. Restricting the trades to continuous trading at the exchanges, we obtain price plots that are free from outliers.\n\ndata.tabledtplyr\n\n\n\n# Define a subset with continuous trades only\nLOB_continuous_trades &lt;- substr(trades[, mmt], start = 1, stop = 2) == \"12\"\n\n# Plot the prices of continuous trades\nggplot(trades[LOB_continuous_trades], \n       aes(x = time, y = price)) + \n  geom_line() + \n  facet_wrap(~date, ncol = 1) + \n  labs(title = \"Trade prices\", y = \"Price\", x = \"Time (seconds past midnight)\")\n\n\n\n\n\n\n\n\n\n\n\ntv_trades |&gt;\n  filter(str_extract(mmt, \"^.{2}\") == \"12\") |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(x = hms::hms(time), y = price)) + \n  geom_line() + \n  facet_wrap(~date, ncol = 1) + \n  labs(title = \"Trade prices\", y = \"Price\", x = \"Time (seconds past midnight)\")\n\n\n\n\n\n\n\n\n\n\n\nFurther detective work reveals that the trade price outliers are not erroneous, they are just stated in pounds rather than in pence. This is clear because the outliers are priced 100 times lower than the other trades. Apparently, some off-exchange trades follow a different price reporting convention.\n\n# View trades with low prices\ntrades[price &lt; 100]\n\n     ticker    price size            mmt       date     time\n  1:  PRU.L 14.80000  635 47------MP---- 2021-06-07 29102.70\n  2:  PRU.L 14.86625  400 45------MP---- 2021-06-07 31868.09\n  3:  PRU.L 14.86062  204 45------MP---- 2021-06-07 32331.08\n  4:  PRU.L 14.85125  540 45------MP---- 2021-06-07 32466.05\n  5:  PRU.L 14.83875 1042 45------MP---- 2021-06-07 39062.65\n ---                                                        \n301:  PRU.L 14.88500    1 47------MP---- 2021-06-11 57966.07\n302:  PRU.L 14.87000  434 47------MP---- 2021-06-11 58540.00\n303:  PRU.L 14.91500  870 47------MP---- 2021-06-11 59286.00\n304:  PRU.L 14.92000 1463 46------MP---- 2021-06-11 59336.00\n305:  PRU.L 14.92000 1463 46------MP---- 2021-06-11 59336.00\n\n\nQuirks in the data are not unusual, and if they go unnoticed they can have strong impact on the market quality measures. The take-away from the outlier analysis is that there is often an explanation for why their prices are off. It is not always as straightforward as here, but it is worthwhile to try to find out what the cause of the deviations is. Other potential explanations are that the time stamps are off (possibly due to delayed reporting) or that the pricing is not done at the market (but in accordance to some derivative contract).\nFor the analysis below, we want to focus on exchange trades. Accordingly, we filter out all trades that are not from the on-exchange continuous trading sessions.\n\ndata.tabledtplyr\n\n\n\n# Retain continuous LOB trades only\ntrades &lt;- trades[LOB_continuous_trades]\n\n\n\n\ntv_trades &lt;- tv_trades |&gt;\n  filter(str_extract(mmt, \"^.{2}\") == \"12\")\n\n\n\n\n\n\nMatching trades to quotes\nTo evaluate the cost of trading, we want to compare the trade price to the fundamental value at the time of trade, as implied by the bid-ask quotes.\nThe objective of matching trades and quotes is to obtain the quotes that prevailed just before the trade. This is straightforward in settings where the trades and quotes are recorded at the same point, such that they are correctly sequenced. In other settings, the timestamps may need to be adjusted due to reporting latencies, or the trade size needs to be matched to changes in quoted depth (Jurkatis, 2021)15.\nFor US data, the most common approach is to match trades to the last quotes available in the millisecond or microsecond before the trade, as prescribed by Holden and Jacobsen (2014)16. There is, however, an active debate which timestamp to use. Several recent papers advocate the use of participant time stamps in trade and quote matching, see references about US timestamps above.\nIn lack of specific guidance for stocks traded in the UK, we match trades to quotes prevailing just before the trade. Based on the assumption that the combined liquidity from LSE and TQE offers the best fundamental value approximation, we match trades from all venues to the EBBO.\nThe merge function in data.table can be called as above by merge(dt1, dt2) (for two data.tables named dt1 and dt2), or simply dt1[dt2]. We use the latter approach here because it allows us to specify what to do when the timestamps do not match exactly. The option roll = TRUE specifies that each observation in trades should be matched to the quotes_ebbo observation with the latest timestamp that is equal or earlier than the trade timestamp. However, we don’t want equal matches, because the quote observation should always be before the trade. To avoid matching to contemporaneous quotes, which may be updated to reflect the impact of the trade itself, we add one microsecond to the quote timestamps before running the merge function. For further understanding and illustration of the rolling join, we refer to the blog post by R-Bloggers.\n\ndata.tabledtplyr\n\n\nFor the sake of completeness, you can load the EBBO quote data which we stored at an intermediate step above as follows.\n\nquotes_ebbo &lt;- fread(file = \"quotes_ebbo.csv\")\n\n\n# Adjust quote time stamps by one microsecond\nsetnames(quotes_ebbo, old = \"time\", new = \"quote_time\")\nquotes_ebbo[, time := quote_time + 0.000001]\n\n# Sort trades and quotes (this specifies the matching criteria for the merge function)\nsetkeyv(trades, cols = c(\"date\", \"time\"))\nsetkeyv(quotes_ebbo, cols = c(\"date\", \"time\"))\n\n# Match trades to quotes prevailing at the time of trade \n# The rolling is done only for the last of the matching variables, in this case \"time\"\n# `mult = \"last\"` specifies that if there are multiple matches with identical timestamps, \n# the last match is retained\ntrades &lt;- quotes_ebbo[trades, roll = TRUE, mult = \"last\"]\n\n\n\nFor the sake of completeness, you can load the EBBO quote data which we stored at an intermediate step above as follows.\n\ntv_quotes_ebbo &lt;- read_csv(\"tv_quotes_ebbo.csv\")\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  mutate(time = time + 0.000001) |&gt;\n  arrange(date, time) |&gt; \n  as_tibble()\n\ntv_trades &lt;- tv_trades |&gt;\n  arrange(date, time) |&gt; \n  as_tibble()\n\ntv_trades &lt;- tv_trades |&gt;\n  left_join(tv_quotes_ebbo, join_by(date, closest(time &gt;= time)), suffix = c(\"\", \"_quotes\")) |&gt;\n  lazy_dt()\n\n\n\n\n\n\nFurther screening\nAs some trades may be matched to crossed or locked quotes, another round of data screening is required. Because such quotes are not considered reliable, we do not include those trades in the liquidity measurement. Furthermore, if there are trades that could not be matched to any quotes, or that lack information on price or size, they should be excluded too.\nThe output shows that 88.5% of the trades at LSE are eligible for the liquidity analysis, and 96.8% of the trades at TQE. The criterion that drives virtually all exclusions in the sample is the locked quotes.\n\ndata.tabledtplyr\n\n\n\n# Flag trades that should be included\ntrades[, include := !crossed & !locked & !large & !is.na(size) & size &gt; 0 &  \n                    !is.na(price) & price &gt; 0 & !is.na(midpoint) & midpoint &gt; 0]\n\n# Report trade filtering stats\ntrades_filters &lt;- trades[, \n    list(crossed = mean(crossed, na.rm = TRUE),\n         locked = mean(locked, na.rm = TRUE),\n         large = mean(large, na.rm = TRUE),\n         no_price = mean(is.na(price) | price == 0),\n         no_size = mean(is.na(size) | size == 0),\n         no_quotes = mean(is.na(midpoint) | midpoint &lt;= 0),\n         included = mean(include)), \n    by = \"ticker\"]\n\ntrades_filters[,\n  lapply(.SD * 100, round, digits = 2), \n  .SDcols = c(\"crossed\", \"locked\", \"large\", \"no_price\", \"no_size\", \"no_quotes\", \"included\"),\n  by = \"ticker\"]\n\n    ticker crossed locked large no_price no_size no_quotes included\n1:   PRU.L    0.30  11.20     0        0       0      0.00    88.50\n2: PRUl.TQ    0.07   3.05     0        0       0      0.02    96.85\n\n\n\n\n\ntv_trades &lt;- tv_trades |&gt;\n  mutate(include = !crossed & !locked & !large & !is.na(size) & size &gt; 0 &\n           !is.na(price) & price &gt; 0 & !is.na(midpoint) & midpoint &gt; 0)\n\ntrades_filters &lt;- tv_trades |&gt;\n  group_by(ticker) |&gt;\n  summarize(\n    crossed = mean(crossed, na.rm = TRUE),\n    locked = mean(locked, na.rm = TRUE),\n    large = mean(large, na.rm = TRUE),\n    no_price = mean(is.na(price) | price == 0),\n    no_size = mean(is.na(size) | size == 0),\n    no_quotes = mean(is.na(midpoint) | midpoint &lt;= 0),\n    included = mean(include)\n  )\n\n# Round percentages and display\ntrades_filters |&gt;\n  mutate(across(crossed:included, ~round(. * 100, digits = 2))) |&gt;\n  select(ticker, crossed, locked, large, no_price, no_size, no_quotes, included) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 8\n  ticker  crossed locked large no_price no_size no_quotes included\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 PRU.L      0.3   11.2      0        0       0      0        88.5\n2 PRUl.TQ    0.07   3.05     0        0       0      0.02     96.8\n\n\n\n\n\n\n\nDirection of trade\nIn empirical market microstructure, we often need to determine the direction of trade. If a trade happens following a buy market order, it is said to be buyer-initiated, and vice versa.\nThe most common tool to determine the direction of trade is the algorithm prescribed by Lee and Ready (1991)17. They primarily recommend the quote rule, saying that a trade is buyer-initiated if the trade price is above the prevailing midpoint, and seller-initiated if it is below. When the price equals the midpoint, Lee and Ready propose the tick rule. It specifies that a trade is buyer-initiated (seller-initiated) if the price is higher (lower) than the closest previous trade with a different price.\nThe quote rule is straightforward to implement using the sign function, which returns +1 when the price deviation from the midpoint is positive and -1 if it is negative. The tick rule, in contrast, requires several steps of code. We create a new data.table, named price_tick, which in addition to date and time observations for each trade, indicates whether a trade is priced higher (+1), lower (-1), or the same (0) as the previous one. We then exclude all trades that don’t imply a price change. Finally, we merge the price_tick and the trades objects, such that each trade is associated with the latest previous price change.\nThe direction of trade can now be determined, using primarily the quote rule, and secondarily the tick rule. The output shows that, in this sample, seller-initiated trades are somewhat more common than buyer-initiated.\n\ndata.tabledtplyr\n\n\n\n# Quote rule (the trade price is compared to the midpoint at the time of the trade)\ntrades[, quote_diff := sign(price - midpoint)]\n\n# Tick rule (each trade is matched to the closest preceding trade price change)\nprice_tick &lt;- data.table(date = trades$date,\n                         time = trades$time,\n                         price_change = c(NA, sign(diff(trades$price))))\n\n# Retain trades that imply a trade price change\nprice_tick &lt;- price_tick[price_change != 0]\n\n# Merge trades and trade price changes\nsetkeyv(trades, c(\"date\", \"time\"))\nsetkeyv(price_tick, c(\"date\", \"time\"))\ntrades &lt;- price_tick[trades, roll = TRUE, mult = \"last\"]\n\n# Apply the Lee-Ready (1991) algorithm\ntrades[, dir := {\n  # 1st step: quote rule\n  direction = quote_diff\n  # 2nd step: tick rule\n  no_direction = is.na(direction) | direction == 0\n    direction[no_direction] = price_change[no_direction] \n    \n    list(direction)},\n    by = \"date\"]\n\ntable(trades$ticker, trades$dir)\n\n         \n             -1     1\n  PRU.L   10854  8821\n  PRUl.TQ  4222  3840\n\n\n\n\nFirst, we compute the sign of the price change based on the quote rule (the trade price is compared to the midpoint at the time of the trade).\n\ntv_trades &lt;- tv_trades |&gt;\n  mutate(quote_diff = sign(price - midpoint))\n\nSecond, each trade is matched to the closest preceding trade price change.\n\nprice_tick &lt;- tv_trades |&gt;\n  transmute(date, time, price_change = c(NA, sign(diff(price)))) |&gt;\n  filter(price_change != 0) |&gt; \n  group_by(date,time)|&gt; \n  slice(n()) |&gt; \n  ungroup() |&gt;\n  as_tibble()\n\nWe merge the price_tick and the trades objects, such that each trade is associated with the latest previous price change.\n\ntv_trades &lt;- tv_trades |&gt;\n  as_tibble() |&gt;\n  left_join(price_tick, join_by(date, closest(time&gt;=time)), suffix = c(\"\", \"_tick\")) |&gt;\n  fill(price_change, .direction = \"up\") |&gt;\n  lazy_dt()\n\nFinally, we apply the Lee-Ready (1991) algorithm\n\ntv_trades &lt;- tv_trades |&gt;\n  group_by(date) |&gt;\n  mutate(dir = case_when(!is.na(quote_diff) & quote_diff != 0 ~ quote_diff,\n                         TRUE ~ price_change)\n  ) |&gt;\n  ungroup() |&gt;\n  lazy_dt()\n\ntv_trades |&gt; \n  count(ticker, dir) |&gt; \n  as_tibble()\n\n# A tibble: 4 × 3\n  ticker    dir     n\n  &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;\n1 PRU.L      -1 10854\n2 PRU.L       1  8821\n3 PRUl.TQ    -1  4222\n4 PRUl.TQ     1  3840"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#liquidity-measures-1",
    "href": "blog/tidy-market-microstructure/index.html#liquidity-measures-1",
    "title": "Tidy Market Microstructure",
    "section": "Liquidity measures",
    "text": "Liquidity measures\n\nEffective spread\nThe relative effective spread is defined as \\(\\text{effective}\\_\\text{spread}^{rel} = 2 D(P - M)/M\\), where \\(P\\) is the trade price and \\(D\\) is the direction of trade. The multiplication by two is to make the effective spread comparable to the quoted spread. As is common in the literature, we use trade size weights when calculating the average effective spread. We multiply the effective spread by 10,000 to express it in basis points.\nWe also calculate the dollar volume, which is simply \\(\\text{dollar}\\_\\text{volume} = P \\times \\text{size}\\), with \\(\\text{size}\\) denoting the trade size. The trading volume is commonly referred to as liquidity in popular press, but in academic papers it rarely used as a liquidity measure. One reason for that is that spikes in volume are usually due to news rather than liquidity shocks. We still include trading volume here, because it often included as a control variable in microstructure event studies. To express the dollar volume in million GBP, we multiply it by \\(10^{-8}\\).\nConsistent with the quote-based measures, the effective spread indicates that PRU is more liquid at LSE than at TQE. The LSE effective spread is 4.18 bps, more than 25% lower than the LSE quoted spread at 5.70 bps. The large difference may be due to trades executed at prices better than visible in the LOB, the effective spreads benchmarked to the consolidated midpoint rather than the respective venue midpoint, or the calculation of the effective spread at times of trade rather than continuously throughout the day. If investors time their trades to reduce trading costs, it makes sense that the effective spread is on average lower than the quoted spread. The interested reader can find out which of these differences drive the wedge between the two measures, by altering the way the average quoted spread is obtained.\n\ndata.tabledtplyr\n\n\n\n# Measure average liquidity for each stock-day\n\n# First order the table\nsetorder(trades, ticker, date, time)\n\ntrades_liquidity &lt;- trades[include == TRUE, {             \n  list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n                                      w = size),\n       volume = sum(price * size))\n  },\n    by = c(\"ticker\", \"date\")]\n\n# Output liquidity measures, averaged across the five trading days for each ticker\ntrades_liquidity[, \n    list(effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n         volume = round(mean(volume * 1e-8), digits = 2)), \n    by = \"ticker\"]\n\n    ticker effective_spread volume\n1:   PRU.L             4.18  13.64\n2: PRUl.TQ             4.52   3.19\n\n\n\n\n\ntv_trades &lt;- tv_trades |&gt;\n  arrange(ticker, date, time)\n\ntv_trades_liquidity &lt;- tv_trades |&gt;\n  filter(include == TRUE) |&gt;\n  group_by(ticker, date) |&gt;\n  summarize(\n    effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, w = size),\n    volume = sum(price * size),\n    .groups = \"drop\"\n  )\n\n# Output liquidity measures, averaged across the five trading days for each ticker\ntv_trades_liquidity |&gt;\n  group_by(ticker) |&gt;\n  summarize(\n    effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n    volume = round(mean(volume * 1e-8), digits = 2)\n  ) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 3\n  ticker  effective_spread volume\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1 PRU.L               4.18  13.6 \n2 PRUl.TQ             4.52   3.19\n\n\n\n\n\nIs the midpoint really a good proxy for the fundamental value of the security? Hagströmer (2021)18 shows that the reliance on the midpoint introduces bias in the effective spread. The bias is because traders are more likely to buy the security when the fundamental value is closer to the ask price, and more inclined to sell when the true value is close to the bid price. To capture this, we also consider the weighted midpoint, which is a proxy that allows the true value to lie anywhere between the best bid and ask prices. It is defined as \\(M_w=(P^BQ^A+P^AQ^B) / (Q^A+Q^B)\\), and denoted midpoint_w in the code below. The weighted midpoint equals the midpoint when \\(Q^A=Q^B\\).\nIn line with Hagströmer (2021), we find that the weighted midpoint version of the effective spread is lower than the conventional measure. At TQE, the difference is about 15%. This is noteworthy, as it overturns the previous finding that the TQE is less liquid than the LSE. Although the quoted spread at the TQE is wider, the results indicate that the traders at TQE are better at timing their trades in accordance to the fundamental value.\nThe reason for that the choice of effective spread measure matters for PRU is that its trading is constrained by the tick size (the quoted spread is almost always one or two ticks). Stocks that are not tick-constrained tend to show smaller differences between the effective spread measures.\n\ndata.tabledtplyr\n\n\n\n# Measure average liquidity for each stock-day, including the effective spread \n# relative to the weighted midpoint\ntrades_liquidity &lt;- trades[include == TRUE, {\n  midpoint_w = (best_bid_price * best_ask_depth + best_ask_price * best_bid_depth) / \n                 (best_ask_depth + best_bid_depth)\n    list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n                                          w = size,\n                                          na.rm = TRUE),\n         effective_spread_w = weighted.mean(2 * dir * (price - midpoint_w) / midpoint, \n                                            w = size,\n                                            na.rm = TRUE))},\n    by = c(\"ticker\", \"date\")]\n\n# Output liquidity measures, averaged across the five trading days for each ticker\ntrades_liquidity[, \n    list(effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n         effective_spread_w = round(mean(effective_spread_w * 1e4), digits = 2)), \n    by = \"ticker\"]\n\n    ticker effective_spread effective_spread_w\n1:   PRU.L             4.18               4.08\n2: PRUl.TQ             4.52               3.94\n\n\n\n\n\ntv_trades_liquidity &lt;- tv_trades |&gt;\n  filter(include == TRUE) |&gt;\n  group_by(ticker, date) |&gt;\n  mutate(midpoint_w = (best_bid_price * best_ask_depth + best_ask_price * best_bid_depth) /\n           (best_ask_depth + best_bid_depth)) |&gt;\n  summarize(\n    effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, w = size, \n                                     na.rm = TRUE),\n    effective_spread_w = weighted.mean(2 * dir * (price - midpoint_w) / midpoint, w = size,\n                                       na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ntv_trades_liquidity |&gt;\n  group_by(ticker) |&gt;\n  summarize(\n    effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n    effective_spread_w = round(mean(effective_spread_w * 1e4), digits = 2)\n  ) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 3\n  ticker  effective_spread effective_spread_w\n  &lt;chr&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n1 PRU.L               4.18               4.08\n2 PRUl.TQ             4.52               3.94\n\n\n\n\n\n\n\nPrice impact and realized spreads\nThe price impact is defined as \\(\\text{price}\\_\\text{impact}^{rel} = 2 D(M_{t+\\Delta}-M)/M\\), where \\(M_{t+\\Delta}\\) is the midpoint holding \\(\\Delta\\) seconds after the trade. It thus captures the signed price change following a trade.\nThe realized spread is defined as \\(\\text{realized}\\_\\text{spread}^{rel} = 2 D(P - M_{t+\\Delta})/M\\). Note that the price impact and the realized spread may be viewed as two components of the effective spread, as \\(\\text{effective}\\_\\text{spread}^{rel}=\\text{price}\\_\\text{impact}^{rel}+\\text{realized}\\_\\text{spread}^{rel}\\).\nThe choice of horizon (\\(\\Delta\\)) for the price impact and realized spread is arbitrary but can be important. The most common choice has traditionally been five minutes, but in modern markets that may even exceed the holding period of short-term investors. For a detailed discussion of this parameter, see Conrad & Wahal (2020).19\nThe code below prepares the variables needed to calculate the realized spread and the price impact at a 60 second horizon (setting \\(\\Delta=60\\)). It loads the consolidated quotes, subtracts 60 seconds to their timestamps, and merges them with the trades. In this way, we have trades that are matched to quotes prevailing just before the trade (from the previous matching) as well as to future quotes.\n\ndata.tabledtplyr\n\n\n\n# Adjust quote time stamps by 60 seconds\nquotes_ebbo &lt;- quotes_ebbo[, time := quote_time - 60]\n\n# Rename variables to indicate that they correspond to quotes 1 minute after the trade\n# The function `paste0` adds the suffix \"_1min\" to each variable\nsetnames(quotes_ebbo, \n         old = c(\"midpoint\", \"crossed\", \"locked\", \"large\"), \n         new = paste0(c(\"midpoint\", \"crossed\", \"locked\", \"large\"), \"_1min\"))\n\n# Merge trades and quotes\nsetkeyv(quotes_ebbo, cols = c(\"date\", \"time\"))\nsetkeyv(trades, cols = c(\"date\", \"time\"))\ntrades &lt;- quotes_ebbo[trades, roll = TRUE, mult = \"last\"]\n\n# Flag valid future quotes\ntrades[, include_1min := !crossed_1min & !locked_1min & !large_1min]\n\n\n\n\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt;\n  mutate(time = time - 60)\n\n# Rename variables to indicate that they correspond to quotes 1 minute after the trade\n# The function `paste0` adds the suffix \"_1min\" to each variable\ntv_quotes_ebbo &lt;- tv_quotes_ebbo |&gt; \n  mutate(across(c(midpoint, crossed, locked, large), \n                ~., \n                .names=\"{col}_1min\"))\n\n# Merge trades and quotes\ntv_trades &lt;- tv_trades |&gt;\n  as_tibble() |&gt;\n  left_join(tv_quotes_ebbo |&gt; select(date, time, contains(\"_1min\")), \n            join_by(date, closest(time&gt;=time)), suffix = c(\"\", \"_quotes\")) |&gt;\n  arrange(ticker, date, time) |&gt;\n  group_by(ticker, date) |&gt;\n  fill(midpoint_1min, crossed_1min, locked_1min, large_1min, .direction = \"down\")\n\n# Flag valid future quotes\ntv_trades &lt;- tv_trades |&gt;\n  mutate(include_1min = !crossed_1min & !locked_1min & !large_1min) |&gt;\n  lazy_dt()\n\n\n\n\nThe next step calculates the liquidity measures. The results show that the price impact exceeds the effective spread. The realized spread is then negative, implying that liquidity providers on average lose money. How can that be? Wouldn’t the market makers who incur losses simply refrain from trading? It could be that the 60-second evaluation does not reflect the market makers’ trading horizon. More likely, perhaps, is that not all traders who post limit orders are in the market to earn the bid-ask spread. It could also be liquidity traders or informed investors who use limit orders to save on transaction costs. Relative to paying the effective spread, earning a negative realized spread may well be an attractive alternative.\n\ndata.tabledtplyr\n\n\n\n# Measure trade-based liquidity\neffective_spread_decomposition &lt;- trades[include & include_1min, {\n    list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n                                  w = size),\n         price_impact = weighted.mean(2 * dir * (midpoint_1min - midpoint) / midpoint, \n                                 w = size),\n         realized_spread = weighted.mean(2 * dir * (price - midpoint_1min) / midpoint, \n                                 w = size))}, \n    by = c(\"ticker\", \"date\")]\n\n# Output the average liquidity measures\neffective_spread_decomposition[, \n    list(effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n         price_impact = round(mean(price_impact * 1e4), digits = 2),\n         realized_spread = round(mean(realized_spread * 1e4), digits = 2)), \n    by = \"ticker\"]\n\n    ticker effective_spread price_impact realized_spread\n1:   PRU.L             4.18         4.75           -0.57\n2: PRUl.TQ             4.52         6.13           -1.61\n\n\n\n\n\ntv_effective_spread_decomposition &lt;- tv_trades |&gt;\n  filter(include == TRUE & include_1min == TRUE) |&gt;\n  group_by(ticker, date) |&gt;\n  summarize(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n                                             w = size),\n              price_impact = weighted.mean(2 * dir * (midpoint_1min - midpoint) / midpoint, \n                                           w = size),\n              realized_spread = weighted.mean(2 * dir * (price - midpoint_1min) / midpoint, \n                                              w = size),\n              .groups = \"drop\")\n\ntv_effective_spread_decomposition |&gt;\n  group_by(ticker) |&gt;\n  summarize(\n    effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n    price_impact = round(mean(price_impact * 1e4), digits = 2),\n    realized_spread = round(mean(realized_spread * 1e4), digits = 2)\n  ) |&gt; \n  as_tibble()\n\n# A tibble: 2 × 4\n  ticker  effective_spread price_impact realized_spread\n  &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1 PRU.L               4.18         4.75           -0.57\n2 PRUl.TQ             4.52         6.13           -1.61\n\n\n\n\n\nThe accuracy of the effective spread decomposition may be improved by replacing the midpoint by the weighted midpoint. We leave the implementation of that to the interested reader."
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#equispaced-returns",
    "href": "blog/tidy-market-microstructure/index.html#equispaced-returns",
    "title": "Tidy Market Microstructure",
    "section": "Equispaced returns",
    "text": "Equispaced returns\nTo get started, we again rely on the consolidated quote data. For the applications below, we drop all LOB variables except the midpoint and the filtering flags.\n\ndata.tabledtplyr\n\n\n\nquotes_ebbo &lt;- fread(file = \"quotes_ebbo.csv\")\n\n\n# Delete variables\nquotes_ebbo[, c(\"best_bid_price\", \"best_bid_depth\", \"best_ask_price\", \n                \"best_ask_depth\", \"duration\") := NULL]\n\n\n\n\ntv_quotes_ebbo &lt;- read_csv(\"tv_quotes_ebbo.csv\") |&gt; \n  select(date, time, midpoint, crossed, locked , large) |&gt;\n  lazy_dt()\n\n\n\n\nTo obtain equispaced observations, we create a time grid with one observation per second. As above, we exclude the first and last minute when setting the opening and closing times. We use the expand.grid function to create a grid of second-by-second observations for each sample date, and then convert it to a data.table.\nFirst we create an equispaced time grid.\n\nsampling_freq  &lt;- 1\nopen_time &lt;- 8 * 3600\nclose_time &lt;- 16.5 * 3600\n\nThe function seq creates a sequence of discrete numbers the by option defines the increment of the sequence, which is here 1 second\n\ntime_grid &lt;- seq(from = open_time + 60, to = close_time - 60, by = sampling_freq)\n\n\ndata.tabledtplyr\n\n\n\n# Repeat the time grid for each date and sort it by date and time\ndates &lt;- unique(quotes_ebbo$date)\nquotes_1sec &lt;- expand.grid(date = dates, time = time_grid)\n\n# Make it a data.table\nquotes_1sec &lt;- data.table(quotes_1sec, key = c(\"date\", \"time\"))\n\n# View the time grid\nquotes_1sec\n\n              date  time\n     1: 2021-06-07 28860\n     2: 2021-06-07 28861\n     3: 2021-06-07 28862\n     4: 2021-06-07 28863\n     5: 2021-06-07 28864\n    ---                 \n152401: 2021-06-11 59336\n152402: 2021-06-11 59337\n152403: 2021-06-11 59338\n152404: 2021-06-11 59339\n152405: 2021-06-11 59340\n\n\n\n\n\ntv_quotes_1sec &lt;- expand_grid(\n  date = tv_quotes_ebbo |&gt; pull(date) |&gt;  unique(), \n  time = time_grid\n)\n\n\n\n\nFor each point in the grid, we want to find the prevailing quote. That is, the last quote update preceding or coinciding with the time in question. Because quotes are valid until cancelled, the same quote may be matched to several consecutive seconds. To avoid bid-ask bounce in the market efficiency measures, we use the midpoint rather than the bid or ask prices.\nIn the same way as we matched trades to quotes above, we use the rolling merge to match the grid times to quotes. Once the merge is done, it is straightforward to set prices that are flagged as problematic to NA, and to calculate returns. We use log-diff returns expressed in basis points (i.e., multiplied by 10,000).\n\ndata.tabledtplyr\n\n\n\n# Sort the quotes\nsetkeyv(quotes_ebbo, cols = c(\"date\", \"time\"))\n\n# Merge the time grid with the quotes\nquotes_1sec &lt;- quotes_ebbo[quotes_1sec, roll = TRUE]\n\n# Set problematic quotes to NA\nquotes_1sec$midpoint[quotes_1sec$crossed|quotes_1sec$locked| quotes_1sec$large] &lt;- NA\n\n# Calculate returns, expressed in basis points\n# The function `diff` returns the first difference of a time series, which in this case \n# is the log of the midpoint. \n# For each date, a leading `NA` is added to make the resulting vector fit the number of \n# observations in `quotes_1sec`. \nquotes_1sec[, return := 1e4 * c(NA, diff(log(midpoint))), by = \"date\"]\n\n\n\n\ntv_quotes_1sec &lt;- tv_quotes_1sec |&gt; \n  left_join(tv_quotes_ebbo |&gt; as_tibble(), join_by(date, closest(time&gt;=time)), suffix = c(\"\", \"_quotes\")) |&gt;\n  mutate(midpoint = na_if(midpoint, locked|crossed|large)) |&gt;\n  group_by(date) |&gt;\n  mutate(return = 1e4 * c(NA, diff(log(midpoint)))) |&gt;\n  lazy_dt()"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#efficiency-and-volatility-measures",
    "href": "blog/tidy-market-microstructure/index.html#efficiency-and-volatility-measures",
    "title": "Tidy Market Microstructure",
    "section": "Efficiency and volatility measures",
    "text": "Efficiency and volatility measures\n\nReturn autocorrelation and realized volatility\nWe obtain the return autocorrelation by applying the cor function to returns and lagged returns. The latter are generated using the shift function with one lag. We account for missing values by specifying the option use = \"complete.obs\". The return autocorrelation comes out at 0.02, a very low number. This is not surprising, as the sample stock is a large firm with high trading activity – two characteristics associated with high market efficiency.\nRealized volatility is defined as the mean of squared returns. We also obtain the return variance to be able to calculate the variance ratio below. Although their definitions differ, the realized volatility and the return variance are almost identical in this data set, 0.32. This is because the mean return is close to zero.\n\ndata.tabledtplyr\n\n\n\n# Measure market efficiency and volatility \nefficiency_1sec &lt;- quotes_1sec[order(date, time), \n  list(return_corr_1sec = cor(return, shift(return, n = 1, type = \"lag\"), \n                              use = \"complete.obs\"),\n         realized_vol_1sec = mean(return^2, na.rm = TRUE),\n         return_var_1sec = var(return, na.rm = TRUE)), \n    by = \"date\"]\n\n# Output an overview of the average efficiency and volatility \nefficiency_1sec[, \n  list(return_corr_1sec = round(mean(return_corr_1sec), digits = 2),\n       realized_vol_1sec = round(mean(realized_vol_1sec), digits = 2),\n       return_var_1sec = round(mean(return_var_1sec), digits = 2))]\n\n   return_corr_1sec realized_vol_1sec return_var_1sec\n1:             0.02              0.32            0.32\n\n\n\n\n\ntv_efficiency_1sec &lt;- tv_quotes_1sec |&gt;\n  arrange(date, time) |&gt;\n  group_by(date) |&gt;\n  summarize(return_corr_1sec = cor(return, lag(return), use = \"complete.obs\"),\n            vol_1sec = mean(return^2, na.rm = TRUE),\n            return_var_1sec = var(return, na.rm = TRUE))\n\ntv_efficiency_1sec |&gt;\n  summarize(across(everything(), \n                   ~round(mean(.), digits = 2)))\n\nSource: local data table [1 x 4]\nCall:   `_DT26`[order(date, time)][, .(return_corr_1sec = cor(return, \n    shift(return, type = \"lag\"), use = \"complete.obs\"), vol_1sec = mean(return^2, \n    na.rm = TRUE), return_var_1sec = var(return, na.rm = TRUE)), \n    keyby = .(date)][, .(date = round(mean(date), digits = 2), \n    return_corr_1sec = round(mean(return_corr_1sec), digits = 2), \n    vol_1sec = round(mean(vol_1sec), digits = 2), return_var_1sec = round(mean(return_var_1sec), \n        digits = 2))]\n\n  date       return_corr_1sec vol_1sec return_var_1sec\n  &lt;date&gt;                &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 2021-06-09             0.02     0.32            0.32\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\n\n\n\n\n\nVariance ratios\nThe variance ratio is defined as \\(var\\_ratio_{\\tau_{1}, \\tau_{2}} = (Var(R_{\\tau_{1}}) \\tau_2) / (Var(R_{\\tau_{2}})  \\tau_1)\\), where \\(\\tau_{1}\\) and \\(\\tau_{2}\\) are two return sampling frequencies, and \\(Var(R_{\\tau_{i}})\\) is the variance of returns sampled at frequency \\(\\tau_{i}\\). Under market efficiency, the variance ratio should be equal to one. Negative and positive deviations from unity are due to frictions. It is usually the absolute deviation from unity that is used as an efficiency measure.\nWe measure the variance ratio for 1-second and 10-second returns. To get the 10-second return variance, the first step is to obtain the 10-second price grid. To do so, we simply take a subset of the grid obtained for the 1-second frequency. We then proceed with the calculation of returns and the return variance in the same way as above.\n\nsampling_freq &lt;- 10 # 10 second grid\ntime_grid &lt;- seq(from = open_time + 60, to = close_time - 60, by = sampling_freq)\n\n\ndata.tabledtplyr\n\n\n\n# Subset the 1-second price grid to get the 10-second price grid \nquotes_10sec &lt;- quotes_1sec[time %in% time_grid,]\n\n# Calculate returns at the 10-second frequency, expressed in basis points\nquotes_10sec[, return := 1e4 * c(NA, diff(log(midpoint))), by = \"date\"]\n\n# Calculate the return variance at the 10-second frequency, daily\nefficiency_10sec &lt;- quotes_10sec[, \n                      list(return_var_10sec = var(return, na.rm = TRUE)), \n                      by = \"date\"]\n\n\n\n\ntv_quotes_10sec &lt;- tv_quotes_1sec |&gt;\n  inner_join(tibble(time = time_grid)) |&gt;\n  group_by(date) |&gt;\n  mutate(return = 1e4 * c(NA, diff(log(midpoint))))\n\nJoining, by = \"time\"\n\ntv_efficiency_10sec &lt;- tv_quotes_10sec |&gt;\n  group_by(date) |&gt;\n  summarize(return_var_10sec = var(return, na.rm = TRUE))\n\n\n\n\nFinally, we merge the 10-second return variance with the 1-second frequency efficiency measures and calculate the variance ratio. The output shows that the 10-second return variance is slightly higher than ten times the 1-second return variance, resulting in a variance ratio that exceeds the efficiency benchmark (unity) by 8%.\n\ndata.tabledtplyr\n\n\n\n# Merge the efficiency measures of different return sampling frequencies\nefficiency &lt;- efficiency_1sec[efficiency_10sec]\n\n# Obtain the variance ratio\nefficiency[, var_ratio := return_var_10sec / (10 * return_var_1sec)]\n\n# Output an overview of the average variance ratio\nefficiency[, \n  list(return_var_1sec = round(mean(return_var_1sec), digits = 3),\n       return_var_10sec = round(mean(return_var_10sec), digits = 3),\n       var_ratio = round(mean(var_ratio), digits = 3))]\n\n   return_var_1sec return_var_10sec var_ratio\n1:           0.318            3.442     1.081\n\n\n\n\n\ntv_efficiency &lt;- tv_efficiency_1sec |&gt;\n  inner_join(tv_efficiency_10sec) |&gt;\n  mutate(var_ratio = return_var_10sec / (10 * return_var_1sec)) |&gt;\n  lazy_dt()\n\nJoining, by = \"date\"\n\ntv_efficiency |&gt;\n  summarize(across(c(return_var_1sec, return_var_10sec, var_ratio), ~round(mean(.), digits = 3)))\n\nSource: local data table [1 x 3]\nCall:   `_DT29`[, .(return_var_1sec = round(mean(return_var_1sec), digits = 3), \n    return_var_10sec = round(mean(return_var_10sec), digits = 3), \n    var_ratio = round(mean(var_ratio), digits = 3))]\n\n  return_var_1sec return_var_10sec var_ratio\n            &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1            0.32             3.44      1.08\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results"
  },
  {
    "objectID": "blog/tidy-market-microstructure/index.html#footnotes",
    "href": "blog/tidy-market-microstructure/index.html#footnotes",
    "title": "Tidy Market Microstructure",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMenkveld, A. J., et al. (342 coauthors) (2023). Non-standard errors. Forthcoming in Journal of Finance. https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=3961574↩︎\nAmihud, Y. (2002). Illiquidity and stock returns: Cross-section and time-series effects. Journal of Financial Markets, 5(1), 31-56. https://doi.org/10.1016/S1386-4181(01)00024-6↩︎\nJahan-Parvar, M. R., & Zikes, F. (2023). When Do Low-Frequency Measures Really Measure Effective Spreads? Evidence from Equity and Foreign Exchange Markets. Review of Financial Studies, 36(10), 4190-4232. https://doi.org/10.1093/rfs/hhad028↩︎\nCampbell, J. Y., Lo, A. W., & MacKinlay, A. C. (1998). The Econometrics of Financial Markets. Princeton University Press.↩︎\nFoucault, T., Pagano, M., & Röell, A. (2013). Market liquidity: Theory, evidence, and policy. Oxford University Press, USA.↩︎\nHasbrouck, J. (2007). Empirical market microstructure: The institutions, economics, and econometrics of securities trading. Oxford University Press.↩︎\nA previous guide to microstructure programming (in SAS) is provided by Boehmer, Broussard and Kallunki (2002): Boehmer, E., Broussard, J. P., & Kallunki, J. P. (2002). Using SAS in financial research. SAS Publishing.↩︎\nPart of the data.table speed advantage is that it uses parallel processing by default. Consequently, one has to be cautious to use it in parallelization, as improper specification may lead to over-parallelization and undermine the performance benefits.↩︎\nScheuch, C., Voigt, S., & Weiss P. (2023). Tidy finance with {R}. Chapman and Hall/CRC. https://doi.org/10.1201/b23237 and https://tidy-finance.org/↩︎\nBartlett, R. P., & McCrary, J. (2019). How rigged are stock markets? Evidence from microsecond timestamps. Journal of Financial Markets, 45, 37-60. https://doi.org/10.1016/j.finmar.2019.06.003↩︎\nHolden, C. W., Pierson, M., & Wu, J. (2023). In the blink of an eye: Exchange-to-SIP latency and trade classification accuracy. Working paper. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441422↩︎\nSchwenk-Nebbe, S. (2022). The participant timestamp: Get the most out of TAQ data. Working paper. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3984827↩︎\nHagströmer, B., & Menkveld, A. J. (2023). Trades, quotes, and information shares. Working paper. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4356262↩︎\nShkilko, A. V., Van Ness, B. F., & Van Ness, R. A. (2008). Locked and crossed markets on NASDAQ and the NYSE. Journal of Financial Markets, 11(3), 308-337. https://www.sciencedirect.com/science/article/pii/S1386418107000031?casa_token=GIqJQiPKyisAAAAA:ZVbaf4SPC0IzxFFLWqgI2papSw2MrEf_lPXCL9OlT_ZnKgA6-pGTl4EYisKuIUaFdx8JY1p3d7o↩︎\nJurkatis, S. (2022). Inferring trade directions in fast markets. Journal of Financial Markets, 58, 100635. https://doi.org/10.1016/j.finmar.2021.100635↩︎\nHolden, C. W., & Jacobsen, S. (2014). Liquidity measurement problems in fast, competitive markets: Expensive and cheap solutions. Journal of Finance, 69(4), 1747-1785. https://doi.org/10.1111/jofi.12127↩︎\nLee, C. M., & Ready, M. J. (1991). Inferring trade direction from intraday data. Journal of Finance, 46(2), 733-746. https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.1991.tb02683.x↩︎\nHagströmer, B. (2021). Bias in the effective bid-ask spread. Journal of Financial Economics, 142(1), 314-337. https://doi.org/10.1016/j.jfineco.2021.04.018↩︎\nConrad, J., & Wahal, S. (2020). The term structure of liquidity provision. Journal of Financial Economics, 136(1), 239-259. https://doi.org/10.1016/j.jfineco.2019.09.008↩︎"
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html",
    "href": "blog/tidy-collaborative-filtering/index.html",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "",
    "text": "Recommender systems are a key component of our digital lifes, ranging from e-commerce, online advertisements, movie recommendations, or more generally all kinds of product recommendations. A recommender system aims to efficiently deliver personalized content to users based on a large pool of potentially relevant information. In this blog post, I illustrate the concept of recommender systems by building a simple stock recommendation tool that relies on publicly available portfolios from the social trading platform wikifolio.com. The resulting recommender proposes stocks to investors who already have their own portfolios and look for new investment opportunities. The underlying assumption is that the wikifolio traders hold stock portfolios that can provide meaningful inspiration for other investors. The resulting stock recommendations of course do not constitute any investment advice and rather serve an illustrative purpose.\nwikifolio.com is the leading social trading platform in Europe, where anyone can publish and monetize their trading strategies through virtual portfolios, which are called wikifolios. The community of wikifolio traders includes full time investors and successful entrepreneurs, as well as experts from different sectors, portfolio managers, and editors of financial magazines. All traders share their trading ideas through fully transparent wikifolios. The wikifolios are easy to track and replicate, by investing in the corresponding, collateralized index certificate. As of writing, there are more than 30k published wikifolios of more than 9k traders, indicating a large diversity of available portfolios for training our recommender.\nThere are essentially three types of recommender models: recommenders via collaborative filtering, recommenders via content-based filtering, and hybrid recommenders (that mix the first two). In this blog post, I focus on the collaborative filtering approach as it requires no information other than portfolios and can provide fairly high precision with little complexity. Nonetheless, I first briefly describe the recommender approaches and refer to Ricci et al. (2011)1 for a comprehensive exposition."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#collaborative-filtering",
    "href": "blog/tidy-collaborative-filtering/index.html#collaborative-filtering",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Collaborative Filtering",
    "text": "Collaborative Filtering\nIn collaborative filtering, recommendations are based on past user interactions and items to produce new recommendations. The central notion is that past user-item interactions are sufficient to detect similar users or items. Broadly speaking, there are two sub-classes of collaborative filtering: the memory-based approach, which essentially searches nearest neighbors based on recorded transactions and is hence model-free, and the model-based approach, where new representations of users and items are built based on some generative pre-estimated model. Theoretically, the memory-based approach has a low bias (since no latent model is assumed) but a high variance (since the recommendations change a lot in the nearest neighbor search). The model-based approach relies on a trained interaction model. It has a relatively higher bias but a lower variance, i.e., recommendations are more stable since they come from a model.\nAdvantages of collaborative filtering include: (i) no information about users or items is required; (ii) a high precision can be achieved with little data; (iii) the more interaction between users and items is available, the more recommendations become accurate. However, the disadvantages of collaborative filtering are: (i) it is impossible to make recommendations to a new user or recommend a new item (cold start problem); (ii) calculating recommendations for millions of users or items consumes a lot of computational power (scalability problem); (iii) if the number of items is large relative to the users and most users only have interacted with a small subset of all items, then the resulting representation has many zero interactions and might hence lead to computational difficulties (sparsity problem)."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#content-based-filering",
    "href": "blog/tidy-collaborative-filtering/index.html#content-based-filering",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Content-Based Filering",
    "text": "Content-Based Filering\nContent-based filtering methods exploit information about users or items to create recommendations by building a model that relates available characteristics of users or items to each other. The recommendation problem is hence cast into a classification problem (the user will like the item or not) or more generally a regression problem (which rating will the user give an item). The classification problem can be item-centered by focusing on available user information and estimating a model for each item. If there are a lot of user-item interactions available, the resulting model is fairly robust, but it is less personalized (as it ignores user characteristics apart from interactions). The classification problem can also be user-centered by working with item features and estimating a model for each user. However, if a user only has a few interactions then the resulting model becomes easily unstable. Content-based filtering can also be neither user nor item-centered by stacking the two feature vectors, hence considering both input simultaneously, and putting them into a neural network.\nThe main advantage of content-based filtering is that it can make recommendations for new users without any interaction history or recommend new items to users. The disadvantages include: (i) training needs a lot of users and item examples for reliable results; (ii) tuning might be much harder in practice than collaborative filtering; (iii) missing information might be a problem since there is no clear solution how to treat missingness in user or item characteristics."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#hybrid-recommenders",
    "href": "blog/tidy-collaborative-filtering/index.html#hybrid-recommenders",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Hybrid Recommenders",
    "text": "Hybrid Recommenders\nHybrid recommender systems combine both collaborative and content-based filtering to overcome the challenges of each approach. There are different hybridization techniques available, e.g., combining the scores of different components (weighted), chosing among different component (switching), following strict priority rules (cascading), presenting outputs from different components at the same time (mixed), etc."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#roc-curves",
    "href": "blog/tidy-collaborative-filtering/index.html#roc-curves",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "ROC curves",
    "text": "ROC curves\nThe first visualization approach comes from signal-detection and is called “Receiver Operating Characteristic” (ROC). The ROC-curve plots the algorithm’s probability of detection (TPR) against the probability of false alarm (FPR).\n\nTPR = TP / (TP + FN) (i.e., share of true positive recommendations relative to all known portfolios)\nFPR = FP / (FP + TN) (i.e., share of incorrect recommendations relative to )\n\nIntuitively, the bigger the area under the ROC curve, the better is the corresponding algorithm.\n\nresults_tbl |&gt;\n  ggplot(aes(FPR, TPR, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"False Positive Rate (FPR)\",\n    y = \"True Positive Rate (TPR)\",\n    title = \"ROC curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\n\nThe figure shows that recommending random items exhibits the lowest TPR for any FPR, so it is the worst among all algorithms (which is not surprising). Association rules, on the other hand, constitute the best algorithm among the current selection. This result is neat because association rule mining is a computationally cheap algorithm, so we could potentially fine-tune or reestimate the model easily."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#precision-recall-curves",
    "href": "blog/tidy-collaborative-filtering/index.html#precision-recall-curves",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Precision-Recall Curves",
    "text": "Precision-Recall Curves\nThe second popular approach is to plot Precision-Recall curves. The two measures are often used in information retrieval problems:\n\nPrecision = TP / (TP + FP) (i.e., correctly recommended items relative to total recommended items)\nRecall = TP / (TP + FN) (i.e., correctly recommended items relative to total number of known useful recommendations)\n\nThe goal is to have a higher precision for any level of recall. In fact, there is trade-off between the two measures since high precision means low recall and vice-versa.\n\nresults_tbl |&gt;\n  ggplot(aes(x = recall, y = precision, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"Recall\", y = \"Precision\",\n    title = \"Precision-Recall curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\n\nAgain, proposing random items exhibits the worst performance, as for any given level of recall, this approach has the lowest precision. Association rules are also the best algorithm with this visualization approach."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#footnotes",
    "href": "blog/tidy-collaborative-filtering/index.html#footnotes",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRicci, F, Rokach, L., Shapira, B. and Kantor, P. (2011). “Recommender Systems Handbook”. https://link.springer.com/book/10.1007/978-0-387-85820-3.↩︎\nHahsler, M. (2022). “recommenderlab: An R Framework for Developing and Testing Recommendation Algorithms”, R package version 1.0.3. https://CRAN.R-project.org/package=recommenderlab.↩︎\nBreese, J.S., Heckerman, D. and Kadie, C. (1998). “Empirical Analysis of Predictive Algorithms for Collaborative Filtering”, Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, Madison, 43-52. https://arxiv.org/pdf/1301.7363.pdf.↩︎"
  },
  {
    "objectID": "blog/op-ed-tidy-finance/index.html",
    "href": "blog/op-ed-tidy-finance/index.html",
    "title": "What is Tidy Finance?",
    "section": "",
    "text": "Empirical finance can be tedious. Many standard tasks, such as cleaning data or forming factor portfolios, require a lot of effort. The code to produce even seminal results is typically opaque. Why should researchers have to reinvent the wheel over and over again?\nTidy Finance with R is our take on how to conduct empirical research in financial economics from scratch. Whether you are an industry professional looking to expand your quant skills, a graduate student diving into the finance world, or an academic researcher, this book shows you how to use R to master applications in asset pricing, portfolio optimization, risk management, and option pricing.\nWe wrote this book to provide a comprehensive guide to using R for financial analysis. Our book collects all the tools we wish we would have had at hand at the beginning of our graduate studies in finance. Without transparent code for standard procedures, numerous replication efforts (and their failures) feel like a waste of resources. We have been there, as probably everybody working with data has. Since we kicked off our careers, we have constantly updated our methods, coding styles, and workflows. Our book reflects our lessons learned. By sharing them, we aim to help others avoid dead ends.\nWorking on problems that countless others have already solved in secrecy is not just tedious, it even may have detrimental effects. In a recent study1 together with hundreds of research teams from across the globe, Albert J. Menkveld, the best-publishing Dutch economist according to Economentop 40, shows that without a standard path to do empirical analysis, results may vary substantially. Even if teams set out to analyze the same research question based on the same data, implementation details are important and deserve more than treatment as subtleties.\nThere will always be multiple acceptable ways to test relevant research questions. So why should it matter that our book lifts our curtain on reproducible finance by providing a fully transparent code base for many typical financial applications? First and foremost, we hope to inspire others to make their research truly reproducible. This is not a purely theoretical exercise: our examples start with data preparation and conclude with communicating results to get readers to do empirical analysis on real data as fast as possible. We believe that the need for precise academic writing does not stop where the code begins. Understanding and agreeing on standard procedures hopefully frees up resources to focus on what matters: a novel research project, a seminar paper, or a thorough analysis for your employer. If our book helps to provide a foundation for discussions on which determinants render code useful, we have achieved much more than we were hoping for at the beginning of this project.\nUnlike typical stylized examples, our book starts with the problems of any serious research project. The often overlooked challenge behind empirical research may seem trivial at first glance: we need data to conduct our analyses. Finance is not an exception: raw data, often hidden behind proprietary financial data sources, requires cleaning before there is any hope of extracting valuable insights from it. While you can despise data cleaning, you cannot ignore it.\nWe describe and provide the code to prepare typical open-source and proprietary financial data sources (e.g., CRSP, Compustat, Mergent FISD, TRACE). We reuse these data in all the subsequent chapters, which we keep as self-contained as possible. The empirical applications range from key concepts of empirical asset pricing (beta estimation, portfolio sorts, performance analysis, Fama-French factors) to modeling and machine learning applications (fixed effects estimation, clustering standard errors, difference-in-difference estimators, ridge regression, Lasso, Elastic net, random forests, neural networks) and portfolio optimization techniques.\nNecessarily, our book reflects our opinionated perspective on how to perform empirical analyses. From our experience as researchers and instructors, we believe in the value of the workflows we teach and apply daily. The entire book rests on two core concepts: coding principles using the tidyverse family of R packages and tidy data.\nWe base our book entirely on the open-source programming language R. R and the tidyverse community provide established tools to perform typical data science tasks, ranging from cleaning and manipulation to plotting and modeling. R is hence the ideal environment to develop an accessible code base for future finance students. The concept of tidy data refers to organizing financial data in a structured and consistent way, allowing for easy analysis and understanding.2 Taken together, tidy data and code help achieve the ultimate goal: to provide a fundamentally human-centered experience that makes it easier to teach, learn, and replicate the code of others – or even your own!\nWe are convinced that empirical research in finance is in desperate need of reproducible code to form standards for otherwise repetitive tasks. Instructors and researchers have already reached out to us with grateful words about our book. Tidy Finance finds its way into lecture halls across the globe already today. Various recent developments support our call for increased transparency. For instance, Cam Harvey, the former editor of the Journal of Finance, and a former president of the American Finance Association, openly argues that the profession needs to tackle the replication crisis.3 Top journals in financial economics increasingly adopt code and data-sharing policies to increase transparency. The industry and academia are aware and concerned (if not alarmed) about these issues, which is why we believe that the timing for publishing Tidy Finance with R could not be better."
  },
  {
    "objectID": "blog/op-ed-tidy-finance/index.html#footnotes",
    "href": "blog/op-ed-tidy-finance/index.html#footnotes",
    "title": "What is Tidy Finance?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMenkveld, A. J. et al. (2022). “Non-standard Errors”. http://dx.doi.org/10.2139/ssrn.3961574↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nWigglesworth, R. (2021). The hidden ‘replication crisis’ of finance. Financial Times. https://www.ft.com/content/9025393f-76da-4b8f-9436-4341485c75d0↩︎"
  },
  {
    "objectID": "blog/historical-sp-500-total-return/index.html",
    "href": "blog/historical-sp-500-total-return/index.html",
    "title": "Construction of a Historical S&P 500 Total Return Index",
    "section": "",
    "text": "I wanted to simulate simple equity savings plans over long time horizons and many different initiation periods for a story with the German news portal t-online. The good thing is that the S&P 500 index provides a great starting point as it is easily available since 1928 via Yahoo Finance. However, I wanted my savings plans to be accumulating, i.e., all cash distributions are reinvested in the savings plan. The S&P index is inadequate for this situation as it is a price index that only tracks its components’ price movements. The S&P 500 Total Return Index tracks the overall performance of the S&P 500 and would be the solution to my problem, but it is only available since 1988.\nFortunately, I came up with a solution using data provided by Robert Shiller and provide the complete code below for future reference. If you spot any errors or have better suggestions, please feel free to create an issue.\nThis is the set of packages I use throughout this post.\n\nlibrary(tidyverse) # for overall grammar\nlibrary(tidyquant) # to download data from yahoo finance\nlibrary(glue)      # to automatically construct figure captions\nlibrary(scales)    # for nicer axis labels \nlibrary(readxl)    # to read Shiller's data \n\nFirst, let us download the S&P 500 Total Return Index from Yahoo Finance. I only consider the closing prices of the last day of each month because my savings plans only transfer funds once a month. In principle, you could also approximate the daily time series, but I believe it will be noiser because Shiller only provides monthly data.\n\nsp500_recent &lt;-  tq_get(\"^SP500TR\", get = \"stock.prices\",\n                        from = \"1988-01-04\", to = \"2023-01-31\") |&gt;\n  select(date, total_return_index = close) |&gt;\n  drop_na() |&gt;\n  group_by(month = ceiling_date(date, \"month\")-1) |&gt;\n  arrange(date) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  select(month, total_return_index)\n\nNext, I download data from Robert Shiller’s website that he used in his great book Irrational Excuberance. I create a temporary file and read the relevant sheet. In particular, the data contains monthly S&P 500 price and dividend data. The original file has a bit of annoying date format that I have to correct before parsing.\n\ntemp &lt;- tempfile(fileext = \".xls\")\n\ndownload.file(url = \"http://www.econ.yale.edu/~shiller/data/ie_data.xls\",\n              destfile = temp, mode='wb')\n\nshiller_historical &lt;- read_excel(temp, sheet = \"Data\", skip = 7) |&gt;\n  transmute(month = ceiling_date(ymd(str_replace(str_c(Date, \".01\"), \"\\\\.1\\\\.\", \"\\\\.10\\\\.\")), \"month\")-1,\n            price = as.numeric(P),\n            dividend = as.numeric(D)) \n\nTo construct the total return index, I need a return that includes dividends. In the next code chunk, I compute monthly total returns of the S&P 500 index by incorporating the monthly dividend paid on the index in the corresponding month. Note that Shiller’s data contains the 12-month moving sum of monthly dividends, hence the division by 12. Admittedly, this is a brute force approximation, but I couldn’t come up with a better solution so far.\n\nshiller_historical &lt;- shiller_historical |&gt;\n  arrange(month) |&gt;\n  mutate(ret = (price + dividend / 12) / lag(price) - 1)\n\nBefore I go back in time, let us check whether the total return computed above is able to match the actual total return since 1988. I start with the first total return index number that is available and use the cumulative product of returns from above to construct the check time series.\n\ncheck &lt;- shiller_historical |&gt;\n  full_join(sp500_recent, by = \"month\") |&gt;\n  filter(!is.na(total_return_index)) |&gt;\n  arrange(month) |&gt;\n  mutate(ret = if_else(row_number() == 1, 0, ret), # ignore first month return\n         total_return_check = total_return_index[1] * cumprod(1 + ret)) |&gt;\n  drop_na()\n\nThe correlation between the actual time series and the check is remarkably high which gives me confidence in the method I propose here.\n\ncheck |&gt; \n  select(total_return_index, total_return_check) |&gt;  \n  cor()\n\n                   total_return_index total_return_check\ntotal_return_index              1.000              0.999\ntotal_return_check              0.999              1.000\n\n\nIn addition, the visual inspection of the two time series in Figure 1 corroborates my confidence. Note that both the actual and the simulated total return indexes start at the same index value.\n\ncheck |&gt;\n  select(month, Actual = total_return_index, Simulated = total_return_check) |&gt;\n  pivot_longer(cols = -month) |&gt;\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = comma)+ \n  labs(x = NULL, y = NULL, color = NULL,\n       title = \"Actual and simulated S&P 500 Total Return index\",\n       subtitle = glue(\"Both indexes start at {min(check$month)}\"))\n\n\n\n\n\n\n\nFigure 1: Simluated and actual S&P 500 Total Return index data move closely together.\n\n\n\n\n\nNow, let us use the same logic to construct the total return index for the time before 1988. Note that I just sort the months in descending order and divide by the cumulative product of the total return from Shiller’s data.\n\nsp500_historical &lt;- sp500_recent |&gt; \n  filter(month == min(month)) |&gt;\n  full_join(shiller_historical |&gt;\n              filter(month &lt;= min(sp500_recent$month)), by = \"month\") |&gt;\n  arrange(desc(month)) |&gt;\n  mutate(ret = if_else(row_number() == 1, 0, ret),\n         total_return_index = total_return_index[1] / cumprod(1 + ret))\n\nBefore we take a look at the results, I also add the S&P price index from Yahoo Finance for comparison.\n\nsp500_price_index &lt;- tq_get(\"^GSPC\", get = \"stock.prices\",\n                            from = \"1928-01-01\", to = \"2023-01-31\") |&gt;\n  select(date, price_index = close) |&gt;\n  drop_na() |&gt;\n  group_by(month = ceiling_date(date, \"month\") - 1) |&gt;\n  arrange(date) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  select(month, price_index)\n\nFinally, let us combine (i) the actual S&P 500 Total Return Index from 1988 until 2023, (ii) the simulated S&P 500 total return index before 1988, and (iii) the S&P 500 price index from 1928 until 2023.\n\nsp500_monthly &lt;- sp500_recent|&gt;\n  bind_rows(sp500_historical |&gt;\n              filter(month &lt; min(sp500_recent$month))  |&gt;\n              select(month, total_return_index)) |&gt;\n  full_join(sp500_price_index |&gt; \n              select(month, price_index), by = \"month\") |&gt;\n  filter(month &gt;= \"1928-01-01\")  |&gt;\n  arrange(month)\nsp500_monthly\n\n# A tibble: 1,141 × 3\n  month      total_return_index price_index\n  &lt;date&gt;                  &lt;dbl&gt;       &lt;dbl&gt;\n1 1928-01-31               1.20        17.6\n2 1928-02-29               1.21        17.3\n3 1928-03-31               1.20        19.3\n4 1928-04-30               1.26        19.8\n5 1928-05-31               1.35        20  \n# ℹ 1,136 more rows\n\n\nFigure 2 shows the dramatic differences in cumulative returns if you only consider price changes, as the S&P 500 Index does, versus total returns with reinvested capital gains. Note that I plot the indexes in log scale, otherwise everything until the last couple of decades would look like a flat line. I believe it is also important to keep the differences between price and performance indexes in mind whenever you compare equity indexes across countries. For instance, the DAX is a performance index by default and should never be compared with the S&P 500 price index.\n\nsp500_monthly |&gt;\n  select(month, \n         `Price Index` = price_index, \n         `Total Return Index` = total_return_index) |&gt;\n  pivot_longer(cols = -month) |&gt;\n  group_by(name) |&gt;\n  arrange(month) |&gt;\n  mutate(value = value / value[1] * 100) |&gt;\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_log10(labels = comma) +\n  scale_x_date(expand = c(0, 0), date_breaks = \"10 years\", date_labels = \"%Y\") + \n  labs(x = NULL, y = NULL, color = NULL,\n       title = \"S&P 500 Price index and Total Return index since 1928\",\n       subtitle = glue(\"Both indexes are normalized to 100 at {min(sp500_monthly$month)}\"))\n\n\n\n\n\n\n\nFigure 2: Using total return data yields dramatically higher cumulative returns over a few decades.\n\n\n\n\n\n\n\n\nFigure 1: Simluated and actual S&P 500 Total Return index data move closely together.\nFigure 2: Using total return data yields dramatically higher cumulative returns over a few decades."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html",
    "href": "blog/crsp-v2-update/index.html",
    "title": "CRSP 2.0 Update",
    "section": "",
    "text": "With commit 6acb50b, Tidy Finance has transitioned to a new version of the CRSP tables distributed by WRDS. In this blog post, we will review the changes that affect our routines."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html#overview-crsp-and-wrds",
    "href": "blog/crsp-v2-update/index.html#overview-crsp-and-wrds",
    "title": "CRSP 2.0 Update",
    "section": "Overview: CRSP and WRDS",
    "text": "Overview: CRSP and WRDS\nTidy Finance shows how to download the most used data in empirical research in finance from the Wharton Research Data Services (WRDS). A main component of this data is the stock return history provided by the Center for Research in Security Prices (CRSP). CRSP is the de-facto gold standard for stock return data covering historical returns from the 1920s until now.\nYou can read all about how to connect to WRDS and download the CRSP data in our chapter WRDS, CRSP, and Compustat in the R version and the Python version."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html#crsp-format-2.0-ciz",
    "href": "blog/crsp-v2-update/index.html#crsp-format-2.0-ciz",
    "title": "CRSP 2.0 Update",
    "section": "CRSP format 2.0 (CIZ)",
    "text": "CRSP format 2.0 (CIZ)\nIn 2022, CRSP rolled out a new format for its data, which is called CRSP’s Stock and Indexes Flat File Format 2.0 (CIZ). This new format is now also available via WRDS. So, what is new in the second version? The most notable items from CRSP’s announcement include:\n\nThey renamed many variables in the daily and monthly data. Not all of these changes make it easier to understand what the variables are, and some names are hard to read.\nThey added additional variables that are sometimes redundant but make querying simpler (e.g., you do not have to merge different data to perform common tasks).\nDelisting returns are now included in the main return time series.\nThey introduce issuer-level data, which ensures unique observations for each issuer (e.g., industry codes).\nThey reworked some flag items by giving them alphanumeric values instead of the previous numeric codes. Moreover, these codes are now also documented in metadata files to facilitate access.\n\nMany of these changes mean that you will not be able to simply use the new data with your existing code. Do not worry. We have you covered and will show you how to use the new data in your previous routines easily.\nYou can access the new data via WRDS using the commands below. For the main data, WRDS basically just added a “_v2” postfix. However, you also see we need fewer tables, as we do not load information on delisting returns separately.\n\nmsf_db &lt;- tbl(wrds, in_schema(\"crsp\", \"msf_v2\"))\nstksecurityinfohist_db &lt;- tbl(wrds, in_schema(\"crsp\", \"stksecurityinfohist\"))\n\nWe do not repost the new routines for downloading the data in this blog. Instead, we ask you to check out the new code in the respective chapter on WRDS, CRSP, and Compustat in the R version and the Python version."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html#a-small-glimpse-at-the-differences",
    "href": "blog/crsp-v2-update/index.html#a-small-glimpse-at-the-differences",
    "title": "CRSP 2.0 Update",
    "section": "A small glimpse at the differences",
    "text": "A small glimpse at the differences\nTo take a look at the new table and compare it to the old one, we use the new tidyfinance R package (see our recent blog post on the initial release here) to download the data. If you do not have the package yet, please install it from CRAN using install.packages(\"tidyfinance\"). For ease of use, we also load the full tidyverse as in all our chapters of the R book.\n\nlibrary(tidyfinance)\nlibrary(tidyverse)\n\nThen, we can download the two data formats using the function download_data_wrds_crsp.\n\ncrsp_v1 &lt;- download_data_wrds_crsp(\"crsp_monthly\", \n                                    version = \"v1\",\n                                    start_date = \"1970-01-01\",\n                                    end_date = \"2022-12-01\")\n\ncrsp_v2 &lt;- download_data_wrds_crsp(\"crsp_monthly\", \n                                    version = \"v2\",\n                                    start_date = \"1970-01-01\",\n                                    end_date = \"2022-12-01\")\n\nNow, do we find any significant differences between the data? Let us look at the number of observations identified by permno and date.\n\nnrow(crsp_v2)\n\n[1] 3107656\n\nnrow(crsp_v1)\n\n[1] 3103640\n\n\nWe see that the new data contains more rows. However, we are not sure where these observations come from or where we lost them in the old data. Let us see if the observations are actually matched for the identifiers with a anti_join().\n\nanti_join(crsp_v2, crsp_v1,\n          by = join_by(\"permno\", \"date\")) |&gt; \n  nrow()\n\n[1] 4041\n\n\nWe find that roughly 4,000 observations are not matched. However, this is a very minor difference.\nNext, we check if the monthly returns are all the same. We compare the new returns to the adjusted returns from version 1.0 to account for delisting in both versions. In fact, these returns should be slightly different, as CRSP states that they have updated the way to compound distributions. They also state this change in compounding as an example, among other things. However, they do not yet provide more information about these other things.\n\ninner_join(crsp_v2 |&gt; \n             select(permno, date, ret_new = ret_excess), \n           crsp_v1 |&gt; \n             select(permno, date, ret_old = ret_excess),\n          by = join_by(\"permno\", \"date\")) |&gt; \n  mutate(ret_differences = !near(ret_new, ret_old, tol = 10^-6)) |&gt; \n  summarize(share_all_ret_differences = sum(ret_differences, na.rm = TRUE)/n())\n\n# A tibble: 1 × 1\n  share_all_ret_differences\n                      &lt;dbl&gt;\n1                     0.123\n\n\nIndeed, we see that not all returns are exactly matched."
  },
  {
    "objectID": "blog/crsp-v2-update/index.html#can-we-keep-using-the-old-crsp-data",
    "href": "blog/crsp-v2-update/index.html#can-we-keep-using-the-old-crsp-data",
    "title": "CRSP 2.0 Update",
    "section": "Can we keep using the old CRSP data?",
    "text": "Can we keep using the old CRSP data?\nUnfortunately, the old format (also called 1.0 (SIZ)) will not receive updates after the end of the year 2024. Nevertheless, WRDS said that the old data will remain in its current place, so we can continue replicating studies with their original data (ignoring changes to the actual data points). Thus, we have included the option to download the legacy data in our tidyfinance R package."
  },
  {
    "objectID": "blog/cir-calibration/index.html",
    "href": "blog/cir-calibration/index.html",
    "title": "CIR Model Calibration using Python",
    "section": "",
    "text": "The Cox–Ingersoll–Ross (CIR)1 model stands as a cornerstone within the vast expanse of Financial Mathematics literature. Originally conceived to refine the well-known Vasicek2 model in Interest Rate Modeling, the CIR model addressed a notable limitation of its predecessor—specifically, the propensity of Gaussian models like Vasicek’s to generate negative interest rates, a feature often deemed undesirable despite the theoretical possibility of negative rates in reality.\nIn this concise exposition, I will delineate the process of calibrating the Cox–Ingersoll–Ross model using Python. From a theoretical point of view, I will define linear models to calibrate the CIR model and test their feasibility via Monte-Carlo simulations."
  },
  {
    "objectID": "blog/cir-calibration/index.html#footnotes",
    "href": "blog/cir-calibration/index.html#footnotes",
    "title": "CIR Model Calibration using Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCox, J. C., Ingersoll Jr, J. E., & Ross, S. A. (1985). A Theory of the Term Structure of Interest Rates. Econometrica, 53(2), 385-408. Link.↩︎\nVasicek, O. (1977). An equilibrium characterization of the term structure. Journal of financial economics, 5(2), 177-188. Link.↩︎\nOrlando, G., Mininni, R. M., and Bufalo, M. (2020). Forecasting interest rates through vasicek and cir models: A partitioning approach. Journal of Forecasting, 39(4):569–579. Link.↩︎"
  },
  {
    "objectID": "blog/convert-raw-trace-data/index.html",
    "href": "blog/convert-raw-trace-data/index.html",
    "title": "Convert Raw TRACE Data to a Local SQLite Database",
    "section": "",
    "text": "Corporate bond research is gaining momentum, but data availability can be a challenge for most researchers. While FINRA makes its TRACE Enhanced Data on U.S. corporate bond trading available through vendors such as Wharton Research Data Services (WRDS), not every researcher has access to these services. Alternatively, FINRA offers its Enhanced Historical Data and Academic Data (the latter exclusively) as one-off purchases. However, organizing this data can be complex and time consuming due to the nested and zipped structure of raw TXT files, making it cumbersome for researchers to explore the exciting world of fixed income securities.\nOur R code enables you organizing the TRACE academic data and TRACE enhanced data by eliminating the complexities of multiple, nested files, simplifying the data conversion process. Drawing from the existing SAS-based solution of Dick-Nielsen (2014) 1 and (2019) 2, you can now easily convert TRACE data into a single, organized SQLite database, allowing for seamless and efficient downstream analysis.\nHere are the key benefits our solution offers:\nWith our R code, you can simplify your corporate bond research and overcome current limitations. Don’t let complex data organizing hold you back! Prepare yourself to be immersed in the extraordinary realm of TRACE, the key to unlock a deeper understanding of over-the-counter transactions and to unveil the mysterious pathways of fixed-income securities. With TRACE as your guide, you will embark on a journey of discovery, unraveling market trends and unearthing invaluable insights that will forever enrich your understanding of this mesmerizing world. Embrace the power of TRACE and unveil the secrets that lie within.\nIn the following, we will discuss first, the structure of the bond trading data “TRACE” provided by FINRA, second the proposed normalized database schema for the generated SQLite database, and finally third, give you a brief hands-on guide to downloading data and using our R code to generate the SQLite."
  },
  {
    "objectID": "blog/convert-raw-trace-data/index.html#final-remarks",
    "href": "blog/convert-raw-trace-data/index.html#final-remarks",
    "title": "Convert Raw TRACE Data to a Local SQLite Database",
    "section": "Final Remarks",
    "text": "Final Remarks\nWe hope that this blog post is helpful to the reader working with TRACE academic data and TRACE enhanced data and in facilitating analyses in this emerging, exciting, and promising field of research.\nAt the end, we would like to thank the editors of Tidy Finance for their helpful suggestions and support with writing this blog post, and our supervisor Prof. Dr. Dirk Schiereck from the Chair of Corporate Finance at TU Darmstadt (Germany) who made the data available for this project."
  },
  {
    "objectID": "blog/convert-raw-trace-data/index.html#footnotes",
    "href": "blog/convert-raw-trace-data/index.html#footnotes",
    "title": "Convert Raw TRACE Data to a Local SQLite Database",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDick-Nielsen, J. (2014). How to clean enhanced TRACE data. Available at [SSRN 2337908] (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2337908).↩︎\nDick-Nielsen, J., & Poulsen, T. K. (2019). How to clean academic trace data. Available at [SSRN 3456082] (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3456082).↩︎"
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html",
    "href": "blog/fama-french-three-vs-five-factors/index.html",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "",
    "text": "In our book chapters Replicating Fama and French Factors (R Version) and Replicating Fama and French Factors (Python Version), we show how to construct factor portfolios that are fairly close to the popular data from Prof. Kenneth French finance data library. In this blog post, I want to elaborate a bit more on the subtle difference between the size data in the Fama-French three (FF3)1 and five (FF5)2 factor data."
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html#analyzing-monthly-factor-data",
    "href": "blog/fama-french-three-vs-five-factors/index.html#analyzing-monthly-factor-data",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "Analyzing monthly factor data",
    "text": "Analyzing monthly factor data\nI first start by downloading the monthly Fama-French factors using the frenchdata package. The currently available data ranges from July 1926 to August 2023. 1926-07-01 to 2023-08-01.\n\nlibrary(tidyverse)\nlibrary(frenchdata)\nlibrary(fixest)\n\nstart_date &lt;- \"1926-07-01\"\nend_date &lt;- \"2023-08-01\"\n\nfactors_ff3_monthly_raw &lt;- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff3_monthly &lt;- factors_ff3_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`)\n\nfactors_ff5_monthly_raw &lt;- download_french_data(\"Fama/French 5 Factors (2x3)\")\nfactors_ff5_monthly &lt;- factors_ff5_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    across(c(RF, `Mkt-RF`, SMB, HML, RMW, CMA), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) \n\nfactors_ff_monthly &lt;- factors_ff3_monthly |&gt; \n  rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff3\")) |&gt; \n  inner_join(\n    factors_ff5_monthly |&gt; \n      select(month, mkt_excess, rf, smb, hml) |&gt; \n      rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff5\")), by = \"month\") |&gt; \n  filter(month &gt;= start_date & month &lt;= end_date)\n\nindustries_ff_monthly_raw &lt;- download_french_data(\"10 Industry Portfolios\")\nindustries_ff_monthly &lt;- industries_ff_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(month = floor_date(ymd(str_c(date, \"01\")), \"month\")) |&gt;\n  mutate(across(where(is.numeric), ~ . / 100)) |&gt;\n  select(month, everything(), -date) |&gt;\n  rename_with(str_to_lower)\n\nLet us first inspect the summary statistics of each factor.\n\nfactors_ff_monthly |&gt; \n  pivot_longer(cols = - month) |&gt; \n  select(name, value) |&gt;\n  drop_na() |&gt;\n  group_by(name) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  ) |&gt; \n  print(n = Inf)\n\n# A tibble: 8 × 9\n  name         mean      sd    min     q05    q50    q95    max     n\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 hml_ff3   0.00284 0.0300  -0.139 -0.0413 0.0022 0.0541 0.128    721\n2 hml_ff5   0.00284 0.0300  -0.139 -0.0413 0.0022 0.0541 0.128    721\n3 mkt_exce… 0.00568 0.0449  -0.232 -0.0726 0.0092 0.0713 0.161    721\n4 mkt_exce… 0.00568 0.0449  -0.232 -0.0726 0.0092 0.0713 0.161    721\n5 rf_ff3    0.00362 0.00266  0      0      0.0038 0.0081 0.0135   721\n6 rf_ff5    0.00362 0.00266  0      0      0.0038 0.0081 0.0135   721\n7 smb_ff3   0.00186 0.0304  -0.172 -0.0421 0.0011 0.0501 0.214    721\n8 smb_ff5   0.00219 0.0302  -0.153 -0.0431 0.001  0.0481 0.183    721\n\n\nThe above table shows that risk free rates rf_*, market excess returns mkt_excess_*, and value factors hml_* show de facto identical value across all statistics for FF3 and FF5. However, the size factors smb_* seem to be different between the data sets. Another way to show the difference is running regressions, as we do in our replication chapters:\n\nmodel_smb &lt;- lm(smb_ff3 ~ smb_ff5, data = factors_ff_monthly)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb_ff3 ~ smb_ff5, data = factors_ff_monthly)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.03543 -0.00192  0.00032  0.00205  0.03373 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.000303   0.000222   -1.36     0.17    \nsmb_ff5      0.987055   0.007338  134.51   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00595 on 719 degrees of freedom\nMultiple R-squared:  0.962, Adjusted R-squared:  0.962 \nF-statistic: 1.81e+04 on 1 and 719 DF,  p-value: &lt;2e-16\n\n\nRegressing the FF3 size factor on its FF5 counterpart yields a coefficient of 0.99 and an R-squared around 96%, so definitely no perfect co-movement.\nIs this difference just an artifact in the data, limited to a certain time period? Figure Figure 1 shows that there are differences throughout the whole sample.\n\nfactors_ff_monthly |&gt; \n  mutate(difference = smb_ff3 - smb_ff5) |&gt; \n  ggplot(aes(x = month, y = difference, fill = difference &gt; 0)) +\n  geom_col() +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  theme(legend.position = \"none\") + \n  labs(\n    x = NULL, y = \"smb_ff3 - smb_ff5\",\n    title = \"Difference between monthly size factors from FF3 and FF5 data\"\n  )\n\n\n\n\n\n\n\nFigure 1: End-of-month difference between monthly size factors from Fama-French three (FF3) and five (FF5) factor data.\n\n\n\n\n\nWhere does this difference come from? In my opinion, there is one likelyexplanation for the differences: the size portfolios portfolio_size and value portfolios portfolio_bm are constructed as independent sorts in FF3, while portfolio_bm, portfolio_op, and portfolio_inv are the result of dependent sorts in FF5 (depending on portfolio_size). In FF5, portfolio_size is then calculated on averages based on portfolio_bm, portfolio_op, and portfolio_inv portfolios. As all portfolios are the result of value-weighted return aggregation, it is hence very likely that these aggregations lead to different values.\nThese subtle differences might even impact your statistical tests. As an example, let us consider one of the industry portfolios from industry_ff_monthly. We use the ‘other’ portfolio, which contains sectors such as mines, construction, entertainment, finance, etc. We run a gression of the corresponding industry portfolios against the market, size, and value factors of FF3 and FF5, respectively.\n\nindustry_returns &lt;- industries_ff_monthly |&gt; \n  select(month, ret_other = other) |&gt; \n  inner_join(factors_ff_monthly, by = \"month\")\n\nmodel_ff3 &lt;- feols(\n  ret_other ~ mkt_excess_ff3 + smb_ff3 + hml_ff3, \n  industry_returns\n)\n\nmodel_ff5 &lt;- feols(\n  ret_other ~ mkt_excess_ff5 + smb_ff5 + hml_ff5, \n  industry_returns\n)\n\netable(model_ff3, model_ff5, coefstat = \"tstat\")\n\n                        model_ff3         model_ff5\nDependent Var.:         ret_other         ret_other\n                                                   \nConstant         0.0017** (2.694)  0.0017** (2.689)\nmkt_excess_ff3   1.139*** (76.67)                  \nsmb_ff3           0.0500* (2.299)                  \nhml_ff3         0.3984*** (18.48)                  \nmkt_excess_ff5                     1.135*** (76.35)\nsmb_ff5                            0.0690** (3.196)\nhml_ff5                           0.3897*** (18.23)\n_______________ _________________ _________________\nVCOV type                     IID               IID\nObservations                  721               721\nR2                        0.89991           0.90059\nAdj. R2                   0.89949           0.90017\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results show that the size factor is only significant at the 5% level for the FF3 data, but it is significant at the 1% level for the FF5 version!"
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html#a-quick-look-at-daily-factors",
    "href": "blog/fama-french-three-vs-five-factors/index.html#a-quick-look-at-daily-factors",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "A quick look at daily factors",
    "text": "A quick look at daily factors\nLet us take a quick look at the daily factors to check whether the difference in size premia exists there as well. If my explanation for the difference is correct, then there should be differences. We can download the daily factor data in a similar fashion as the monthly data.\n\nfactors_ff3_daily_raw &lt;- download_french_data(\"Fama/French 3 Factors [Daily]\")\nfactors_ff3_daily &lt;- factors_ff3_daily_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = ymd(date),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`)\n\nfactors_ff5_daily_raw &lt;- download_french_data(\"Fama/French 5 Factors (2x3) [Daily]\")\nfactors_ff5_daily &lt;- factors_ff5_daily_raw$subsets$data[[1]] |&gt;\n  mutate(\n    date = ymd(date),\n    across(c(RF, `Mkt-RF`, SMB, HML, RMW, CMA), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) \n\nfactors_ff_daily &lt;- factors_ff3_daily |&gt; \n  rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff3\")) |&gt; \n  inner_join(\n    factors_ff5_daily |&gt; \n      select(date, mkt_excess, rf, smb, hml) |&gt; \n      rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff5\")), by = \"date\")  |&gt; \n  filter(date &gt;= start_date & date &lt;= end_date)\n\nmodel_smb &lt;- lm(smb_ff3 ~ smb_ff5, data = factors_ff_daily)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb_ff3 ~ smb_ff5, data = factors_ff_daily)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.019389 -0.000393  0.000026  0.000419  0.013334 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.50e-05   1.02e-05   -1.48     0.14    \nsmb_ff5      9.72e-01   1.88e-03  517.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00125 on 15121 degrees of freedom\nMultiple R-squared:  0.946, Adjusted R-squared:  0.946 \nF-statistic: 2.67e+05 on 1 and 15121 DF,  p-value: &lt;2e-16\n\n\nRegressing the FF3 size factor on its FF5 counterpart yields a coefficient of 0.97 and an R-squared around 95%, so again no perfect co-movement. Unreported results of the distributions and differences over time confirm the regression results and are in line with differences among monthly factor data."
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html#conclusion",
    "href": "blog/fama-french-three-vs-five-factors/index.html#conclusion",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "Conclusion",
    "text": "Conclusion\nAre there any implications for empirical applications? In my opinion, you should be careful when you want to test your portfolios against FF3 and FF5 factors. It is strictly speaking not correct to just use a subsample of factors from FF5 if you want to test against the FF3 factors. I rather recommend downloading both FF3 and FF5 and run tests with each data set separately.\n\n\n\nFigure 1: End-of-month difference between monthly size factors from Fama-French three (FF3) and five (FF5) factor data."
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html#footnotes",
    "href": "blog/fama-french-three-vs-five-factors/index.html#footnotes",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFama, E. F.; French, K. R. (1993). “Common risk factors in the returns on stocks and bonds”. Journal of Financial Economics. 33: 3–56. https://doi.org/10.1016/0304-405X(93)90023-5↩︎\nFama, E. F.; French, K. R. (2015). “A Five-Factor Asset Pricing Model”. Journal of Financial Economics. 116: 1–22. CiteSeerX 10.1.1.645.3745. https://doi.org/10.1016/j.jfineco.2014.10.010↩︎"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html",
    "href": "blog/nse-portfolio-sorts/index.html",
    "title": "Non-standard errors in portfolio sorts",
    "section": "",
    "text": "Welcome to our latest blog post where we delve into the intriguing world of non-standard errors1, a crucial aspect of academic research, which also is addressed in financial economics. These non-standard errors often stem from the methodological decisions we make, adding an extra dimension of uncertainty to the estimates we report. I had the pleasure of working with Stefan Voigt on a contribution to Menkveld et al. (2023), which was an exciting opportunity to shape the first discussions on non-standard errors.\nOne of the goals of Tidy Finance has always been focused on promoting reproducibility in finance research. We began this endeavor by introducing a chapter on Size sorts and p-hacking, which initiated some analysis of portfolio sorting choices. Recently, my fellow authors, Dominik Walter and Rüdiger Weber, and I published an update to our working paper, Non-Standard Errors in Portfolio Sorts2. This paper delves into the impact of methodological choices on return differentials derived from portfolio sorts. Our conclusion? We need to accept non-standard errors in empirical asset pricing. By reporting the entire range of return differentials, we simultaneously deepen our economic understanding of return anomalies and improve trust.\nThis blog post will guide you through conducting portfolio sorts which keep non-standard errors in perspective. We explore the multitude of possible decisions that can impact return differentials, allowing us to estimate a premium’s distribution rather than a single return differential. The code is inspired by Tidy Finance with R and the replication code for Walter, Weber, and Weiss (2023), which can be found in this Github repository. By the end of this post, you will have the knowledge to sort portfolios based on asset growth in nearly 70,000 different ways.\nWhile this post is detailed, it is not overly complex. However, if you are new to R or portfolio sorts, I recommend first checking out our chapter on Size sorts and p-hacking. Due to the length constraints, we will be skipping over certain details related to implementation and the economic background (you can find these in the WWW paper). If there are particular aspects you would like to delve into further, please feel free to reach out."
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#merge-data",
    "href": "blog/nse-portfolio-sorts/index.html#merge-data",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Merge data",
    "text": "Merge data\nOne key decision node is the sorting variable lag. However, merging data is an expensive operation, and doing it repeatedly is unnecessary. Hence, we merge the data in the three possible lag configurations and store them as separate tibbles. Thereby, we can later reference the correct table instead of merging the desired output.\nFirst, let us consider the Fama-French (FF) lag. Here, we consider accounting information published in year \\(t-1\\) starting from July of year \\(t\\). That is, we use the accounting information published 6 to 18 months ago. We first match the accounting data to the stock market data before we fill in the missing observations. A few pitfalls exist when using the fill()-function. First, one might easily forget to order and group the data. Second, the function does not care how outdated the information becomes. In principle, you can end up with data that is decades old. Therefore, we ensure that these filled data points are not older than 12 months. This is achieved with the new variable sv_age_check, which serves as a filter for outdated data. Finally, notice that this code provides much flexibility. All variables with prefixes sv_ and filter_ get filled. So you can easily adapt my code to your needs.\n\ndata_FF &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month) |&gt;\n  left_join(\n    compustat |&gt;\n      mutate(\n        sorting_date = floor_date(datadate, \"year\") %m+% months(18),\n        sv_age_check = sorting_date\n      ) |&gt;\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n# Fill variables and ensure timeliness of data\ndata_FF &lt;- data_FF |&gt;\n  arrange(permno, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |&gt;\n  ungroup() |&gt;\n  filter(sv_age_check &gt; month %m-% months(12)) |&gt;\n  select(-sv_age_check, -sorting_date, -gvkey)\n\nNext, we create the basis with lags of three and six months. The process is exactly the same as above for the FF lag, but without the floor_date() as we apply a constant lag to all observations. Again, we make sure that after the call to fill() our information does not become too old.\n\n# 3 months\n## Merge data\ndata_3m &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month) |&gt;\n  left_join(\n    compustat |&gt;\n      mutate(\n        sorting_date = floor_date(datadate, \"month\") %m+% months(3),\n        sv_age_check = sorting_date\n      ) |&gt;\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n## Fill variables and ensure timeliness of data\ndata_3m &lt;- data_3m |&gt;\n  arrange(permno, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |&gt;\n  ungroup() |&gt;\n  filter(sv_age_check &gt; month %m-% months(12)) |&gt;\n  select(-sv_age_check, -sorting_date, -gvkey)\n\n# 6 months\n## Merge data\ndata_6m &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month) |&gt;\n  left_join(\n    compustat |&gt;\n      mutate(\n        sorting_date = floor_date(datadate, \"month\") %m+% months(6),\n        sv_age_check = sorting_date\n      ) |&gt;\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n## Fill variables and ensure timeliness of data\ndata_6m &lt;- data_6m |&gt;\n  arrange(permno, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |&gt;\n  ungroup() |&gt;\n  filter(sv_age_check &gt; month %m-% months(12)) |&gt;\n  select(-sv_age_check, -sorting_date, -gvkey)"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#functions",
    "href": "blog/nse-portfolio-sorts/index.html#functions",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Functions",
    "text": "Functions\nWe write functions that complete specific tasks and then combine them to generate the desired output. Breaking it up into smaller steps makes the whole process more tractable and easier to test.\n\nSelect the sample\nThe first function gets the name handle_data() because it is intended to select the sample according to the sample construction choices. The function first selects the data based on the desired sorting variable lag (specified in sv_lag). Then, we apply the various filters we discussed above. As you see, this is relatively simple, but it already covers our sample construction nodes.\n\nhandle_data &lt;- function(include_financials, include_utilities,\n                        drop_smallNYSE_at, drop_price_at, drop_stock_age_at,\n                        drop_earnings, drop_bookequity,\n                        sv_lag) {\n  # Select dataset\n  if (sv_lag == \"FF\") data_all &lt;- data_FF\n  if (sv_lag == \"3m\") data_all &lt;- data_3m\n  if (sv_lag == \"6m\") data_all &lt;- data_6m\n\n  # Size filter based on NYSE percentile\n  if (drop_smallNYSE_at &gt; 0) {\n    data_all &lt;- data_all |&gt;\n      group_by(month) |&gt;\n      mutate(NYSE_breakpoint = quantile(\n        mktcap_lag[exchange == \"NYSE\"],\n        drop_smallNYSE_at\n      )) |&gt;\n      ungroup() |&gt;\n      filter(mktcap_lag &gt;= NYSE_breakpoint) |&gt;\n      select(-NYSE_breakpoint)\n  }\n\n  # Exclude industries\n  data_all &lt;- data_all |&gt;\n    filter(if (include_financials) TRUE else !grepl(\"Finance\", industry)) |&gt;\n    filter(if (include_utilities) TRUE else !grepl(\"Utilities\", industry))\n\n  # Book equity filter\n  if (drop_bookequity) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_be &gt; 0)\n  }\n\n  # Earnings filter\n  if (drop_earnings) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_earnings &gt; 0)\n  }\n\n  # Stock age filter\n  if (drop_stock_age_at &gt; 0) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_stock_age &gt;= drop_stock_age_at)\n  }\n\n  # Price filter\n  if (drop_price_at &gt; 0) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_price &gt;= drop_price_at)\n  }\n\n  # Define ME\n  data_all &lt;- data_all |&gt;\n    mutate(me = mktcap_lag) |&gt;\n    drop_na(me) |&gt;\n    select(-starts_with(\"filter_\"), -industry)\n\n  # Return\n  return(data_all)\n}\n\n\n\nAssign portfolios\nNext, we define a function that assigns portfolios based on the specified sorting variable, the number of portfolios, and the exchanges. The function only works on a single cross-section of data, i.e., it has to be applied to individual months of data. The central part of the function is to compute the \\(n\\) breakpoints based on the exchange filter. Then, findInterval() assigns the respective portfolio number.\nThe function also features two sanity checks. First, it does not assign portfolios if there are too few stocks in the cross-section. Second, sometimes the sorting variable creates scenarios where some portfolios are overpopulated. For example, if the variable in question is bounded from below by 0. In such a case, an unexpectedly large number of firms might end up in the lowest bucket, covering multiple quantiles.\n\nassign_portfolio &lt;- function(data, sorting_variable,\n                             n_portfolios, exchanges) {\n  # Escape small sets (i.e., less than 10 firms per portfolio)\n  if (nrow(data) &lt; n_portfolios * 10) {\n    return(NA)\n  }\n\n  # Compute breakpoints\n  breakpoints &lt;- data |&gt;\n    filter(grepl(exchanges, exchange)) |&gt;\n    pull(all_of(sorting_variable)) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  # Assign portfolios\n  portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull(all_of(sorting_variable)),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n\n  # Check if breakpoints are well defined\n  if (length(unique(breakpoints)) == n_portfolios + 1) {\n    return(portfolios)\n  } else {\n    print(breakpoints)\n    cat(paste0(\n      \"\\n Breakpoint issue! Month \",\n      as.Date(as.numeric(cur_group())),\n      \"\\n\"\n    ))\n    stop()\n  }\n}\n\n\n\nSingle and double sorts\nOur goal is to construct portfolios for single sorts, independent double sorts, and dependent double sorts. Hence, our next three functions do exactly that. The double sorts considered always take a first sort on market equity (the variable me) before sorting on the actual sorting variable.\nLet us start with single sorts. As you see, we group by month as the function assign_portfolio() we wrote above handles one cross-section at a time. The rest of the function just passes the arguments to the portfolio assignment.\n\nsort_single &lt;- function(data, sorting_variable,\n                        exchanges, n_portfolios_main) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(portfolio = assign_portfolio(\n      data = pick(all_of(sorting_variable), exchange),\n      sorting_variable = sorting_variable,\n      n_portfolios = n_portfolios_main,\n      exchanges = exchanges\n    )) |&gt;\n    drop_na(portfolio) |&gt;\n    ungroup()\n}\n\nFor double sorts, things are more interesting. First, we have the issue of independent and dependent double sorts. An independent sort considers the two sorting variables (size and asset growth) independently. In contrast, dependent sorts are, in our case, first sorting on size and within these buckets on asset growth. We group by the secondary portfolio to achieve the dependent sort before generating the main portfolios. Second, we need to generate an overall portfolio of the two sorts - we will see this later.\n\nsort_double_independent &lt;- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(\n      portfolio_secondary = assign_portfolio(\n        data = pick(me, exchange),\n        sorting_variable = \"me\",\n        n_portfolios = n_portfolios_secondary,\n        exchanges = exchanges\n      ),\n      portfolio_main = assign_portfolio(\n        data = pick(all_of(sorting_variable), exchange),\n        sorting_variable = sorting_variable,\n        n_portfolios = n_portfolios_main,\n        exchanges = exchanges\n      ),\n      portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)\n    ) |&gt;\n    drop_na(portfolio_main, portfolio_secondary) |&gt;\n    ungroup()\n}\n\nsort_double_dependent &lt;- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(portfolio_secondary = assign_portfolio(\n      data = pick(me, exchange),\n      sorting_variable = \"me\",\n      n_portfolios = n_portfolios_secondary,\n      exchanges = exchanges\n    )) |&gt;\n    drop_na(portfolio_secondary) |&gt;\n    group_by(month, portfolio_secondary) |&gt;\n    mutate(\n      portfolio_main = assign_portfolio(\n        data = pick(all_of(sorting_variable), exchange),\n        sorting_variable = sorting_variable,\n        n_portfolios = n_portfolios_main,\n        exchanges = exchanges\n      ),\n      portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)\n    ) |&gt;\n    drop_na(portfolio_main) |&gt;\n    ungroup()\n}\n\n\n\nAnnual vs monthly rebalancing\nNow, we still have one decision node to cover: Rebalancing. We can either rebalance annually in July or monthly. To achieve this, we write two more functions - the last functions before finishing up. Let us start with monthly rebalancing because it is much easier. All we need to do is to use the assigned portfolio numbers to generate portfolio returns. Inside the function, we use three if() calls to decide the sorting method. Notice that the double sorts use the simple average for aggregating the extreme portfolios of the size buckets.\n\nrebalance_monthly &lt;- function(data, sorting_variable, sorting_method,\n                              n_portfolios_main, n_portfolios_secondary,\n                              exchanges, value_weighted) {\n  # Single sort\n  if (sorting_method == \"single\") {\n    data_rets &lt;- data |&gt;\n      sort_single(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main\n      ) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        .groups = \"drop\"\n      )\n  }\n\n  # Double independent sort\n  if (sorting_method == \"dbl_ind\") {\n    data_rets &lt;- data |&gt;\n      sort_double_independent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |&gt;\n      group_by(month, portfolio_main) |&gt;\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |&gt;\n      rename(portfolio = portfolio_main)\n  }\n\n  # Double dependent sort\n  if (sorting_method == \"dbl_dep\") {\n    data_rets &lt;- data |&gt;\n      sort_double_dependent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |&gt;\n      group_by(month, portfolio_main) |&gt;\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |&gt;\n      rename(portfolio = portfolio_main)\n  }\n\n  return(data_rets)\n}\n\nNow, let us move to the annual rebalancing. Here, we first assign a portfolio on the data in July based on single or independent/dependent double sorts. Then, we fill the remaining months forward before computing returns. Hence, we need one extra step for each sort.\n\nrebalance_annually &lt;- function(data, sorting_variable, sorting_method,\n                               n_portfolios_main, n_portfolios_secondary,\n                               exchanges, value_weighted) {\n  data_sorting &lt;- data |&gt;\n    filter(month(month) == 7) |&gt;\n    group_by(month)\n\n  # Single sort\n  if (sorting_method == \"single\") {\n    # Assign portfolios\n    data_sorting &lt;- data_sorting |&gt;\n      sort_single(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main\n      ) |&gt;\n      select(permno, month, portfolio) |&gt;\n      mutate(sorting_month = month)\n  }\n\n  # Double independent sort\n  if (sorting_method == \"dbl_ind\") {\n    # Assign portfolios\n    data_sorting &lt;- data_sorting |&gt;\n      sort_double_independent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |&gt;\n      mutate(sorting_month = month)\n  }\n\n  # Double dependent sort\n  if (sorting_method == \"dbl_dep\") {\n    # Assign portfolios\n    data_sorting &lt;- data_sorting |&gt;\n      sort_double_dependent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |&gt;\n      mutate(sorting_month = month)\n  }\n\n  # Compute portfolio return\n  if (sorting_method == \"single\") {\n    data |&gt;\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |&gt;\n      group_by(permno) |&gt;\n      arrange(month) |&gt;\n      fill(portfolio, sorting_month) |&gt;\n      filter(sorting_month &gt;= month %m-% months(12)) |&gt;\n      drop_na(portfolio) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        .groups = \"drop\"\n      )\n  } else {\n    data |&gt;\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |&gt;\n      group_by(permno) |&gt;\n      arrange(month) |&gt;\n      fill(portfolio_main, portfolio_secondary, portfolio, sorting_month) |&gt;\n      filter(sorting_month &gt;= month %m-% months(12)) |&gt;\n      drop_na(portfolio_main, portfolio_secondary) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |&gt;\n      group_by(month, portfolio_main) |&gt;\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |&gt;\n      rename(portfolio = portfolio_main)\n  }\n}\n\n\n\nCombining the functions\nNow, we have everything to compute our 69,120 portfolio sorts for asset growth to understand the variation our decisions induce. To achieve this, our function considers all choices as arguments and passes them to the sample selection and portfolio construction functions.\nFinally, the function computes the return differential for each month. Since we are only interested in the mean here, we simply take the mean of these time series and call it our premium estimate.\n\nexecute_sorts &lt;- function(sorting_variable, drop_smallNYSE_at,\n                          include_financials, include_utilities,\n                          drop_bookequity, drop_earnings,\n                          drop_stock_age_at, drop_price_at,\n                          sv_lag, formation_time,\n                          n_portfolios_main, sorting_method,\n                          n_portfolios_secondary, exchanges,\n                          value_weighted) {\n  # Select data\n  data_sorts &lt;- handle_data(\n    include_financials = include_financials,\n    include_utilities = include_utilities,\n    drop_smallNYSE_at = drop_smallNYSE_at,\n    drop_price_at = drop_price_at,\n    drop_stock_age_at = drop_stock_age_at,\n    drop_earnings = drop_earnings,\n    drop_bookequity = drop_bookequity,\n    sv_lag = sv_lag\n  )\n\n  # Rebalancing\n  ## Monthly\n  if (formation_time == \"monthly\") {\n    data_return &lt;- rebalance_monthly(\n      data = data_sorts,\n      sorting_variable = sorting_variable,\n      sorting_method = sorting_method,\n      n_portfolios_main = n_portfolios_main,\n      n_portfolios_secondary = n_portfolios_secondary,\n      exchanges = exchanges,\n      value_weighted = value_weighted\n    )\n  }\n\n  ## Annually\n  if (formation_time == \"FF\") {\n    data_return &lt;- rebalance_annually(\n      data = data_sorts,\n      sorting_variable = sorting_variable,\n      sorting_method = sorting_method,\n      n_portfolios_main = n_portfolios_main,\n      n_portfolios_secondary = n_portfolios_secondary,\n      exchanges = exchanges,\n      value_weighted = value_weighted\n    )\n  }\n\n  # Compute return differential\n  data_return |&gt;\n    group_by(month) |&gt;\n    summarize(\n      premium = ret[portfolio == max(portfolio)] - ret[portfolio == min(portfolio)],\n      .groups = \"drop\"\n    ) |&gt;\n    pull(premium) |&gt;\n    mean() * 100\n}"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#applying-the-functions",
    "href": "blog/nse-portfolio-sorts/index.html#applying-the-functions",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Applying the functions",
    "text": "Applying the functions\nFinally, we have data, decisions, and functions. Indeed, we are now ready to implement the portfolio sort, right? Yes! Just let me briefly discuss how the implementation works.\nWe have a grid with 69,120 specifications. For each of these specifications, we want to estimate a return differential. This is most easily achieved with a pmap() call. However, we want to parallelize the operation to leverage the multiple cores of our device. Hence, we have to use the package furrr and a future_pmap() instead. As a side note, in most cases we also have to increase the maximum memory for each worker, which can be done with options().\n\nlibrary(furrr)\n\noptions(future.globals.maxSize = 891289600)\n\nplan(multisession, workers = availableCores())\n\nWith the parallel environment set and ready to go, we map the arguments our function needs into the final function execute_sorts() from above. Then, we go and have some tea. And some lunch, breakfast, second breakfast, and so on. In short, it takes a while - depending on your device even more than a day. Each result requires roughly 13 seconds, but you must remember that you are computing 69,120 results.\n\ndata_premia &lt;- setup_grid |&gt;\n  mutate(premium_estimate = future_pmap(\n    .l = list(\n      sorting_variable, drop_smallNYSE_at, include_financials,\n      include_utilities, drop_bookequity, drop_earnings,\n      drop_stock_age_at, drop_price_at, sv_lag,\n      formation_time, n_portfolios_main, sorting_method,\n      n_portfolios_secondary, exchanges, value_weighted\n    ),\n    .f = ~ execute_sorts(\n      sorting_variable = ..1,\n      drop_smallNYSE_at = ..2,\n      include_financials = ..3,\n      include_utilities = ..4,\n      drop_bookequity = ..5,\n      drop_earnings = ..6,\n      drop_stock_age_at = ..7,\n      drop_price_at = ..8,\n      sv_lag = ..9,\n      formation_time = ..10,\n      n_portfolios_main = ..11,\n      sorting_method = ..12,\n      n_portfolios_secondary = ..13,\n      exchanges = ..14,\n      value_weighted = ..15\n    )\n  ))\n\nNow you have all the estimates for the premium. However, one last step has to be considered when you actually investigate the premium. The portfolio sorting algorithm we constructed here is always long in the firms with a high value for the sorting variable and short in firms with low characteristics. This provides a very general way of doing it. Hence, you simply correct for this effect at the end by multiplying the column by -1 if your sorting variable predicts an inverse relation between the sorting variable and expected returns. Otherwise, you can ignore the following chunk.\n\ndata_premia &lt;- data_premia |&gt;\n  mutate(premium_estimate = premium_estimate * -1)"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#footnotes",
    "href": "blog/nse-portfolio-sorts/index.html#footnotes",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMenkveld, A. J. et al. (2023). “Non-standard Errors”, Journal of Finance (forthcoming). http://dx.doi.org/10.2139/ssrn.3961574↩︎\nWalter, D., Weber, R., and Weiss, P. (2023). “Non-Standard Errors in Portfolio Sorts”. http://dx.doi.org/10.2139/ssrn.4164117↩︎\nCooper, M. J., Gulen, H., and Schill, M. J. (2008). “Asset growth and the cross‐section of stock returns”, The Journal of Finance, 63(4), 1609-1651.↩︎"
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html",
    "href": "blog/r-tidyfinance-0-1-0/index.html",
    "title": "tidyfinance 0.1.0",
    "section": "",
    "text": "We are happy to annouce the initial release of the tidyfinance R package on CRAN. The package contains a set of helper functions for empirical research in financial economics, addressing a variety of topics covered in Tidy Finance with R (TFWR). We designed the package to provide easy shortcuts for the applications that we discuss in the book. If you want to inspect the details of the package or propose new features, feel free to visit the package repository on Github.\nIn this blog post, we demonstrate the features of the initial release. We decided to focus on functions that allow you to download the data that we use in TFWR."
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#install-the-package",
    "href": "blog/r-tidyfinance-0-1-0/index.html#install-the-package",
    "title": "tidyfinance 0.1.0",
    "section": "Install the package",
    "text": "Install the package\nYou can install the released version of tidyfinance from CRAN via:\n\ninstall.packages(\"tidyfinance\")\n\nYou can install the development version of tidyfinance from GitHub using the pak package:\n\npak::pak(\"tidy-finance/r-tidyfinance\")"
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#download-data",
    "href": "blog/r-tidyfinance-0-1-0/index.html#download-data",
    "title": "tidyfinance 0.1.0",
    "section": "Download data",
    "text": "Download data\nLet’s start by loading the package\n\nlibrary(tidyfinance)\n\nThe main function is download_data(type, start_date, end_date) with supported type:\n\nlist_supported_types()\n\n# A tibble: 20 × 3\n  type                  dataset_name                   domain  \n  &lt;chr&gt;                 &lt;chr&gt;                          &lt;chr&gt;   \n1 factors_q5_daily      q5_factors_daily_2022.csv      Global Q\n2 factors_q5_weekly     q5_factors_weekly_2022.csv     Global Q\n3 factors_q5_weekly_w2w q5_factors_weekly_w2w_2022.csv Global Q\n4 factors_q5_monthly    q5_factors_monthly_2022.csv    Global Q\n5 factors_q5_quarterly  q5_factors_quarterly_2022.csv  Global Q\n# ℹ 15 more rows\n\n\nSo, for instance, if you want to download monthly Fama-French Three-Factor data, you can call:\n\ndownload_data(\"factors_ff3_monthly\", \"2020-01-01\", \"2020-12-31\")\n\n# A tibble: 12 × 5\n  date       risk_free mkt_excess     smb     hml\n  &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 2020-01-01    0.0013    -0.0011 -0.0311 -0.0625\n2 2020-02-01    0.0012    -0.0813  0.0107 -0.0381\n3 2020-03-01    0.0013    -0.134  -0.0483 -0.139 \n4 2020-04-01    0          0.136   0.0245 -0.0133\n5 2020-05-01    0.0001     0.0558  0.0247 -0.0488\n# ℹ 7 more rows\n\n\nUnder the hood, the function uses the frenchdata package (see its documentation here) and applies some cleaning steps, as in TFWR. If you haven’t installed frenchdata yet, you’ll get prompted to install it first before you can download this specific data type.\nYou can also access q-Factor data in this way, by calling:\n\ndownload_data(\"factors_q5_daily\", \"2020-01-01\", \"2020-12-31\")\n\n# A tibble: 253 × 7\n  date       risk_free mkt_excess       me       ia      roe       eg\n  &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2020-01-02  0.000055    0.00863 -0.0112  -0.00171  6.84e-4  3.41e-3\n2 2020-01-03  0.000055   -0.00673  0.00234 -0.00193 -1.55e-3  6.83e-4\n3 2020-01-06  0.000055    0.00360 -0.00360 -0.00409 -4.78e-3  6.11e-4\n4 2020-01-07  0.000055   -0.00192 -0.00139 -0.00322 -5.12e-3 -2.74e-3\n5 2020-01-08  0.000055    0.00467 -0.00108 -0.00121  4.56e-3  6.14e-3\n# ℹ 248 more rows\n\n\nTo ensure that we can extend the functionality of the download functions for specific types, we also provide domain-specific download functions. The download_data(\"factors_ff3_monthly\") actually calls download_data_factors(\"factors_ff3_monthly\"), which in turn calls download_data_factors_ff(\"factors_ff3_monthly\"). Why did we decide to have these nested function approach?\nSuppose that the q-Factor data changes its URL path and our original function does not work anymore. In this case, you can replace the default url value in download_data_factors_q(type, start_date, end_date, url) to apply the usual cleaning steps.\nThis feature becomes more apparent for other data sources such as wrds_crsp_monthly. Note that you need to have valid WRDS credentials and need to set them correctly (check ?get_wrds_connection and WRDS, CRSP, and Compustat in TFWR). If you want to download the standard monthly CRSP data, you can call:\n\ndownload_data(\"wrds_crsp_monthly\", \"2020-01-01\", \"2020-12-31\")\n\nIf you want to add further columns, you can add them via ... to download_data_wrds_crsp(), for instance:\n\ndownload_data_wrds_crsp(\"wrds_crsp_monthly\", \"2020-01-01\", \"2020-12-31\", mthvol)\n\nNote that the function downloads CRSP v2 as default, as we do in our book since February 2024. If you want to download the old version of CRSP before the update, you can use the version = v1 parameter in download_data_wrds_crsp() .\nAs another example, you can do the same for Compustat:\n\ndownload_data_wrds_compustat(\"wrds_compustat_annual\", \"2000-01-01\", \"2020-12-31\", acoxar, amc, aldo)\n\nCheck out the list of supported types and the corresponding download functions for more information on the respective customization options. We decided to provide limited functionality for the initial release on purpose and rather respond to community request than overengineer the package from the start."
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#browse-content-from-tfwr",
    "href": "blog/r-tidyfinance-0-1-0/index.html#browse-content-from-tfwr",
    "title": "tidyfinance 0.1.0",
    "section": "Browse content from TFWR",
    "text": "Browse content from TFWR\nWe include functions to check out content from TFWR in your browser. If you want to list all available R chapters, simply call the following function:\n\nlist_tidy_finance_chapters()\n\n [1] \"setting-up-your-environment\"                \n [2] \"introduction-to-tidy-finance\"               \n [3] \"accessing-and-managing-financial-data\"      \n [4] \"wrds-crsp-and-compustat\"                    \n [5] \"trace-and-fisd\"                             \n [6] \"other-data-providers\"                       \n [7] \"beta-estimation\"                            \n [8] \"univariate-portfolio-sorts\"                 \n [9] \"size-sorts-and-p-hacking\"                   \n[10] \"value-and-bivariate-sorts\"                  \n[11] \"replicating-fama-and-french-factors\"        \n[12] \"fama-macbeth-regressions\"                   \n[13] \"fixed-effects-and-clustered-standard-errors\"\n[14] \"difference-in-differences\"                  \n[15] \"factor-selection-via-machine-learning\"      \n[16] \"option-pricing-via-machine-learning\"        \n[17] \"parametric-portfolio-policies\"              \n[18] \"constrained-optimization-and-backtesting\"   \n[19] \"wrds-dummy-data\"                            \n[20] \"cover-and-logo-design\"                      \n[21] \"clean-enhanced-trace-with-r\"                \n[22] \"proofs\"                                     \n[23] \"hex-sticker\"                                \n[24] \"changelog\"                                  \n\n\nThe function returns a character vector containing the names of the chapters available in TFWR. If you want to look at a specific chapter, you can call:\n\nopen_tidy_finance_website(\"beta-estimation\")\n\nThis opens either the specific chapter you requested or the main index page in your default web browser."
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#regression-helpers",
    "href": "blog/r-tidyfinance-0-1-0/index.html#regression-helpers",
    "title": "tidyfinance 0.1.0",
    "section": "Regression helpers",
    "text": "Regression helpers\nWe discuss winsorization in TFWR, so we figured providing this function could be useful:\n\nlibrary(tibble)\nlibrary(dplyr)\n\nset.seed(123)\ndata &lt;- tibble(x = rnorm(100)) |&gt; \n  arrange(x)\n\ndata |&gt; \n  mutate(x_winsorized = winsorize(x, 0.01))\n\n# A tibble: 100 × 2\n      x x_winsorized\n  &lt;dbl&gt;        &lt;dbl&gt;\n1 -2.31        -1.97\n2 -1.97        -1.97\n3 -1.69        -1.69\n4 -1.55        -1.55\n5 -1.27        -1.27\n# ℹ 95 more rows\n\n\nIf you rather want to replace the bottom and top quantiles of your distribution with missing values, then you can use trim()\n\ndata |&gt; \n  mutate(x_trimmed = trim(x, 0.01))\n\n# A tibble: 100 × 2\n      x x_trimmed\n  &lt;dbl&gt;     &lt;dbl&gt;\n1 -2.31     NA   \n2 -1.97     -1.97\n3 -1.69     -1.69\n4 -1.55     -1.55\n5 -1.27     -1.27\n# ℹ 95 more rows\n\n\nWe also discuss the importance of providing summary statistics of your data, so there is also a function for that:\n\ncreate_summary_statistics(data, x, detail = TRUE)\n\n# A tibble: 1 × 15\n  variable     n   mean    sd   min   q01   q05   q10    q25    q50\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 x          100 0.0904 0.913 -2.31 -1.97 -1.27 -1.07 -0.494 0.0618\n# ℹ 5 more variables: q75 &lt;dbl&gt;, q90 &lt;dbl&gt;, q95 &lt;dbl&gt;, q99 &lt;dbl&gt;,\n#   max &lt;dbl&gt;"
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#experimental-functions",
    "href": "blog/r-tidyfinance-0-1-0/index.html#experimental-functions",
    "title": "tidyfinance 0.1.0",
    "section": "Experimental functions",
    "text": "Experimental functions\nWe have two more experimental functions in the sense that it is unclear in which direction they might evolve. First you can assign portfolios based on a sorting variable using assign_portfolio():\n\ndata &lt;- tibble(\n  id = 1:100,\n  exchange = sample(c(\"NYSE\", \"NASDAQ\"), 100, replace = TRUE),\n  market_cap = runif(100, 1e6, 1e9)\n)\n\ndata |&gt; \n  mutate(\n    portfolio = assign_portfolio(\n      pick(everything()), \"market_cap\", n_portfolios = 5, exchanges = c(\"NYSE\"))\n  )\n\n# A tibble: 100 × 4\n     id exchange market_cap portfolio\n  &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;int&gt;\n1     1 NASDAQ   784790691.         4\n2     2 NASDAQ    10420475.         1\n3     3 NASDAQ   779286817.         4\n4     4 NYSE     729661261.         4\n5     5 NASDAQ   630501721.         3\n# ℹ 95 more rows\n\n\nSecond, you can estimate the coefficients of a linear model specified by one or more independent variable using estimate_model():\n\ndata &lt;- tibble(\n  ret_excess = rnorm(100),\n  mkt_excess = rnorm(100),\n  smb = rnorm(100),\n  hml = rnorm(100)\n)\n\nestimate_model(data, \"ret_excess ~ mkt_excess + smb + hml\")\n\n  mkt_excess     smb    hml\n1    -0.0399 -0.0287 0.0207"
  },
  {
    "objectID": "blog/r-tidyfinance-0-1-0/index.html#concluding-remarks",
    "href": "blog/r-tidyfinance-0-1-0/index.html#concluding-remarks",
    "title": "tidyfinance 0.1.0",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nWe are curious to learn in which direction we should extend the package, so please consider opening an issue in the package repository. For instance, we could support more data sources, add more parameters to the download_* family of functions, or we could put more emphasis on the generality of portfolio assignment or other modeling functions."
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html",
    "href": "blog/tidy-finance-dummy-data/index.html",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "",
    "text": "Since we published our book Tidy Finance with R, we have received feedback from readers who don’t have access to WRDS that they cannot run the code we provide. To alleviate their constraints, we decided to create a dummy database that contains the WRDS tables and corresponding columns such that all code chunks in our book can be executed with this dummy database. The resulting database can be found through this link (around 50 MB). Just download the database and put it into your data folder (I already renamed it to tidy_finance.sqlite). Note that we do not create dummy data for macro tables because they can be freely downloaded from the original sources - check out Accessing and Managing Financial Data.\nWe deliberately use the dummy label because the data is not meaningful in the sense that it allows readers to actually replicate the results of the book. For legal reasons, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books.\nTo generate the dummy database, we use the following packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nLet us initialize a tidy_finance.sqlite database or connect to your existing one. Be careful, if you already downloaded the data from WRDS, then the code in this blog post will overwrite your data!\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\nSince we draw random numbers for most of the columns, we also define a seed to ensure that the generated numbers are replicable. We also initialize vectors of dates of different frequencies over 10 years that we then use to create yearly, monthly, and daily data, respectively.\nset.seed(1234)\n\nstart_date &lt;- as.Date(\"2003-01-01\")\nend_date &lt;- as.Date(\"2022-12-31\")\n\ntime_series_years &lt;- seq(year(start_date), year(end_date), 1)\ntime_series_months &lt;- seq(start_date, end_date, \"1 month\")\ntime_series_days &lt;- seq(start_date, end_date, \"1 day\")"
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html#create-stock-dummy-data",
    "href": "blog/tidy-finance-dummy-data/index.html#create-stock-dummy-data",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "Create stock dummy data",
    "text": "Create stock dummy data\nLet us start with the core data used throughout the book: stock and firm characteristics. We first generate a table with a cross-section of stock identifiers with unique permno and gvkey values, as well as associated exchcd, exchange, industry, and siccd values. The generated data is based on the characteristics of stocks in the crsp_monthly table of the original database, ensuring that the generated stocks roughly reflect the distribution of industries and exchanges in the original data, but the identifiers and corresponding exchanges or industries do not reflect actual firms. Similarly, the permno-gvkey combinations are purely nonsensical and should not be used together with actual CRSP or Compustat data.\n\nnumber_of_stocks &lt;- 100\n\nindustries &lt;- tibble(\n  industry = c(\"Agriculture\", \"Construction\", \"Finance\", \n               \"Manufacturing\", \"Mining\", \"Public\", \"Retail\", \n               \"Services\", \"Transportation\", \"Utilities\", \n               \"Wholesale\"),\n  n = c(81, 287, 4682, 8584, 1287, 1974, 1571, 4277, 1249, \n        457, 904),\n  prob = c(0.00319, 0.0113, 0.185, 0.339, 0.0508, 0.0779, \n           0.0620, 0.169, 0.0493, 0.0180, 0.0357)\n)\n\nexchanges &lt;- exchanges &lt;- tibble(\n  exchange = c(\"AMEX\", \"NASDAQ\", \"NYSE\"),\n  n = c(2893, 17236, 5553),\n  prob = c(0.113, 0.671, 0.216)\n)\n\nstock_identifiers &lt;- 1:number_of_stocks |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        permno = x,\n        gvkey = as.character(x + 10000),\n        exchange = sample(exchanges$exchange, 1, \n                          prob = exchanges$prob),\n        industry = sample(industries$industry, 1, \n                          prob = industries$prob)\n      ) |&gt; \n        mutate(\n          exchcd = case_when(\n            exchange == \"NYSE\" ~ sample(c(1, 31), n()),\n            exchange == \"AMEX\" ~ sample(c(2, 32), n()),\n            exchange == \"NASDAQ\" ~ sample(c(3, 33), n())\n          ),\n          siccd = case_when(\n            industry == \"Agriculture\" ~ sample(1:999, n()),\n            industry == \"Mining\" ~ sample(1000:1499, n()),\n            industry == \"Construction\" ~ sample(1500:1799, n()),\n            industry == \"Manufacturing\" ~ sample(1800:3999, n()),\n            industry == \"Transportation\" ~ sample(4000:4899, n()),\n            industry == \"Utilities\" ~ sample(4900:4999, n()),\n            industry == \"Wholesale\" ~ sample(5000:5199, n()),\n            industry == \"Retail\" ~ sample(5200:5999, n()),\n            industry == \"Finance\" ~ sample(6000:6799, n()),\n            industry == \"Services\" ~ sample(7000:8999, n()),\n            industry == \"Public\" ~ sample(9000:9999, n())\n          )\n        )\n    }\n  )\n\nNext, we construct three panels of stock data with varying frequencies: yearly, monthly, and daily. We begin by creating the stock_panel_yearly panel. To achieve this, we combine the stock_identifiers table with a new table containing the variable year from time_series_years. The expand_grid() function ensures that we get all possible combinations of the two tables. After combining, we select only the gvkey and year columns for our final yearly panel.\nNext, we construct the stock_panel_monthly panel. Similar to the yearly panel, we use the expand_grid() function to combine stock_identifiers with a new table that has the month variable from time_series_months. After merging, we select the columns permno, gvkey, month, siccd, industry, exchcd, and exchange to form our monthly panel.\nLastly, we create the stock_panel_daily panel. We combine stock_identifiers with a table containing the date variable from time_series_days. After merging, we retain only the permno and date columns for our daily panel.\n\nstock_panel_yearly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(year = time_series_years)\n) |&gt; \n  select(gvkey, year)\n\nstock_panel_monthly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(month = time_series_months)\n) |&gt; \n  select(permno, gvkey, month, siccd, industry, exchcd, exchange)\n\nstock_panel_daily &lt;- expand_grid(\n  stock_identifiers, \n  tibble(date = time_series_days)\n)|&gt; \n  select(permno, date)\n\n\nDummy beta table\nWe then proceed to create dummy beta values for our stock_panel_monthly table. We generate monthly beta values beta_monthly using the rnorm() function with a mean and standard deviation of 1. For daily beta values beta_daily, we take the dummy monthly beta and add a small random noise to it. This noise is generated again using the rnorm() function, but this time we divide the random values by 100 to ensure they are small deviations from the monthly beta.\n\nbeta_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    beta_monthly = rnorm(n(), mean = 1, sd = 1),\n    beta_daily = beta_monthly + rnorm(n()) / 100\n  )\n\ndbWriteTable(\n  tidy_finance,\n  \"beta\", \n  beta_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy compustat table\nTo create dummy firm characteristics, we take all columns from the compustat table and create random numbers between 0 and 1. For simplicity, we set the datadate for each firm-year observation to the last day of the year, although it is empirically not the case. We then use the !!! operator to unlist and execute a list of commands. This trick actually helps us to avoid typing the same function for each column individually.\n\nrelevant_columns &lt;- c(\n  \"seq\", \"ceq\", \"at\", \"lt\", \"txditc\", \"txdb\", \"itcb\", \n  \"pstkrv\", \"pstkl\", \"pstk\", \"capx\", \"oancf\", \"sale\", \n  \"cogs\", \"xint\", \"xsga\", \"be\", \"op\", \"at_lag\", \"inv\"\n)\n\ncommands &lt;- unlist(\n  map(\n    relevant_columns, \n    ~rlang::exprs(!!..1 := runif(n()))\n  )\n)\n\ncompustat_dummy &lt;- stock_panel_yearly |&gt; \n  mutate(\n    datadate = ymd(str_c(year, \"12\", \"31\")),\n    !!!commands\n  )\n\ndbWriteTable(\n  tidy_finance, \n  \"compustat\", \n  compustat_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_monthly table\nThe crsp_monthly table only lacks a few more columns compared to stock_panel_monthly: the returns ret drawn from a normal distribution, the excess returns ret_excess with small deviations from the returns, the shares outstanding shrout and the last price per month altprc both drawn from uniform distributions, and the market capitalization mktcap as the product of shrout and altprc.\n\ncrsp_monthly_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    date = ceiling_date(month, \"month\") - 1,\n    ret = pmax(rnorm(n()), -1),\n    ret_excess = pmax(ret - runif(n(), 0, 0.0025), -1),\n    shrout = runif(n(), 1, 50) * 1000,\n    altprc = runif(n(), 0, 1000),\n    mktcap = shrout * altprc\n  ) |&gt; \n  group_by(permno) |&gt; \n  arrange(month) |&gt; \n  mutate(mktcap_lag = lag(mktcap)) |&gt; \n  ungroup()\n\ndbWriteTable(\n  tidy_finance, \n  \"crsp_monthly\",\n  crsp_monthly_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_daily table\nThe crsp_daily table only contains a month column and the daily excess returns ret_excess as additional columns to stock_panel_daily.\n\ncrsp_daily_dummy &lt;- stock_panel_daily |&gt; \n  mutate(\n    month = floor_date(date, \"month\"),\n    ret_excess = pmax(rnorm(n()), -1)\n  )\n\ndbWriteTable(\n  tidy_finance,\n  \"crsp_daily\",\n  crsp_daily_dummy, \n  overwrite = TRUE\n)"
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html#create-bond-dummy-data",
    "href": "blog/tidy-finance-dummy-data/index.html#create-bond-dummy-data",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "Create bond dummy data",
    "text": "Create bond dummy data\nLastly, we move to the bond data that we use in our books.\n\nDummy fisd data\nTo create dummy data with the structure of Mergent FISD, we calculate the empirical probabilities of actual bonds for several variables: maturity, offering_amt, interest_frequency, coupon, and sic_code. We use these probabilities to sample a small cross-section of bonds with completely made up complete_cusip, issue_id, and issuer_id.\n\nnumber_of_bonds &lt;- 100\n\nfisd_dummy &lt;- 1:number_of_bonds |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        complete_cusip = str_to_upper(\n          str_c(\n            sample(c(letters, 0:9), 12, replace = TRUE), \n            collapse = \"\"\n          )\n        ),\n      )\n    }\n  ) |&gt; \n  mutate(\n    maturity = sample(time_series_days, n(), replace = TRUE),\n    offering_amt = sample(seq(1:100) * 100000, n(), replace = TRUE),\n    offering_date = maturity - sample(seq(1:25) * 365, n(),replace = TRUE),\n    dated_date = offering_date - sample(-10:10, n(), replace = TRUE),\n    interest_frequency = sample(c(0, 1, 2, 4, 12), n(), replace = TRUE),\n    coupon = sample(seq(0, 2, by = 0.1), n(), replace = TRUE),\n    last_interest_date = pmax(maturity, offering_date, dated_date),\n    issue_id = row_number(),\n    issuer_id = sample(1:250, n(), replace = TRUE),\n    sic_code = as.character(sample(seq(1:9)*1000, n(), replace = TRUE))\n  )\n  \ndbWriteTable(\n  tidy_finance, \n  \"fisd\", \n  fisd_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy trace_enhanced data\nFinally, we create a dummy bond transaction data for the fictional CUSIPs of the dummy fisd data. We take the date range that we also analyze in the book and ensure that we have at least five transactions per day to fulfill a filtering step in the book.\n\nstart_date &lt;- as.Date(\"2014-01-01\")\nend_date &lt;- as.Date(\"2016-11-30\")\n\nbonds_panel &lt;- expand_grid(\n  fisd_dummy |&gt; \n    select(cusip_id = complete_cusip),\n  tibble(\n    trd_exctn_dt = seq(start_date, end_date, \"1 day\")\n  )\n)\n\ntrace_enhanced_dummy &lt;- bind_rows(\n  bonds_panel, bonds_panel, \n  bonds_panel, bonds_panel, \n  bonds_panel) |&gt; \n  mutate(\n    trd_exctn_tm = str_c(\n      sample(0:24, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE)\n    ),\n    rptd_pr = runif(n(), 10, 200),\n    entrd_vol_qt = sample(1:20, n(), replace = TRUE) * 1000,\n    yld_pt = runif(n(), -10, 10),\n    rpt_side_cd = sample(c(\"B\", \"S\"), n(), replace = TRUE),\n    cntra_mp_id = sample(c(\"C\", \"D\"), n(), replace = TRUE)\n  ) \n  \ndbWriteTable(\n  tidy_finance, \n  \"trace_enhanced\", \n  trace_enhanced_dummy, \n  overwrite = TRUE\n)\n\nAs stated in the introduction, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books. You can find the database with the dummy data here."
  },
  {
    "objectID": "blog/user-conference-2022/index.html",
    "href": "blog/user-conference-2022/index.html",
    "title": "Tidy Finance at the useR!2022 Conference",
    "section": "",
    "text": "We had the pleasure of presenting our book at the virtual useR!2022 conference with 1,227 registered participants from 96 countries. It was great fun! Since it was recorded, you can find the video of my presentation below."
  },
  {
    "objectID": "blog/workshops-for-ukraine/index.html",
    "href": "blog/workshops-for-ukraine/index.html",
    "title": "Tidy Finance at Workshops for Ukraine",
    "section": "",
    "text": "Dariia Mykhailyshyna, an Economics PhD student at the University of Bologna, set up a collection of workshops that can be accessed in exchange for a donation in support of Ukraine. We contributed two workshops based on our book Tidy Finance With R. The seminars are recorded and available on demand, and the collection is continuously expanded with interesting topics. Check out the extensive workshop program to register for upcoming events and get recordings and materials of the previous workshops.\nYou can find the workshop descriptions of our contributions below. We believe in making a humanitarian contribution to this cause and would appreciate it if you consider this tremendous effort."
  },
  {
    "objectID": "blog/workshops-for-ukraine/index.html#financial-data-in-r",
    "href": "blog/workshops-for-ukraine/index.html#financial-data-in-r",
    "title": "Tidy Finance at Workshops for Ukraine",
    "section": "Financial Data in R",
    "text": "Financial Data in R\nThis workshop explores financial data available for research and practical applications in financial economics. It relies on material available on www.tidy-finance.org and covers: (1) How to access freely available data from Yahoo!Finance and other vendors. (2) Where to find the data most commonly used in academic research. This main part covers data from CRSP, Compustat, and TRACE. (3) How to store and access data for your research project efficiently. (4) What other data providers are available and how to access their services within R."
  },
  {
    "objectID": "blog/workshops-for-ukraine/index.html#empirical-asset-pricing-in-r",
    "href": "blog/workshops-for-ukraine/index.html#empirical-asset-pricing-in-r",
    "title": "Tidy Finance at Workshops for Ukraine",
    "section": "Empirical Asset Pricing in R",
    "text": "Empirical Asset Pricing in R\nThis workshop explores empirical asset pricing and combines explanations of theoretical concepts with practical implementations. The course relies on material available on www.tidy-finance.org and proceeds in three steps: (1) We dive into the most used data sources and show how to work with data from WRDS, forming the basis for the analysis. We also briefly introduce some other possible sources of financial data. (2) We show how to implement the capital asset pricing model in rolling-window regressions. (3) We introduce the widely used method of portfolio sorts in empirical asset pricing. During the workshop, we will combine some theoretical insights with hands-on implementations in R."
  },
  {
    "objectID": "contribute.html",
    "href": "contribute.html",
    "title": "Contribute to Tidy Finance",
    "section": "",
    "text": "Join our mission to support reproducible finance by contributing to the Tidy Finance Blog. The blog follows our endeavors to increase transparency in financial economics and opens a new channel for you. We actively encourage the finance community to share their insights on coding.\nTidy Finance is an open-source project to augment how research is conducted. We hope to change the way people think about sharing their code. The Tidy Finance Blog is open to everybody who wants to share their code, discuss paper replication, preparation of typical datasets or highlight novel empirical applications in financial economics. The blog is an excellent tool to promote your research and share your thought processes with the community.\nWe strive to make Tidy Finance a trusted, up-to-date resource for all topics in financial economics. Therefore, we commit our time to ensure high-quality content - a commitment that we extend to the Tidy Finance Blog by providing editorial-like processes. While we aim to be as open as possible, we will also carefully assess all contributions."
  },
  {
    "objectID": "contribute.html#who-should-contribute-to-the-tidy-finance-blog",
    "href": "contribute.html#who-should-contribute-to-the-tidy-finance-blog",
    "title": "Contribute to Tidy Finance",
    "section": "Who should contribute to the Tidy Finance Blog?",
    "text": "Who should contribute to the Tidy Finance Blog?\nEverybody should consider writing a blog post on Tidy Finance. We do not exclude anybody from the finance community. We always appreciate relevant contributions that help the target audiences of Tidy Finance (i.e., researchers, students, and professionals). Let us move forward together."
  },
  {
    "objectID": "contribute.html#how-can-i-contribute",
    "href": "contribute.html#how-can-i-contribute",
    "title": "Contribute to Tidy Finance",
    "section": "How can I contribute?",
    "text": "How can I contribute?\nIf you want to contribute to the Tidy Finance Blog, we provide a simple three-step process for your blog post.\n\nSend a proposal for your blog post to blog@tidy-finance.org.\nAfter our confirmation, write and submit your blog post.\nWork with us on potential revisions before reading your published post.\n\nWe are grateful for every contribution. This process is our attempt to provide fairness to all parties. From the perspective of the readers, the other authors, and the whole project, ensuring the quality and relevance of all contributions is paramount. Hence, we cannot publish all blog posts without thoroughly reviewing the submissions.\nWe ask you to submit a proposal before you write an entire post to ensure your efforts are not wasted. In this proposal, we want to learn about your idea and how it will contribute to Tidy Finance. Therefore, do not hesitate to submit a proposal. We guarantee full anonymity and an open attitude to your suggestion. Nevertheless, we do not make promises regarding a positive decision.\nOnce you receive our thumbs-up, you can start working on your contribution. The target format is a quarto document we can render as part of our website repository. When you have a first draft ready, send us an HTML output, and we will provide timely feedback.\nFinally, we will publish your post on the Tidy Finance blog with your name added to the list of contributors to reproducible finance. Unfortunately, we cannot provide you with a cape. Then again, not all heroes wear capes."
  },
  {
    "objectID": "contribute.html#part-of-the-future",
    "href": "contribute.html#part-of-the-future",
    "title": "Contribute to Tidy Finance",
    "section": "Part of the future",
    "text": "Part of the future\nThe Tidy Finance Blog is the place to contribute stand-alone applications in financial economics with guaranteed quality and relevance. While you publish under an open-source license, you retain sole authorship of your work. Furthermore, in future editions of “Tidy Finance with R,” we will acknowledge external contributions directly by referring to the Tidy Finance blog and citing your work."
  },
  {
    "objectID": "contribute.html#what-if-i-do-not-use-the-tidyverse-or-r",
    "href": "contribute.html#what-if-i-do-not-use-the-tidyverse-or-r",
    "title": "Contribute to Tidy Finance",
    "section": "What if I do not use the tidyverse or R?",
    "text": "What if I do not use the tidyverse or R?\nTidy Finance is strongly connected to the tidyverse and R. Yet, we do not rule out extending our horizon and learning new tricks. You can make non-tidyverse and non-R suggestions. However, we need a sufficient reason to break with our tradition."
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html",
    "href": "python/accessing-and-managing-financial-data.html",
    "title": "Accessing and Managing Financial Data",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we suggest a way to organize your financial data. Everybody who has experience with data is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome when using different data formats and across different projects. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages.\nThis chapter shows how to import different open-source datasets. Specifically, our data comes from the application programming interface (API) of Yahoo!Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series. We store all the data in a single database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\nFirst, we load the Python packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nimport pandas as pd\nimport numpy as np\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on.\nstart_date = \"1960-01-01\"\nend_date = \"2022-12-31\"",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#fama-french-data",
    "href": "python/accessing-and-managing-financial-data.html#fama-french-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Fama-French Data",
    "text": "Fama-French Data\nWe start by downloading some famous Fama-French factors (e.g., Fama and French 1993) and portfolio returns commonly used in empirical asset pricing. Fortunately, the pandas-datareader package provides a simple interface to read data from Kenneth French’s Data Library.\n\nimport pandas_datareader as pdr\n\nWe can use the pdr.DataReader() function of the package to download monthly Fama-French factors. The set Fama/French 3 Factors contains the return time series of the market (mkt_excess), size (smb), and value (hml) factors alongside the risk-free rates (rf). Note that we have to do some manual work to parse all the columns correctly and scale them appropriately, as the raw Fama-French data comes in a unique data format. For precise descriptions of the variables, we suggest consulting Prof. Kenneth French’s finance data library directly. If you are on the website, check the raw data files to appreciate the time you can save thanks to pandas_datareader.\n\nfactors_ff3_monthly_raw = pdr.DataReader(\n  name=\"F-F_Research_Data_Factors\",\n  data_source=\"famafrench\", \n  start=start_date, \n  end=end_date)[0]\n\nfactors_ff3_monthly = (factors_ff3_monthly_raw\n  .divide(100)\n  .reset_index(names=\"month\")\n  .assign(month=lambda x: pd.to_datetime(x[\"month\"].astype(str)))\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns={\"mkt-rf\": \"mkt_excess\"})\n)\n\nWe also download the set 5 Factors (2x3), which additionally includes the return time series of the profitability (rmw) and investment (cma) factors. We demonstrate how the monthly factors are constructed in Replicating Fama and French Factors.\n\nfactors_ff5_monthly_raw = pdr.DataReader(\n  name=\"F-F_Research_Data_5_Factors_2x3\",\n  data_source=\"famafrench\", \n  start=start_date, \n  end=end_date)[0]\n\nfactors_ff5_monthly = (factors_ff5_monthly_raw\n  .divide(100)\n  .reset_index(names=\"month\")\n  .assign(month=lambda x: pd.to_datetime(x[\"month\"].astype(str)))\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns={\"mkt-rf\": \"mkt_excess\"})\n)\n\nIt is straightforward to download the corresponding daily Fama-French factors with the same function.\n\nfactors_ff3_daily_raw = pdr.DataReader(\n  name=\"F-F_Research_Data_Factors_daily\",\n  data_source=\"famafrench\", \n  start=start_date, \n  end=end_date)[0]\n\nfactors_ff3_daily = (factors_ff3_daily_raw\n  .divide(100)\n  .reset_index(names=\"date\")\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns={\"mkt-rf\": \"mkt_excess\"})\n)\n\nIn a subsequent chapter, we also use the monthly returns from ten industry portfolios, so let us fetch that data, too.\n\nindustries_ff_monthly_raw = pdr.DataReader(\n  name=\"10_Industry_Portfolios\",\n  data_source=\"famafrench\", \n  start=start_date, \n  end=end_date)[0]\n\nindustries_ff_monthly = (industries_ff_monthly_raw\n  .divide(100)\n  .reset_index(names=\"month\")\n  .assign(month=lambda x: pd.to_datetime(x[\"month\"].astype(str)))\n  .rename(str.lower, axis=\"columns\")\n)\n\nIt is worth taking a look at all available portfolio return time series from Kenneth French’s homepage. You should check out the other sets by calling pdr.famafrench.get_available_datasets().",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#q-factors",
    "href": "python/accessing-and-managing-financial-data.html#q-factors",
    "title": "Accessing and Managing Financial Data",
    "section": "q-Factors",
    "text": "q-Factors\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the Hou, Xue, and Zhang (2014) q-factor model. We refer to the extended background information provided by the original authors for further information. The q-factors can be downloaded directly from the authors’ homepage from within pd.read_csv(). \nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the “R_”-prescript using regular expressions and write all column names in lowercase. We then query the data to select observations between the start and end dates. Finally, we use the double asterisk (**) notation in the assign function to apply the same transform of dividing by 100 to all four factors by iterating through them. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on try. You can check out style guides available online, e.g., Hadley Wickham’s tidyverse style guide.\n\nfactors_q_monthly_link = (\n  \"https://global-q.org/uploads/1/2/2/6/122679606/\"\n  \"q5_factors_monthly_2022.csv\"\n)\n\nfactors_q_monthly = (pd.read_csv(factors_q_monthly_link)\n  .assign(\n    month=lambda x: (\n      pd.to_datetime(x[\"year\"].astype(str) + \"-\" +\n        x[\"month\"].astype(str) + \"-01\"))\n  )\n  .drop(columns=[\"R_F\", \"R_MKT\", \"year\"])\n  .rename(columns=lambda x: x.replace(\"R_\", \"\").lower())\n  .query(f\"month &gt;= '{start_date}' and month &lt;= '{end_date}'\")\n  .assign(\n    **{col: lambda x: x[col]/100 for col in [\"me\", \"ia\", \"roe\", \"eg\"]}\n  )\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "href": "python/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "title": "Accessing and Managing Financial Data",
    "section": "Macroeconomic Predictors",
    "text": "Macroeconomic Predictors\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. Welch and Goyal (2008) comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data until 2022 on Amit Goyal’s website. Since the data is an XLSX-file stored on a public Google Drive location, we need additional packages to access the data directly from our Python session. Usually, you need to authenticate if you interact with Google drive directly in Python. Since the data is stored via a public link, we can proceed without any authentication.\n\nsheet_id = \"1g4LOaRj4TvwJr9RIaA_nwrXXWTOy46bP\"\nsheet_name = \"macro_predictors.xlsx\"\nmacro_predictors_link = (\n  f\"https://docs.google.com/spreadsheets/d/{sheet_id}\" \n  f\"/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n)\n\nNext, we read in the new data and transform the columns into the variables that we later use:\n\nThe dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (Campbell and Shiller 1988; Campbell and Yogo 2006).\nDividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).\nEarnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index (Campbell and Shiller 1988).\nDividend payout ratio (de), the difference between the log of dividends and the log of earnings (Lamont 1998).\nStock variance (svar), the sum of squared daily returns on the S&P 500 index (Guo 2006).\nBook-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average (Kothari and Shanken 1997).\nNet equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks (Campbell, Hilscher, and Szilagyi 2008).\nTreasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis (Campbell 1987).\nLong-term yield (lty), the long-term government bond yield from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nLong-term rate of returns (ltr), the long-term government bond returns from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nTerm spread (tms), the difference between the long-term yield on government bonds and the Treasury bill (Campbell 1987).\nDefault yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields (Fama and French 1989).\nInflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics (Campbell and Vuolteenaho 2004).\n\nFor variable definitions and the required data transformations, you can consult the material on Amit Goyal’s website.\n\nmacro_predictors = (\n  pd.read_csv(macro_predictors_link, thousands=\",\")\n  .assign(\n    month=lambda x: pd.to_datetime(x[\"yyyymm\"], format=\"%Y%m\"),\n    dp=lambda x: np.log(x[\"D12\"])-np.log(x[\"Index\"]),\n    dy=lambda x: np.log(x[\"D12\"])-np.log(x[\"Index\"].shift(1)),\n    ep=lambda x: np.log(x[\"E12\"])-np.log(x[\"Index\"]),\n    de=lambda x: np.log(x[\"D12\"])-np.log(x[\"E12\"]),\n    tms=lambda x: x[\"lty\"]-x[\"tbl\"],\n    dfy=lambda x: x[\"BAA\"]-x[\"AAA\"]\n  )\n  .rename(columns={\"b/m\": \"bm\"})\n  .get([\"month\", \"dp\", \"dy\", \"ep\", \"de\", \"svar\", \"bm\", \n        \"ntis\", \"tbl\", \"lty\", \"ltr\", \"tms\", \"dfy\", \"infl\"])\n  .query(\"month &gt;= @start_date and month &lt;= @end_date\")\n  .dropna()\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "href": "python/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Other Macroeconomic Data",
    "text": "Other Macroeconomic Data\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. As an illustration, we use the already familiar pandas-datareader package to fetch consumer price index (CPI) data that can be found under the CPIAUCNS key.\n\ncpi_monthly = (pdr.DataReader(\n    name=\"CPIAUCNS\", \n    data_source=\"fred\", \n    start=start_date, \n    end=end_date\n  )\n  .reset_index(names=\"month\")\n  .rename(columns={\"CPIAUCNS\": \"cpi\"})\n  .assign(cpi=lambda x: x[\"cpi\"]/x[\"cpi\"].iloc[-1])\n)\n\nNote that we use the assign() in the last line to set the current (latest) price level as the reference inflation level. To download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the PCU2122212122210 key.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#setting-up-a-database",
    "href": "python/accessing-and-managing-financial-data.html#setting-up-a-database",
    "title": "Accessing and Managing Financial Data",
    "section": "Setting Up a Database",
    "text": "Setting Up a Database\nNow that we have downloaded some (freely available) data from the web into the memory of our Python session, let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code.\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an SQLite-database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured SQL database engine. Note that SQL (Structured Query Language) is a standard language for accessing and manipulating databases.\n\nimport sqlite3\n\nAn SQLite-database is easily created - the code below is really all there is. You do not need any external software. Otherwise, date columns are stored and retrieved as integers. We will use the resulting file tidy_finance.sqlite in the subfolder data for all subsequent chapters to retrieve our data.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the pandas function to_sql(), which copies the data to our SQLite-database.\n\n(factors_ff3_monthly\n  .to_sql(name=\"factors_ff3_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\nNow, if we want to have the whole table in memory, we need to call pd.read_sql_query() with the corresponding query. You will see that we regularly load the data into the memory in the next chapters.\n\npd.read_sql_query(\n  sql=\"SELECT month, rf FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\n\n\n\n\n\n\n\n\nmonth\nrf\n\n\n\n\n0\n1960-01-01\n0.0033\n\n\n1\n1960-02-01\n0.0029\n\n\n2\n1960-03-01\n0.0035\n\n\n3\n1960-04-01\n0.0019\n\n\n4\n1960-05-01\n0.0027\n\n\n...\n...\n...\n\n\n751\n2022-08-01\n0.0019\n\n\n752\n2022-09-01\n0.0019\n\n\n753\n2022-10-01\n0.0023\n\n\n754\n2022-11-01\n0.0029\n\n\n755\n2022-12-01\n0.0033\n\n\n\n\n756 rows × 2 columns\n\n\n\n\nThe last couple of code chunks are really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages.\nBefore we move on to the next data source, let us also store the other six tables in our new SQLite database.\n\ndata_dict = {\n  \"factors_ff5_monthly\": factors_ff5_monthly,\n  \"factors_ff3_daily\": factors_ff3_daily,\n  \"industries_ff_monthly\": industries_ff_monthly, \n  \"factors_q_monthly\": factors_q_monthly,\n  \"macro_predictors\": macro_predictors,\n  \"cpi_monthly\": cpi_monthly\n}\n\nfor key, value in data_dict.items():\n    value.to_sql(name=key,\n                 con=tidy_finance, \n                 if_exists=\"replace\",\n                 index=False)\n\nFrom now on, all you need to do to access data that is stored in the database is to follow two steps: (i) Establish the connection to the SQLite-database and (ii) execute the query to fetch the data. For your convenience, the following steps show all you need in a compact fashion.\n\nimport pandas as pd\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_q_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM factors_q_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "href": "python/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "title": "Accessing and Managing Financial Data",
    "section": "Managing SQLite Databases",
    "text": "Managing SQLite Databases\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\nTo optimize the database file, you can run the VACUUM command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the execute() function.\n\ntidy_finance.execute(\"VACUUM\")\n\nThe VACUUM command actually performs a couple of additional cleaning steps, which you can read about in this tutorial.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#exercises",
    "href": "python/accessing-and-managing-financial-data.html#exercises",
    "title": "Accessing and Managing Financial Data",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the monthly Fama-French factors manually from Kenneth French’s data library and read them in via pd.read_csv(). Validate that you get the same data as via the pandas-datareader package.\nDownload the daily Fama-French 5 factors using the pdr.DataReader() package. After the successful download and conversion to the column format that we used above, compare the rf, mkt_excess, smb, and hml columns of factors_ff3_daily to factors_ff5_daily. Discuss any differences you might find.",
    "crumbs": [
      "R",
      "Financial Data",
      "Accessing and Managing Financial Data"
    ]
  },
  {
    "objectID": "python/changelog.html",
    "href": "python/changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "You can find every single change in our commit history. We collect the most important changes for Tidy Finance with Python in the list below.\n\nMay 15, 2024, Commit 2bb2e07: We added a new subsection about creating environment variables to Setting Up Your Environment.\nMay 15, 2024, Commit adccfc9: We updated the filters in CRSP download, so that correct historical information is used and daily and monthly data are aligned.\nApril 17, 2024, Commit b8c32aa: Corrects a typo in the TRACE download.\nApril 15, 2024, Commit c0f5cc0: Fixes the definition of the dividend yield.",
    "crumbs": [
      "R",
      "Appendix",
      "Changelog"
    ]
  },
  {
    "objectID": "python/colophon.html",
    "href": "python/colophon.html",
    "title": "Colophon",
    "section": "",
    "text": "In this appendix chapter, we provide details on the package versions used in this edition. The book was built with Python (Python Software Foundation 2023) version 3.10.11 and the following packages:\n\n\n\n\n\n\n\nPackage\nVersion\n\n\n\n\n0\nanyio\n4.0.0\n\n\n1\nappdirs\n1.4.4\n\n\n2\nappnope\n0.1.3\n\n\n3\nargon2-cffi\n23.1.0\n\n\n4\nargon2-cffi-bindings\n21.2.0\n\n\n5\narrow\n1.3.0\n\n\n6\nastor\n0.8.1\n\n\n7\nasttokens\n2.4.0\n\n\n8\nasync-lru\n2.0.4\n\n\n9\nattrs\n23.1.0\n\n\n10\nBabel\n2.13.0\n\n\n11\nbackcall\n0.2.0\n\n\n12\nbeautifulsoup4\n4.12.2\n\n\n13\nbleach\n6.1.0\n\n\n14\ncertifi\n2023.7.22\n\n\n15\ncffi\n1.16.0\n\n\n16\ncharset-normalizer\n3.3.0\n\n\n17\nclick\n8.1.7\n\n\n18\ncomm\n0.1.4\n\n\n19\ncontourpy\n1.1.1\n\n\n20\ncycler\n0.12.1\n\n\n21\nCython\n3.0.3\n\n\n22\ndebugpy\n1.8.0\n\n\n23\ndecorator\n5.1.1\n\n\n24\ndefusedxml\n0.7.1\n\n\n25\net-xmlfile\n1.1.0\n\n\n26\nexceptiongroup\n1.1.3\n\n\n27\nexecuting\n2.0.0\n\n\n28\nfastjsonschema\n2.18.1\n\n\n29\nfonttools\n4.43.1\n\n\n30\nformulaic\n0.6.6\n\n\n31\nfqdn\n1.5.1\n\n\n32\nfrozendict\n2.3.8\n\n\n33\nhtml5lib\n1.1\n\n\n34\nhttpimport\n1.3.1\n\n\n35\nidna\n3.4\n\n\n36\nimportlib-metadata\n6.8.0\n\n\n37\ninterface-meta\n1.3.0\n\n\n38\nipykernel\n6.25.2\n\n\n39\nipython\n8.16.1\n\n\n40\nipython-genutils\n0.2.0\n\n\n41\nipywidgets\n8.1.1\n\n\n42\nisoduration\n20.11.0\n\n\n43\njedi\n0.18.2\n\n\n44\nJinja2\n3.1.2\n\n\n45\njoblib\n1.3.2\n\n\n46\njson5\n0.9.14\n\n\n47\njsonpointer\n2.4\n\n\n48\njsonschema\n4.19.1\n\n\n49\njsonschema-specifications\n2023.7.1\n\n\n50\njupyter\n1.0.0\n\n\n51\njupyter-cache\n0.6.1\n\n\n52\njupyter-console\n6.6.3\n\n\n53\njupyter-events\n0.7.0\n\n\n54\njupyter-lsp\n2.2.0\n\n\n55\njupyter_client\n8.3.1\n\n\n56\njupyter_core\n5.4.0\n\n\n57\njupyter_server\n2.7.3\n\n\n58\njupyter_server_terminals\n0.4.4\n\n\n59\njupyterlab\n4.0.6\n\n\n60\njupyterlab-pygments\n0.2.2\n\n\n61\njupyterlab-widgets\n3.0.9\n\n\n62\njupyterlab_server\n2.25.0\n\n\n63\nkiwisolver\n1.4.5\n\n\n64\nlinearmodels\n5.3\n\n\n65\nlxml\n4.9.3\n\n\n66\nMarkupSafe\n2.1.3\n\n\n67\nmatplotlib\n3.8.0\n\n\n68\nmatplotlib-inline\n0.1.6\n\n\n69\nmistune\n3.0.2\n\n\n70\nmizani\n0.9.3\n\n\n71\nmultitasking\n0.0.11\n\n\n72\nmypy-extensions\n1.0.0\n\n\n73\nnbclient\n0.7.4\n\n\n74\nnbconvert\n7.9.2\n\n\n75\nnbformat\n5.9.2\n\n\n76\nnest-asyncio\n1.5.8\n\n\n77\nnotebook\n7.0.4\n\n\n78\nnotebook_shim\n0.2.3\n\n\n79\nnumpy\n1.26.0\n\n\n80\nopenpyxl\n3.1.2\n\n\n81\noverrides\n7.4.0\n\n\n82\npackaging\n23.2\n\n\n83\npandas\n2.1.1\n\n\n84\npandas-datareader\n0.10.0\n\n\n85\npandocfilters\n1.5.0\n\n\n86\nparso\n0.8.3\n\n\n87\npatsy\n0.5.3\n\n\n88\npeewee\n3.16.3\n\n\n89\npexpect\n4.8.0\n\n\n90\npickleshare\n0.7.5\n\n\n91\nPillow\n10.0.1\n\n\n92\nplatformdirs\n3.11.0\n\n\n93\nplotnine\n0.12.3\n\n\n94\nprometheus-client\n0.17.1\n\n\n95\nprompt-toolkit\n3.0.39\n\n\n96\npsutil\n5.9.5\n\n\n97\npsycopg2-binary\n2.9.9\n\n\n98\nptyprocess\n0.7.0\n\n\n99\npure-eval\n0.2.2\n\n\n100\npycparser\n2.21\n\n\n101\nPygments\n2.16.1\n\n\n102\npyhdfe\n0.2.0\n\n\n103\npyparsing\n3.1.1\n\n\n104\npython-dateutil\n2.8.2\n\n\n105\npython-dotenv\n1.0.0\n\n\n106\npython-json-logger\n2.0.7\n\n\n107\npytz\n2023.3.post1\n\n\n108\nPyYAML\n6.0.1\n\n\n109\npyzmq\n25.1.1\n\n\n110\nqtconsole\n5.4.4\n\n\n111\nQtPy\n2.4.0\n\n\n112\nreferencing\n0.30.2\n\n\n113\nregtabletotext\n0.0.11\n\n\n114\nrequests\n2.31.0\n\n\n115\nrfc3339-validator\n0.1.4\n\n\n116\nrfc3986-validator\n0.1.1\n\n\n117\nrpds-py\n0.10.4\n\n\n118\nscikit-learn\n1.3.1\n\n\n119\nscipy\n1.11.3\n\n\n120\nSend2Trash\n1.8.2\n\n\n121\nsetuptools-scm\n7.1.0\n\n\n122\nsix\n1.16.0\n\n\n123\nsniffio\n1.3.0\n\n\n124\nsoupsieve\n2.5\n\n\n125\nSQLAlchemy\n2.0.21\n\n\n126\nstack-data\n0.6.3\n\n\n127\nstatsmodels\n0.14.0\n\n\n128\ntabulate\n0.9.0\n\n\n129\nterminado\n0.17.1\n\n\n130\nthreadpoolctl\n3.2.0\n\n\n131\ntinycss2\n1.2.1\n\n\n132\ntomli\n2.0.1\n\n\n133\ntornado\n6.3.3\n\n\n134\ntraitlets\n5.11.2\n\n\n135\ntypes-python-dateutil\n2.8.19.14\n\n\n136\ntyping_extensions\n4.8.0\n\n\n137\ntzdata\n2023.3\n\n\n138\nuri-template\n1.3.0\n\n\n139\nurllib3\n2.0.6\n\n\n140\nwcwidth\n0.2.8\n\n\n141\nwebcolors\n1.13\n\n\n142\nwebencodings\n0.5.1\n\n\n143\nwebsocket-client\n1.6.4\n\n\n144\nwidgetsnbextension\n4.0.9\n\n\n145\nwrapt\n1.15.0\n\n\n146\nyfinance\n0.2.31\n\n\n147\nzipp\n3.17.0\n\n\n\n\n\n\n\n\n\n\nReferences\n\nPython Software Foundation. 2023. “Python Language Reference, Version 3.10.11.”",
    "crumbs": [
      "R",
      "Appendix",
      "Colophon"
    ]
  },
  {
    "objectID": "python/cover-image.html",
    "href": "python/cover-image.html",
    "title": "Cover Image",
    "section": "",
    "text": "The cover of the book is inspired by the fast growing generative art community in R. Generative art refers to art that in whole or in part has been created with the use of an autonomous system. Instead of creating random dynamics, we rely on what is core to the book: The evolution of financial markets. Each circle corresponds to one of the twelve Fama-French industry portfolios, whereas each bar represents the average annual return between 1927 and 2022. The bar color is determined by the standard deviation of returns for each industry. The few lines of code below replicate the entire figure.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas_datareader as pdr\n\nfrom datetime import datetime\nfrom matplotlib.colors import LinearSegmentedColormap\n\nmain_colors = [\"#3B9AB2\", \"#78B7C5\", \"#EBCC2A\", \"#E1AF00\", \"#F21A00\"]\ncolormap = LinearSegmentedColormap.from_list(\"custom_colormap\", main_colors)\n\nindustries_ff_daily_raw = pdr.DataReader(\n  name=\"12_Industry_Portfolios_daily\",\n  data_source=\"famafrench\", \n  start=\"1927-01-01\", \n  end=\"2022-12-31\")[0]\n\nindustries_ff_daily = (industries_ff_daily_raw\n  .divide(100)\n  .reset_index(names=\"date\")\n  .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n  .rename(str.lower, axis=\"columns\")\n)\n\nindustries_long = (industries_ff_daily\n  .melt(id_vars=\"date\", var_name=\"name\", value_name=\"value\")\n)\n                          \nindustries_order = sorted(industries_long[\"name\"].unique())\n\ndata_plot = (industries_long\n  .assign(year=industries_long[\"date\"].dt.to_period(\"Y\"))\n  .groupby([\"year\", \"name\"])\n  .aggregate(total=(\"value\", \"mean\"),\n             vola=(\"value\", \"std\"))\n  .reset_index()\n  .assign(\n    vola_ntile=lambda x: pd.qcut(x[\"vola\"], 42, labels=False)\n  )\n)\n\ndpi = 300\nwidth = 2400/dpi\nheight = 1800/dpi\nnum_cols = 4\nnum_rows = int(len(industries_order)/num_cols)\nfig, axs = plt.subplots(\n  num_rows, num_cols,\n  constrained_layout=True,\n  subplot_kw={\"projection\": \"polar\"},\n  figsize=(width, height),\n  dpi=dpi\n)\naxs = axs.flatten()\n\nfor i in enumerate(industries_order):\n\n    df = data_plot.copy().query(f'name == \"{i[1]}\"')\n    min_value = df[\"total\"].min()\n    max_value = df[\"total\"].max()\n    std_value = df[\"total\"].std()\n    df[\"total\"] = 2*(df[\"total\"]-min_value)/(max_value-min_value)-1\n\n    angles = np.linspace(0, 2*np.pi, len(df), endpoint=False)\n    values = df[\"total\"].values\n    width = 2*np.pi/len(values)\n    offset = np.pi/2\n\n    ax = axs[i[0]]\n    ax.set_theta_offset(offset)\n    ax.set_ylim(-std_value*1400, 1)\n    ax.set_frame_on(False)\n    ax.xaxis.grid(False)\n    ax.yaxis.grid(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    color_values = df[\"vola_ntile\"].values\n    normalize = plt.Normalize(min(color_values), max(color_values))\n    colors = colormap(normalize(color_values))\n\n    ax.bar(\n      angles, values,\n      width=width, color=colors, edgecolor=\"white\", linewidth=0.2\n    )\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=-0.2, hspace=-0.1)\nplt.gcf().savefig(\n  \"images/cover-image.png\", dpi = 300, pad_inches=0, transparent=False\n)",
    "crumbs": [
      "R",
      "Appendix",
      "Cover Image"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html",
    "href": "python/factor-selection-via-machine-learning.html",
    "title": "Factor Selection via Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nThe aim of this chapter is twofold. From a data science perspective, we introduce scikit-learn, a collection of packages for modeling and machine learning (ML). scikit-learn comes with a handy workflow for all sorts of typical prediction tasks. From a finance perspective, we address the notion of factor zoo (Cochrane 2011) using ML methods. We introduce Lasso, Ridge, and Elastic Net regression as a special case of penalized regression models. Then, we explain the concept of cross-validation for model tuning with Elastic Net regularization as a popular example. We implement and showcase the entire cycle from model specification, training, and forecast evaluation within the scikit-learn universe. While the tools can generally be applied to an abundance of interesting asset pricing problems, we apply penalized regressions for identifying macroeconomic variables and asset pricing factors that help explain a cross-section of industry portfolios.\nIn previous chapters, we illustrate that stock characteristics such as size provide valuable pricing information in addition to the market beta. Such findings question the usefulness of the Capital Asset Pricing Model. In fact, during the last decades, financial economists discovered a plethora of additional factors which may be correlated with the marginal utility of consumption (and would thus deserve a prominent role in pricing applications). The search for factors that explain the cross-section of expected stock returns has produced hundreds of potential candidates, as noted more recently by Harvey, Liu, and Zhu (2016), Harvey (2017), Mclean and Pontiff (2016), and Hou, Xue, and Zhang (2020). Therefore, given the multitude of proposed risk factors, the challenge these days rather is: do we believe in the relevance of hundreds of risk factors? During recent years, promising methods from the field of ML got applied to common finance applications. We refer to Mullainathan and Spiess (2017) for a treatment of ML from the perspective of an econometrician, Nagel (2021) for an excellent review of ML practices in asset pricing, Easley et al. (2020) for ML applications in (high-frequency) market microstructure, and Dixon, Halperin, and Bilokon (2020) for a detailed treatment of all methodological aspects.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "href": "python/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "title": "Factor Selection via Machine Learning",
    "section": "Brief Theoretical Background",
    "text": "Brief Theoretical Background\nThis is a book about doing empirical work in a tidy manner, and we refer to any of the many excellent textbook treatments of ML methods and especially penalized regressions for some deeper discussion. Excellent material is provided, for instance, by Hastie, Tibshirani, and Friedman (2009), Gareth et al. (2013), and De Prado (2018). Instead, we briefly summarize the idea of Lasso and Ridge regressions as well as the more general Elastic Net. Then, we turn to the fascinating question on how to implement, tune, and use such models with the scikit-learn package.\nTo set the stage, we start with the definition of a linear model: Suppose we have data \\((y_t, x_t), t = 1,\\ldots, T\\), where \\(x_t\\) is a \\((K \\times 1)\\) vector of regressors and \\(y_t\\) is the response for observation \\(t\\). The linear model takes the form \\(y_t = \\beta' x_t + \\varepsilon_t\\) with some error term \\(\\varepsilon_t\\) and has been studied in abundance. For \\(K\\leq T\\), the well-known ordinary-least square (OLS) estimator for the \\((K \\times 1)\\) vector \\(\\beta\\) minimizes the sum of squared residuals and is then \\[\\hat{\\beta}^\\text{ols} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t. \\tag{1}\\] \nWhile we are often interested in the estimated coefficient vector \\(\\hat\\beta^\\text{ols}\\), ML is about the predictive performance most of the time. For a new observation \\(\\tilde{x}_t\\), the linear model generates predictions such that \\[\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta^\\text{ols}{}' \\tilde x_t. \\tag{2}\\] Is this the best we can do? Not necessarily: instead of minimizing the sum of squared residuals, penalized linear models can improve predictive performance by choosing other estimators \\(\\hat{\\beta}\\) with lower variance than the estimator \\(\\hat\\beta^\\text{ols}\\). At the same time, it seems appealing to restrict the set of regressors to a few meaningful ones, if possible. In other words, if \\(K\\) is large (such as for the number of proposed factors in the asset pricing literature), it may be a desirable feature to select reasonable factors and set \\(\\hat\\beta^{\\text{ols}}_k = 0\\) for some redundant factors.\nIt should be clear that the promised benefits of penalized regressions, i.e., reducing the mean squared error (MSE), come at a cost. In most cases, reducing the variance of the estimator introduces a bias such that \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). What is the effect of such a bias-variance trade-off? To understand the implications, assume the following data-generating process for \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2) \\tag{3}\\] We want to recover \\(f(x)\\), which denotes some unknown functional which maps the relationship between \\(x\\) and \\(y\\). While the properties of \\(\\hat\\beta^\\text{ols}\\) as an unbiased estimator may be desirable under some circumstances, they are certainly not if we consider predictive accuracy. Alternative predictors \\(\\hat{f}(x)\\) could be more desirable: For instance, the MSE depends on our model choice as follows: \\[\\begin{aligned}\nMSE &=E\\left(\\left(y-\\hat{f}(x)\\right)^2\\right)=E\\left(\\left(f(x)+\\epsilon-\\hat{f}(x)\\right)^2\\right)\\\\\n&= \\underbrace{E\\left(\\left(f(x)-\\hat{f}(x)\\right)^2\\right)}_{\\text{total quadratic error}}+\\underbrace{E\\left(\\epsilon^2\\right)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(x)^2\\right)+E\\left(f(x)^2\\right)-2E\\left(f(x)\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(x)^2\\right)+f(x)^2-2f(x)E\\left(\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(x)\\right)}_{\\text{variance of model}}+ \\underbrace{E\\left((f(x)-\\hat{f}(x))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2.\n\\end{aligned} \\tag{4}\\] While no model can reduce \\(\\sigma_\\varepsilon^2\\), a biased estimator with small variance may have a lower MSE than an unbiased estimator.\n\nRidge regression\n\nOne biased estimator is known as Ridge regression. Hoerl and Kennard (1970) propose to minimize the sum of squared errors while simultaneously imposing a penalty on the \\(L_2\\) norm of the parameters \\(\\hat\\beta\\). Formally, this means that for a penalty factor \\(\\lambda\\geq 0\\), the minimization problem takes the form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq c\\). Here \\(c\\geq 0\\) is a constant that depends on the choice of \\(\\lambda\\). The larger \\(\\lambda\\), the smaller \\(c\\) (technically speaking, there is a one-to-one relationship between \\(\\lambda\\), which corresponds to the Lagrangian of the minimization problem above and \\(c\\)). Here, \\(X = \\left(x_1 \\ldots x_T\\right)'\\) and \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). A closed-form solution for the resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda I\\right)^{-1}X'y,\n\\tag{5}\\] where \\(I\\) is the identity matrix of dimension \\(K\\). A couple of observations are worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta^\\text{ols}\\) for \\(\\lambda = 0\\) and \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) for \\(\\lambda\\rightarrow \\infty\\). Also for \\(\\lambda &gt; 0\\), \\(\\left(X'X + \\lambda I\\right)\\) is non-singular even if \\(X'X\\) is which means that \\(\\hat\\beta^\\text{ridge}\\) exists even if \\(\\hat\\beta\\) is not defined. However, note also that the Ridge estimator requires careful choice of the hyperparameter \\(\\lambda\\) which controls the amount of regularization: a larger value of \\(\\lambda\\) implies shrinkage of the regression coefficient toward 0; a smaller value of \\(\\lambda\\) reduces the bias of the resulting estimator.\n\nNote that \\(X\\) usually contains an intercept column with ones. As a general rule, the associated intercept coefficient is not penalized. In practice, this often implies that \\(y\\) is simply demeaned before computing \\(\\hat\\beta^\\text{ridge}\\).\n\nWhat about the statistical properties of the Ridge estimator? First, the bad news is that \\(\\hat\\beta^\\text{ridge}\\) is a biased estimator of \\(\\beta\\). However, the good news is that (under homoscedastic error terms) the variance of the Ridge estimator is guaranteed to be smaller than the variance of the OLS estimator. We encourage you to verify these two statements in the Exercises. As a result, we face a trade-off: The Ridge regression sacrifices some unbiasedness to achieve a smaller variance than the OLS estimator.\n\n\nLasso\n\nAn alternative to Ridge regression is the Lasso (least absolute shrinkage and selection operator). Similar to Ridge regression, the Lasso (Tibshirani 1996) is a penalized and biased estimator. The main difference to Ridge regression is that Lasso does not only shrink coefficients but effectively selects variables by setting coefficients for irrelevant variables to zero. Lasso implements a \\(L_1\\) penalization on the parameters such that: \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right)\\text{ s.t. } \\sum\\limits_{k=1}^K|\\beta_k| &lt; c(\\lambda). \\tag{6}\\] There is no closed-form solution for \\(\\hat\\beta^\\text{Lasso}\\) in the above maximization problem, but efficient algorithms exist (e.g., the glmnet package for R and Python). Like for Ridge regression, the hyperparameter \\(\\lambda\\) has to be specified beforehand.\nThe corresponding Lagrangian reads as follows \\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned} \\tag{7}\\]\n\n\nElastic Net\nThe Elastic Net (Zou and Hastie 2005) combines \\(L_1\\) with \\(L_2\\) penalization and encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. In terms of the Lagrangian, this more general framework considers the following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2 \\tag{8}\\] Now, we have to choose two hyperparameters: the shrinkage factor \\(\\lambda\\) and the weighting parameter \\(\\rho\\). The Elastic Net resembles Lasso for \\(\\rho = 0\\) and Ridge regression for \\(\\rho = 1\\). While the glmnet package provides efficient algorithms to compute the coefficients of penalized regressions, it is a good exercise to implement Ridge and Lasso estimation on your own before you use the scikit-learn back-end.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#python-packages",
    "href": "python/factor-selection-via-machine-learning.html#python-packages",
    "title": "Factor Selection via Machine Learning",
    "section": "Python Packages",
    "text": "Python Packages\nTo get started, we load the required packages and data. The main focus is on the workflow behind the scikit-learn (Pedregosa et al. 2011) package collection.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nfrom plotnine import * \nfrom mizani.formatters import percent_format, date_format\nfrom mizani.breaks import date_breaks\nfrom itertools import product\nfrom sklearn.model_selection import (\n  train_test_split, GridSearchCV, TimeSeriesSplit, cross_val_score\n)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#data-preparation",
    "href": "python/factor-selection-via-machine-learning.html#data-preparation",
    "title": "Factor Selection via Machine Learning",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn this analysis, we use four different data sources that we load from our SQLite database introduced in Accessing and Managing Financial Data. We start with two different sets of factor portfolio returns which have been suggested as representing practical risk factor exposure and thus should be relevant when it comes to asset pricing applications.\n\nThe standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big, and high-minus-low book-to-market valuation sorts) defined in Fama and French (1992) and Fama and French (1993).\nMonthly q-factor returns from Hou, Xue, and Zhang (2014). The factors contain the size factor, the investment factor, the return-on-equity factor, and the expected growth factor.\n\nNext, we include macroeconomic predictors which may predict the general stock market economy. Macroeconomic variables effectively serve as conditioning information such that their inclusion hints at the relevance of conditional models instead of unconditional asset pricing. We refer the interested reader to Cochrane (2009) on the role of conditioning information.\n\nOur set of macroeconomic predictors comes from Welch and Goyal (2008). The data has been updated by the authors until 2021 and contains monthly variables that have been suggested as good predictors for the equity premium. Some of the variables are the dividend price ratio, earnings price ratio, stock variance, net equity expansion, treasury bill rate, and inflation.\n\nFinally, we need a set of test assets. The aim is to understand which of the plenty factors and macroeconomic variable combinations prove helpful in explaining our test assets’ cross-section of returns. In line with many existing papers, we use monthly portfolio returns from ten different industries according to the definition from Kenneth French’s homepage as test assets.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_ff3_monthly = (pd.read_sql_query(\n     sql=\"SELECT * FROM factors_ff3_monthly\",\n     con=tidy_finance,\n     parse_dates={\"month\"})\n  .add_prefix(\"factor_ff_\")\n)\n\nfactors_q_monthly = (pd.read_sql_query(\n    sql=\"SELECT * FROM factors_q_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n  .add_prefix(\"factor_q_\")\n)\n\nmacro_predictors = (pd.read_sql_query(\n    sql=\"SELECT * FROM macro_predictors\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n .add_prefix(\"macro_\")\n)\n\nindustries_ff_monthly = (pd.read_sql_query(\n    sql=\"SELECT * FROM industries_ff_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n  .melt(id_vars=\"month\", var_name=\"industry\", value_name=\"ret\")\n)\n\nWe combine all the monthly observations into one dataframe.\n\ndata = (industries_ff_monthly\n  .merge(factors_ff3_monthly, \n         how=\"left\", left_on=\"month\", right_on=\"factor_ff_month\")\n  .merge(factors_q_monthly, \n         how=\"left\", left_on=\"month\", right_on=\"factor_q_month\")\n  .merge(macro_predictors, \n         how=\"left\", left_on=\"month\", right_on=\"macro_month\") \n  .assign(ret_excess=lambda x: x[\"ret\"] - x[\"factor_ff_rf\"]) \n  .drop(columns=[\"ret\", \"factor_ff_month\", \"factor_q_month\", \"macro_month\"])\n  .dropna()\n)\n\nOur data contains 22 columns of regressors with the 13 macro-variables and 8 factor returns for each month. Figure 1 provides summary statistics for the 10 monthly industry excess returns in percent. One can see that the dispersion in the excess returns varies widely across industries. \n\ndata_plot = (ggplot(data, \n  aes(x=\"industry\", y=\"ret_excess\")) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(x=\"\", y=\"\", \n       title=\"Excess return distributions by industry in percent\") + \n   scale_y_continuous(labels=percent_format())\n)\ndata_plot.draw()\n\n\n\n\n\n\n\nFigure 1: The box plots show the monthly dispersion of returns for 10 different industries.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#machine-learning-workflow",
    "href": "python/factor-selection-via-machine-learning.html#machine-learning-workflow",
    "title": "Factor Selection via Machine Learning",
    "section": "Machine Learning Workflow",
    "text": "Machine Learning Workflow\nTo illustrate penalized linear regressions, we employ the scikit-learn collection of packages for modeling and ML. Using the ideas of Ridge and Lasso regressions, the following example guides you through (i) pre-processing the data (data split and variable mutation), (ii) building models, (iii) fitting models, and (iv) tuning models to create the “best” possible predictions.\n\nPre-process data\nWe want to explain excess returns with all available predictors. The regression equation thus takes the form \\[r_{t} = \\alpha_0 + \\left(\\tilde f_t \\otimes \\tilde z_t\\right)B + \\varepsilon_t  \\tag{9}\\] where \\(r_t\\) is the vector of industry excess returns at time \\(t\\), \\(\\otimes\\) denotes the Kronecker product and \\(\\tilde f_t\\) and \\(\\tilde z_t\\) are the (standardized) vectors of factor portfolio returns and macroeconomic variables.\nWe hence perform the following pre-processing steps:\n\nWe exclude the column month from the analysis\nWe include all interaction terms between factors and macroeconomic predictors\nWe demean and scale each regressor such that the standard deviation is one\n\nScaling is often necessary in machine learning applications, especially when combining variables of different magnitudes or units, or when using algorithms sensitive to feature scales (e.g., gradient descent-based algorithms). We use ColumnTransformer() to scale all regressors using StandardScaler(). The remainder=\"drop\" ensures that only the specified columns are retained in the output, and others are dropped. The option verbose_feature_names_out=False ensures that the output feature names remain unchanged. Also note that we use the zip() function to pair each element from column_names with its corresponding list of values from new_column_values, creating tuples, and then convert these tuples into a dictionary using dict() from which we create a dataframe.\n\nmacro_variables = data.filter(like=\"macro\").columns\nfactor_variables = data.filter(like=\"factor\").columns\n\ncolumn_combinations = list(product(macro_variables, factor_variables))\n\nnew_column_values = []\nfor macro_column, factor_column in column_combinations:\n    new_column_values.append(data[macro_column] * data[factor_column])\n\ncolumn_names = [\" x \".join(t) for t in column_combinations]\nnew_columns = pd.DataFrame(dict(zip(column_names, new_column_values)))\n\ndata = pd.concat([data, new_columns], axis=1)\n\npreprocessor = ColumnTransformer(\n  transformers=[\n    (\"scale\", StandardScaler(), \n    [col for col in data.columns \n      if col not in [\"ret_excess\", \"month\", \"industry\"]])\n  ],\n  remainder=\"drop\",\n  verbose_feature_names_out=False\n)\n\n\n\nBuild a model\n Next, we can build an actual model based on our pre-processed data. In line with the definition above, we estimate regression coefficients of a Lasso regression such that we get\n\\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned} \\tag{10}\\] In the application at hand, \\(X\\) contains 104 columns with all possible interactions between factor returns and macroeconomic variables. We want to emphasize that the workflow for any model is very similar, irrespective of the specific model. As you will see further below, it is straightforward to fit Ridge regression coefficients and, later, Neural networks or Random forests with similar code. For now, we start with the linear regression model with an arbitrarily chosen value for the penalty factor \\(\\lambda\\) (denoted as alpha=0.007 in the code below). In the setup below, l1_ratio denotes the value of \\(1-\\rho\\), hence setting l1_ratio=1 implies the Lasso.\n\nlm_model = ElasticNet(\n  alpha=0.007,\n  l1_ratio=1, \n  max_iter=5000, \n  fit_intercept=False\n)  \n\nlm_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", lm_model)\n])\n\nThat’s it - we are done! The object lm_model_pipeline contains the definition of our model with all required information, in particular the pre-processing steps and the regression model.\n\n\nFit a model\nWith the pipeline from above, we are ready to fit it to the data. Typically, we use training data to fit the model. The training data is pre-processed according to our recipe steps, and the Lasso regression coefficients are computed. For illustrative purposes, we focus on the manufacturing industry for now.\n\ndata_manufacturing = data.query(\"industry == 'manuf'\")\ntraining_date = \"2011-12-01\"\n\ndata_manufacturing_training = (data_manufacturing\n  .query(f\"month&lt;'{training_date}'\")\n)\n\nlm_fit = lm_pipeline.fit(\n  data_manufacturing_training, \n  data_manufacturing_training.get(\"ret_excess\")\n)\n\nFirst, we focus on the in-sample predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) Figure 2 illustrates the projections for the entire time series of the manufacturing industry portfolio returns.\n\npredicted_values = (pd.DataFrame({\n    \"Fitted value\": lm_fit.predict(data_manufacturing),\n    \"Realization\": data_manufacturing.get(\"ret_excess\")\n  })\n  .assign(month = data_manufacturing[\"month\"])\n  .melt(id_vars=\"month\", var_name=\"Variable\", value_name=\"return\")\n)\n\npredicted_values_plot = (\n  ggplot(predicted_values, \n         aes(x=\"month\", y=\"return\", \n             color=\"Variable\", linetype=\"Variable\")) +\n  annotate(\n    \"rect\",\n    xmin=data_manufacturing_training[\"month\"].max(),\n    xmax=data_manufacturing[\"month\"].max(),\n    ymin=-np.inf, ymax=np.inf,\n    alpha=0.25, fill=\"#808080\"\n  ) + \n  geom_line() +\n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"Monthly realized and fitted manufacturing risk premia\") +\n  scale_x_datetime(breaks=date_breaks(\"5 years\"), \n                   labels=date_format(\"%Y\")) +\n  scale_y_continuous(labels=percent_format())\n)\npredicted_values_plot.draw()\n\n\n\n\n\n\n\nFigure 2: The figure shows monthly realized and fitted manufacturing industry risk premium. The grey area corresponds to the out of sample period.\n\n\n\n\n\nWhat do the estimated coefficients look like? To analyze these values, it is worth computing the coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. The code below estimates the coefficients for the Lasso and Ridge regression for the processed training data sample for a grid of different \\(\\lambda\\)’s.\n\nx = preprocessor.fit_transform(data_manufacturing)\ny = data_manufacturing[\"ret_excess\"]\n\nalphas = np.logspace(-5, 5, 100)\n\ncoefficients_lasso = []\nfor a in alphas:\n    lasso = Lasso(alpha=a, fit_intercept=False)\n    coefficients_lasso.append(lasso.fit(x, y).coef_)\n    \ncoefficients_lasso = (pd.DataFrame(coefficients_lasso)\n  .assign(alpha=alphas, model=\"Lasso\")\n  .melt(id_vars=[\"alpha\", \"model\"])\n)\n    \ncoefficients_ridge = []\nfor a in alphas:\n    ridge = Ridge(alpha=a, fit_intercept=False)\n    coefficients_ridge.append(ridge.fit(x, y).coef_)\n\ncoefficients_ridge = (pd.DataFrame(coefficients_ridge)\n  .assign(alpha=alphas, model=\"Ridge\")\n  .melt(id_vars=[\"alpha\", \"model\"])\n)\n\nThe dataframes lasso_coefficients and ridge_coefficients contain an entire sequence of estimated coefficients for multiple values of the penalty factor \\(\\lambda\\). Figure 3 illustrates the trajectories of the regression coefficients as a function of the penalty factor. Both Lasso and Ridge coefficients converge to zero as the penalty factor increases.\n\ncoefficients_plot = (\n  ggplot(pd.concat([coefficients_lasso, coefficients_ridge]), \n         aes(x=\"alpha\", y=\"value\", color=\"variable\")) + \n  geom_line()  +\n  facet_wrap(\"model\") +\n  labs(x=\"Penalty factor (lambda)\", y=\"\",\n       title=\"Estimated coefficient paths for different penalty factors\") +\n  scale_x_log10() +\n  theme(legend_position=\"none\"))\ncoefficients_plot.draw()\n\n\n\n\n\n\n\nFigure 3: The figure shows estimated coefficient paths for different penalty factors. The penalty parameters are chosen iteratively to resemble the path from no penalization to a model that excludes all variables.\n\n\n\n\n\n\n\nTune a model\nTo compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , we simply imposed an arbitrary value for the penalty hyperparameter \\(\\lambda\\). Model tuning is the process of optimally selecting such hyperparameters through cross-validation.\nThe goal for choosing \\(\\lambda\\) (or any other hyperparameter, e.g., \\(\\rho\\) for the Elastic Net) is to find a way to produce predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, the MSPE is not directly observable. We can only compute an estimate because our data is random and because we do not observe the entire population.\nObviously, if we train an algorithm on the same data that we use to compute the error, our estimate \\(\\text{MSPE}\\) would indicate way better predictive accuracy than what we can expect in real out-of-sample data. The result is called overfitting.\nCross-validation is a technique that allows us to alleviate this problem. We approximate the true MSPE as the average of many MSPE obtained by creating predictions for \\(K\\) new random samples of the data, none of them used to train the algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). In practice, this is done by carving out a piece of our data and pretending it is an independent sample. We again divide the data into a training set and a test set. The MSPE on the test set is our measure for actual predictive ability, while we use the training set to fit models with the aim to find the optimal hyperparameter values. To do so, we further divide our training sample into (several) subsets, fit our model for a grid of potential hyperparameter values (e.g., \\(\\lambda\\)), and evaluate the predictive accuracy on an independent sample. This works as follows:\n\nSpecify a grid of hyperparameters\nObtain predictors \\(\\hat{y}_i(\\lambda)\\) to denote the predictors for the used parameters \\(\\lambda\\)\nCompute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2 .\n\\tag{11}\\] With K-fold cross-validation, we do this computation \\(K\\) times. Simply pick a validation set with \\(M=T/K\\) observations at random and think of these as random samples \\(y_1^k, \\dots, y_{\\tilde{T}}^k\\), with \\(k=1\\).\n\nHow should you pick \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original data. However, larger values of \\(K\\) will have much higher computation time. scikit-learn provides all required tools to conduct \\(K\\)-fold cross-validation. We just have to update our model specification. In our case, we specify the penalty factor \\(\\lambda\\) as well as the mixing factor \\(\\rho\\) as free parameters.\nFor our sample, we consider a time-series cross-validation sample. This means that we tune our models with 20 random samples of length five years with a validation period of four years. For a grid of possible hyperparameters, we then fit the model for each fold and evaluate \\(\\hat{\\text{MSPE}}\\) in the corresponding validation set. Finally, we select the model specification with the lowest MSPE in the validation set. First, we define the cross-validation folds based on our training data only.\nThen, we evaluate the performance for a grid of different penalty values. scikit-learn provides functionalities to construct a suitable grid of hyperparameters with GridSearchCV(). The code chunk below creates a \\(10 \\times 3\\) hyperparameters grid. Then, the method fit() evaluates all the models for each fold.\n\ninitial_years = 5\nassessment_months = 48\nn_splits = int(len(data_manufacturing)/assessment_months) - 1\nlength_of_year = 12\nalphas = np.logspace(-6, 2, 100)\n\ndata_folds = TimeSeriesSplit(\n  n_splits=n_splits, \n  test_size=assessment_months, \n  max_train_size=initial_years * length_of_year\n)\n\nparams = {\n  \"regressor__alpha\": alphas,\n  \"regressor__l1_ratio\": (0.0, 0.5, 1)\n}\n\nfinder = GridSearchCV(\n  lm_pipeline,\n  param_grid=params,\n  scoring=\"neg_root_mean_squared_error\",\n  cv=data_folds\n)\n\nfinder = finder.fit(\n  data_manufacturing, data_manufacturing.get(\"ret_excess\")\n)\n\nAfter the tuning process, we collect the evaluation metrics (the root mean-squared error in our example) to identify the optimal model. Figure 4 illustrates the average validation set’s root mean-squared error for each value of \\(\\lambda\\) and \\(\\rho\\).\n\nvalidation = (pd.DataFrame(finder.cv_results_)\n  .assign(\n    mspe=lambda x: -x[\"mean_test_score\"],\n    param_regressor__alpha=lambda x: pd.to_numeric(\n      x[\"param_regressor__alpha\"], errors=\"coerce\"\n    )\n  )\n)\n\nvalidation_plot = (ggplot(validation, \n  aes(x=\"param_regressor__alpha\", y=\"mspe\", \n      color=\"param_regressor__l1_ratio\",\n      shape=\"param_regressor__l1_ratio\",\n      group=\"param_regressor__l1_ratio\")) +\n  geom_point() + \n  geom_line() +\n  labs(x =\"Penalty factor (lambda)\", y=\"Root MSPE\", \n       title=\"Root MSPE for different penalty factors\",\n       color=\"Proportion of Lasso Penalty\",\n       shape=\"Proportion of Lasso Penalty\") +\n  scale_x_log10() + \n  guides(linetype=\"none\")\n)\nvalidation_plot.draw()\n\n\n\n\n\n\n\nFigure 4: The figure shows root MSPE for different penalty factors. Evaluation of manufacturing excess returns for different penalty factors (lambda) and proportions of Lasso penalty (rho). 1.0 indicates Lasso, 0.5 indicates Elastic Net, and 0.0 indicates Ridge.\n\n\n\n\n\nFigure 4 shows that the MSPE drops faster for Lasso and Elastic Net compared to Ridge regressions as penalty factor increases. However, for higher penalty factors, the MSPE for Ridge regressions dips below the others, which both slightly increase again above a certain threshold. Recall that the larger the regularization, the more restricted the model becomes. The best performing model yields a penalty parameter (alpha) of 0.0063 and a mixture factor (\\(\\rho\\)) of 0.5.\n\n\nFull workflow\nOur starting point was the question: Which factors determine industry returns? While Avramov et al. (2023) provide a Bayesian analysis related to the research question above, we choose a simplified approach: To illustrate the entire workflow, we now run the penalized regressions for all ten industries. We want to identify relevant variables by fitting Lasso models for each industry returns time series. More specifically, we perform cross-validation for each industry to identify the optimal penalty factor \\(\\lambda\\).\nFirst, we define the Lasso model with one tuning parameter.\n\nlm_model = Lasso(fit_intercept=False, max_iter=5000)\n\nparams = {\"regressor__alpha\": alphas}\n\nlm_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", lm_model)\n])\n\nThe following task can be easily parallelized to reduce computing time, but we use a simple loop for ease of exposition.\n\nall_industries = data[\"industry\"].drop_duplicates()\n\nresults = []\nfor industry in all_industries:\n  print(industry)\n  finder = GridSearchCV(\n    lm_pipeline,\n    param_grid=params,\n    scoring=\"neg_mean_squared_error\",\n    cv=data_folds\n  )\n\n  finder = finder.fit(\n    data.query(\"industry == @industry\"),\n    data.query(\"industry == @industry\").get(\"ret_excess\")\n  )\n  results.append(\n    pd.DataFrame(finder.best_estimator_.named_steps.regressor.coef_ != 0)\n  )\n\nselected_factors = (\n  pd.DataFrame(\n    lm_pipeline[:-1].get_feature_names_out(),\n    columns=[\"variable\"]\n  )\n  .assign(variable = lambda x: (\n    x[\"variable\"].str.replace(\"factor_|ff_|q_|macro_\",\"\"))\n  )\n  .assign(**dict(zip(all_industries, results)))\n  .melt(id_vars=\"variable\", var_name =\"industry\")\n  .query(\"value == True\")\n)\n\nWhat has just happened? In principle, exactly the same as before but instead of computing the Lasso coefficients for one industry, we did it for ten sequentially. Now, we just have to do some housekeeping and keep only variables that Lasso does not set to zero. We illustrate the results in a heat map in Figure 5.\n\nselected_factors_plot = (\n  ggplot(selected_factors, \n         aes(x=\"variable\", y=\"industry\")) +\n  geom_tile() +\n  labs(x=\"\", y=\"\", \n       title=\"Selected variables for different industries\") +\n  coord_flip() +\n  scale_x_discrete(limits=reversed) +\n  theme(axis_text_x=element_text(rotation=70, hjust=1),\n        figure_size=(6.4, 6.4))\n)\nselected_factors_plot.draw()\n\n\n\n\n\n\n\nFigure 5: The figure shows selected variables for different industries. Dark areas indicate that the estimated Lasso regression coefficient is not set to zero. White fields show which variables get assigned a value of exactly zero.\n\n\n\n\n\nThe heat map in Figure 5 conveys two main insights: first, we see that many factors, macroeconomic variables, and interaction terms are not relevant for explaining the cross-section of returns across the industry portfolios. In fact, only factor_ff_mkt_excess and its interaction with macro_bm a role for several industries. Second, there seems to be quite some heterogeneity across different industries. While barely any variable is selected by Lasso for Utilities, many factors are selected for, e.g., Durable and Manufacturing, but the selected factors do not necessarily coincide. In other words, there seems to be a clear picture that we do not need many factors, but Lasso does not provide a factor that consistently provides pricing abilities across industries.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/factor-selection-via-machine-learning.html#exercises",
    "href": "python/factor-selection-via-machine-learning.html#exercises",
    "title": "Factor Selection via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and lambda and then returns the Ridge estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_2\\) penalty.\nCompute the \\(L_2\\) norm (\\(\\beta'\\beta\\)) for the regression coefficients based on the predictive regression from the previous exercise for a range of \\(\\lambda\\)’s and illustrate the effect of penalization in a suitable figure.\nNow, write a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and \\(\\lambda\\) and then returns the Lasso estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_1\\) penalty.\nAfter you understand what Ridge and Lasso regressions are doing, familiarize yourself with the glmnet package’s documentation. It is a thoroughly tested and well-established package that provides efficient code to compute the penalized regression coefficients for Ridge and Lasso and for combinations, commonly called Elastic Nets.\n\n\n\n\nFigure 1: The box plots show the monthly dispersion of returns for 10 different industries.\nFigure 2: The figure shows monthly realized and fitted manufacturing industry risk premium. The grey area corresponds to the out of sample period.\nFigure 3: The figure shows estimated coefficient paths for different penalty factors. The penalty parameters are chosen iteratively to resemble the path from no penalization to a model that excludes all variables.\nFigure 4: The figure shows root MSPE for different penalty factors. Evaluation of manufacturing excess returns for different penalty factors (lambda) and proportions of Lasso penalty (rho). 1.0 indicates Lasso, 0.5 indicates Elastic Net, and 0.0 indicates Ridge.\nFigure 5: The figure shows selected variables for different industries. Dark areas indicate that the estimated Lasso regression coefficient is not set to zero. White fields show which variables get assigned a value of exactly zero.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html",
    "href": "python/fixed-effects-and-clustered-standard-errors.html",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we provide an intuitive introduction to the two popular concepts of fixed effects regressions and clustered standard errors. When working with regressions in empirical finance, you will sooner or later be confronted with discussions around how you deal with omitted variables bias and dependence in your residuals. The concepts we introduce in this chapter are designed to address such concerns.\nWe focus on a classical panel regression common to the corporate finance literature (e.g., Fazzari et al. 1988; Erickson and Whited 2012; Gulen and Ion 2015): firm investment modeled as a function that increases in firm cash flow and firm investment opportunities.\nTypically, this investment regression uses quarterly balance sheet data provided via Compustat because it allows for richer dynamics in the regressors and more opportunities to construct variables. As we focus on the implementation of fixed effects and clustered standard errors, we use the annual Compustat data from our previous chapters and leave the estimation using quarterly data as an exercise. We demonstrate below that the regression based on annual data yields qualitatively similar results to estimations based on quarterly data from the literature, namely confirming the positive relationships between investment and the two regressors.\nThe current chapter relies on the following set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport datetime as dt\nimport itertools\nimport linearmodels as lm\n\nfrom regtabletotext import prettify_result, prettify_result\nCompared to previous chapters, we introduce linearmodels (Sheppard 2023), which provides tools for estimating various econometric models such as panel regressions.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and annual Compustat as data sources from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. In particular, Compustat provides balance sheet and income statement data on a firm level, while CRSP provides market valuations. \n\ntidy_finance = sqlite3.connect(\n  database=\"data/tidy_finance_python.sqlite\"\n)\n\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT gvkey, month, mktcap FROM crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\ncompustat = pd.read_sql_query(\n  sql=(\"SELECT datadate, gvkey, year, at, be, capx, oancf, txdb \"\n       \"FROM compustat\"),\n  con=tidy_finance,\n  parse_dates={\"datadate\"}\n)\n\nThe classical investment regressions model is the capital investment of a firm as a function of operating cash flows and Tobin’s q, a measure of a firm’s investment opportunities. We start by constructing investment and cash flows which are usually normalized by lagged total assets of a firm. In the following code chunk, we construct a panel of firm-year observations, so we have both cross-sectional information on firms as well as time-series information for each firm.\n\ndata_investment = (compustat\n  .assign(\n    month=lambda x: (\n      pd.to_datetime(x[\"datadate\"]).dt.to_period(\"M\").dt.to_timestamp()\n    )\n  )\n  .merge(compustat.get([\"gvkey\", \"year\", \"at\"])\n         .rename(columns={\"at\": \"at_lag\"})\n         .assign(year=lambda x: x[\"year\"]+1), \n         on=[\"gvkey\", \"year\"], how=\"left\")\n  .query(\"at &gt; 0 and at_lag &gt; 0\")\n  .assign(investment=lambda x: x[\"capx\"]/x[\"at_lag\"],\n          cash_flows=lambda x: x[\"oancf\"]/x[\"at_lag\"])                   \n)\n\ndata_investment = (data_investment\n  .merge(data_investment.get([\"gvkey\", \"year\", \"investment\"])\n          .rename(columns={\"investment\": \"investment_lead\"})\n          .assign(year=lambda x: x[\"year\"]-1), \n         on=[\"gvkey\", \"year\"], how=\"left\")\n)\n\nTobin’s q is the ratio of the market value of capital to its replacement costs. It is one of the most common regressors in corporate finance applications (e.g., Fazzari et al. 1988; Erickson and Whited 2012). We follow the implementation of Gulen and Ion (2015) and compute Tobin’s q as the market value of equity (mktcap) plus the book value of assets (at) minus book value of equity (be) plus deferred taxes (txdb), all divided by book value of assets (at). Finally, we only keep observations where all variables of interest are non-missing, and the reported book value of assets is strictly positive.\n\ndata_investment = (data_investment\n  .merge(crsp_monthly, on=[\"gvkey\", \"month\"], how=\"left\")\n  .assign(\n    tobins_q=lambda x: (\n      (x[\"mktcap\"]+x[\"at\"]-x[\"be\"]+x[\"txdb\"])/x[\"at\"]\n    )\n  )\n  .get([\"gvkey\", \"year\", \"investment_lead\", \"cash_flows\", \"tobins_q\"])\n  .dropna()\n)\n\nAs the variable construction typically leads to extreme values that are most likely related to data issues (e.g., reporting errors), many papers include winsorization of the variables of interest. Winsorization involves replacing values of extreme outliers with quantiles on the respective end. The following function implements the winsorization for any percentage cut that should be applied on either end of the distributions. In the specific example, we winsorize the main variables (investment, cash_flows, and tobins_q) at the one percent level.1\n\ndef winsorize(x, cut):\n    \"\"\"Winsorize returns at cut level.\"\"\"\n    \n    tmp_x = x.copy()\n    upper_quantile=np.nanquantile(tmp_x, 1 - cut)\n    lower_quantile=np.nanquantile(tmp_x, cut)\n    tmp_x[tmp_x &gt; upper_quantile]=upper_quantile\n    tmp_x[tmp_x &lt; lower_quantile]=lower_quantile\n    \n    return tmp_x\n\ndata_investment = (data_investment\n  .assign(\n    investment_lead=lambda x: winsorize(x[\"investment_lead\"], 0.01),\n    cash_flows=lambda x: winsorize(x[\"cash_flows\"], 0.01),\n    tobins_q=lambda x: winsorize(x[\"tobins_q\"], 0.01)\n  )\n)\n\nBefore proceeding to any estimations, we highly recommend tabulating summary statistics of the variables that enter the regression. These simple tables allow you to check the plausibility of your numerical variables, as well as spot any obvious errors or outliers. Additionally, for panel data, plotting the time series of the variable’s mean and the number of observations is a useful exercise to spot potential problems.\n\ndata_investment_summary = (data_investment\n  .melt(id_vars=[\"gvkey\", \"year\"], var_name=\"measure\",\n        value_vars=[\"investment_lead\", \"cash_flows\", \"tobins_q\"])\n  .get([\"measure\", \"value\"])\n  .groupby(\"measure\")\n  .describe(percentiles=[0.05, 0.5, 0.95])\n)\nnp.round(data_investment_summary, 2)\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\ncount\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nmeasure\n\n\n\n\n\n\n\n\n\n\n\n\ncash_flows\n127468.0\n0.01\n0.27\n-1.56\n-0.47\n0.06\n0.27\n0.48\n\n\ninvestment_lead\n127468.0\n0.06\n0.08\n0.00\n0.00\n0.03\n0.21\n0.46\n\n\ntobins_q\n127468.0\n2.00\n1.70\n0.57\n0.79\n1.39\n5.37\n10.91",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nTo illustrate fixed effects regressions, we use the linearmodels package, which is both computationally powerful and flexible with respect to model specifications. We start out with the basic investment regression using the simple model \\[ \\text{Investment}_{i,t+1} = \\alpha + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t}, \\tag{1}\\] where \\(\\varepsilon_t\\) is i.i.d. normally distributed across time and firms. We use the PanelOLS()-function to estimate the simple model so that the output has the same structure as the other regressions below.\n\nmodel_ols = lm.PanelOLS.from_formula(\n  formula=\"investment_lead ~ cash_flows + tobins_q + 1\",\n  data=data_investment.set_index([\"gvkey\", \"year\"]),\n).fit()\nprettify_result(model_ols)\n\nPanel OLS Model:\ninvestment_lead ~ cash_flows + tobins_q + 1\n\nCovariance Type: Unadjusted\n\nCoefficients:\n            Estimate  Std. Error  t-Statistic  p-Value\nIntercept      0.042       0.000      127.470      0.0\ncash_flows     0.049       0.001       61.776      0.0\ntobins_q       0.007       0.000       57.078      0.0\n\nSummary statistics:\n- Number of observations: 127,468\n- R-squared (incl. FE): 0.043, Within R-squared: 0.039\n\n\n\nAs expected, the regression output shows significant coefficients for both variables. Higher cash flows and investment opportunities are associated with higher investment. However, the simple model actually may have a lot of omitted variables, so our coefficients are most likely biased. As there is a lot of unexplained variation in our simple model (indicated by the rather low adjusted R-squared), the bias in our coefficients is potentially severe, and the true values could be above or below zero. Note that there are no clear cutoffs to decide when an R-squared is high or low, but it depends on the context of your application and on the comparison of different models for the same data.\nOne way to tackle the issue of omitted variable bias is to get rid of as much unexplained variation as possible by including fixed effects; i.e., model parameters that are fixed for specific groups (e.g., Wooldridge 2010). In essence, each group has its own mean in fixed effects regressions. The simplest group that we can form in the investment regression is the firm level. The firm fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t}, \\tag{2}\\] where \\(\\alpha_i\\) is the firm fixed effect and captures the firm-specific mean investment across all years. In fact, you could also compute firms’ investments as deviations from the firms’ average investments and estimate the model without the fixed effects. The idea of the firm fixed effect is to remove the firm’s average investment, which might be affected by firm-specific variables that you do not observe. For example, firms in a specific industry might invest more on average. Or you observe a young firm with large investments but only small concurrent cash flows, which will only happen in a few years. This sort of variation is unwanted because it is related to unobserved variables that can bias your estimates in any direction.\nTo include the firm fixed effect, we use gvkey (Compustat’s firm identifier) as follows:\n\nmodel_fe_firm = lm.PanelOLS.from_formula(\n  formula=\"investment_lead ~ cash_flows + tobins_q + EntityEffects\",\n  data=data_investment.set_index([\"gvkey\", \"year\"]),\n).fit()\nprettify_result(model_fe_firm)\n\nPanel OLS Model:\ninvestment_lead ~ cash_flows + tobins_q + EntityEffects\n\nCovariance Type: Unadjusted\n\nCoefficients:\n            Estimate  Std. Error  t-Statistic  p-Value\ncash_flows     0.014       0.001       15.217      0.0\ntobins_q       0.011       0.000       82.093      0.0\n\nIncluded Fixed Effects:\n        Total\nEntity  14350\n\nSummary statistics:\n- Number of observations: 127,468\n- R-squared (incl. FE): 0.585, Within R-squared: 0.057\n\n\n\nThe regression output shows a lot of unexplained variation at the firm level that is taken care of by including the firm fixed effect as the adjusted R-squared rises above 50 percent. In fact, it is more interesting to look at the within R-squared that shows the explanatory power of a firm’s cash flow and Tobin’s q on top of the average investment of each firm. We can also see that the coefficients changed slightly in magnitude but not in sign.\nThere is another source of variation that we can get rid of in our setting: average investment across firms might vary over time due to macroeconomic factors that affect all firms, such as economic crises. By including year fixed effects, we can take out the effect of unobservables that vary over time. The two-way fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\alpha_t + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t}, \\tag{3}\\] where \\(\\alpha_t\\) is the time fixed effect. Here you can think of higher investments during an economic expansion with simultaneously high cash flows. You can include a time fixed effects by using “TimeEffects” in the formula of PanelOLS.\n\nmodel_fe_firmyear = lm.PanelOLS.from_formula(\n  formula=(\"investment_lead ~ cash_flows + tobins_q + EntityEffects\" \n           \" + TimeEffects\"),\n  data=data_investment.set_index([\"gvkey\", \"year\"]),\n).fit()\nprettify_result(model_fe_firmyear)\n\nPanel OLS Model:\ninvestment_lead ~ cash_flows + tobins_q + EntityEffects + TimeEffects\n\nCovariance Type: Unadjusted\n\nCoefficients:\n            Estimate  Std. Error  t-Statistic  p-Value\ncash_flows     0.018       0.001       19.356      0.0\ntobins_q       0.010       0.000       75.414      0.0\n\nIncluded Fixed Effects:\n        Total\nEntity  14350\nTime       35\n\nSummary statistics:\n- Number of observations: 127,468\n- R-squared (incl. FE): 0.606, Within R-squared: 0.057\n\n\n\nThe inclusion of time fixed effects did only marginally affect the R-squared and the coefficients, which we can interpret as a good thing as it indicates that the coefficients are not driven by an omitted variable that varies over time.\nHow can we further improve the robustness of our regression results? Ideally, we want to get rid of unexplained variation at the firm-year level, which means we need to include more variables that vary across firm and time and are likely correlated with investment. Note that we cannot include firm-year fixed effects in our setting because then cash flows and Tobin’s q are colinear with the fixed effects, and the estimation becomes void.\nBefore we discuss the properties of our estimation errors, we want to point out that regression tables are at the heart of every empirical analysis, where you compare multiple models. Fortunately, the results.compare() function provides a convenient way to tabulate the regression output (with many parameters to customize and even print the output in LaTeX). We recommend printing \\(t\\)-statistics rather than standard errors in regression tables because the latter are typically very hard to interpret across coefficients that vary in size. We also do not print p-values because they are sometimes misinterpreted to signal the importance of observed effects (Wasserstein and Lazar 2016). The \\(t\\)-statistics provide a consistent way to interpret changes in estimation uncertainty across different model specifications.\n\nprettify_result([model_ols, model_fe_firm, model_fe_firmyear])\n\nDependent var.  investment_lead  investment_lead  investment_lead\n\nIntercept       0.042 (127.47)\ncash_flows       0.049 (61.78)    0.014 (15.22)    0.018 (19.36)\ntobins_q         0.007 (57.08)    0.011 (82.09)    0.01 (75.41)\n\nFixed effects                        Entity        Entity, Time\nVCOV type         Unadjusted       Unadjusted       Unadjusted\nObservations        127,468          127,468          127,468\nR2 (incl. FE)        0.043            0.585            0.606\nWithin R2            0.039            0.057            0.057\n\nNote: t-statistics in parentheses",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Clustering Standard Errors",
    "text": "Clustering Standard Errors\nApart from biased estimators, we usually have to deal with potentially complex dependencies of our residuals with each other. Such dependencies in the residuals invalidate the i.i.d. assumption of OLS and lead to biased standard errors. With biased OLS standard errors, we cannot reliably interpret the statistical significance of our estimated coefficients.\nIn our setting, the residuals may be correlated across years for a given firm (time-series dependence), or, alternatively, the residuals may be correlated across different firms (cross-section dependence). One of the most common approaches to dealing with such dependence is the use of clustered standard errors (Petersen 2008). The idea behind clustering is that the correlation of residuals within a cluster can be of any form. As the number of clusters grows, the cluster-robust standard errors become consistent (Donald and Lang 2007; Wooldridge 2010). A natural requirement for clustering standard errors in practice is hence a sufficiently large number of clusters. Typically, around at least 30 to 50 clusters are seen as sufficient (Cameron, Gelbach, and Miller 2011).\nInstead of relying on the i.i.d. assumption, we can use the cov_type=\"clustered\" option in the fit()-function as above. The code chunk below applies both one-way clustering by firm as well as two-way clustering by firm and year.\n\nmodel_cluster_firm = lm.PanelOLS.from_formula(\n  formula=(\"investment_lead ~ cash_flows + tobins_q + EntityEffects\"\n           \" + TimeEffects\"),\n  data=data_investment.set_index([\"gvkey\", \"year\"]),\n).fit(cov_type=\"clustered\", cluster_entity=True, cluster_time=False)\n\nmodel_cluster_firmyear = lm.PanelOLS.from_formula(\n  formula=(\"investment_lead ~ cash_flows + tobins_q + EntityEffects\"\n           \" + TimeEffects\"),\n  data=data_investment.set_index([\"gvkey\", \"year\"]),\n).fit(cov_type=\"clustered\", cluster_entity=True, cluster_time=True)\n\n The table below shows the comparison of the different assumptions behind the standard errors. In the first column, we can see highly significant coefficients on both cash flows and Tobin’s q. By clustering the standard errors on the firm level, the \\(t\\)-statistics of both coefficients drop in half, indicating a high correlation of residuals within firms. If we additionally cluster by year, we see a drop, particularly for Tobin’s q, again. Even after relaxing the assumptions behind our standard errors, both coefficients are still comfortably significant as the \\(t\\)-statistics are well above the usual critical values of 1.96 or 2.576 for two-tailed significance tests.\n\nprettify_result([\n  model_fe_firmyear, model_cluster_firm, model_cluster_firmyear\n])\n\nDependent var.  investment_lead  investment_lead  investment_lead\n\ncash_flows       0.018 (19.36)    0.018 (10.57)    0.018 (9.03)\ntobins_q         0.01 (75.41)     0.01 (33.44)     0.01 (14.64)\n\nFixed effects    Entity, Time     Entity, Time     Entity, Time\nVCOV type         Unadjusted        Clustered        Clustered\nObservations        127,468          127,468          127,468\nR2 (incl. FE)        0.606            0.606            0.606\nWithin R2            0.057            0.057            0.057\n\nNote: t-statistics in parentheses\n\n\nInspired by Abadie et al. (2017), we want to close this chapter by highlighting that choosing the right dimensions for clustering is a design problem. Even if the data is informative about whether clustering matters for standard errors, they do not tell you whether you should adjust the standard errors for clustering. Clustering at too aggregate levels can hence lead to unnecessarily inflated standard errors.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#exercises",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#exercises",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Exercises",
    "text": "Exercises\n\nEstimate the two-way fixed effects model with two-way clustered standard errors using quarterly Compustat data from WRDS.\nFollowing Peters and Taylor (2017), compute Tobin’s q as the market value of outstanding equity mktcap plus the book value of debt (dltt + dlc) minus the current assets atc and everything divided by the book value of property, plant and equipment ppegt. What is the correlation between the measures of Tobin’s q? What is the impact on the two-way fixed effects regressions?",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/fixed-effects-and-clustered-standard-errors.html#footnotes",
    "href": "python/fixed-effects-and-clustered-standard-errors.html#footnotes",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that in pandas, when you index a dataframee, you receive a reference to the original dataframee. Consequently, modifying a subset will directly impact the initial dataframee. To prevent unintended changes to the original dataframee, it is advisable to use the copy() method as we do here in the winsorize function.↩︎",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html",
    "href": "python/introduction-to-tidy-finance.html",
    "title": "Introduction to Tidy Finance",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nThe main aim of this chapter is to familiarize yourself with pandas (McKinney 2010) and numpy (Harris et al. 2020), the main workhorses for data analysis in Python. We start by downloading and visualizing stock data from Yahoo!Finance. Then, we move to a simple portfolio choice problem and construct the efficient frontier. These examples introduce you to our approach of Tidy Finance.",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#working-with-stock-market-data",
    "href": "python/introduction-to-tidy-finance.html#working-with-stock-market-data",
    "title": "Introduction to Tidy Finance",
    "section": "Working with Stock Market Data",
    "text": "Working with Stock Market Data\nAt the start of each session, we load the required Python packages. Throughout the entire book, we always use pandas and numpy to perform a number of data manipulations. In this chapter, we also load the convenient yfinance (Aroussi 2023) package to download price data.\n\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\n\nNote that import pandas as pd implies that we can call all pandas functions later with a simple pd.function(). Instead, utilizing from pandas import * is generally discouraged, as it leads to namespace pollution. This statement imports all functions and classes from pandas into your current namespace, potentially causing conflicts with functions you define or those from other imported libraries. Using the pd abbreviation is a very convenient way to prevent this.\nWe first download daily prices for one stock symbol, e.g., the Apple stock, AAPL, directly from the data provider Yahoo!Finance. To download the data, you can use the function yf.download(). The data from Yahoo!Finance comes as a dataframe, a two-dimensional, tabular data structure in which each row is indexed, and each column has a name. After the download, we apply a set of functions directly on the dataframe. First, we put the date index into a separate column. Second, we add the column symbol that stores the ticker information, and finally, we rename all columns to lowercase names. Dataframes allow for chaining all these operations sequentially through using .. \n\nprices = (yf.download(\n    tickers=\"AAPL\", \n    start=\"2000-01-01\", \n    end=\"2022-12-31\", \n    progress=False\n  )\n  .reset_index()\n  .assign(symbol=\"AAPL\")\n  .rename(columns={\n    \"Date\": \"date\", \n    \"Open\": \"open\", \n    \"High\": \"high\",\n    \"Low\": \"low\",\n    \"Close\": \"close\", \n    \"Adj Close\": \"adjusted\", \n    \"Volume\": \"volume\"}\n  )\n)\nprices.head().round(3)\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadjusted\nvolume\nsymbol\n\n\n\n\n0\n2000-01-03\n0.936\n1.004\n0.908\n0.999\n0.846\n535796800\nAAPL\n\n\n1\n2000-01-04\n0.967\n0.988\n0.903\n0.915\n0.775\n512377600\nAAPL\n\n\n2\n2000-01-05\n0.926\n0.987\n0.920\n0.929\n0.786\n778321600\nAAPL\n\n\n3\n2000-01-06\n0.948\n0.955\n0.848\n0.848\n0.718\n767972800\nAAPL\n\n\n4\n2000-01-07\n0.862\n0.902\n0.853\n0.888\n0.752\n460734400\nAAPL\n\n\n\n\n\n\n\n\n yf.download() downloads stock market data from Yahoo!Finance. The above code chunk returns a dataframe with eight quite self-explanatory columns: date, the market prices at the open, high, low, and close, the adjusted price in USD, the daily volume (in the number of traded shares), and the symbol. The adjusted prices are corrected for anything that might affect the stock price after the market closes, e.g., stock splits and dividends. These actions affect the quoted prices but have no direct impact on the investors who hold the stock. Therefore, we often rely on adjusted prices when it comes to analyzing the returns an investor would have earned by holding the stock continuously.\nNext, we use the plotnine (Kibirige 2023b) package to visualize the time series of adjusted prices in Figure 1. This package takes care of visualization tasks based on the principles of the Grammar of Graphics (Wilkinson 2012). Note that generally, we do not recommend using the * import style. However, we use it here only for the plotting functions, which are distinct to plotnine and have very plotting-related names. So, the risk of misuse through a polluted namespace is marginal.\n\nfrom plotnine import *\n\nCreating figures becomes very intuitive with the Grammar of Graphics, as the following code chunk demonstrates.\n\nprices_figure = (\n  ggplot(prices, \n         aes(y=\"adjusted\", x=\"date\")) +\n  geom_line() +\n  labs(x=\"\", y=\"\",\n       title=\"Apple stock prices from 2000 to 2022\")\n)\nprices_figure.draw()\n\n\n\n\n\n\n\nFigure 1: The figure shows Apple stock prices between the beginning of 2000 and the end of 2022. Prices are in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n\n Instead of analyzing prices, we compute daily returns defined as\n\\[r_t = p_t / p_{t-1} - 1,\n\\tag{1}\\]\nwhere \\(p_t\\) is the adjusted price on day \\(t\\). In that context, the function pct_change() is helpful because it computes this percentage change.\n\nreturns = (prices\n  .sort_values(\"date\")\n  .assign(ret=lambda x: x[\"adjusted\"].pct_change())\n  .get([\"symbol\", \"date\", \"ret\"])\n)\n\n\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nret\n\n\n\n\n0\nAAPL\n2000-01-03\nNaN\n\n\n1\nAAPL\n2000-01-04\n-0.084\n\n\n2\nAAPL\n2000-01-05\n0.015\n\n\n3\nAAPL\n2000-01-06\n-0.087\n\n\n4\nAAPL\n2000-01-07\n0.047\n\n\n\n\n\n\n\n\nThe resulting dataframe contains three columns, where the last contains the daily returns (ret). Note that the first entry naturally contains a missing value (NaN) because there is no previous price. Obviously, the use of pct_change() would be meaningless if the time series is not ordered by ascending dates. The function sort_values() provides a convenient way to order observations in the correct way for our application. In case you want to order observations by descending dates, you can use the parameter ascending=False.\nFor the upcoming examples, we remove missing values, as these would require separate treatment when computing, e.g., sample averages. In general, however, make sure you understand why NA values occur and carefully examine if you can simply get rid of these observations. The dataframe dropna() method kicks out all rows that contain a missing value in any column.\n\nreturns = returns.dropna() \n\nNext, we visualize the distribution of daily returns in a histogram in Figure 2, where we also introduce the mizani (Kibirige 2023a) package for formatting functions. Additionally, we add a dashed line that indicates the five percent quantile of the daily returns to the histogram, which is a (crude) proxy for the worst return of the stock with a probability of at most five percent. The five percent quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators. We refer to Tsay (2010) for a more thorough introduction to stylized facts of returns.\n\nfrom mizani.formatters import percent_format\n\nquantile_05 = returns[\"ret\"].quantile(0.05)\n\nreturns_figure = (\n  ggplot(returns, aes(x=\"ret\")) +\n  geom_histogram(bins=100) +\n  geom_vline(aes(xintercept=quantile_05), \n                 linetype=\"dashed\") +\n  labs(x=\"\", y=\"\",\n       title=\"Distribution of daily Apple stock returns\") +\n  scale_x_continuous(labels=percent_format())\n)\nreturns_figure.draw()\n\n\n\n\n\n\n\nFigure 2: The figure shows a histogram of daily Apple stock returns in percent. The dotted vertical line indicates the historical five percent quantile.\n\n\n\n\n\nHere, bins=100 determines the number of bins used in the illustration and, hence, implicitly the width of the bins. Before proceeding, make sure you understand how to use the geom geom_vline() to add a dashed line that indicates the five percent quantile of the daily returns. A typical task before proceeding with any data is to compute summary statistics for the main variables of interest.\n\npd.DataFrame(returns[\"ret\"].describe()).round(3).T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nret\n5786.0\n0.001\n0.025\n-0.519\n-0.01\n0.001\n0.013\n0.139\n\n\n\n\n\n\n\n\nWe see that the maximum daily return was 13.9 percent. Perhaps not surprisingly, the average daily return is close to but slightly above 0. In line with the illustration above, the large losses on the day with the minimum returns indicate a strong asymmetry in the distribution of returns.\nYou can also compute these summary statistics for each year individually by imposing .groupby(returns[\"date\"].dt.year), where the call .dt.year returns the year of a date variable. More specifically, the few lines of code below compute the summary statistics from above for individual groups of data defined by year. The summary statistics, therefore, allow an eyeball analysis of the time-series dynamics of the return distribution.\n\n(returns[\"ret\"]\n  .groupby(returns[\"date\"].dt.year)\n  .describe()\n  .round(3)\n)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2000\n251.0\n-0.003\n0.055\n-0.519\n-0.034\n-0.002\n0.027\n0.137\n\n\n2001\n248.0\n0.002\n0.039\n-0.172\n-0.023\n-0.001\n0.027\n0.129\n\n\n2002\n252.0\n-0.001\n0.031\n-0.150\n-0.019\n-0.003\n0.018\n0.085\n\n\n2003\n252.0\n0.002\n0.023\n-0.081\n-0.012\n0.002\n0.015\n0.113\n\n\n2004\n252.0\n0.005\n0.025\n-0.056\n-0.009\n0.003\n0.016\n0.132\n\n\n2005\n252.0\n0.003\n0.024\n-0.092\n-0.010\n0.003\n0.017\n0.091\n\n\n2006\n251.0\n0.001\n0.024\n-0.063\n-0.014\n-0.002\n0.014\n0.118\n\n\n2007\n251.0\n0.004\n0.024\n-0.070\n-0.009\n0.003\n0.018\n0.105\n\n\n2008\n253.0\n-0.003\n0.037\n-0.179\n-0.024\n-0.001\n0.019\n0.139\n\n\n2009\n252.0\n0.004\n0.021\n-0.050\n-0.009\n0.002\n0.015\n0.068\n\n\n2010\n252.0\n0.002\n0.017\n-0.050\n-0.006\n0.002\n0.011\n0.077\n\n\n2011\n252.0\n0.001\n0.017\n-0.056\n-0.009\n0.001\n0.011\n0.059\n\n\n2012\n250.0\n0.001\n0.019\n-0.064\n-0.008\n0.000\n0.012\n0.089\n\n\n2013\n252.0\n0.000\n0.018\n-0.124\n-0.009\n-0.000\n0.011\n0.051\n\n\n2014\n252.0\n0.001\n0.014\n-0.080\n-0.006\n0.001\n0.010\n0.082\n\n\n2015\n252.0\n0.000\n0.017\n-0.061\n-0.009\n-0.001\n0.009\n0.057\n\n\n2016\n252.0\n0.001\n0.015\n-0.066\n-0.006\n0.001\n0.008\n0.065\n\n\n2017\n251.0\n0.002\n0.011\n-0.039\n-0.004\n0.001\n0.007\n0.061\n\n\n2018\n251.0\n-0.000\n0.018\n-0.066\n-0.009\n0.001\n0.009\n0.070\n\n\n2019\n252.0\n0.003\n0.016\n-0.100\n-0.005\n0.003\n0.012\n0.068\n\n\n2020\n253.0\n0.003\n0.029\n-0.129\n-0.010\n0.002\n0.017\n0.120\n\n\n2021\n252.0\n0.001\n0.016\n-0.042\n-0.008\n0.001\n0.012\n0.054\n\n\n2022\n251.0\n-0.001\n0.022\n-0.059\n-0.016\n-0.001\n0.014\n0.089",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "href": "python/introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "title": "Introduction to Tidy Finance",
    "section": "Scaling Up the Analysis",
    "text": "Scaling Up the Analysis\nAs a next step, we generalize the code from before such that all the computations can handle an arbitrary vector of stock symbols (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nThis is where the magic starts: tidy data makes it extremely easy to generalize the computations from before to as many assets as you like. The following code takes any vector of symbols, e.g., symbol=[\"AAPL\", \"MMM\", \"BA\"], and automates the download as well as the plot of the price time series. In the end, we create the table of summary statistics for an arbitrary number of assets. We perform the analysis with data from all current constituents of the Dow Jones Industrial Average index. \nWe first download a table with DOW Jones constituents from an external website.\n\nurl = (\"https://www.ssga.com/us/en/institutional/etfs/library-content/\"\n       \"products/fund-data/etfs/us/holdings-daily-us-en-dia.xlsx\")\n\nsymbols = (pd.read_excel(url, skiprows=4, nrows=30)\n  .get(\"Ticker\")\n  .tolist()\n)\n\nNext, we can use yf.download() to download prices for all stock symbols in the above list and again chain a couple of pandas dataframe functions to create a tidy dataset.\n\nindex_prices = (yf.download(\n    tickers=symbols, \n    start=\"2000-01-01\", \n    end=\"2022-12-31\", \n    progress=False\n  ))\n\nindex_prices = (index_prices\n  .stack()\n  .reset_index(level=1, drop=False)\n  .reset_index()\n  .rename(columns={\n    \"Date\": \"date\",\n    \"level_1\": \"symbol\",\n    \"Open\": \"open\",\n    \"High\": \"high\",\n    \"Low\": \"low\",\n    \"Close\": \"close\",\n    \"Adj Close\": \"adjusted\",\n    \"Volume\": \"volume\"}\n  )\n)\n\nThe resulting dataframe contains 165,593 daily observations for 30 different stock symbols. Figure 3 illustrates the time series of downloaded adjusted prices for each of the constituents of the Dow Jones index. We again draw on the mizani package, but this time we use its useful date formatting function to get nicer axis labels. Make sure you understand every single line of code! What are the arguments of aes()? Which alternative geoms could you use to visualize the time series? Hint: If you do not know the answers, try to change the code to see what difference your intervention causes.\n\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\n\nindex_prices_figure = (\n  ggplot(index_prices, \n         aes(y=\"adjusted\", x=\"date\", color=\"symbol\")) +\n geom_line() +\n labs(x=\"\", y=\"\", color=\"\",\n      title=\"Stock prices of DOW index constituents\") +\n theme(legend_position=\"none\") +\n scale_x_datetime(date_breaks=\"5 years\", date_labels=\"%Y\")\n)\nindex_prices_figure.draw()\n\n\n\n\n\n\n\nFigure 3: The figure shows the stock prices of DOW index constituents. Prices are in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n\nDo you notice the small differences relative to the code we used before? yf.download(symbols) returns a dataframe for several symbols as well. All we need to do to illustrate all symbols simultaneously is to include color=\"symbol\" in the ggplot aesthetics. In this way, we generate a separate line for each symbol. Of course, there are simply too many lines on this graph to identify the individual stocks properly, but it illustrates the point well.\nThe same holds for stock returns. Before computing the returns, we use groupby(\"symbol\") such that the assign() command is performed to calculate the returns for each symbol individually and assign it to the variable ret in the dataframe. The same logic also applies to the computation of summary statistics: groupby(\"symbol\") is the key to aggregating the time series into symbol-specific variables of interest.\n\nall_returns = (index_prices\n  .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted\"].pct_change())\n  .get([\"symbol\", \"date\", \"ret\"])\n  .dropna(subset=\"ret\")\n)\n\n(all_returns\n  .groupby(\"symbol\")[\"ret\"]\n  .describe()\n  .round(3)\n)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\n\n\nAAPL\n5786.0\n0.001\n0.025\n-0.519\n-0.010\n0.001\n0.013\n0.139\n\n\nAMGN\n5786.0\n0.000\n0.020\n-0.134\n-0.009\n0.000\n0.009\n0.151\n\n\nAMZN\n5786.0\n0.001\n0.032\n-0.248\n-0.012\n0.000\n0.014\n0.345\n\n\nAXP\n5786.0\n0.001\n0.023\n-0.176\n-0.009\n0.000\n0.010\n0.219\n\n\nBA\n5786.0\n0.001\n0.022\n-0.238\n-0.010\n0.001\n0.011\n0.243\n\n\nCAT\n5786.0\n0.001\n0.020\n-0.145\n-0.010\n0.001\n0.011\n0.147\n\n\nCRM\n4664.0\n0.001\n0.027\n-0.271\n-0.012\n0.000\n0.014\n0.260\n\n\nCSCO\n5786.0\n0.000\n0.024\n-0.162\n-0.009\n0.000\n0.010\n0.244\n\n\nCVX\n5786.0\n0.001\n0.018\n-0.221\n-0.008\n0.001\n0.009\n0.227\n\n\nDIS\n5786.0\n0.000\n0.019\n-0.184\n-0.009\n0.000\n0.009\n0.160\n\n\nDOW\n954.0\n0.001\n0.026\n-0.217\n-0.012\n0.000\n0.014\n0.209\n\n\nGS\n5786.0\n0.001\n0.023\n-0.190\n-0.010\n0.000\n0.011\n0.265\n\n\nHD\n5786.0\n0.001\n0.019\n-0.287\n-0.008\n0.001\n0.009\n0.141\n\n\nHON\n5786.0\n0.001\n0.019\n-0.174\n-0.008\n0.001\n0.009\n0.282\n\n\nIBM\n5786.0\n0.000\n0.017\n-0.155\n-0.007\n0.000\n0.008\n0.120\n\n\nINTC\n5786.0\n0.000\n0.024\n-0.220\n-0.010\n0.000\n0.011\n0.201\n\n\nJNJ\n5786.0\n0.000\n0.012\n-0.158\n-0.005\n0.000\n0.006\n0.122\n\n\nJPM\n5786.0\n0.001\n0.024\n-0.207\n-0.009\n0.000\n0.010\n0.251\n\n\nKO\n5786.0\n0.000\n0.013\n-0.101\n-0.005\n0.000\n0.006\n0.139\n\n\nMCD\n5786.0\n0.001\n0.015\n-0.159\n-0.006\n0.001\n0.007\n0.181\n\n\nMMM\n5786.0\n0.000\n0.015\n-0.129\n-0.006\n0.001\n0.008\n0.126\n\n\nMRK\n5786.0\n0.000\n0.017\n-0.268\n-0.007\n0.000\n0.008\n0.130\n\n\nMSFT\n5786.0\n0.001\n0.019\n-0.156\n-0.008\n0.000\n0.009\n0.196\n\n\nNKE\n5786.0\n0.001\n0.019\n-0.198\n-0.008\n0.001\n0.009\n0.155\n\n\nPG\n5786.0\n0.000\n0.013\n-0.302\n-0.005\n0.000\n0.006\n0.120\n\n\nTRV\n5786.0\n0.001\n0.018\n-0.208\n-0.007\n0.001\n0.008\n0.256\n\n\nUNH\n5786.0\n0.001\n0.020\n-0.186\n-0.008\n0.001\n0.010\n0.348\n\n\nV\n3723.0\n0.001\n0.019\n-0.136\n-0.008\n0.001\n0.010\n0.150\n\n\nVZ\n5786.0\n0.000\n0.015\n-0.118\n-0.007\n0.000\n0.007\n0.146\n\n\nWMT\n5786.0\n0.000\n0.015\n-0.114\n-0.007\n0.000\n0.007\n0.117",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "href": "python/introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "title": "Introduction to Tidy Finance",
    "section": "Other Forms of Data Aggregation",
    "text": "Other Forms of Data Aggregation\nOf course, aggregation across variables other than symbol can also make sense. For instance, suppose you are interested in answering the question: Are days with high aggregate trading volume likely followed by days with high aggregate trading volume? To provide some initial analysis on this question, we take the downloaded data and compute aggregate daily trading volume for all Dow Jones constituents in USD. Recall that the column volume is denoted in the number of traded shares. Thus, we multiply the trading volume with the daily adjusted closing price to get a proxy for the aggregate trading volume in USD. Scaling by 1e9 (Python can handle scientific notation) denotes daily trading volume in billion USD.\n\ntrading_volume = (index_prices\n  .assign(trading_volume=lambda x: (x[\"volume\"]*x[\"adjusted\"])/1e9)\n  .groupby(\"date\")[\"trading_volume\"]\n  .sum()\n  .reset_index()\n  .assign(trading_volume_lag=lambda x: x[\"trading_volume\"].shift(periods=1))\n)\n\ntrading_volume_figure = (\n  ggplot(trading_volume, \n          aes(x=\"date\", y=\"trading_volume\")) +\n  geom_line() +\n  labs(x=\"\", y=\"\",\n       title=(\"Aggregate daily trading volume of DOW index constituents \"\n              \"in billion USD\"))  +\n scale_x_datetime(date_breaks=\"5 years\", date_labels=\"%Y\")\n)\ntrading_volume_figure.draw()\n\n\n\n\n\n\n\nFigure 4: The figure shows the total daily trading volume in billion USD.\n\n\n\n\n\nFigure 4 indicates a clear upward trend in aggregated daily trading volume. In particular, since the outbreak of the COVID-19 pandemic, markets have processed substantial trading volumes, as analyzed, for instance, by Goldstein, Koijen, and Mueller (2021). One way to illustrate the persistence of trading volume would be to plot volume on day \\(t\\) against volume on day \\(t-1\\) as in the example below. In Figure 5, we add a dotted 45°-line to indicate a hypothetical one-to-one relation by geom_abline(), addressing potential differences in the axes’ scales.\n\ntrading_volume_figure = (\n  ggplot(trading_volume, \n         aes(x=\"trading_volume_lag\", y=\"trading_volume\")) +\n  geom_point() +\n  geom_abline(aes(intercept=0, slope=1), linetype=\"dashed\") +\n  labs(x=\"Previous day aggregate trading volume\",\n       y=\"Aggregate trading volume\",\n       title=(\"Persistence in daily trading volume of DOW constituents \"\n              \"in billion USD\"))\n)\ntrading_volume_figure.draw()\n\n\n\n\n\n\n\nFigure 5: The figure a scatterplot of aggregate trading volume against previous-day aggregate trading volume.",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#portfolio-choice-problems",
    "href": "python/introduction-to-tidy-finance.html#portfolio-choice-problems",
    "title": "Introduction to Tidy Finance",
    "section": "Portfolio Choice Problems",
    "text": "Portfolio Choice Problems\nIn the previous part, we show how to download stock market data and inspect it with graphs and summary statistics. Now, we move to a typical question in Finance: How to allocate wealth across different assets optimally. The standard framework for optimal portfolio selection considers investors that prefer higher future returns but dislike future return volatility (defined as the square root of the return variance, i.e., the risk): the mean-variance investor (Markowitz 1952).\n An essential tool to evaluate portfolios in the mean-variance context is the efficient frontier, the set of portfolios that satisfies the condition that no other portfolio exists with a higher expected return but with the same volatility, see, e.g., Merton (1972). We compute and visualize the efficient frontier for several stocks. First, we extract each asset’s monthly returns. In order to keep things simple, we work with a balanced panel and exclude DOW constituents for which we do not observe a price on every single trading day since the year 2000.\n\nprices = (index_prices\n  .groupby(\"symbol\")\n  .apply(lambda x: x.assign(counts=x[\"adjusted\"].dropna().count()))\n  .reset_index(drop=True)\n  .query(\"counts == counts.max()\")\n)\n\nNext, we transform the returns from a tidy dataframe into a \\((T \\times N)\\) matrix with one column for each of the \\(N\\) symbols and one row for each of the \\(T\\) trading days to compute the sample average return vector \\[\\hat\\mu = \\frac{1}{T}\\sum\\limits_{t=1}^T r_t, \\tag{2}\\] where \\(r_t\\) is the \\(N\\) vector of returns on date \\(t\\) and the sample covariance matrix \\[\\hat\\Sigma = \\frac{1}{T-1}\\sum\\limits_{t=1}^T (r_t - \\hat\\mu)(r_t - \\hat\\mu)'. \\tag{3}\\] We achieve this by using pivot() with the new column names from the column symbol and setting the values to adjusted.\nIn financial econometrics, a core focus falls on problems that arise if the investor has to rely on estimates \\(\\hat\\mu\\) and \\(\\hat\\Sigma\\) instead of using the vector of expected returns \\(\\mu\\) and the variance-covariance matrix \\(\\Sigma\\). We highlight the impact of estimation uncertainty on the portfolio performance in various backtesting applications in Parametric Portfolio Policies and Constrained Optimization and Backtesting.\nFor now, we focus on a much more restricted set of assumptions: The \\(N\\) assets are fixed, and the first two moments of the distribution of the returns are determined by the parameters \\(\\mu\\) and \\(\\Sigma\\). Thus, even though we proceed with the vector of sample average returns and the sample variance-covariance matrix, those will be handled as the true parameters of the return distribution for the rest of this chapter. We, therefore, refer to \\(\\Sigma\\) and \\(\\mu\\) instead of explicitly highlighting that the sample moments are estimates. \n\nreturns_matrix = (prices\n  .pivot(columns=\"symbol\", values=\"adjusted\", index=\"date\")\n  .resample(\"m\")\n  .last()\n  .pct_change()\n  .dropna()\n)\nmu = np.array(returns_matrix.mean()).T\nsigma = np.array(returns_matrix.cov())\n\nThen, we compute the minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) as well as the expected return \\(\\omega_\\text{mvp}'\\mu\\) and volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) of this portfolio. Recall that the minimum variance portfolio is the vector of portfolio weights that are the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\sum\\limits_{i=1}^N\\omega_i = 1. \\tag{4}\\] The constraint that weights sum up to one simply implies that all funds are distributed across the available asset universe, i.e., there is no possibility to retain cash. It is easy to show analytically that \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\), where \\(\\iota\\) is a vector of ones and \\(\\Sigma^{-1}\\) is the inverse of \\(\\Sigma\\). We provide the proof of the analytical solution in Proofs.\n\nN = returns_matrix.shape[1]\niota = np.ones(N)\nsigma_inv = np.linalg.inv(sigma) \n\nmvp_weights = sigma_inv @ iota\nmvp_weights = mvp_weights/mvp_weights.sum()\nmvp_return = mu.T @ mvp_weights\nmvp_volatility = np.sqrt(mvp_weights.T @ sigma @ mvp_weights)\nmvp_moments = pd.DataFrame({\"value\": [mvp_return, mvp_volatility]},\n                           index=[\"average_ret\", \"volatility\"])\nmvp_moments.round(3)\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\naverage_ret\n0.008\n\n\nvolatility\n0.032\n\n\n\n\n\n\n\n\nThe command np.linalg.inv() returns the inverse of a matrix such that np.linalg.inv(sigma) delivers \\(\\Sigma^{-1}\\) (if a unique solution exists).\nNote that the monthly volatility of the minimum variance portfolio is of the same order of magnitude as the daily standard deviation of the individual components. Thus, the diversification benefits in terms of risk reduction are tremendous!\nNext, we set out to find the weights for a portfolio that achieves, as an example, three times the expected return of the minimum variance portfolio. However, mean-variance investors are not interested in any portfolio that achieves the required return but rather in the efficient portfolio, i.e., the portfolio with the lowest standard deviation. If you wonder where the solution \\(\\omega_\\text{eff}\\) comes from: The efficient portfolio is chosen by an investor who aims to achieve minimum variance given a minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, their objective function is to choose \\(\\omega_\\text{eff}\\) as the solution to \\[\\omega_\\text{eff}(\\bar{\\mu}) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}. \\tag{5}\\]\nIn Proofs, we show that the efficient portfolio takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right)\\] where \\(C:= \\iota'\\Sigma^{-1}\\iota\\), \\(D:= \\iota'\\Sigma^{-1}\\mu\\), \\(E:= \\mu'\\Sigma^{-1}\\mu\\), and \\(\\tilde\\lambda = 2\\frac{\\bar\\mu - D/C}{E-D^2/C}\\).\nThe code below implements the analytic solution to this optimization problem for a benchmark return \\(\\bar\\mu\\), which we set to 3 times the expected return of the minimum variance portfolio. We encourage you to verify that it is correct.\n\nbenchmark_multiple = 3\nmu_bar = benchmark_multiple*mvp_return\nC = iota.T @ sigma_inv @ iota\nD = iota.T @ sigma_inv @ mu\nE = mu.T @ sigma_inv @ mu\nlambda_tilde = 2*(mu_bar-D/C)/(E-D**2/C)\nefp_weights = mvp_weights+lambda_tilde/2*(sigma_inv @ mu-D*mvp_weights)",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#the-efficient-frontier",
    "href": "python/introduction-to-tidy-finance.html#the-efficient-frontier",
    "title": "Introduction to Tidy Finance",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n The mutual fund separation theorem states that as soon as we have two efficient portfolios (such as the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and the efficient portfolio for a higher required level of expected returns \\(\\omega_\\text{eff}(\\bar{\\mu})\\), we can characterize the entire efficient frontier by combining these two portfolios. That is, any linear combination of the two portfolio weights will again represent an efficient portfolio. The code below implements the construction of the efficient frontier, which characterizes the highest expected return achievable at each level of risk. To understand the code better, make sure to familiarize yourself with the inner workings of the for loop.\n\nlength_year = 12\na = np.arange(-0.4, 2.0, 0.01)\nres = pd.DataFrame(columns=[\"mu\", \"sd\"], index=a).astype(float)\n\nfor i in a:\n    w = (1-i)*mvp_weights+i*efp_weights\n    res.loc[i, \"mu\"] = (w.T @ mu)*length_year\n    res.loc[i, \"sd\"] = np.sqrt(w.T @ sigma @ w)*np.sqrt(length_year)\n\nThe code above proceeds in two steps: First, we compute a vector of combination weights \\(a\\), and then we evaluate the resulting linear combination with \\(a\\in\\mathbb{R}\\):\n\\[\\omega^* = a\\omega_\\text{eff}(\\bar\\mu) + (1-a)\\omega_\\text{mvp} = \\omega_\\text{mvp} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) \\tag{6}\\] with \\(\\lambda^* = 2\\frac{a\\bar\\mu + (1-a)\\tilde\\mu - D/C}{E-D^2/C}\\). It follows that \\(\\omega^* = \\omega_\\text{eff}\\left(a\\bar\\mu + (1-a)\\tilde\\mu\\right)\\), in other words, \\(\\omega^*\\) is an efficient portfolio that proofs the mutual fund separation theorem.\nFinally, it is simple to visualize the efficient frontier alongside the two efficient portfolios within one powerful figure using the ggplot function from plotnine (see Figure 6). We also add the individual stocks in the same call. We compute annualized returns based on the simple assumption that monthly returns are independent and identically distributed. Thus, the average annualized return is just twelve times the expected monthly return.\n\nmvp_return = (mu.T @ mvp_weights)*length_year\nmvp_volatility = (np.sqrt(mvp_weights.T @ sigma @ mvp_weights)* \n                  np.sqrt(length_year))\nefp_return = mu_bar*length_year\nefp_volatility = (np.sqrt(efp_weights.T @ sigma @ efp_weights)* \n                  np.sqrt(length_year))\n\nres_figure = (\n  ggplot(res, aes(x=\"sd\", y=\"mu\")) +\n  geom_point() +\n  geom_point(\n    pd.DataFrame({\"mu\": [mvp_return, efp_return],\n                  \"sd\": [mvp_volatility, efp_volatility]}),\n    size=4\n  ) +\n  geom_point(\n    pd.DataFrame({\"mu\": mu*length_year,\n                  \"sd\": np.sqrt(np.diag(sigma))*np.sqrt(length_year)})\n  ) +\n  labs(x=\"Annualized standard deviation\",\n       y=\"Annualized expected return\",\n       title=\"Efficient frontier for DOW index constituents\") +\n  scale_x_continuous(labels=percent_format()) +\n  scale_y_continuous(labels=percent_format())\n)\nres_figure.draw()\n\n\n\n\n\n\n\nFigure 6: The figure shows the efficient frontier for DOW index constituents. The big dots indicate the location of the minimum variance and the efficient portfolio that delivers three times the expected return of the minimum variance portfolio, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe line in Figure 6 indicates the efficient frontier: the set of portfolios a mean-variance efficient investor would choose from. Compare the performance relative to the individual assets (the dots); it should become clear that diversifying yields massive performance gains (at least as long as we take the parameters \\(\\Sigma\\) and \\(\\mu\\) as given).",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#exercises",
    "href": "python/introduction-to-tidy-finance.html#exercises",
    "title": "Introduction to Tidy Finance",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily prices for another stock market symbol of your choice from Yahoo!Finance with yf.download() from the yfinance package. Plot two time series of the ticker’s unadjusted and adjusted closing prices. Explain the differences.\nCompute daily net returns for an asset of your choice and visualize the distribution of daily returns in a histogram using 100 bins. Also, use geom_vline() to add a dashed red vertical line that indicates the five percent quantile of the daily returns. Compute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns.\nTake your code from before and generalize it such that you can perform all the computations for an arbitrary vector of tickers (e.g., ticker = [\"AAPL\", \"MMM\", \"BA\"]). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nAre days with high aggregate trading volume often also days with large absolute returns? Find an appropriate visualization to analyze the question using the ticker AAPL.\nCompute monthly returns from the downloaded stock market prices. Compute the vector of historical average returns and the sample variance-covariance matrix. Compute the minimum variance portfolio weights and the portfolio volatility and average returns. Visualize the mean-variance efficient frontier. Choose one of your assets and identify the portfolio which yields the same historical volatility but achieves the highest possible average return.\nIn the portfolio choice analysis, we restricted our sample to all assets trading every day since 2000. How is such a decision a problem when you want to infer future expected portfolio performance from the results?\nThe efficient frontier characterizes the portfolios with the highest expected return for different levels of risk. Identify the portfolio with the highest expected return per standard deviation. Which famous performance measure is close to the ratio of average returns to the standard deviation of returns?\n\n\n\n\nFigure 1: The figure shows Apple stock prices between the beginning of 2000 and the end of 2022. Prices are in USD, adjusted for dividend payments and stock splits.\nFigure 2: The figure shows a histogram of daily Apple stock returns in percent. The dotted vertical line indicates the historical five percent quantile.\nFigure 3: The figure shows the stock prices of DOW index constituents. Prices are in USD, adjusted for dividend payments and stock splits.\nFigure 4: The figure shows the total daily trading volume in billion USD.\nFigure 5: The figure a scatterplot of aggregate trading volume against previous-day aggregate trading volume.\nFigure 6: The figure shows the efficient frontier for DOW index constituents. The big dots indicate the location of the minimum variance and the efficient portfolio that delivers three times the expected return of the minimum variance portfolio, respectively. The small dots indicate the location of the individual constituents.",
    "crumbs": [
      "R",
      "Getting Started",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "python/other-data-providers.html",
    "href": "python/other-data-providers.html",
    "title": "Other Data Providers",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn the previous chapters, we introduced many ways to get financial data that researchers regularly use. We showed how to load data into Python from Yahoo!Finance (using the yfinance package) and from Kenneth French’s data library (using the pandas-datareader package). We also presented commonly used file types, such as comma-separated or Excel files. Then, we introduced remotely connecting to WRDS and downloading data from there. However, this is only a subset of the vast amounts of data available online these days.\nIn this chapter, we provide an overview of common alternative data providers for which direct access via Python packages exists. Such a list requires constant adjustments because both data providers and access methods change. However, we want to emphasize two main insights. First, the number of Python packages that provide access to (financial) data is large. Too large actually to survey here exhaustively. Instead, we can only cover the tip of the iceberg. Second, Python provides the functionalities to access any form of files or data available online. Thus, even if a desired data source does not come with a well-established Python package, chances are high that data can be retrieved by establishing your own API connection (using the Python requests package) or by scrapping the content.\nIn our non-exhaustive list below, we restrict ourselves to listing data sources accessed through easy-to-use Python packages. For further inspiration on potential data sources, we recommend reading the Awesome Quant curated list of insanely awesome libraries, packages, and resources for Quants (Quantitative Finance). In fact, the pandas-datareader package provides comprehensive access to a lot of databases, including some of those listed below.\nAlso, the requests library in Python provides a versatile and direct way to interact with APIs (Application Programming Interfaces) offered by various financial data providers. The package simplifies the process of making HTTP requests, handling authentication, and parsing the received data.\nApart from the list below, we want to advertise some amazing data compiled by others. First, there is Open Source Asset Pricing related to Chen and Zimmermann (2022). They provide return data for over 200 trading strategies with different time periods and specifications. The authors also provide signals and explanations of the factor construction. Moreover, in the same spirit, Global factor data provides the data related to Jensen2022b. They provide return data for characteristic-managed portfolios from around the world. The database includes factors for 153 characteristics in 13 themes, using data from 93 countries. Finally, we want to mention the TAQ data providing trades and quotes data for NYSE, Nasdaq, and regional exchanges, which is available via WRDS.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "python/other-data-providers.html#exercises",
    "href": "python/other-data-providers.html#exercises",
    "title": "Other Data Providers",
    "section": "Exercises",
    "text": "Exercises\n\nSelect one of the data sources in the table above and retrieve some data. Browse the homepage of the data provider or the package documentation to find inspiration on which type of data is available to you and how to download the data into your Python session.\nGenerate summary statistics of the data you retrieved and provide some useful visualization. The possibilities are endless: Maybe there is some interesting economic event you want to analyze, such as stock market responses to Twitter activity.",
    "crumbs": [
      "R",
      "Financial Data",
      "Other Data Providers"
    ]
  },
  {
    "objectID": "python/proofs.html",
    "href": "python/proofs.html",
    "title": "Proofs",
    "section": "",
    "text": "In this appendix chapter, we collect the proofs that we refer to throughout the book.",
    "crumbs": [
      "R",
      "Appendix",
      "Proofs"
    ]
  },
  {
    "objectID": "python/proofs.html#optimal-portfolio-choice",
    "href": "python/proofs.html#optimal-portfolio-choice",
    "title": "Proofs",
    "section": "Optimal Portfolio Choice",
    "text": "Optimal Portfolio Choice\n\nMinimum variance portfolio\nThe minimum variance portfolio weights are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega= 1,\\] where \\(\\iota\\) is an \\((N \\times 1)\\) vector of ones. The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1).\\] We can solve the first-order conditions of the Lagrangian equation: \\[\n\\begin{aligned}\n& \\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega} = 0 \\Leftrightarrow 2\\Sigma \\omega = \\lambda\\iota \\Rightarrow \\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota \\\\ \\end{aligned}\n\\] Next, the constraint that weights have to sum up to one delivers: \\(1 = \\iota'\\omega = \\frac{\\lambda}{2}\\iota'\\Sigma^{-1}\\iota \\Rightarrow \\lambda = \\frac{2}{\\iota'\\Sigma^{-1}\\iota}.\\) Finally, plug-in the derived value of \\(\\lambda\\) to get \\[\n\\begin{aligned}\n\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}.\n\\end{aligned}\n\\]\n\n\nEfficient portfolio\nConsider an investor who aims to achieve minimum variance given a desired expected return \\(\\bar{\\mu}\\), that is: \\[\\omega_\\text{eff}\\left(\\bar{\\mu}\\right) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1) - \\tilde{\\lambda}(\\omega'\\mu - \\bar{\\mu}). \\] We can solve the first-order conditions to get \\[\n\\begin{aligned}\n2\\Sigma \\omega &= \\lambda\\iota + \\tilde\\lambda \\mu\\\\\n\\Rightarrow\\omega &= \\frac{\\lambda}{2}\\Sigma^{-1}\\iota + \\frac{\\tilde\\lambda}{2}\\Sigma^{-1}\\mu.\n\\end{aligned}\n\\]\nNext, the two constraints (\\(w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}\\)) imply \\[\n\\begin{aligned}\n1 &= \\iota'\\omega = \\frac{\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\iota}_{C} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\mu}_D\\\\\n\\Rightarrow \\lambda&= \\frac{2 - \\tilde\\lambda D}{C}\\\\\n\\bar\\mu &= \\mu'\\omega = \\frac{\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\iota}_{D} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\mu}_E = \\frac{1}{2}\\left(\\frac{2 - \\tilde\\lambda D}{C}\\right)D+\\frac{\\tilde\\lambda}{2}E  \\\\&=\\frac{D}{C}+\\frac{\\tilde\\lambda}{2}\\left(E - \\frac{D^2}{C}\\right)\\\\\n\\Rightarrow \\tilde\\lambda &= 2\\frac{\\bar\\mu - D/C}{E-D^2/C}.\n\\end{aligned}\n\\] As a result, the efficient portfolio weight takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right).\\] Thus, the efficient portfolio allocates wealth in the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and a levered (self-financing) portfolio to increase the expected return.\nNote that the portfolio weights sum up to one as \\[\\iota'\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) = D - D = 0\\text{ so }\\iota'\\omega_\\text{eff} = \\iota'\\omega_\\text{mvp} = 1.\\] Finally, the expected return of the efficient portfolio is \\[\\mu'\\omega_\\text{eff} = \\frac{D}{C} + \\bar\\mu - \\frac{D}{C} = \\bar\\mu.\\]\n\n\nEquivalence between certainty equivalent maximization and minimum variance optimization\nWe argue that an investor with a quadratic utility function with certainty equivalent \\[\\max_\\omega CE(\\omega) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen with the aim to minimize volatility given a pre-specified level or expected returns \\[\\min_\\omega \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1.\\] Note the difference: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines their optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return they want to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk-averse investors (higher \\(\\gamma\\)) will chose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nTo proof this equivalence, consider first the optimal portfolio weights for a certainty equivalent maximizing investor. The first-order condition reads \\[\n\\begin{aligned}\n\\mu - \\lambda \\iota &= \\gamma \\Sigma \\omega \\\\\n\\Leftrightarrow \\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\lambda\\iota\\right)\n\\end{aligned}\n\\] Next, we make use of the constraint \\(\\iota'\\omega = 1\\). \\[\n\\begin{aligned}\n\\iota'\\omega &= 1 = \\frac{1}{\\gamma}\\left(\\iota'\\Sigma^{-1}\\mu - \\lambda\\iota'\\Sigma^{-1}\\iota\\right)\\\\\n\\Rightarrow \\lambda &= \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right).\n\\end{aligned}\n\\] Plugging in the value of \\(\\lambda\\) reveals the desired portfolio for an investor with risk aversion \\(\\gamma\\). \\[\n\\begin{aligned}\n\\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right)\\right) \\\\\n\\Rightarrow \\omega &= \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\iota'\\Sigma^{-1}\\right)\\mu\\\\\n&= \\omega_\\text{mvp} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1}\\mu - \\frac{\\iota'\\Sigma^{-1}\\mu}{\\iota'\\Sigma^{-1}\\iota}\\Sigma^{-1}\\iota\\right).\n\\end{aligned}\n\\] The resulting weights correspond to the efficient portfolio with desired return \\(\\bar r\\) such that (in the notation of book) \\[\\frac{1}{\\gamma} = \\frac{\\tilde\\lambda}{2} = \\frac{\\bar\\mu - D/C}{E - D^2/C}\\] which implies that the desired return is just \\[\\bar\\mu = \\frac{D}{C} + \\frac{1}{\\gamma}\\left({E - D^2/C}\\right)\\] which is \\(\\frac{D}{C} = \\mu'\\omega_\\text{mvp}\\) for \\(\\gamma\\rightarrow \\infty\\) as expected. For instance, letting \\(\\gamma \\rightarrow \\infty\\) implies \\(\\bar\\mu = \\frac{D}{C} = \\omega_\\text{mvp}'\\mu\\).",
    "crumbs": [
      "R",
      "Appendix",
      "Proofs"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html",
    "href": "python/setting-up-your-environment.html",
    "title": "Setting Up Your Environment",
    "section": "",
    "text": "We aim to lower the bar for starting empirical research in financial economics. We want to make using Python easy for you. However, given that Tidy Finance is a platform that supports multiple programming languages, we also consider the possibility that you are unfamiliar with Python. Maybe you are transitioning from R to Python, i.e., following the journey of Tidy Finance, which started in R. Hence, we provide you with a simple guide to get started with Python. If you have not used Python before, you will be able to use it after reading this chapter.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#python-environment",
    "href": "python/setting-up-your-environment.html#python-environment",
    "title": "Setting Up Your Environment",
    "section": "Python Environment",
    "text": "Python Environment\nA Python environment is a self-contained directory or folder containing a specific version of the Python installation with a set of packages and dependencies. In order to isolate and manage the specific dependencies of the Tidy Finance with Python project, a so-called virtual environment is a reliable way to ensure that it will work consistently and reliably on different systems and over time.\nThere are many ways to install a Python version and environments on your system. We present two ways that we found most convenient to write this book and maintain our website: (i) Installation via Anaconda along with using Python in Spyder and (ii) installation via RStudio.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#installation-via-anaconda",
    "href": "python/setting-up-your-environment.html#installation-via-anaconda",
    "title": "Setting Up Your Environment",
    "section": "Installation via Anaconda",
    "text": "Installation via Anaconda\nFirst, we need to install Python itself via Anaconda. You can download the latest version of Anaconda from the official Anaconda website. After downloading the respective version for your operating system, follow the installation instructions.\nSecond, we now describe how to set up a Python virtual environment specific to Tidy Finance on your local system. This book uses Python version 3.10.11 to set up the environment for both Windows and Mac. As we write this book, it is not the latest version of Python. The reason for this is that we wanted (i) a stable code base and (ii) the content of the book to be usable for all kinds of users, especially for those who might rely on corporate version controls and are not able to install new Python distributions.\nFor the installation, we use the Anaconda Python distribution you downloaded in the step before.1 Additionally, you need the packages listed in the provided requirements.txt-file in a dedicated folder for the project. You can find the detailed list of packages in the Colophon.\nWe recommend you start with the package installation right away. After you have prepared your system, you can open the Anaconda prompt and install your virtual environment with the following commands:\n\nconda create -p C:\\Apps\\Anaconda3\\envs\\tidy_finance_environment python==3.10.11 (Confirm with y)\nconda activate C:\\Apps\\Anaconda3\\envs\\tidy_finance_environment\npip install -r \"&lt;Tidy-Finance-with-Python Folder&gt;\\requirements.txt\"\n\nAll other packages found with the command pip list are installed automatically as dependencies with the required packages in the file requirements.txt. Note that we make reference to two distinct folders. The first one, C:\\Apps\\Anaconda3\\envs\\tidy_finance_environment refers to the location of your Python environment used for Tidy Finance. Apart from that, you should store your data, program codes, and scripts in another location: Your Tidy Finance working folder.\nNow, you are basically ready to go. However, you will now need a Python integrated development environment (IDE) to make your coding experience pleasant.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#python-ide",
    "href": "python/setting-up-your-environment.html#python-ide",
    "title": "Setting Up Your Environment",
    "section": "Python IDE",
    "text": "Python IDE\nIf you are new to coding, you will not have an IDE for Python. We recommend using Spyder if you plan to code only in Python as it comes with Anaconda. If you don’t use Anaconda, you can download the software for your operating system from the official website. Then, follow the installation instructions. To add the previously created virtual environment to Spyder, Go to Tools → Preferences → Python Interpreter → “Use the following interpreter” and add C:\\Apps\\Anaconda3\\envs\\tidy_finance_environment\\python.exe.\nAnother increasingly popular code editor for data analysis is Visual Studio Code (VS Code), as it supports a variety of programming languages, including Python and R. We refer to this tutorial if you want to get started with VS Code. There are many more ways to set up a Python IDE, so we refer to this page in the Python wiki for more inspiration.\nIf you also plan to try R, you should get a multipurpose tool: RStudio. You can get your RStudio version from Posit (i.e., the company that created RStudio, which was previously called RStudio itself). When you follow the instructions, you will see that Posit asks you to install R; you need to do so to make RStudio feasible for Python. Then, select the virtual environment in RStudio. Alternatively, you can also start with the installation guide starting from RStudio, which we present below.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#installation-via-rstudio",
    "href": "python/setting-up-your-environment.html#installation-via-rstudio",
    "title": "Setting Up Your Environment",
    "section": "Installation via RStudio",
    "text": "Installation via RStudio\nYou can also install Python and set up your environment directly from RStudio. This approach has the big advantage that you can switch between R and Python code smoothly. We believe that being able to switch between different programming languages is a tremendously valuable skill, so we set up a repository containing all the files that you need to achieve this goal: Tidy Finance Environment. To set up this environment locally, follow these steps:\n\nInstall R and RStudio.\nClone the Tidy Finance Environment repository directly in RStudio by clicking on File/New Project/ and selecting Version Control. Then, click Git and provide the repository address https://github.com/tidy-finance/environment. RStudio will then automatically open the project in the new environment.\nInstall the reticulate R package: install.packages(\"reticulate\").\nUse reticulate to install Python: reticulate::install_python(version = \"3.10.11\", force = TRUE).\nTell renv to use Python: renv::use_python(\"PATH\").\n\n\"PATH\" on Mac: \"~/.pyenv/versions/3.10.11/bin/python\".\n\"PATH\" on Windows: \"C:/Users/&lt;User&gt;/AppData/Local/r-reticulate/ r-reticulate/pyenv/pyenv-win/versions/3.10.11/python.exe\" where &lt;User&gt; is your username.\n\nTell renv to install all required packages: renv::restore().\n\nNow you are ready to execute all code that you can find in this book or its sibling Tidy Finance with R.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#creating-environment-variables",
    "href": "python/setting-up-your-environment.html#creating-environment-variables",
    "title": "Setting Up Your Environment",
    "section": "Creating Environment Variables",
    "text": "Creating Environment Variables\nIf you plan to share your own code with collaborators or the public, you may encounter the situation that your projects require sensitive information, such as login credentials, that you don’t want to publish. Environment variables are widely used in software development projects because they provide a flexible and secure way to configure applications and store secrets. In later chapters, we use such environment variables to store private login data for a remote database.\nYou can use .env-files to store environment variables. Upon startup, Python projects often use libraries like python-dotenv to load these environment variables from a .env-file. .env-files can be placed at the project level and are not meant to be committed to version control, ensuring that sensitive information remains private.\nFirst, you need to install the python-dotenv library if you haven’t already:\n\npip install python-dotenv\n\nThen, create a .env-file in your project directory. You can add variables to this file. For the purpose of this book, we create and save the following variables (where user and password are our private login credentials):\nWRDS_USER=user\nWRDS_PASSWORD=password\nTo access these environment variables in your Python code, load the environment variables at the start of your Python script using python-dotenv:\n\nfrom dotenv import load_dotenv\nimport os\nload_dotenv()\n\nwrds_user = os.getenv(\"WRDS_USER\")\nwrds_password = os.getenv(\"WRDS_PASSWORD\")\n\nNote that you can also store other login credentials, API keys, or file paths in the same environment file.\nIf you use version control, make sure that the .env-file is included in your .gitignore to avoid committing sensitive information to your repository.",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/setting-up-your-environment.html#footnotes",
    "href": "python/setting-up-your-environment.html#footnotes",
    "title": "Setting Up Your Environment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that you can also install a newer version of Python. We only require the environment set up in the previous step to use Python version 3.10.11. The neat aspect is Python’s capability to accommodate version control in this respect.↩︎",
    "crumbs": [
      "R",
      "Getting Started",
      "Setting Up Your Environment"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html",
    "href": "python/trace-and-fisd.html",
    "title": "TRACE and FISD",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we dive into the US corporate bond market. Bond markets are far more diverse than stock markets, as most issuers have multiple bonds outstanding simultaneously with potentially very different indentures. This market segment is exciting due to its size (roughly ten trillion USD outstanding), heterogeneity of issuers (as opposed to government bonds), market structure (mostly over-the-counter trades), and data availability. We introduce how to use bond characteristics from FISD and trade reports from TRACE and provide code to download and clean TRACE in Python.\nMany researchers study liquidity in the US corporate bond market, with notable contributions from Bessembinder, Maxwell, and Venkataraman (2006), Edwards, Harris, and Piwowar (2007), and O’Hara and Zhou (2021), among many others. We do not cover bond returns here, but you can compute them from TRACE data. Instead, we refer to studies on the topic such as Bessembinder et al. (2008), Bai, Bali, and Wen (2019), and Kelly, Palhares, and Pruitt (2021) and a survey by Huang and Shi (2021).\nThis chapter also draws on the resources provided by the project Open Source Bond Asset Pricing and their related publication, i.e., Dickerson, Mueller, and Robotti (2023). We encourage you to visit their website to check out the additional resources they provide. Moreover, WRDS provides bond returns computed from TRACE data at a monthly frequency.\nThe current chapter relies on the following set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport httpimport\n\nfrom plotnine import *\nfrom sqlalchemy import create_engine\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format, comma_format\nCompared to previous chapters, we load httpimport (Torakis 2023) to source code provided in the public Gist. Note that you should be careful with loading anything from the web via this method, and it is highly discouraged to use any unsecured “HTTP” links. Also, you might encounter a problem when using this from a corporate computer that prevents downloading data through a firewall.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#bond-data-from-wrds",
    "href": "python/trace-and-fisd.html#bond-data-from-wrds",
    "title": "TRACE and FISD",
    "section": "Bond Data from WRDS",
    "text": "Bond Data from WRDS\nBoth bond databases we need are available on WRDS to which we establish the PostgreSQL connection described in WRDS, CRSP, and Compustat. Additionally, we connect to our local SQLite-database to store the data we download.\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nconnection_string = (\n  \"postgresql+psycopg2://\"\n  f\"{os.getenv('WRDS_USER')}:{os.getenv('WRDS_PASSWORD')}\"\n  \"@wrds-pgdata.wharton.upenn.edu:9737/wrds\"\n)\n\nwrds = create_engine(connection_string, pool_pre_ping=True)\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#mergent-fisd",
    "href": "python/trace-and-fisd.html#mergent-fisd",
    "title": "TRACE and FISD",
    "section": "Mergent FISD",
    "text": "Mergent FISD\nFor research on US corporate bonds, the Mergent Fixed Income Securities Database (FISD) is the primary resource for bond characteristics. There is a detailed manual on WRDS, so we only cover the necessary subjects here. FISD data comes in two main variants, namely, centered on issuers or issues. In either case, the most useful identifiers are CUSIPs. 9-digit CUSIPs identify securities issued by issuers. The issuers can be identified from the first six digits of a security CUSIP, which is also called a 6-digit CUSIP. Both stocks and bonds have CUSIPs. This connection would, in principle, allow matching them easily, but due to changing issuer details, this approach only yields small coverage.\nWe use the issue-centered version of FISD to identify the subset of US corporate bonds that meet the standard criteria (Bessembinder, Maxwell, and Venkataraman 2006). The WRDS table fisd_mergedissue contains most of the information we need on a 9-digit CUSIP level. Due to the diversity of corporate bonds, details in the indenture vary significantly. We focus on common bonds that make up the majority of trading volume in this market without diverging too much in indentures.\nThe following chunk connects to the data and selects the bond sample to remove certain bond types that are less commonly used (see, e.g., Dick-Nielsen, Feldhütter, and Lando 2012; O’Hara and Zhou 2021, among many others). In particular, we use the filters listed below. Note that we also treat missing values in these flags.\n\nKeep only senior bonds (security_level = 'SEN').\nExclude bonds which are secured lease obligations (slob = 'N' OR slob IS NULL).\nExclude secured bonds (security_pledge IS NULL).\nExclude asset-backed bonds (asset_backed = 'N' OR asset_backed IS NULL).\nExclude defeased bonds ((defeased = 'N' OR defeased IS NULL) AND defeased_date IS NULL).\nKeep only the bond types US Corporate Debentures ('CDEB'), US Corporate Medium Term Notes ('CMTN'), US Corporate Zero Coupon Notes and Bonds ('CMTZ', 'CZ'), and US Corporate Bank Note ('USBN').\nExclude bonds that are payable in kind ((pay_in_kind != 'Y' OR pay_in_kind IS NULL) AND pay_in_kind_exp_date IS NULL).\nExclude foreign (yankee == \"N\" OR is.na(yankee)) and Canadian issuers (canadian = 'N' OR canadian IS NULL).\nExclude bonds denominated in foreign currency (foreign_currency = 'N').\nKeep only fixed (F) and zero (Z) coupon bonds with additional requirements of fix_frequency IS NULL, coupon_change_indicator = 'N' and annual, semi-annual, quarterly, or monthly interest frequencies.\nExclude bonds that were issued under SEC Rule 144A (rule_144a = 'N').\nExlcude privately placed bonds (private_placement = 'N' OR private_placement IS NULL).\nExclude defaulted bonds (defaulted = 'N' AND filing_date IS NULL AND settlement IS NULL).\nExclude convertible (convertible = 'N'), putable (putable = 'N' OR putable IS NULL), exchangeable (exchangeable = 'N' OR exchangeable IS NULL), perpetual (perpetual = 'N'), or preferred bonds (preferred_security = 'N' OR preferred_security IS NULL).\nExclude unit deal bonds ((unit_deal = 'N' OR unit_deal IS NULL)).\n\n\nfisd_query = (\n  \"SELECT complete_cusip, maturity, offering_amt, offering_date, \"\n         \"dated_date, interest_frequency, coupon, last_interest_date, \"\n         \"issue_id, issuer_id \"\n    \"FROM fisd.fisd_mergedissue \"\n    \"WHERE security_level = 'SEN' \"\n          \"AND (slob = 'N' OR slob IS NULL) \"\n          \"AND security_pledge IS NULL \"              \n          \"AND (asset_backed = 'N' OR asset_backed IS NULL) \"\n          \"AND (defeased = 'N' OR defeased IS NULL) \"\n          \"AND defeased_date IS NULL \"\n          \"AND bond_type IN ('CDEB', 'CMTN', 'CMTZ', 'CZ', 'USBN') \"\n          \"AND (pay_in_kind != 'Y' OR pay_in_kind IS NULL) \"\n          \"AND pay_in_kind_exp_date IS NULL \"\n          \"AND (yankee = 'N' OR yankee IS NULL) \"\n          \"AND (canadian = 'N' OR canadian IS NULL) \"\n          \"AND foreign_currency = 'N' \"\n          \"AND coupon_type IN ('F', 'Z') \"\n          \"AND fix_frequency IS NULL \"\n          \"AND coupon_change_indicator = 'N' \"\n          \"AND interest_frequency IN ('0', '1', '2', '4', '12') \"\n          \"AND rule_144a = 'N' \"\n          \"AND (private_placement = 'N' OR private_placement IS NULL) \"\n          \"AND defaulted = 'N' \"\n          \"AND filing_date IS NULL \"\n          \"AND settlement IS NULL \"\n          \"AND convertible = 'N' \"\n          \"AND exchange IS NULL \"\n          \"AND (putable = 'N' OR putable IS NULL) \"\n          \"AND (unit_deal = 'N' OR unit_deal IS NULL) \"\n          \"AND (exchangeable = 'N' OR exchangeable IS NULL) \"\n          \"AND perpetual = 'N' \"\n          \"AND (preferred_security = 'N' OR preferred_security IS NULL)\"\n)\n\nfisd = pd.read_sql_query(\n  sql=fisd_query,\n  con=wrds,\n  dtype={\"complete_cusip\": str, \"interest_frequency\": str, \n         \"issue_id\": int, \"issuer_id\": int},\n  parse_dates={\"maturity\", \"offering_date\", \n               \"dated_date\", \"last_interest_date\"}\n)\n\nWe also pull issuer information from fisd_mergedissuer regarding the industry and country of the firm that issued a particular bond. Then, we filter to include only US-domiciled firms’ bonds. We match the data by issuer_id.\n\nfisd_issuer_query = (\n  \"SELECT issuer_id, sic_code, country_domicile \"\n    \"FROM fisd.fisd_mergedissuer\"\n)\n\nfisd_issuer = pd.read_sql_query(\n  sql=fisd_issuer_query,\n  con=wrds,\n  dtype={\"issuer_id\": int, \"sic_code\": str, \"country_domicile\": str}\n)\n\nfisd = (fisd\n  .merge(fisd_issuer, how=\"inner\", on=\"issuer_id\")\n  .query(\"country_domicile == 'USA'\")\n  .drop(columns=\"country_domicile\")\n)\n\nFinally, we save the bond characteristics to our local database. This selection of bonds also constitutes the sample for which we will collect trade reports from TRACE below.\n\n(fisd\n  .to_sql(name=\"fisd\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\nThe FISD database also contains other data. The issue-based file contains information on covenants, i.e., restrictions included in bond indentures to limit specific actions by firms (e.g., Handler, Jankowitsch, and Weiss 2021). The FISD redemption database also contains information on callable bonds. Moreover, FISD also provides information on bond ratings. We do not need either here.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#trace",
    "href": "python/trace-and-fisd.html#trace",
    "title": "TRACE and FISD",
    "section": "TRACE",
    "text": "TRACE\nThe Financial Industry Regulatory Authority (FINRA) provides the Trade Reporting and Compliance Engine (TRACE). In TRACE, dealers that trade corporate bonds must report such trades individually. Hence, we observe trade messages in TRACE that contain information on the bond traded, the trade time, price, and volume. TRACE comes in two variants: standard and enhanced TRACE. We show how to download and clean enhanced TRACE as it contains uncapped volume, a crucial quantity missing in the standard distribution. Moreover, enhanced TRACE also provides information on the respective parties’ roles and the direction of the trade report. These items become essential in cleaning the messages.\nWhy do we repeatedly talk about cleaning TRACE? Trade messages are submitted within a short time window after a trade is executed (less than 15 minutes). These messages can contain errors, and the reporters subsequently correct them or they cancel a trade altogether. The cleaning needs are described by Dick-Nielsen (2009) in detail, and Dick-Nielsen (2014) shows how to clean the enhanced TRACE data using SAS. We do not go into the cleaning steps here, since the code is lengthy and is not our focus here. However, downloading and cleaning enhanced TRACE data is straightforward with our setup.\nWe store code for cleaning enhanced TRACE with Python on the following GitHub Gist. Clean Enhanced TRACE with Python also contains the code for reference. We only need to source the code from the Gist, which we can do with the code below using httpimport. In the chunk, we explicitly load the necessary function interpreting the Gist as a module (i.e., you could also use it as a module and precede the function calls with the module’s name). Alternatively, you can also go to the Gist, download it, and manually execute it. The clean_enhanced_trace() function takes a vector of CUSIPs, a connection to WRDS explained in WRDS, CRSP, and Compustat, and a start and end date, respectively.\n\ngist_url = (\n  \"https://gist.githubusercontent.com/patrick-weiss/\"\n  \"86ddef6de978fbdfb22609a7840b5d8b/raw/\"\n  \"8fbcc6c6f40f537cd3cd37368be4487d73569c6b/\"\n)\n\nwith httpimport.remote_repo(gist_url):\n  from clean_enhanced_TRACE_python import clean_enhanced_trace\n\nThe TRACE database is considerably large. Therefore, we only download subsets of data at once. Specifying too many CUSIPs over a long time horizon will result in very long download times and a potential failure due to the size of the request to WRDS. The size limit depends on many parameters, and we cannot give you a guideline here. For the applications in this book, we need data around the Paris Agreement in December 2015 and download the data in sets of 1000 bonds, which we define below.\n\ncusips = list(fisd[\"complete_cusip\"].unique())\nbatch_size = 1000\nbatches = np.ceil(len(cusips)/batch_size).astype(int)\n\nFinally, we run a loop in the same style as in WRDS, CRSP, and Compustat where we download daily returns from CRSP. For each of the CUSIP sets defined above, we call the cleaning function and save the resulting output. We add new data to the existing table for batch two and all following batches.\n\nfor j in range(1, batches + 1):\n  \n  cusip_batch = cusips[\n    ((j-1)*batch_size):(min(j*batch_size, len(cusips)))\n  ]\n  \n  cusip_batch_formatted = \", \".join(f\"'{cusip}'\" for cusip in cusip_batch)\n  cusip_string = f\"({cusip_batch_formatted})\"\n\n  trace_enhanced_sub = clean_enhanced_trace(\n    cusips=cusip_string,\n    connection=wrds, \n    start_date=\"'01/01/2014'\", \n    end_date=\"'11/30/2016'\"\n  )\n  \n  if not trace_enhanced_sub.empty:\n      if j == 1:\n        if_exists_string = \"replace\"\n      else:\n        if_exists_string = \"append\"\n\n      trace_enhanced_sub.to_sql(\n        name=\"trace_enhanced\", \n        con=tidy_finance, \n        if_exists=if_exists_string, \n        index=False\n      )\n    \n  print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#insights-into-corporate-bonds",
    "href": "python/trace-and-fisd.html#insights-into-corporate-bonds",
    "title": "TRACE and FISD",
    "section": "Insights into Corporate Bonds",
    "text": "Insights into Corporate Bonds\nWhile many news outlets readily provide information on stocks and the underlying firms, corporate bonds are not covered frequently. Additionally, the TRACE database contains trade-level information, potentially new to students. Therefore, we provide you with some insights by showing some summary statistics.\nWe start by looking into the number of bonds outstanding over time and compare it to the number of bonds traded in our sample. First, we compute the number of bonds outstanding for each quarter around the Paris Agreement from 2014 to 2016.\n\ndates = pd.date_range(start=\"2014-01-01\", end=\"2016-11-30\", freq=\"Q\")\n\nbonds_outstanding = (pd.DataFrame({\"date\": dates})\n  .merge(fisd[[\"complete_cusip\"]], how=\"cross\")\n  .merge(fisd[[\"complete_cusip\", \"offering_date\", \"maturity\"]],\n         on=\"complete_cusip\", how=\"left\")\n  .assign(offering_date=lambda x: x[\"offering_date\"].dt.floor(\"D\"),\n          maturity=lambda x: x[\"maturity\"].dt.floor(\"D\"))\n  .query(\"date &gt;= offering_date & date &lt;= maturity\")\n  .groupby(\"date\")\n  .size()\n  .reset_index(name=\"count\")\n  .assign(type=\"Outstanding\")\n)\n\nNext, we look at the bonds traded each quarter in the same period. Notice that we load the complete trace table from our database, as we only have a single part of it in the environment from the download loop above.\n\ntrace_enhanced = pd.read_sql_query(\n  sql=(\"SELECT cusip_id, trd_exctn_dt, rptd_pr, entrd_vol_qt, yld_pt \" \n        \"FROM trace_enhanced\"),\n  con=tidy_finance,\n  parse_dates={\"trd_exctn_dt\"}\n)\n\nbonds_traded = (trace_enhanced\n  .assign(\n    date=lambda x: (\n      (x[\"trd_exctn_dt\"]-pd.offsets.MonthBegin(1))\n        .dt.to_period(\"Q\").dt.start_time\n    )\n  )\n  .groupby(\"date\")\n  .aggregate(count=(\"cusip_id\", \"nunique\"))\n  .reset_index()\n  .assign(type=\"Traded\")\n)\n\nFinally, we plot the two time series in Figure 1.\n\nbonds_combined = pd.concat(\n  [bonds_outstanding, bonds_traded], ignore_index=True\n)\n\nbonds_figure = (\n  ggplot(bonds_combined, \n         aes(x=\"date\", y=\"count\", color=\"type\", linetype=\"type\")) +\n  geom_line() +\n  labs(x=\"\", y=\"\", color=\"\", linetype=\"\",\n       title=\"Number of bonds outstanding and traded each quarter\") +\n  scale_x_datetime(breaks=date_breaks(\"1 year\"), labels=date_format(\"%Y\")) +\n  scale_y_continuous(labels=comma_format())\n)\nbonds_figure.draw()\n\n\n\n\n\n\n\nFigure 1: Number of corporate bonds outstanding each quarter as reported by Mergent FISD and the number of traded bonds from enhanced TRACE between 2014 and end of 2016.\n\n\n\n\n\nWe see that the number of bonds outstanding increases steadily between 2014 and 2016. During our sample period of trade data, we see that the fraction of bonds trading each quarter is roughly 60 percent. The relatively small number of traded bonds means that many bonds do not trade through an entire quarter. This lack of trading activity illustrates the generally low level of liquidity in the corporate bond market, where it can be hard to trade specific bonds. Does this lack of liquidity mean that corporate bond markets are irrelevant in terms of their size? With over 7,500 traded bonds each quarter, it is hard to say that the market is small. However, let us also investigate the characteristics of issued corporate bonds. In particular, we consider maturity (in years), coupon, and offering amount (in million USD).\n\naverage_characteristics = (fisd\n  .assign(\n    maturity=lambda x: (x[\"maturity\"] - x[\"offering_date\"]).dt.days/365,\n    offering_amt=lambda x: x[\"offering_amt\"]/10**3\n  )\n  .melt(var_name=\"measure\",\n        value_vars=[\"maturity\", \"coupon\", \"offering_amt\"], \n        value_name=\"value\")\n  .dropna()\n  .groupby(\"measure\")[\"value\"]\n  .describe(percentiles=[.05, .50, .95])\n  .drop(columns=\"count\")\n)\naverage_characteristics.round(2)\n\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nmeasure\n\n\n\n\n\n\n\n\n\n\n\ncoupon\n2.78\n3.59\n0.00\n0.00\n0.00\n9.00\n39.00\n\n\nmaturity\n5.96\n7.20\n-6.24\n1.04\n4.02\n24.97\n180.13\n\n\noffering_amt\n145.92\n386.22\n0.00\n0.24\n3.74\n763.60\n15000.00\n\n\n\n\n\n\n\n\nWe see that the average bond in our sample period has an offering amount of over 357 million USD with a median of 200 million USD, which both cannot be considered small. The average bond has a maturity of ten years and pays around 6 percent in coupons.\nFinally, let us compute some summary statistics for the trades in this market. To this end, we show a summary based on aggregate information daily. In particular, we consider the trade size (in million USD) and the number of trades.\n\naverage_trade_size = (trace_enhanced\n  .groupby(\"trd_exctn_dt\")\n  .aggregate(\n    trade_size=(\"entrd_vol_qt\", lambda x: (\n      sum(x*trace_enhanced.loc[x.index, \"rptd_pr\"]/100)/10**6)\n    ),\n    trade_number=(\"trd_exctn_dt\", \"size\")\n  )\n  .reset_index()\n  .melt(id_vars=[\"trd_exctn_dt\"], var_name=\"measure\",\n        value_vars=[\"trade_size\", \"trade_number\"], value_name=\"value\")\n  .groupby(\"measure\")[\"value\"]\n  .describe(percentiles=[.05, .50, .95])\n  .drop(columns=\"count\")\n)\naverage_trade_size.round(0)\n\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nmeasure\n\n\n\n\n\n\n\n\n\n\n\ntrade_number\n25914.0\n5444.0\n439.0\n17844.0\n26051.0\n34383.0\n40839.0\n\n\ntrade_size\n12968.0\n3577.0\n17.0\n6128.0\n13421.0\n17850.0\n21312.0\n\n\n\n\n\n\n\n\nOn average, nearly 26 billion USD of corporate bonds are traded daily in nearly 13,000 transactions. We can, hence, conclude that the corporate bond market is indeed significant in terms of trading volume and activity.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/trace-and-fisd.html#exercises",
    "href": "python/trace-and-fisd.html#exercises",
    "title": "TRACE and FISD",
    "section": "Exercises",
    "text": "Exercises\n\nCompute the amount outstanding across all bonds over time. Make sure to subtract all matured bonds. How would you describe the resulting plot?\nCompute the number of days each bond is traded (accounting for the bonds’ maturities and issuances). Start by looking at the number of bonds traded each day in a graph similar to the one above. How many bonds trade on more than 75 percent of trading days?\nWRDS provides more information from Mergent FISD such as ratings in the table fisd_ratings. Download the ratings table and plot the distribution of ratings for the different rating providers. How would you map the different providers to a common numeric rating scale? \n\n\n\n\nFigure 1: Number of corporate bonds outstanding each quarter as reported by Mergent FISD and the number of traded bonds from enhanced TRACE between 2014 and end of 2016.",
    "crumbs": [
      "R",
      "Financial Data",
      "TRACE and FISD"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html",
    "href": "python/value-and-bivariate-sorts.html",
    "title": "Value and Bivariate Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with Python. You can find the equivalent chapter for the sibling Tidy Finance with R here.\nIn this chapter, we extend the univariate portfolio analysis of Univariate Portfolio Sorts to bivariate sorts, which means we assign stocks to portfolios based on two characteristics. Bivariate sorts are regularly used in the academic asset pricing literature and are the basis for factors in the Fama-French three-factor model. However, some scholars also use sorts with three grouping variables. Conceptually, portfolio sorts are easily applicable in higher dimensions.\nWe form portfolios on firm size and the book-to-market ratio. To calculate book-to-market ratios, accounting data is required, which necessitates additional steps during portfolio formation. In the end, we demonstrate how to form portfolios on two sorting variables using so-called independent and dependent portfolio sorts.\nThe current chapter relies on this set of Python packages.\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport sqlite3\nCompared to previous chapters, we introduce the datetime module that is part of the Python standard library for manipulating dates.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#data-preparation",
    "href": "python/value-and-bivariate-sorts.html#data-preparation",
    "title": "Value and Bivariate Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we load the necessary data from our SQLite database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. We conduct portfolio sorts based on the CRSP sample but keep only the necessary columns in our memory. We use the same data sources for firm size as in Size Sorts and P-Hacking.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=(\"SELECT permno, gvkey, month, ret_excess, mktcap, \" \n         \"mktcap_lag, exchange FROM crsp_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"month\"})\n  .dropna()\n)\n\nFurther, we utilize accounting data. The most common source of accounting data is Compustat. We only need book equity data in this application, which we select from our database. Additionally, we convert the variable datadate to its monthly value, as we only consider monthly returns here and do not need to account for the exact date.\n\nbook_equity = (pd.read_sql_query(\n    sql=\"SELECT gvkey, datadate, be FROM compustat\",\n    con=tidy_finance, \n    parse_dates={\"datadate\"})\n  .dropna()\n  .assign(\n    month=lambda x: (\n      pd.to_datetime(x[\"datadate\"]).dt.to_period(\"M\").dt.to_timestamp()\n    )\n  )\n)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#book-to-market-ratio",
    "href": "python/value-and-bivariate-sorts.html#book-to-market-ratio",
    "title": "Value and Bivariate Sorts",
    "section": "Book-to-Market Ratio",
    "text": "Book-to-Market Ratio\nA fundamental problem in handling accounting data is the look-ahead bias; we must not include data in forming a portfolio that was not available knowledge at the time. Of course, researchers have more information when looking into the past than agents actually had at that moment. However, abnormal excess returns from a trading strategy should not rely on an information advantage because the differential cannot be the result of informed agents’ trades. Hence, we have to lag accounting information.\nWe continue to lag market capitalization and firm size by one month. Then, we compute the book-to-market ratio, which relates a firm’s book equity to its market equity. Firms with high (low) book-to-market ratio are called value (growth) firms. After matching the accounting and market equity information from the same month, we lag book-to-market by six months. This is a sufficiently conservative approach because accounting information is usually released well before six months pass. However, in the asset pricing literature, even longer lags are used as well.1\nHaving both variables, i.e., firm size lagged by one month and book-to-market lagged by six months, we merge these sorting variables to our returns using the sorting_date-column created for this purpose. The final step in our data preparation deals with differences in the frequency of our variables. Returns and firm size are recorded monthly. Yet, the accounting information is only released on an annual basis. Hence, we only match book-to-market to one month per year and have eleven empty observations. To solve this frequency issue, we carry the latest book-to-market ratio of each firm to the subsequent months, i.e., we fill the missing observations with the most current report. This is done via the fillna(method=\"ffill\")-function after sorting by date and firm (which we identify by permno and gvkey) and on a firm basis (which we do by .groupby() as usual). We filter out all observations with accounting data that is older than a year. As the last step, we remove all rows with missing entries because the returns cannot be matched to any annual report.\n\nme = (crsp_monthly\n  .assign(sorting_date=lambda x: x[\"month\"]+pd.DateOffset(months=1))\n  .rename(columns={\"mktcap\": \"me\"})\n  .get([\"permno\", \"sorting_date\", \"me\"])\n)\n\nbm = (book_equity\n  .merge(crsp_monthly, how=\"inner\", on=[\"gvkey\", \"month\"])\n  .assign(bm=lambda x: x[\"be\"]/x[\"mktcap\"],\n          sorting_date=lambda x: x[\"month\"]+pd.DateOffset(months=6))\n  .assign(comp_date=lambda x: x[\"sorting_date\"])\n  .get([\"permno\", \"gvkey\", \"sorting_date\", \"comp_date\", \"bm\"])\n)\n\ndata_for_sorts = (crsp_monthly\n  .merge(bm, \n         how=\"left\", \n         left_on=[\"permno\", \"gvkey\", \"month\"], \n         right_on=[\"permno\", \"gvkey\", \"sorting_date\"])\n  .merge(me, \n         how=\"left\", \n         left_on=[\"permno\", \"month\"], \n         right_on=[\"permno\", \"sorting_date\"])\n  .get([\"permno\", \"gvkey\", \"month\", \"ret_excess\", \n        \"mktcap_lag\", \"me\", \"bm\", \"exchange\", \"comp_date\"])\n)\n\ndata_for_sorts = (data_for_sorts\n  .sort_values(by=[\"permno\", \"gvkey\", \"month\"])\n  .groupby([\"permno\", \"gvkey\"])\n  .apply(lambda x: x.assign(\n      bm=x[\"bm\"].fillna(method=\"ffill\"), \n      comp_date=x[\"comp_date\"].fillna(method=\"ffill\")\n    )\n  )\n  .reset_index(drop=True)\n  .assign(threshold_date = lambda x: (x[\"month\"]-pd.DateOffset(months=12)))\n  .query(\"comp_date &gt; threshold_date\")\n  .drop(columns=[\"comp_date\", \"threshold_date\"])\n  .dropna()\n)\n\nThe last step of preparation for the portfolio sorts is the computation of breakpoints. We continue to use the same function, allowing for the specification of exchanges to be used for the breakpoints. Additionally, we reintroduce the argument sorting_variable into the function for defining different sorting variables.\n\ndef assign_portfolio(data, exchanges, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolio for a given sorting variable.\"\"\"\n    \n    breakpoints = (data\n      .query(f\"exchange in {exchanges}\")\n      .get(sorting_variable)\n      .quantile(np.linspace(0, 1, num=n_portfolios+1), \n                interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    breakpoints.iloc[0] = -np.Inf\n    breakpoints.iloc[breakpoints.size-1] = np.Inf\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nAfter these data preparation steps, we present bivariate portfolio sorts on an independent and dependent basis.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#independent-sorts",
    "href": "python/value-and-bivariate-sorts.html#independent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Independent Sorts",
    "text": "Independent Sorts\nBivariate sorts create portfolios within a two-dimensional space spanned by two sorting variables. It is then possible to assess the return impact of either sorting variable by the return differential from a trading strategy that invests in the portfolios at either end of the respective variables spectrum. We create a five-by-five matrix using book-to-market and firm size as sorting variables in our example below. We end up with 25 portfolios. Since we are interested in the value premium (i.e., the return differential between high and low book-to-market firms), we go long the five portfolios of the highest book-to-market firms and short the five portfolios of the lowest book-to-market firms. The five portfolios at each end are due to the size splits we employed alongside the book-to-market splits.\nTo implement the independent bivariate portfolio sort, we assign monthly portfolios for each of our sorting variables separately to create the variables portfolio_bm and portfolio_me, respectively. Then, these separate portfolios are combined to the final sort stored in portfolio_combined. After assigning the portfolios, we compute the average return within each portfolio for each month. Additionally, we keep the book-to-market portfolio as it makes the computation of the value premium easier. The alternative would be to disaggregate the combined portfolio in a separate step. Notice that we weigh the stocks within each portfolio by their market capitalization, i.e., we decide to value-weight our returns.\n\nvalue_portfolios = (data_for_sorts\n  .groupby(\"month\")\n  .apply(lambda x: x.assign(\n      portfolio_bm=assign_portfolio(\n        data=x, sorting_variable=\"bm\", n_portfolios=5, exchanges=[\"NYSE\"]\n      ),\n      portfolio_me=assign_portfolio(\n        data=x, sorting_variable=\"me\", n_portfolios=5, exchanges=[\"NYSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"month\", \"portfolio_bm\", \"portfolio_me\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nEquipped with our monthly portfolio returns, we are ready to compute the value premium. However, we still have to decide how to invest in the five high and the five low book-to-market portfolios. The most common approach is to weigh these portfolios equally, but this is yet another researcher’s choice. Then, we compute the return differential between the high and low book-to-market portfolios and show the average value premium.\n\nvalue_premium = (value_portfolios\n  .groupby([\"month\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"value_premium\": (\n        x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() - \n          x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean()\n      )\n    })\n  )\n  .aggregate({\"value_premium\": \"mean\"})\n)\n\nThe resulting monthly value premium is 0.43 percent with an annualized return of 5.3 percent.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#dependent-sorts",
    "href": "python/value-and-bivariate-sorts.html#dependent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Dependent Sorts",
    "text": "Dependent Sorts\nIn the previous exercise, we assigned the portfolios without considering the second variable in the assignment. This protocol is called independent portfolio sorts. The alternative, i.e., dependent sorts, creates portfolios for the second sorting variable within each bucket of the first sorting variable. In our example below, we sort firms into five size buckets, and within each of those buckets, we assign firms to five book-to-market portfolios. Hence, we have monthly breakpoints that are specific to each size group. The decision between independent and dependent portfolio sorts is another choice for the researcher. Notice that dependent sorts ensure an equal amount of stocks within each portfolio.\nTo implement the dependent sorts, we first create the size portfolios by calling assign_portfolio() with sorting_variable=\"me\". Then, we group our data again by month and by the size portfolio before assigning the book-to-market portfolio. The rest of the implementation is the same as before. Finally, we compute the value premium.\n\nvalue_portfolios = (data_for_sorts\n  .groupby(\"month\")\n  .apply(lambda x: x.assign(\n      portfolio_me=assign_portfolio(\n        data=x, sorting_variable=\"me\", n_portfolios=5, exchanges=[\"NYSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"month\", \"portfolio_me\"])\n  .apply(lambda x: x.assign(\n      portfolio_bm=assign_portfolio(\n        data=x, sorting_variable=\"bm\", n_portfolios=5, exchanges=[\"NYSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"month\", \"portfolio_bm\", \"portfolio_me\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nvalue_premium = (value_portfolios\n  .groupby([\"month\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"value_premium\": (\n        x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() -\n          x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean()\n      )\n    })\n  )\n  .aggregate({\"value_premium\": \"mean\"})\n)\n\nThe monthly value premium from dependent sorts is 0.38 percent, which translates to an annualized premium of 4.6 percent per year.\nOverall, we show how to conduct bivariate portfolio sorts in this chapter. In one case, we sort the portfolios independently of each other. Yet we also discuss how to create dependent portfolio sorts. Along the lines of Size Sorts and P-Hacking, we see how many choices a researcher has to make to implement portfolio sorts, and bivariate sorts increase the number of choices.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#exercises",
    "href": "python/value-and-bivariate-sorts.html#exercises",
    "title": "Value and Bivariate Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nIn Size Sorts and P-Hacking, we examine the distribution of market equity. Repeat this analysis for book equity and the book-to-market ratio (alongside a plot of the breakpoints, i.e., deciles).\nWhen we investigate the portfolios, we focus on the returns exclusively. However, it is also of interest to understand the characteristics of the portfolios. Write a function to compute the average characteristics for size and book-to-market across the 25 independently and dependently sorted portfolios.\nAs for the size premium, also the value premium constructed here does not follow Fama and French (1993). Implement a p-hacking setup as in Size Sorts and P-Hacking to find a premium that comes closest to their HML premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#footnotes",
    "href": "python/value-and-bivariate-sorts.html#footnotes",
    "title": "Value and Bivariate Sorts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe definition of a time lag is another choice a researcher has to make, similar to breakpoint choices as we describe in Size Sorts and P-Hacking.↩︎",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Value and Bivariate Sorts"
    ]
  },
  {
    "objectID": "python/wrds-dummy-data.html",
    "href": "python/wrds-dummy-data.html",
    "title": "WRDS Dummy Data",
    "section": "",
    "text": "In this appendix chapter, we alleviate the constraints of readers who don’t have access to WRDS and hence cannot run the code that we provide. We show how to create a dummy database that contains the WRDS tables and corresponding columns such that all code chunks in this book can be executed with this dummy database. We do not create dummy data for tables of macroeconomic variables because they can be freely downloaded from the original sources; check out Accessing and Managing Financial Data.\nWe deliberately use the dummy label because the data is not meaningful in the sense that it allows readers to actually replicate the results of the book. For legal reasons, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books.\nTo generate the dummy database, we use the following packages:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport string\nLet us initialize a SQLite database (tidy_finance_python.sqlite) or connect to your existing one. Be careful, if you already downloaded the data from WRDS, then the code in this chapter will overwrite your data!\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\nSince we draw random numbers for most of the columns, we also define a seed to ensure that the generated numbers are replicable. We also initialize vectors of dates of different frequencies over ten years that we then use to create yearly, monthly, and daily data, respectively.\nnp.random.seed(1234)\n\nstart_date = pd.Timestamp(\"2003-01-01\")\nend_date = pd.Timestamp(\"2022-12-31\")\n\ndummy_years = np.arange(start_date.year, end_date.year+1, 1)\ndummy_months = pd.date_range(start_date, end_date, freq=\"MS\") \ndummy_days = pd.date_range(start_date, end_date, freq=\"D\")",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "python/wrds-dummy-data.html#create-stock-dummy-data",
    "href": "python/wrds-dummy-data.html#create-stock-dummy-data",
    "title": "WRDS Dummy Data",
    "section": "Create Stock Dummy Data",
    "text": "Create Stock Dummy Data\nLet us start with the core data used throughout the book: stock and firm characteristics. We first generate a table with a cross-section of stock identifiers with unique permno and gvkey values, as well as associated exchcd, exchange, industry, and siccd values. The generated data is based on the characteristics of stocks in the crsp_monthly table of the original database, ensuring that the generated stocks roughly reflect the distribution of industries and exchanges in the original data, but the identifiers and corresponding exchanges or industries do not reflect actual firms. Similarly, the permno-gvkey combinations are purely nonsensical and should not be used together with actual CRSP or Compustat data.\n\nnumber_of_stocks = 100\n\nindustries = pd.DataFrame({\n  \"industry\": [\"Agriculture\", \"Construction\", \"Finance\",\n               \"Manufacturing\", \"Mining\", \"Public\", \"Retail\", \n               \"Services\", \"Transportation\", \"Utilities\", \"Wholesale\"],\n  \"n\": [81, 287, 4682, 8584, 1287, 1974, 1571, 4277, 1249, 457, 904],\n  \"prob\": [0.00319, 0.0113, 0.185, 0.339, 0.0508, 0.0779, \n           0.0620, 0.169, 0.0493, 0.0180, 0.03451]\n})\n\nexchanges = pd.DataFrame({\n  \"exchange\": [\"AMEX\", \"NASDAQ\", \"NYSE\"],\n  \"n\": [2893, 17236, 5553],\n  \"prob\": [0.113, 0.671, 0.216]\n})\n\nstock_identifiers_list = []\nfor x in range(1, number_of_stocks+1):\n  exchange = np.random.choice(exchanges[\"exchange\"], p=exchanges[\"prob\"])\n  industry = np.random.choice(industries[\"industry\"], p=industries[\"prob\"])\n\n  exchcd_mapping = {\n    \"NYSE\": np.random.choice([1, 31]),\n    \"AMEX\": np.random.choice([2, 32]),\n    \"NASDAQ\": np.random.choice([3, 33])\n  }\n\n  siccd_mapping = {\n    \"Agriculture\": np.random.randint(1, 1000),\n    \"Mining\": np.random.randint(1000, 1500),\n    \"Construction\": np.random.randint(1500, 1800),\n    \"Manufacturing\": np.random.randint(1800, 4000),\n    \"Transportation\": np.random.randint(4000, 4900),\n    \"Utilities\": np.random.randint(4900, 5000),\n    \"Wholesale\": np.random.randint(5000, 5200),\n    \"Retail\": np.random.randint(5200, 6000),\n    \"Finance\": np.random.randint(6000, 6800),\n    \"Services\": np.random.randint(7000, 9000),\n    \"Public\": np.random.randint(9000, 10000)\n  }\n\n  stock_identifiers_list.append({\n    \"permno\": x,\n    \"gvkey\": str(x+10000),\n    \"exchange\": exchange,\n    \"industry\": industry,\n    \"exchcd\": exchcd_mapping[exchange],\n    \"siccd\": siccd_mapping[industry]\n  })\n\nstock_identifiers = pd.DataFrame(stock_identifiers_list)\n\nNext, we construct three panels of stock data with varying frequencies: yearly, monthly, and daily. We begin by creating the stock_panel_yearly panel. To achieve this, we combine the stock_identifiers table with a new table containing the variable year from dummy_years. The expand_grid() function ensures that we get all possible combinations of the two tables. After combining, we select only the gvkey and year columns for our final yearly panel.\nNext, we construct the stock_panel_monthly panel. Similar to the yearly panel, we use the expand_grid() function to combine stock_identifiers with a new table that has the month variable from dummy_months. After merging, we select the columns permno, gvkey, month, siccd, industry, exchcd, and exchange to form our monthly panel.\nLastly, we create the stock_panel_daily panel. We combine stock_identifiers with a table containing the date variable from dummy_days. After merging, we retain only the permno and date columns for our daily panel.\n\nstock_panel_yearly = pd.DataFrame({\n  \"gvkey\": np.tile(stock_identifiers[\"gvkey\"], len(dummy_years)),\n  \"year\": np.repeat(dummy_years, len(stock_identifiers))\n})\n\nstock_panel_monthly = pd.DataFrame({\n  \"permno\": np.tile(stock_identifiers[\"permno\"], len(dummy_months)),\n  \"gvkey\": np.tile(stock_identifiers[\"gvkey\"], len(dummy_months)),\n  \"month\": np.repeat(dummy_months, len(stock_identifiers)),\n  \"siccd\": np.tile(stock_identifiers[\"siccd\"], len(dummy_months)),\n  \"industry\": np.tile(stock_identifiers[\"industry\"], len(dummy_months)),\n  \"exchcd\": np.tile(stock_identifiers[\"exchcd\"], len(dummy_months)),\n  \"exchange\": np.tile(stock_identifiers[\"exchange\"], len(dummy_months))\n})\n\nstock_panel_daily = pd.DataFrame({\n  \"permno\": np.tile(stock_identifiers[\"permno\"], len(dummy_days)),\n  \"date\": np.repeat(dummy_days, len(stock_identifiers))\n})\n\n\nDummy beta table\nWe then proceed to create dummy beta values for our stock_panel_monthly table. We generate monthly beta values beta_monthly using the rnorm() function with a mean and standard deviation of 1. For daily beta values beta_daily, we take the dummy monthly beta and add a small random noise to it. This noise is generated again using the rnorm() function, but this time we divide the random values by 100 to ensure they are small deviations from the monthly beta.\n\nbeta_dummy = (stock_panel_monthly\n  .assign(\n    beta_monthly=np.random.normal(\n      loc=1, scale=1, size=len(stock_panel_monthly)\n    ),\n    beta_daily=lambda x: (\n      x[\"beta_monthly\"]+np.random.normal(scale=0.01, size=len(x))\n    )\n  )\n)\n\n(beta_dummy\n  .to_sql(name=\"beta\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n\n\n\nDummy compustat table\nTo create dummy firm characteristics, we take all columns from the compustat table and create random numbers between 0 and 1. For simplicity, we set the datadate for each firm-year observation to the last day of the year, although it is empirically not the case. \n\nrelevant_columns = [\n  \"seq\", \"ceq\", \"at\", \"lt\", \"txditc\", \"txdb\", \"itcb\", \n  \"pstkrv\", \"pstkl\", \"pstk\", \"capx\", \"oancf\", \"sale\", \n  \"cogs\", \"xint\", \"xsga\", \"be\", \"op\", \"at_lag\", \"inv\"\n]\n\ncommands = {\n  col: np.random.rand(len(stock_panel_yearly)) for col in relevant_columns\n}\n\ncompustat_dummy = (\n  stock_panel_yearly\n  .assign(\n    datadate=lambda x: pd.to_datetime(x[\"year\"].astype(str)+\"-12-31\")\n  )\n  .assign(**commands)\n)\n\n(compustat_dummy\n  .to_sql(name=\"compustat\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\n\n\nDummy crsp_monthly table\nThe crsp_monthly table only lacks a few more columns compared to stock_panel_monthly: the returns ret drawn from a normal distribution, the excess returns ret_excess with small deviations from the returns, the shares outstanding shrout and the last price per month altprc both drawn from uniform distributions, and the market capitalization mktcap as the product of shrout and altprc. \n\ncrsp_monthly_dummy = (stock_panel_monthly\n  .assign(\n    date=lambda x: x[\"month\"]+pd.offsets.MonthEnd(1),\n    ret=lambda x: np.fmax(np.random.normal(size=len(x)), -1),\n    ret_excess=lambda x: (\n      np.fmax(x[\"ret\"]-np.random.uniform(0, 0.0025, len(x)), -1)\n    ),\n    shrout=1000*np.random.uniform(1, 50, len(stock_panel_monthly)),\n    altprc=np.random.uniform(0, 1000, len(stock_panel_monthly)))\n  .assign(mktcap=lambda x: x[\"shrout\"]*x[\"altprc\"])\n  .sort_values(by=[\"permno\", \"month\"])\n  .assign(\n    mktcap_lag=lambda x: (x.groupby(\"permno\")[\"mktcap\"].shift(1))\n  )\n  .reset_index(drop=True)\n)\n\n(crsp_monthly_dummy\n  .to_sql(name=\"crsp_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\n\n\nDummy crsp_daily table\nThe crsp_daily table only contains a month column and the daily excess returns ret_excess as additional columns to stock_panel_daily.\n\ncrsp_daily_dummy = (stock_panel_daily\n  .assign(\n    month=lambda x: x[\"date\"].dt.to_period('M').dt.start_time,\n    ret_excess=lambda x: np.fmax(np.random.normal(size=len(x)), -1)\n  )\n  .reset_index(drop=True)\n)\n\n(crsp_daily_dummy\n  .to_sql(name=\"crsp_daily\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "python/wrds-dummy-data.html#create-bond-dummy-data",
    "href": "python/wrds-dummy-data.html#create-bond-dummy-data",
    "title": "WRDS Dummy Data",
    "section": "Create Bond Dummy Data",
    "text": "Create Bond Dummy Data\nLastly, we move to the bond data that we use in our books.\n\nDummy fisd data\nTo create dummy data with the structure of Mergent FISD, we calculate the empirical probabilities of actual bonds for several variables: maturity, offering_amt, interest_frequency, coupon, and sic_code. We use these probabilities to sample a small cross-section of bonds with completely made up complete_cusip, issue_id, and issuer_id.\n\nnumber_of_bonds = 100\n\ndef generate_cusip():\n  \"\"\"Generate cusip.\"\"\"\n  \n  characters = list(string.ascii_uppercase+string.digits)  # Convert to list\n  cusip = (\"\".join(np.random.choice(characters, size=12))).upper()\n    \n  return cusip\n\nfisd_dummy = (pd.DataFrame({\n    \"complete_cusip\": [generate_cusip() for _ in range(number_of_bonds)]\n  })\n  .assign(\n    maturity=lambda x: np.random.choice(dummy_days, len(x), replace=True),\n    offering_amt=lambda x: np.random.choice(\n      np.arange(1, 101)*100000, len(x), replace=True\n    )\n  )\n  .assign(\n    offering_date=lambda x: (\n      x[\"maturity\"]-pd.to_timedelta(\n        np.random.choice(np.arange(1, 26)*365, len(x), replace=True), \n        unit=\"D\"\n      )\n    )\n  )\n  .assign(\n    dated_date=lambda x: (\n      x[\"offering_date\"]-pd.to_timedelta(\n        np.random.choice(np.arange(-10, 11), len(x), replace=True), \n        unit=\"D\"\n      )\n    ),\n    interest_frequency=lambda x: np.random.choice(\n      [0, 1, 2, 4, 12], len(x), replace=True\n    ),\n    coupon=lambda x: np.random.choice(\n      np.arange(0, 2.1, 0.1), len(x), replace=True\n    )\n  )\n  .assign(\n    last_interest_date=lambda x: (\n      x[[\"maturity\", \"offering_date\", \"dated_date\"]].max(axis=1)\n    ),\n    issue_id=lambda x: x.index+1,\n    issuer_id=lambda x: np.random.choice(\n      np.arange(1, 251), len(x), replace=True\n    ),\n    sic_code=lambda x: (np.random.choice(\n      np.arange(1, 10)*1000, len(x), replace=True)\n    ).astype(str)\n  )\n)\n\n(fisd_dummy\n  .to_sql(name=\"fisd\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\n\n\nDummy trace_enhanced data\nFinally, we create a dummy bond transaction data for the fictional CUSIPs of the dummy fisd data. We take the date range that we also analyze in the book and ensure that we have at least five transactions per day to fulfill a filtering step in the book. \n\nnumber_of_bonds = 100\nstart_date = pd.Timestamp(\"2014-01-01\")\nend_date = pd.Timestamp(\"2016-11-30\")\n\nbonds_panel = pd.DataFrame({\n  \"cusip_id\": np.tile(\n    fisd_dummy[\"complete_cusip\"], \n    (end_date-start_date).days+1\n  ),\n  \"trd_exctn_dt\": np.repeat(\n    pd.date_range(start_date, end_date), len(fisd_dummy)\n  )\n})\n\ntrace_enhanced_dummy = (pd.concat([bonds_panel]*5)\n  .assign(\n    trd_exctn_tm = lambda x: pd.to_datetime(\n      x[\"trd_exctn_dt\"].astype(str)+\" \" +\n      np.random.randint(0, 24, size=len(x)).astype(str)+\":\" +\n      np.random.randint(0, 60, size=len(x)).astype(str)+\":\" +\n      np.random.randint(0, 60, size=len(x)).astype(str)\n    ),\n    rptd_pr=np.random.uniform(10, 200, len(bonds_panel)*5),\n    entrd_vol_qt=1000*np.random.choice(\n      range(1,21), len(bonds_panel)*5, replace=True\n    ),\n    yld_pt=np.random.uniform(-10, 10, len(bonds_panel)*5),\n    rpt_side_cd=np.random.choice(\n      [\"B\", \"S\"], len(bonds_panel)*5, replace=True\n    ),\n    cntra_mp_id=np.random.choice(\n      [\"C\", \"D\"], len(bonds_panel)*5, replace=True\n    )\n  )\n  .reset_index(drop=True)\n)\n\n(trace_enhanced_dummy\n  .to_sql(name=\"trace_enhanced\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\nAs stated in the introduction, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout this book.",
    "crumbs": [
      "R",
      "Appendix",
      "WRDS Dummy Data"
    ]
  },
  {
    "objectID": "r/beta-estimation.html",
    "href": "r/beta-estimation.html",
    "title": "Beta Estimation",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we introduce an important concept in financial economics: the exposure of an individual stock to changes in the market portfolio. According to the Capital Asset Pricing Model (CAPM) of Sharpe (1964), Lintner (1965), and Mossin (1966), cross-sectional variation in expected asset returns should be a function of the covariance between the excess return of the asset and the excess return on the market portfolio. The regression coefficient of excess market returns on excess stock returns is usually called the market beta. We show an estimation procedure for the market betas. We do not go into details about the foundations of market beta but simply refer to any treatment of the CAPM for further information. Instead, we provide details about all the functions that we use to compute the results. In particular, we leverage useful computational concepts: rolling-window estimation and parallelization.\nWe use the following R packages throughout this chapter:\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(slider)\nlibrary(furrr)\nCompared to previous chapters, we introduce slider (Vaughan 2021) for sliding window functions, and furrr (Vaughan and Dancho 2022) to apply mapping functions in parallel.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#estimating-beta-using-monthly-returns",
    "href": "r/beta-estimation.html#estimating-beta-using-monthly-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta Using Monthly Returns",
    "text": "Estimating Beta Using Monthly Returns\nThe estimation procedure is based on a rolling-window estimation, where we may use either monthly or daily returns and different window lengths. First, let us start with loading the monthly CRSP data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, month, industry, ret_excess) |&gt;\n  collect()\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(month, mkt_excess) |&gt;\n  collect()\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(factors_ff3_monthly, join_by(month))\n\nTo estimate the CAPM regression coefficients\n\\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{i, t}\n\\] we regress stock excess returns ret_excess on excess returns of the market portfolio mkt_excess. R provides a simple solution to estimate (linear) models with the function lm(). lm() requires a formula as input that is specified in a compact symbolic form. An expression of the form y ~ model is interpreted as a specification that the response y is modeled by a linear predictor specified symbolically by model. Such a model consists of a series of terms separated by + operators. In addition to standard linear models, lm() provides a lot of flexibility. You should check out the documentation for more information. To start, we restrict the data only to the time series of observations in CRSP that correspond to Apple’s stock (i.e., to permno 14593 for Apple) and compute \\(\\hat\\alpha_i\\) as well as \\(\\hat\\beta_i\\).\n\nfit &lt;- lm(ret_excess ~ mkt_excess,\n  data = crsp_monthly |&gt;\n    filter(permno == \"14593\")\n)\n\nsummary(fit)\n\n\nCall:\nlm(formula = ret_excess ~ mkt_excess, data = filter(crsp_monthly, \n    permno == \"14593\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5170 -0.0589  0.0001  0.0610  0.3947 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.01019    0.00508     2.0    0.046 *  \nmkt_excess   1.38909    0.11142    12.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.113 on 502 degrees of freedom\nMultiple R-squared:  0.236, Adjusted R-squared:  0.235 \nF-statistic:  155 on 1 and 502 DF,  p-value: &lt;2e-16\n\n\nlm() returns an object of class lm which contains all information we usually care about with linear models. summary() returns an overview of the estimated parameters. coefficients(fit) would return only the estimated coefficients. The output above indicates that Apple moves excessively with the market as the estimated \\(\\hat\\beta_i\\) is above one (\\(\\hat\\beta_i \\approx 1.4\\)).",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#rolling-window-estimation",
    "href": "r/beta-estimation.html#rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Rolling-Window Estimation",
    "text": "Rolling-Window Estimation\nAfter we estimated the regression coefficients on an example, we scale the estimation of \\(\\beta_i\\) to a whole different level and perform rolling-window estimations for the entire CRSP sample. The following function implements the CAPM regression for a data frame (or a part thereof) containing at least min_obs observations to avoid huge fluctuations if the time series is too short. If the condition is violated, that is, the time series is too short, the function returns a missing value.\n\nestimate_capm &lt;- function(data, min_obs = 1) {\n  if (nrow(data) &lt; min_obs) {\n    beta &lt;- as.numeric(NA)\n  } else {\n    fit &lt;- lm(ret_excess ~ mkt_excess, data = data)\n    beta &lt;- as.numeric(coefficients(fit)[2])\n  }\n  return(beta)\n}\n\nNext, we define a function that does the rolling estimation. The slide_period function is able to handle months in its window input in a straightforward manner. We thus avoid using any time-series package (e.g., zoo) and converting the data to fit the package functions, but rather stay in the world of the tidyverse.\nThe following function takes input data and slides across the month vector, considering only a total of months months. The function essentially performs three steps: (i) arrange all rows, (ii) compute betas by sliding across months, and (iii) return a tibble with months and corresponding beta estimates (again particularly useful in the case of daily data). As we demonstrate further below, we can also apply the same function to daily returns data.\n\nroll_capm_estimation &lt;- function(data, months, min_obs) {\n  data &lt;- data |&gt;\n    arrange(month)\n\n  betas &lt;- slide_period_vec(\n    .x = data,\n    .i = data$month,\n    .period = \"month\",\n    .f = ~ estimate_capm(., min_obs),\n    .before = months - 1,\n    .complete = FALSE\n  )\n\n  return(tibble(\n    month = unique(data$month),\n    beta = betas\n  ))\n}\n\nBefore we attack the whole CRSP sample, let us focus on a couple of examples for well-known firms.\n\nexamples &lt;- tribble(\n  ~permno, ~company,\n  14593, \"Apple\",\n  10107, \"Microsoft\",\n  93436, \"Tesla\",\n  17778, \"Berkshire Hathaway\"\n)\n\nIf we want to estimate rolling betas for Apple, we can use mutate(). We take a total of 5 years of data and require at least 48 months with return data to compute our betas. Check out the Exercises if you want to compute beta for different time periods.\n\nbeta_example &lt;- crsp_monthly |&gt;\n  filter(permno == examples$permno[1]) |&gt;\n  mutate(roll_capm_estimation(pick(everything()), months = 60, min_obs = 48)) |&gt;\n  drop_na()\nbeta_example\n\n# A tibble: 457 × 6\n  permno month      industry      ret_excess mkt_excess  beta\n   &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1  14593 1984-12-01 Manufacturing     0.170      0.0184  2.05\n2  14593 1985-01-01 Manufacturing    -0.0108     0.0799  1.90\n3  14593 1985-02-01 Manufacturing    -0.152      0.0122  1.88\n4  14593 1985-03-01 Manufacturing    -0.112     -0.0084  1.89\n5  14593 1985-04-01 Manufacturing    -0.0467    -0.0096  1.90\n# ℹ 452 more rows\n\n\nIt is actually quite simple to perform the rolling-window estimation for an arbitrary number of stocks, which we visualize in the following code chunk and the resulting Figure 1.\n\nbeta_examples &lt;- crsp_monthly |&gt;\n  inner_join(examples, join_by(permno)) |&gt;\n  group_by(permno) |&gt;\n  mutate(roll_capm_estimation(pick(everything()), months = 60, min_obs = 48)) |&gt;\n  ungroup() |&gt;\n  select(permno, company, month, beta) |&gt;\n  drop_na()\n\nbeta_examples |&gt;\n  ggplot(aes(\n    x = month, \n    y = beta, \n    color = company,\n    linetype = company)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly beta estimates for example stocks using 5 years of data\"\n  )\n\n\n\n\n\n\n\nFigure 1: The CAPM betas are estimated with monthly data and a rolling window of length 5 years based on adjusted excess returns from CRSP. We use market excess returns from Kenneth French data library.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#parallelized-rolling-window-estimation",
    "href": "r/beta-estimation.html#parallelized-rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Parallelized Rolling-Window Estimation",
    "text": "Parallelized Rolling-Window Estimation\nEven though we could now just apply the function using group_by() on the whole CRSP sample, we advise against doing it as it is computationally quite expensive. Remember that we have to perform rolling-window estimations across all stocks and time periods. However, this estimation problem is an ideal scenario to employ the power of parallelization. Parallelization means that we split the tasks which perform rolling-window estimations across different workers (or cores on your local machine).\nFirst, we nest() the data by permno. Nested data means we now have a list of permno with corresponding time series data and an industry label. We get one row of output for each unique combination of non-nested variables which are permno and industry.\n\ncrsp_monthly_nested &lt;- crsp_monthly |&gt;\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_monthly_nested\n\n# A tibble: 25,974 × 3\n  permno industry      data              \n   &lt;dbl&gt; &lt;chr&gt;         &lt;list&gt;            \n1  10000 Manufacturing &lt;tibble [16 × 3]&gt; \n2  10001 Utilities     &lt;tibble [378 × 3]&gt;\n3  10002 Finance       &lt;tibble [324 × 3]&gt;\n4  10003 Finance       &lt;tibble [118 × 3]&gt;\n5  10005 Mining        &lt;tibble [65 × 3]&gt; \n# ℹ 25,969 more rows\n\n\nAlternatively, we could have created the same nested data by excluding the variables that we do not want to nest, as in the following code chunk. However, for many applications it is desirable to explicitly state the variables that are nested into the data list-column, so that the reader can track what ends up in there.\n\ncrsp_monthly_nested &lt;- crsp_monthly |&gt;\n  nest(data = -c(permno, industry))\n\nNext, we want to apply the roll_capm_estimation() function to each stock. This situation is an ideal use case for map(), which takes a list or vector as input and returns an object of the same length as the input. In our case, map() returns a single data frame with a time series of beta estimates for each stock. Therefore, we use unnest() to transform the list of outputs to a tidy data frame.\n\ncrsp_monthly_nested |&gt;\n  inner_join(examples, join_by(permno)) |&gt;\n  mutate(beta = map(\n    data,\n    ~ roll_capm_estimation(., months = 60, min_obs = 48)\n  )) |&gt;\n  unnest(beta) |&gt;\n  select(permno, month, beta_monthly = beta) |&gt;\n  drop_na()\n\n# A tibble: 1,461 × 3\n  permno month      beta_monthly\n   &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;\n1  10107 1990-03-01         1.39\n2  10107 1990-04-01         1.38\n3  10107 1990-05-01         1.43\n4  10107 1990-06-01         1.43\n5  10107 1990-07-01         1.45\n# ℹ 1,456 more rows\n\n\nHowever, instead, we want to perform the estimations of rolling betas for different stocks in parallel. If you have a Windows or Mac machine, it makes most sense to define multisession, which means that separate R processes are running in the background on the same machine to perform the individual jobs. If you check out the documentation of plan(), you can also see other ways to resolve the parallelization in different environments. Note that we use availableCores() to determine the number of cores available for parallelization, but keep one core free for other tasks. Some machines might freeze if all cores are busy with Python jobs. \n\nn_cores = availableCores() - 1\nplan(multisession, workers = n_cores)\n\nUsing eight cores, the estimation for our sample of around 25k stocks takes around 20 minutes. Of course, you can speed up things considerably by having more cores available to share the workload or by having more powerful cores. Notice the difference in the code below? All you need to do is to replace map() with future_map().\n\nbeta_monthly &lt;- crsp_monthly_nested |&gt;\n  mutate(beta = future_map(\n    data, ~ roll_capm_estimation(., months = 60, min_obs = 48)\n  )) |&gt;\n  unnest(c(beta)) |&gt;\n  select(permno, month, beta_monthly = beta) |&gt;\n  drop_na()",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#estimating-beta-using-daily-returns",
    "href": "r/beta-estimation.html#estimating-beta-using-daily-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta Using Daily Returns",
    "text": "Estimating Beta Using Daily Returns\nBefore we provide some descriptive statistics of our beta estimates, we implement the estimation for the daily CRSP sample as well. Depending on the application, you might either use longer horizon beta estimates based on monthly data or shorter horizon estimates based on daily returns.\nFirst, we load daily CRSP data. Note that the sample is large compared to the monthly data, so make sure to have enough memory available.\n\ncrsp_daily &lt;- tbl(tidy_finance, \"crsp_daily\") |&gt;\n  select(permno, month, date, ret_excess) |&gt;\n  collect()\n\nWe also need the daily Fama-French market excess returns.\n\nfactors_ff3_daily &lt;- tbl(tidy_finance, \"factors_ff3_daily\") |&gt;\n  select(date, mkt_excess) |&gt;\n  collect()\n\nWe make sure to keep only relevant data to save memory space. However, note that your machine might not have enough memory to read the whole daily CRSP sample. In this case, we refer you to the exercises and try working with loops as in WRDS, CRSP, and Compustat.\n\ncrsp_daily &lt;- crsp_daily |&gt;\n  inner_join(factors_ff3_daily, join_by(date)) |&gt;\n  select(permno, month, ret_excess, mkt_excess)\n\nJust like above, we nest the data by permno for parallelization.\n\ncrsp_daily_nested &lt;- crsp_daily |&gt;\n  nest(data = c(month, ret_excess, mkt_excess))\n\nThis is what the estimation looks like for a couple of examples using map(). For the daily data, we use the same function as above but only take 3 months of data and require at least 50 daily return observations in these months. These restrictions help us to retrieve somewhat smooth coefficient estimates.\n\ncrsp_daily_nested |&gt;\n  inner_join(examples, join_by(permno)) |&gt;\n  mutate(beta_daily = map(\n    data,\n    ~ roll_capm_estimation(., months = 3, min_obs = 50)\n  )) |&gt;\n  unnest(c(beta_daily)) |&gt;\n  select(permno, month, beta_daily = beta) |&gt;\n  drop_na()\n\n# A tibble: 1,639 × 3\n  permno month      beta_daily\n   &lt;dbl&gt; &lt;date&gt;          &lt;dbl&gt;\n1  10107 1986-05-01      0.898\n2  10107 1986-06-01      0.906\n3  10107 1986-07-01      0.822\n4  10107 1986-08-01      0.900\n5  10107 1986-09-01      1.01 \n# ℹ 1,634 more rows\n\n\nFor the sake of completeness, we tell our session again to use multiple workers for parallelization.\n\nplan(multisession, workers = n_cores)\n\nThe code chunk for beta estimation using daily returns now looks very similar to the one for monthly data. The whole estimation takes around 30 minutes using eight cores and 16gb memory.\n\nbeta_daily &lt;- crsp_daily_nested |&gt;\n  mutate(beta_daily = future_map(\n    data, ~ roll_capm_estimation(., months = 3, min_obs = 50)\n  )) |&gt;\n  unnest(c(beta_daily)) |&gt;\n  select(permno, month, beta_daily = beta) |&gt;\n  drop_na()",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#comparing-beta-estimates",
    "href": "r/beta-estimation.html#comparing-beta-estimates",
    "title": "Beta Estimation",
    "section": "Comparing Beta Estimates",
    "text": "Comparing Beta Estimates\nWhat is a typical value for stock betas? To get some feeling, we illustrate the dispersion of the estimated \\(\\hat\\beta_i\\) across different industries and across time below. Figure 2 shows that typical business models across industries imply different exposure to the general market economy. However, there are barely any firms that exhibit a negative exposure to the market factor.\n\ncrsp_monthly |&gt;\n  left_join(beta_monthly, join_by(permno, month)) |&gt;\n  drop_na(beta_monthly) |&gt;\n  group_by(industry, permno) |&gt;\n  summarize(beta = mean(beta_monthly), \n            .groups = \"drop\") |&gt;\n  ggplot(aes(x = reorder(industry, beta, FUN = median), y = beta)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Firm-specific beta distributions by industry\"\n  )\n\n\n\n\n\n\n\nFigure 2: The box plots show the average firm-specific beta estimates by industry.\n\n\n\n\n\nNext, we illustrate the time-variation in the cross-section of estimated betas. Figure 3 shows the monthly deciles of estimated betas (based on monthly data) and indicates an interesting pattern: First, betas seem to vary over time in the sense that during some periods, there is a clear trend across all deciles. Second, the sample exhibits periods where the dispersion across stocks increases in the sense that the lower decile decreases and the upper decile increases, which indicates that for some stocks the correlation with the market increases while for others it decreases. Note also here: stocks with negative betas are a rare exception.\n\nbeta_monthly |&gt;\n  drop_na(beta_monthly) |&gt;\n  group_by(month) |&gt;\n  reframe(\n    x = quantile(beta_monthly, seq(0.1, 0.9, 0.1)),\n    quantile = 100 * seq(0.1, 0.9, 0.1)\n  ) |&gt;\n  ggplot(aes(\n    x = month, \n    y = x, \n    color = as_factor(quantile),\n    linetype = as_factor(quantile)\n    )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly deciles of estimated betas\",\n  )\n\n\n\n\n\n\n\nFigure 3: Each line corresponds to the monthly cross-sectional quantile of the estimated CAPM beta.\n\n\n\n\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table. Then, we use the table to plot a comparison of beta estimates for our example stocks in Figure 4.\n\nbeta &lt;- beta_monthly |&gt;\n  full_join(beta_daily, join_by(permno, month)) |&gt;\n  arrange(permno, month)\n\nbeta |&gt;\n  inner_join(examples, join_by(permno)) |&gt;\n  pivot_longer(cols = c(beta_monthly, beta_daily)) |&gt;\n  drop_na() |&gt;\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = name, \n    linetype = name\n    )) +\n  geom_line() +\n  facet_wrap(~company, ncol = 1) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL, \n    title = \"Comparison of beta estimates using monthly and daily data\"\n  )\n\n\n\n\n\n\n\nFigure 4: CAPM betas are computed using 5 years of monthly or 3 months of daily data. The two lines show the monthly estimates based on a rolling window for few exemplary stocks.\n\n\n\n\n\nThe estimates in Figure 4 look as expected. As you can see, it really depends on the estimation window and data frequency how your beta estimates turn out.\nFinally, we write the estimates to our database such that we can use them in later chapters.\n\ndbWriteTable(tidy_finance,\n  \"beta\",\n  value = beta,\n  overwrite = TRUE\n)\n\nWhenever you perform some kind of estimation, it also makes sense to do rough plausibility tests. A possible check is to plot the share of stocks with beta estimates over time. This descriptive helps us discover potential errors in our data preparation or estimation procedure. For instance, suppose there was a gap in our output where we do not have any betas. In this case, we would have to go back and check all previous steps to find out what went wrong.\n\nbeta_long &lt;- crsp_monthly |&gt;\n  left_join(beta, join_by(permno, month)) |&gt;\n  pivot_longer(cols = c(beta_monthly, beta_daily))\n\nbeta_long |&gt;\n  group_by(month, name) |&gt;\n  summarize(share = sum(!is.na(value)) / n(), \n            .groups = \"drop\") |&gt;\n  ggplot(aes(\n    x = month, \n    y = share, \n    color = name,\n    linetype = name\n    )) +\n  geom_line() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"End-of-month share of securities with beta estimates\"\n  ) +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\nFigure 5: The two lines show the share of securities with beta estimates using 5 years of monthly or 3 months of daily data.\n\n\n\n\n\nFigure 5 does not indicate any troubles, so let us move on to the next check.\nWe also encourage everyone to always look at the distributional summary statistics of variables. You can easily spot outliers or weird distributions when looking at such tables.\n\nbeta_long |&gt;\n  select(name, value) |&gt;\n  drop_na() |&gt;\n  group_by(name) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  )\n\n# A tibble: 2 × 9\n  name          mean    sd   min    q05   q50   q95   max       n\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 beta_daily   0.752 0.925 -43.7 -0.440 0.690  2.23  56.6 3289061\n2 beta_monthly 1.10  0.714 -13.0  0.127 1.04   2.33  11.8 2266003\n\n\nThe summary statistics also look plausible for the two estimation procedures.\nFinally, since we have two different estimators for the same theoretical object, we expect the estimators should be at least positively correlated (although not perfectly as the estimators are based on different sample periods and frequencies).\n\nbeta |&gt;\n  select(beta_daily, beta_monthly) |&gt;\n  cor(use = \"complete.obs\")\n\n             beta_daily beta_monthly\nbeta_daily        1.000        0.322\nbeta_monthly      0.322        1.000\n\n\nIndeed, we find a positive correlation between our beta estimates. In the subsequent chapters, we mainly use the estimates based on monthly data, as most readers should be able to replicate them due to potential memory limitations that might arise with the daily data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/beta-estimation.html#exercises",
    "href": "r/beta-estimation.html#exercises",
    "title": "Beta Estimation",
    "section": "Exercises",
    "text": "Exercises\n\nCompute beta estimates based on monthly data using 1, 3, and 5 years of data and impose a minimum number of observations of 10, 28, and 48 months with return data, respectively. How strongly correlated are the estimated betas?\nCompute beta estimates based on monthly data using 5 years of data and impose different numbers of minimum observations. How does the share of permno-month observations with successful beta estimates vary across the different requirements? Do you find a high correlation across the estimated betas?\nInstead of using future_map(), perform the beta estimation in a loop (using either monthly or daily data) for a subset of 100 permnos of your choice. Verify that you get the same results as with the parallelized code from above.\nFilter out the stocks with negative betas. Do these stocks frequently exhibit negative betas, or do they resemble estimation errors?\nCompute beta estimates for multi-factor models such as the Fama-French three-factor model. For that purpose, you extend your regression to \\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\sum\\limits_{j=1}^k\\beta_{i,k}(r_{j, t}-r_{f,t})+\\varepsilon_{i, t}\n\\] where \\(r_{j, t}\\) are the \\(k\\) factor returns. Thus, you estimate 4 parameters (\\(\\alpha_i\\) and the slope coefficients). Provide some summary statistics of the cross-section of firms and their exposure to the different factors.\n\n\n\n\nFigure 1: The CAPM betas are estimated with monthly data and a rolling window of length 5 years based on adjusted excess returns from CRSP. We use market excess returns from Kenneth French data library.\nFigure 2: The box plots show the average firm-specific beta estimates by industry.\nFigure 3: Each line corresponds to the monthly cross-sectional quantile of the estimated CAPM beta.\nFigure 4: CAPM betas are computed using 5 years of monthly or 3 months of daily data. The two lines show the monthly estimates based on a rolling window for few exemplary stocks.\nFigure 5: The two lines show the share of securities with beta estimates using 5 years of monthly or 3 months of daily data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "r/clean-enhanced-trace-with-r.html",
    "href": "r/clean-enhanced-trace-with-r.html",
    "title": "Clean Enhanced TRACE with R",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\n\n\nThis appendix contains code to clean enhanced TRACE with R. It is also available via the following GitHub gist. Hence, you could also source the function with devtools::source_gist(\"3a05b3ab281563b2e94858451c2eb3a4\"). We need this function in Chapter TRACE and FISD to download and clean enhanced TRACE trade messages following Dick-Nielsen (2009) and Dick-Nielsen (2014) for enhanced TRACE specifically. Relatedly, WRDS provides SAS code and there is Python code available by the project Open Source Bond Asset Pricing.\nThe function takes a vector of CUSIPs (in cusips), a connection to WRDS (connection) explained in Chapter 3, and a start and end date (start_date and end_date, respectively). Specifying too many CUSIPs will result in very slow downloads and a potential failure due to the size of the request to WRDS. The dates should be within the coverage of TRACE itself, i.e., starting after 2002, and the dates should be supplied using the class date. The output of the function contains all valid trade messages for the selected CUSIPs over the specified period.\n\nclean_enhanced_trace &lt;- function(cusips,\n                                 connection,\n                                 start_date = as.Date(\"2002-01-01\"),\n                                 end_date = today()) {\n\n  # Packages (required)\n  library(dplyr)\n  library(lubridate)\n  library(dbplyr)\n  library(RPostgres)\n\n  # Function checks ---------------------------------------------------------\n  # Input parameters\n  ## Cusips\n  if (length(cusips) == 0 | any(is.na(cusips))) stop(\"Check cusips.\")\n\n  ## Dates\n  if (!is.Date(start_date) | !is.Date(end_date)) stop(\"Dates needed\")\n  if (start_date &lt; as.Date(\"2002-01-01\")) stop(\"TRACE starts later.\")\n  if (end_date &gt; today()) stop(\"TRACE does not predict the future.\")\n  if (start_date &gt;= end_date) stop(\"Date conflict.\")\n\n  ## Connection\n  if (!dbIsValid(connection)) stop(\"Connection issue.\")\n\n  # Enhanced Trace ----------------------------------------------------------\n  # Main file\n  trace_all &lt;- tbl(\n    connection,\n    I(\"trace.trace_enhanced\")\n  ) |&gt;\n    filter(cusip_id %in% cusips) |&gt;\n    filter(trd_exctn_dt &gt;= start_date & trd_exctn_dt &lt;= end_date) |&gt;\n    select(\n      cusip_id, msg_seq_nb, orig_msg_seq_nb,\n      entrd_vol_qt, rptd_pr, yld_pt, rpt_side_cd, cntra_mp_id,\n      trd_exctn_dt, trd_exctn_tm, trd_rpt_dt, trd_rpt_tm,\n      pr_trd_dt, trc_st, asof_cd, wis_fl,\n      days_to_sttl_ct, stlmnt_dt, spcl_trd_fl\n    ) |&gt;\n    collect()\n\n  # Enhanced Trace: Post 06-02-2012 -----------------------------------------\n  # Trades (trc_st = T) and correction (trc_st = R)\n  trace_post_TR &lt;- trace_all |&gt;\n    filter(\n      (trc_st == \"T\" | trc_st == \"R\"),\n      trd_rpt_dt &gt;= as.Date(\"2012-02-06\")\n    )\n\n  # Cancellations (trc_st = X) and correction cancellations (trc_st = C)\n  trace_post_XC &lt;- trace_all |&gt;\n    filter(\n      (trc_st == \"X\" | trc_st == \"C\"),\n      trd_rpt_dt &gt;= as.Date(\"2012-02-06\")\n    )\n\n  # Cleaning corrected and cancelled trades\n  trace_post_TR &lt;- trace_post_TR |&gt;\n    anti_join(trace_post_XC,\n      join_by(\n        cusip_id, msg_seq_nb, entrd_vol_qt,\n        rptd_pr, rpt_side_cd, cntra_mp_id,\n        trd_exctn_dt, trd_exctn_tm\n      )\n    )\n\n  # Reversals (trc_st = Y)\n  trace_post_Y &lt;- trace_all |&gt;\n    filter(\n      trc_st == \"Y\",\n      trd_rpt_dt &gt;= as.Date(\"2012-02-06\")\n    )\n\n  # Clean reversals\n  ## match the orig_msg_seq_nb of the Y-message to\n  ## the msg_seq_nb of the main message\n  trace_post &lt;- trace_post_TR |&gt;\n    anti_join(trace_post_Y,\n      join_by(\n        cusip_id,\n        msg_seq_nb == orig_msg_seq_nb,\n        entrd_vol_qt, rptd_pr, rpt_side_cd,\n        cntra_mp_id, trd_exctn_dt, trd_exctn_tm\n      )\n    )\n\n\n  # Enhanced TRACE: Pre 06-02-2012 ------------------------------------------\n  # Cancellations (trc_st = C)\n  trace_pre_C &lt;- trace_all |&gt;\n    filter(\n      trc_st == \"C\",\n      trd_rpt_dt &lt; as.Date(\"2012-02-06\")\n    )\n\n  # Trades w/o cancellations\n  ## match the orig_msg_seq_nb of the C-message\n  ## to the msg_seq_nb of the main message\n  trace_pre_T &lt;- trace_all |&gt;\n    filter(\n      trc_st == \"T\",\n      trd_rpt_dt &lt; as.Date(\"2012-02-06\")\n    ) |&gt;\n    anti_join(trace_pre_C,\n      join_by(\n        cusip_id,\n        msg_seq_nb == orig_msg_seq_nb,\n        entrd_vol_qt, rptd_pr, rpt_side_cd,\n        cntra_mp_id, trd_exctn_dt, trd_exctn_tm\n      )\n    )\n\n  # Corrections (trc_st = W) - W can also correct a previous W\n  trace_pre_W &lt;- trace_all |&gt;\n    filter(\n      trc_st == \"W\",\n      trd_rpt_dt &lt; as.Date(\"2012-02-06\")\n    )\n\n  # Implement corrections in a loop\n  ## Correction control\n  correction_control &lt;- nrow(trace_pre_W)\n  correction_control_last &lt;- nrow(trace_pre_W)\n\n  ## Correction loop\n  while (correction_control &gt; 0) {\n    # Corrections that correct some msg\n    trace_pre_W_correcting &lt;- trace_pre_W |&gt;\n      semi_join(trace_pre_T,\n        join_by(\n          cusip_id, trd_exctn_dt,\n          orig_msg_seq_nb == msg_seq_nb\n        )\n      )\n\n    # Corrections that do not correct some msg\n    trace_pre_W &lt;- trace_pre_W |&gt;\n      anti_join(trace_pre_T,\n        join_by(cusip_id, trd_exctn_dt,\n          orig_msg_seq_nb == msg_seq_nb\n        )\n      )\n\n    # Delete msgs that are corrected and add correction msgs\n    trace_pre_T &lt;- trace_pre_T |&gt;\n      anti_join(trace_pre_W_correcting,\n        join_by(\n          cusip_id, trd_exctn_dt,\n          msg_seq_nb == orig_msg_seq_nb\n        )\n      ) |&gt;\n      union_all(trace_pre_W_correcting)\n\n    # Escape if no corrections remain or they cannot be matched\n    correction_control &lt;- nrow(trace_pre_W)\n    if (correction_control == correction_control_last) {\n      correction_control &lt;- 0\n    }\n    correction_control_last &lt;- nrow(trace_pre_W)\n  }\n\n\n  # Clean reversals\n  ## Record reversals\n  trace_pre_R &lt;- trace_pre_T |&gt;\n    filter(asof_cd == \"R\") |&gt;\n    group_by(\n      cusip_id, trd_exctn_dt, entrd_vol_qt,\n      rptd_pr, rpt_side_cd, cntra_mp_id\n    ) |&gt;\n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |&gt;\n    mutate(seq = row_number()) |&gt;\n    ungroup()\n\n  ## Remove reversals and the reversed trade\n  trace_pre &lt;- trace_pre_T |&gt;\n    filter(is.na(asof_cd) | !(asof_cd %in% c(\"R\", \"X\", \"D\"))) |&gt;\n    group_by(\n      cusip_id, trd_exctn_dt, entrd_vol_qt,\n      rptd_pr, rpt_side_cd, cntra_mp_id\n    ) |&gt;\n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |&gt;\n    mutate(seq = row_number()) |&gt;\n    ungroup() |&gt;\n    anti_join(trace_pre_R,\n      join_by(\n        cusip_id, trd_exctn_dt, entrd_vol_qt,\n        rptd_pr, rpt_side_cd, cntra_mp_id, seq\n      )\n    ) |&gt;\n    select(-seq)\n\n\n  # Agency trades -----------------------------------------------------------\n  # Combine pre and post trades\n  trace_clean &lt;- trace_post |&gt;\n    union_all(trace_pre)\n\n  # Keep angency sells and unmatched agency buys\n  ## Agency sells\n  trace_agency_sells &lt;- trace_clean |&gt;\n    filter(\n      cntra_mp_id == \"D\",\n      rpt_side_cd == \"S\"\n    )\n\n  # Agency buys that are unmatched\n  trace_agency_buys_filtered &lt;- trace_clean |&gt;\n    filter(\n      cntra_mp_id == \"D\",\n      rpt_side_cd == \"B\"\n    ) |&gt;\n    anti_join(trace_agency_sells,\n      join_by(\n        cusip_id, trd_exctn_dt,\n        entrd_vol_qt, rptd_pr\n      )\n    )\n\n  # Agency clean\n  trace_clean &lt;- trace_clean |&gt;\n    filter(cntra_mp_id == \"C\") |&gt;\n    union_all(trace_agency_sells) |&gt;\n    union_all(trace_agency_buys_filtered)\n\n\n  # Additional Filters ------------------------------------------------------\n  trace_add_filters &lt;- trace_clean |&gt;\n    mutate(days_to_sttl_ct2 = stlmnt_dt - trd_exctn_dt) |&gt;\n    filter(\n      is.na(days_to_sttl_ct) | as.numeric(days_to_sttl_ct) &lt;= 7,\n      is.na(days_to_sttl_ct2) | as.numeric(days_to_sttl_ct2) &lt;= 7,\n      wis_fl == \"N\",\n      is.na(spcl_trd_fl) | spcl_trd_fl == \"\",\n      is.na(asof_cd) | asof_cd == \"\"\n    )\n\n\n  # Output ------------------------------------------------------------------\n  # Only keep necessary columns\n  trace_final &lt;- trace_add_filters |&gt;\n    arrange(cusip_id, trd_exctn_dt, trd_exctn_tm) |&gt;\n    select(\n      cusip_id, trd_exctn_dt, trd_exctn_tm,\n      rptd_pr, entrd_vol_qt, yld_pt, rpt_side_cd, cntra_mp_id\n    ) |&gt;\n    mutate(trd_exctn_tm = format(\n      as_datetime(\n        trd_exctn_tm, \n        tz = \"America/New_York\"), \n      \"%H:%M:%S\"\n      )\n    )\n  \n  # Return\n  return(trace_final)\n}\n\n\n\n\n\nReferences\n\nDick-Nielsen, Jens. 2009. “Liquidity biases in TRACE.” The Journal of Fixed Income 19 (2): 43–55. https://doi.org/10.3905/jfi.2009.19.2.043.\n\n\n———. 2014. “How to clean enhanced TRACE data.” Working Paper. https://ssrn.com/abstract=2337908.",
    "crumbs": [
      "R",
      "Appendix",
      "Clean Enhanced TRACE with R"
    ]
  },
  {
    "objectID": "r/cover-and-logo-design.html",
    "href": "r/cover-and-logo-design.html",
    "title": "Cover and Logo Design",
    "section": "",
    "text": "The cover of the book is inspired by the fast growing generative art community in R. Generative art refers to art that in whole or in part has been created with the use of an autonomous system. Instead of creating random dynamics, we rely on what is core to the book: The evolution of financial markets. Each circle in the cover figure corresponds to daily market return within one year of our sample. Deviations from the circle line indicate positive or negative returns. The colors are determined by the standard deviation of market returns during the particular year. The few lines of code below replicate the entire figure. We use the Wes Andersen color palette (also throughout the entire book), provided by the package wesanderson (Ram and Wickham 2018)\n\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff3_daily &lt;- tbl(\n  tidy_finance,\n  \"factors_ff3_daily\"\n) |&gt;\n  collect()\n\ndata_plot &lt;- factors_ff3_daily |&gt;\n  select(date, mkt_excess) |&gt;\n  group_by(year = floor_date(date, \"year\")) |&gt;\n  mutate(group_id = cur_group_id())\n\ndata_plot &lt;- data_plot |&gt;\n  group_by(group_id) |&gt;\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) |&gt;\n  filter(year &gt;= \"1962-01-01\" & year &lt;= \"2021-12-31\")\n\nlevels &lt;- data_plot |&gt;\n  distinct(group_id, vola) |&gt;\n  arrange(vola) |&gt;\n  pull(vola)\n\ncp &lt;- coord_polar(\n  direction = -1,\n  clip = \"on\"\n)\n\ncp$is_free &lt;- function() TRUE\ncolors &lt;- wes_palette(\"Zissou1\",\n  n_groups(data_plot),\n  type = \"continuous\"\n)\n\ncover &lt;- data_plot |&gt;\n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    group = group_id,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id,\n    ncol = 10,\n    scales = \"free\"\n  ) +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\nggsave(\n plot = cover,\n width = 10,\n height = 6,\n filename = \"images/cover.png\",\n bg = \"white\"\n)\n\nTo generate our logo, we focus on year 2021 - the end of the sample period at the time we published tidy-finance.org for the first time.\n\nlogo &lt;- data_plot |&gt;\n  ungroup() |&gt; \n  filter(year == \"2021-01-01\") |&gt; \n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) +\n  scale_fill_manual(values =  \"white\") \n\nggsave(\n plot = logo,\n width = 840,\n height = 840,\n units = \"px\",\n filename = \"images/logo-website-white.png\",\n)\n\nggsave(\n plot = logo +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 840,\n height = 840,\n units = \"px\",\n filename = \"images/logo-website.png\",\n)\n\nHere is the code to generate the vector graphics for our buttons.\n\nbutton_r &lt;- data_plot |&gt;\n  ungroup() |&gt; \n  filter(year == \"2000-01-01\") |&gt; \n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) \n\nggsave(\n plot = button_r +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-r-blue.svg\",\n)\n\nggsave(\n plot = button_r +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[4]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-r-orange.svg\",\n)\n\nbutton_python &lt;- data_plot |&gt;\n  ungroup() |&gt; \n  filter(year == \"1991-01-01\") |&gt; \n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) \n\nggsave(\n plot = button_python +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-python-blue.svg\",\n)\n\nggsave(\n plot = button_python +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[4]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-python-orange.svg\",\n)\n\n\n\n\n\nReferences\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson palette generator. https://CRAN.R-project.org/package=wesanderson.",
    "crumbs": [
      "R",
      "Appendix",
      "Cover and Logo Design"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html",
    "href": "r/factor-selection-via-machine-learning.html",
    "title": "Factor Selection via Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nThe aim of this chapter is twofold. From a data science perspective, we introduce tidymodels, a collection of packages for modeling and machine learning (ML) using tidyverse principles. tidymodels comes with a handy workflow for all sorts of typical prediction tasks. From a finance perspective, we address the notion of factor zoo (Cochrane 2011) using ML methods. We introduce Lasso and Ridge regression as a special case of penalized regression models. Then, we explain the concept of cross-validation for model tuning with Elastic Net regularization as a popular example. We implement and showcase the entire cycle from model specification, training, and forecast evaluation within the tidymodels universe. While the tools can generally be applied to an abundance of interesting asset pricing problems, we apply penalized regressions for identifying macroeconomic variables and asset pricing factors that help explain a cross-section of industry portfolios.\nIn previous chapters, we illustrate that stock characteristics such as size provide valuable pricing information in addition to the market beta. Such findings question the usefulness of the Capital Asset Pricing Model. In fact, during the last decades, financial economists discovered a plethora of additional factors which may be correlated with the marginal utility of consumption (and would thus deserve a prominent role in pricing applications). The search for factors that explain the cross-section of expected stock returns has produced hundreds of potential candidates, as noted more recently by Harvey, Liu, and Zhu (2016), Mclean and Pontiff (2016), and Hou, Xue, and Zhang (2020). Therefore, given the multitude of proposed risk factors, the challenge these days rather is: do we believe in the relevance of 300+ risk factors? During recent years, promising methods from the field of ML got applied to common finance applications. We refer to Mullainathan and Spiess (2017) for a treatment of ML from the perspective of an econometrician, Nagel (2021) for an excellent review of ML practices in asset pricing, Easley et al. (2020) for ML applications in (high-frequency) market microstructure, and Dixon, Halperin, and Bilokon (2020) for a detailed treatment of all methodological aspects.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "href": "r/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "title": "Factor Selection via Machine Learning",
    "section": "Brief Theoretical Background",
    "text": "Brief Theoretical Background\nThis is a book about doing empirical work in a tidy manner, and we refer to any of the many excellent textbook treatments of ML methods and especially penalized regressions for some deeper discussion. Excellent material is provided, for instance, by Hastie, Tibshirani, and Friedman (2009), Gareth et al. (2013), and De Prado (2018). Instead, we briefly summarize the idea of Lasso and Ridge regressions as well as the more general Elastic Net. Then, we turn to the fascinating question on how to implement, tune, and use such models with the tidymodels workflow.\nTo set the stage, we start with the definition of a linear model: suppose we have data \\((y_t, x_t), t = 1,\\ldots, T\\), where \\(x_t\\) is a \\((K \\times 1)\\) vector of regressors and \\(y_t\\) is the response for observation \\(t\\). The linear model takes the form \\(y_t = \\beta' x_t + \\varepsilon_t\\) with some error term \\(\\varepsilon_t\\) and has been studied in abundance. The well-known ordinary-least square (OLS) estimator for the \\((K \\times 1)\\) vector \\(\\beta\\) minimizes the sum of squared residuals and is then \\[\\hat{\\beta}^\\text{ols} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t.\\] \nWhile we are often interested in the estimated coefficient vector \\(\\hat\\beta^\\text{ols}\\), ML is about the predictive performance most of the time. For a new observation \\(\\tilde{x}_t\\), the linear model generates predictions such that \\[\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta^\\text{ols}{}' \\tilde x_t.\\] Is this the best we can do? Not really: instead of minimizing the sum of squared residuals, penalized linear models can improve predictive performance by choosing other estimators \\(\\hat{\\beta}\\) with lower variance than the estimator \\(\\hat\\beta^\\text{ols}\\). At the same time, it seems appealing to restrict the set of regressors to a few meaningful ones, if possible. In other words, if \\(K\\) is large (such as for the number of proposed factors in the asset pricing literature), it may be a desirable feature to select reasonable factors and set \\(\\hat\\beta^{\\text{ols}}_k = 0\\) for some redundant factors.\nIt should be clear that the promised benefits of penalized regressions, i.e., reducing the mean squared error (MSE), come at a cost. In most cases, reducing the variance of the estimator introduces a bias such that \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). What is the effect of such a bias-variance trade-off? To understand the implications, assume the following data-generating process for \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2)\\] We want to recover \\(f(x)\\), which denotes some unknown functional which maps the relationship between \\(x\\) and \\(y\\). While the properties of \\(\\hat\\beta^\\text{ols}\\) as an unbiased estimator may be desirable under some circumstances, they are certainly not if we consider predictive accuracy. Alternative predictors \\(\\hat{f}(x)\\) could be more desirable: For instance, the MSE depends on our model choice as follows: \\[\\begin{aligned}\nMSE &=E((y-\\hat{f}(x))^2)=E((f(x)+\\epsilon-\\hat{f}(x))^2)\\\\\n&= \\underbrace{E((f(x)-\\hat{f}(x))^2)}_{\\text{total quadratic error}}+\\underbrace{E(\\epsilon^2)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(x)^2\\right)+E\\left(f(x)^2\\right)-2E\\left(f(x)\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(x)^2\\right)+f(x)^2-2f(x)E\\left(\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(x)\\right)}_{\\text{variance of model}}+ \\underbrace{E\\left((f(x)-\\hat{f}(x))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2.\n\\end{aligned}\\] While no model can reduce \\(\\sigma_\\varepsilon^2\\), a biased estimator with small variance may have a lower MSE than an unbiased estimator.\n\nRidge regression\n\nOne biased estimator is known as Ridge regression. Hoerl and Kennard (1970) propose to minimize the sum of squared errors while simultaneously imposing a penalty on the \\(L_2\\) norm of the parameters \\(\\hat\\beta\\). Formally, this means that for a penalty factor \\(\\lambda\\geq 0\\), the minimization problem takes the form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq c\\). Here \\(c\\geq 0\\) is a constant that depends on the choice of \\(\\lambda\\). The larger \\(\\lambda\\), the smaller \\(c\\) (technically speaking, there is a one-to-one relationship between \\(\\lambda\\), which corresponds to the Lagrangian of the minimization problem above and \\(c\\)). Here, \\(X = \\left(x_1  \\ldots  x_T\\right)'\\) and \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). A closed-form solution for the resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda I\\right)^{-1}X'y.\\] A couple of observations are worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta^\\text{ols}\\) for \\(\\lambda = 0\\) and \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) for \\(\\lambda\\rightarrow \\infty\\). Also for \\(\\lambda &gt; 0\\), \\(\\left(X'X + \\lambda I\\right)\\) is non-singular even if \\(X'X\\) is which means that \\(\\hat\\beta^\\text{ridge}\\) exists even if \\(\\hat\\beta\\) is not defined. However, note also that the Ridge estimator requires careful choice of the hyperparameter \\(\\lambda\\) which controls the amount of regularization: a larger value of \\(\\lambda\\) implies shrinkage of the regression coefficient toward 0, a smaller value of \\(\\lambda\\) reduces the bias of the resulting estimator.\n\nNote that \\(X\\) usually contains an intercept column with ones. As a general rule, the associated intercept coefficient is not penalized. In practice, this often implies that \\(y\\) is simply demeaned before computing \\(\\hat\\beta^\\text{ridge}\\).\n\nWhat about the statistical properties of the Ridge estimator? First, the bad news is that \\(\\hat\\beta^\\text{ridge}\\) is a biased estimator of \\(\\beta\\). However, the good news is that (under homoscedastic error terms) the variance of the Ridge estimator is guaranteed to be smaller than the variance of the OLS estimator. We encourage you to verify these two statements in the Exercises. As a result, we face a trade-off: The Ridge regression sacrifices some unbiasedness to achieve a smaller variance than the OLS estimator.\n\n\nLasso\n\nAn alternative to Ridge regression is the Lasso (least absolute shrinkage and selection operator). Similar to Ridge regression, the Lasso (Tibshirani 1996) is a penalized and biased estimator. The main difference to Ridge regression is that Lasso does not only shrink coefficients but effectively selects variables by setting coefficients for irrelevant variables to zero. Lasso implements a \\(L_1\\) penalization on the parameters such that: \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right)\\text{ s.t. } \\sum\\limits_{k=1}^K|\\beta_k| &lt; c(\\lambda).\\] There is no closed-form solution for \\(\\hat\\beta^\\text{Lasso}\\) in the above maximization problem, but efficient algorithms exist (e.g., the R package glmnet). Like for Ridge regression, the hyperparameter \\(\\lambda\\) has to be specified beforehand.\n\n\nElastic Net\nThe Elastic Net (Zou and Hastie 2005) combines \\(L_1\\) with \\(L_2\\) penalization and encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. This more general framework considers the following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2\\] Now, we have to choose two hyperparameters: the shrinkage factor \\(\\lambda\\) and the weighting parameter \\(\\rho\\). The Elastic Net resembles Lasso for \\(\\rho = 0\\) and Ridge regression for \\(\\rho = 1\\). While the R package glmnet provides efficient algorithms to compute the coefficients of penalized regressions, it is a good exercise to implement Ridge and Lasso estimation on your own before you use the glmnet package or the tidymodels back-end.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#data-preparation",
    "href": "r/factor-selection-via-machine-learning.html#data-preparation",
    "title": "Factor Selection via Machine Learning",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the required R packages and data. The main focus is on the workflow behind the tidymodels package collection (Kuhn and Wickham 2020). Kuhn and Silge (2018) provide a thorough introduction into all tidymodels components. glmnet (Simon et al. 2011) was developed and released in sync with Tibshirani (1996) and provides an R implementation of Elastic Net estimation. The package timetk (Dancho and Vaughan 2022) provides useful tools for time series data wrangling.\n\nlibrary(RSQLite)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(scales)\nlibrary(furrr)\nlibrary(glmnet)\nlibrary(timetk)\n\nIn this analysis, we use four different data sources that we load from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. We start with two different sets of factor portfolio returns which have been suggested as representing practical risk factor exposure and thus should be relevant when it comes to asset pricing applications.\n\nThe standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big, and high-minus-low book-to-market valuation sorts) defined in Fama and French (1992) and Fama and French (1993)\nMonthly q-factor returns from Hou, Xue, and Zhang (2014). The factors contain the size factor, the investment factor, the return-on-equity factor, and the expected growth factor\n\nNext, we include macroeconomic predictors which may predict the general stock market economy. Macroeconomic variables effectively serve as conditioning information such that their inclusion hints at the relevance of conditional models instead of unconditional asset pricing. We refer the interested reader to Cochrane (2009) on the role of conditioning information.\n\nOur set of macroeconomic predictors comes from Welch and Goyal (2008). The data has been updated by the authors until 2021 and contains monthly variables that have been suggested as good predictors for the equity premium. Some of the variables are the dividend price ratio, earnings price ratio, stock variance, net equity expansion, treasury bill rate, and inflation\n\nFinally, we need a set of test assets. The aim is to understand which of the plenty factors and macroeconomic variable combinations prove helpful in explaining our test assets’ cross-section of returns. In line with many existing papers, we use monthly portfolio returns from 10 different industries according to the definition from Kenneth French’s homepage as test assets.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  collect() |&gt;\n  rename_with(~ str_c(\"factor_ff_\", .), -month)\n\nfactors_q_monthly &lt;- tbl(tidy_finance, \"factors_q_monthly\") |&gt;\n  collect() |&gt;\n  rename_with(~ str_c(\"factor_q_\", .), -month)\n\nmacro_predictors &lt;- tbl(tidy_finance, \"macro_predictors\") |&gt;\n  collect() |&gt;\n  rename_with(~ str_c(\"macro_\", .), -month) |&gt;\n  select(-macro_rp_div)\n\nindustries_ff_monthly &lt;- tbl(tidy_finance, \"industries_ff_monthly\") |&gt;\n  collect() |&gt;\n  pivot_longer(-month,\n    names_to = \"industry\", values_to = \"ret\"\n  ) |&gt;\n  arrange(desc(industry)) |&gt; \n  mutate(industry = as_factor(industry))\n\nWe combine all the monthly observations into one dataframe.\n\ndata &lt;- industries_ff_monthly |&gt;\n  left_join(factors_ff3_monthly, join_by(month)) |&gt;\n  left_join(factors_q_monthly, join_by(month)) |&gt;\n  left_join(macro_predictors, join_by(month)) |&gt;\n  mutate(\n    ret = ret - factor_ff_rf\n  ) |&gt;\n  select(month, industry, ret_excess = ret, everything()) |&gt;\n  drop_na()\n\nOur data contains 22 columns of regressors with the 13 macro-variables and 8 factor returns for each month. Figure 1 provides summary statistics for the 10 monthly industry excess returns in percent.\n\ndata |&gt;\n  group_by(industry) |&gt;\n  ggplot(aes(x = industry, y = ret_excess)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Excess return distributions by industry in percent\"\n  ) +\n  scale_y_continuous(\n    labels = percent\n  )\n\n\n\n\n\n\n\nFigure 1: The box plots show the monthly dispersion of returns for 10 different industries.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#the-tidymodels-workflow",
    "href": "r/factor-selection-via-machine-learning.html#the-tidymodels-workflow",
    "title": "Factor Selection via Machine Learning",
    "section": "The tidymodels Workflow",
    "text": "The tidymodels Workflow\nTo illustrate penalized linear regressions, we employ the tidymodels collection of packages for modeling and ML using tidyverse principles. You can simply use install.packages(\"tidymodels\") to get access to all the related packages. We recommend checking out the work of Kuhn and Silge (2018): They continuously write on their great book ‘Tidy Modeling with R’ using tidy principles.\nThe tidymodels workflow encompasses the main stages of the modeling process: pre-processing of data, model fitting, and post-processing of results. As we demonstrate below, tidymodels provides efficient workflows that you can update with low effort.\nUsing the ideas of Ridge and Lasso regressions, the following example guides you through (i) pre-processing the data (data split and variable mutation), (ii) building models, (iii) fitting models, and (iv) tuning models to create the “best” possible predictions.\nTo start, we restrict our analysis to just one industry: Manufacturing. We first split the sample into a training and a test set. For that purpose, tidymodels provides the function initial_time_split() from the rsample package (Silge et al. 2022). The split takes the last 20% of the data as a test set, which is not used for any model tuning. We use this test set to evaluate the predictive accuracy in an out-of-sample scenario.\n\nsplit &lt;- initial_time_split(\n  data |&gt;\n    filter(industry == \"manuf\") |&gt;\n    select(-industry),\n  prop = 4 / 5\n)\nsplit\n\n&lt;Training/Testing/Total&gt;\n&lt;536/135/671&gt;\n\n\nThe object split simply keeps track of the observations of the training and the test set. We can call the training set with training(split), while we can extract the test set with testing(split).\n\nPre-process data\nRecipes help you pre-process your data before training your model. Recipes are a series of pre-processing steps such as variable selection, transformation, or conversion of qualitative predictors to indicator variables. Each recipe starts with a formula that defines the general structure of the dataset and the role of each variable (regressor or dependent variable). For our dataset, our recipe contains the following steps before we fit any model:\n\nOur formula defines that we want to explain excess returns with all available predictors. The regression equation thus takes the form \\[r_{t} = \\alpha_0 + \\left(\\tilde f_t \\otimes \\tilde z_t\\right)B + \\varepsilon_t \\] where \\(r_t\\) is the vector of industry excess returns at time \\(t\\) and \\(\\tilde f_t\\) and \\(\\tilde z_t\\) are the (standardized) vectors of factor portfolio returns and macroeconomic variables\nWe exclude the column month from the analysis\nWe include all interaction terms between factors and macroeconomic predictors\nWe demean and scale each regressor such that the standard deviation is one\n\n\nrec &lt;- recipe(ret_excess ~ ., data = training(split)) |&gt;\n  step_rm(month) |&gt;\n  step_interact(terms = ~ contains(\"factor\"):contains(\"macro\")) |&gt;\n  step_normalize(all_predictors())\n\nA table of all available recipe steps can be found in the tidymodels documentation. As of 2024, more than 150 different processing steps are available! One important point: The definition of a recipe does not trigger any calculations yet but rather provides a description of the tasks to be applied. As a result, it is very easy to reuse recipes for different models and thus make sure that the outcomes are comparable as they are based on the same input. In the example above, it does not make a difference whether you use the input data = training(split) or data = testing(split). All that matters at this early stage are the column names and types.\nWe can apply the recipe to any data with a suitable structure. The code below combines two different functions: prep() estimates the required parameters from a training set that can be applied to other datasets later. bake() applies the processed computations to new data.\n\ndata_prep &lt;- prep(rec, training(split))\n\nThe object data_prep contains information related to the different preprocessing steps applied to the training data: E.g., it is necessary to compute sample means and standard deviations to center and scale the variables.\n\ndata_bake &lt;- bake(data_prep,\n  new_data = testing(split)\n)\ndata_bake\n\n# A tibble: 135 × 126\n  factor_ff_mkt_excess factor_ff_smb factor_ff_hml factor_ff_rf\n                 &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1              -1.73          -1.10        -0.711         -1.78\n2               2.35           0.949       -0.0937        -1.78\n3              -0.156         -0.122       -0.284         -1.78\n4               0.0646        -0.256        0.425         -1.78\n5               0.995          0.563       -0.456         -1.78\n# ℹ 130 more rows\n# ℹ 122 more variables: factor_q_me &lt;dbl&gt;, factor_q_ia &lt;dbl&gt;,\n#   factor_q_roe &lt;dbl&gt;, factor_q_eg &lt;dbl&gt;, macro_dp &lt;dbl&gt;,\n#   macro_dy &lt;dbl&gt;, macro_ep &lt;dbl&gt;, macro_de &lt;dbl&gt;,\n#   macro_svar &lt;dbl&gt;, macro_bm &lt;dbl&gt;, macro_ntis &lt;dbl&gt;,\n#   macro_tbl &lt;dbl&gt;, macro_lty &lt;dbl&gt;, macro_ltr &lt;dbl&gt;,\n#   macro_tms &lt;dbl&gt;, macro_dfy &lt;dbl&gt;, macro_infl &lt;dbl&gt;, …\n\n\nNote that the resulting data contains the 132 observations from the test set and 126 columns. Why so many? Recall that the recipe states to compute every possible interaction term between the factors and predictors, which increases the dimension of the data matrix substantially.\nYou may ask at this stage: why should I use a recipe instead of simply using the data wrangling commands such as mutate() or select()? tidymodels beauty is that a lot is happening under the hood. Recall, that for the simple scaling step, you actually have to compute the standard deviation of each column, then store this value, and apply the identical transformation to a different dataset, e.g., testing(split). A prepped recipe stores these values and hands them on once you bake() a novel dataset. Easy as pie with tidymodels, isn’t it?\n\n\nBuild a model\n Next, we can build an actual model based on our pre-processed data. In line with the definition above, we estimate regression coefficients of a Lasso regression such that we get \\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned}\\] We want to emphasize that the tidymodels workflow for any model is very similar, irrespective of the specific model. As you will see further below, it is straightforward to fit Ridge regression coefficients and, later, Neural networks or Random forests with basically the same code. The structure is always as follows: create a so-called workflow() and use the fit() function. A table with all available model APIs is available here. For now, we start with the linear regression model with a given value for the penalty factor \\(\\lambda\\). In the setup below, mixture denotes the value of \\(\\rho\\), hence setting mixture = 1 implies the Lasso.\n\nlm_model &lt;- linear_reg(\n  penalty = 0.0001,\n  mixture = 1\n) |&gt;\n  set_engine(\"glmnet\", intercept = FALSE)\n\nThat’s it - we are done! The object lm_model contains the definition of our model with all required information. Note that set_engine(\"glmnet\") indicates the API character of the tidymodels workflow: Under the hood, the package glmnet is doing the heavy lifting, while linear_reg() provides a unified framework to collect the inputs. The workflow ends with combining everything necessary for the serious data science workflow, namely, a recipe and a model.\n\nlm_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(lm_model)\nlm_fit\n\n══ Workflow ═════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ─────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_rm()\n• step_interact()\n• step_normalize()\n\n── Model ────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-04\n  mixture = 1\n\nEngine-Specific Arguments:\n  intercept = FALSE\n\nComputational engine: glmnet \n\n\n\n\nFit a model\nWith the workflow from above, we are ready to use fit(). Typically, we use training data to fit the model. The training data is pre-processed according to our recipe steps, and the Lasso regression coefficients are computed. First, we focus on the predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) Figure 2 illustrates the projections for the entire time series of the manufacturing industry portfolio returns. The grey area indicates the out-of-sample period, which we did not use to fit the model.\n\npredicted_values &lt;- lm_fit |&gt;\n  fit(data = training(split)) |&gt;\n  predict(data |&gt; filter(industry == \"manuf\")) |&gt;\n  bind_cols(data |&gt; filter(industry == \"manuf\")) |&gt;\n  select(month,\n    \"Fitted value\" = .pred,\n    \"Realization\" = ret_excess\n  ) |&gt;\n  pivot_longer(-month, names_to = \"Variable\")\n\n\npredicted_values |&gt;\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = Variable,\n    linetype = Variable\n    )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"Monthly realized and fitted manufacturing industry risk premia\"\n  ) +\n  scale_x_date(\n    breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"5 years\"\n      )\n    },\n    minor_breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"1 years\"\n      )\n    },\n    expand = c(0, 0),\n    labels = date_format(\"%Y\")\n  ) +\n  scale_y_continuous(\n    labels = percent\n  ) +\n  annotate(\"rect\",\n    xmin = testing(split) |&gt; pull(month) |&gt; min(),\n    xmax = testing(split) |&gt; pull(month) |&gt; max(),\n    ymin = -Inf, ymax = Inf,\n    alpha = 0.5, fill = \"grey70\"\n  )\n\n\n\n\n\n\n\nFigure 2: The grey area corresponds to the out of sample period.\n\n\n\n\n\nWhat do the estimated coefficients look like? To analyze these values and to illustrate the difference between the tidymodels workflow and the underlying glmnet package, it is worth computing the coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. The code below estimates the coefficients for the Lasso and Ridge regression for the processed training data sample. Note that glmnet actually takes a vector y and the matrix of regressors \\(X\\) as input. Moreover, glmnet requires choosing the penalty parameter \\(\\alpha\\), which corresponds to \\(\\rho\\) in the notation above. When using the tidymodels model API, such details do not need consideration.\n\nx &lt;- data_bake |&gt;\n  select(-ret_excess) |&gt;\n  as.matrix()\ny &lt;- data_bake |&gt; pull(ret_excess)\n\nfit_lasso &lt;- glmnet(\n  x = x,\n  y = y,\n  alpha = 1,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nfit_ridge &lt;- glmnet(\n  x = x,\n  y = y,\n  alpha = 0,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nThe objects fit_lasso and fit_ridge contain an entire sequence of estimated coefficients for multiple values of the penalty factor \\(\\lambda\\). Figure 3 illustrates the trajectories of the regression coefficients as a function of the penalty factor. Both Lasso and Ridge coefficients converge to zero as the penalty factor increases.\n\nbind_rows(\n  tidy(fit_lasso) |&gt; mutate(Model = \"Lasso\"),\n  tidy(fit_ridge) |&gt; mutate(Model = \"Ridge\")\n) |&gt;\n  rename(\"Variable\" = term) |&gt;\n  ggplot(aes(x = lambda, y = estimate, color = Variable)) +\n  geom_line() +\n  scale_x_log10() +\n  facet_wrap(~Model, scales = \"free_x\") +\n  labs(\n    x = \"Penalty factor (lambda)\", y = NULL,\n    title = \"Estimated coefficient paths for different penalty factors\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 3: The penalty parameters are chosen iteratively to resemble the path from no penalization to a model that excludes all variables.\n\n\n\n\n\n\nOne word of caution: The package glmnet computes estimates of the coefficients \\(\\hat\\beta\\) based on numerical optimization procedures. As a result, the estimated coefficients for the special case with no regularization (\\(\\lambda = 0\\)) can deviate from the standard OLS estimates.\n\n\n\nTune a model\nTo compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , we simply imposed a value for the penalty hyperparameter \\(\\lambda\\). Model tuning is the process of optimally selecting such hyperparameters. tidymodels provides extensive tuning options based on so-called cross-validation. Again, we refer to any treatment of cross-validation to get a more detailed discussion of the statistical underpinnings. Here we focus on the general idea and the implementation with tidymodels.\nThe goal for choosing \\(\\lambda\\) (or any other hyperparameter, e.g., \\(\\rho\\) for the Elastic Net) is to find a way to produce predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, the MSPE is not directly observable. We can only compute an estimate because our data is random and because we do not observe the entire population.\nObviously, if we train an algorithm on the same data that we use to compute the error, our estimate \\(\\text{MSPE}\\) would indicate way better predictive accuracy than what we can expect in real out-of-sample data. The result is called overfitting.\nCross-validation is a technique that allows us to alleviate this problem. We approximate the true MSPE as the average of many MSPE obtained by creating predictions for \\(K\\) new random samples of the data, none of them used to train the algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). In practice, this is done by carving out a piece of our data and pretending it is an independent sample. We again divide the data into a training set and a test set. The MSPE on the test set is our measure for actual predictive ability, while we use the training set to fit models with the aim to find the optimal hyperparameter values. To do so, we further divide our training sample into (several) subsets, fit our model for a grid of potential hyperparameter values (e.g., \\(\\lambda\\)), and evaluate the predictive accuracy on an independent sample. This works as follows:\n\nSpecify a grid of hyperparameters\nObtain predictors \\(\\hat{y}_i(\\lambda)\\) to denote the predictors for the used parameters \\(\\lambda\\)\nCompute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2\n\\] With K-fold cross-validation, we do this computation \\(K\\) times. Simply pick a validation set with \\(M=T/K\\) observations at random and think of these as random samples \\(y_1^k, \\dots, y_{\\tilde{T}}^k\\), with \\(k=1\\)\n\nHow should you pick \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original data. However, larger values of \\(K\\) will have much higher computation time. tidymodels provides all required tools to conduct \\(K\\)-fold cross-validation. We just have to update our model specification and let tidymodels know which parameters to tune. In our case, we specify the penalty factor \\(\\lambda\\) as well as the mixing factor \\(\\rho\\) as free parameters. Note that it is simple to change an existing workflow with update_model().\n\nlm_model &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) |&gt;\n  set_engine(\"glmnet\")\n\nlm_fit &lt;- lm_fit |&gt;\n  update_model(lm_model)\n\nFor our sample, we consider a time-series cross-validation sample. This means that we tune our models with 20 random samples of length five years with a validation period of four years. For a grid of possible hyperparameters, we then fit the model for each fold and evaluate \\(\\hat{\\text{MSPE}}\\) in the corresponding validation set. Finally, we select the model specification with the lowest MSPE in the validation set. First, we define the cross-validation folds based on our training data only.\n\ndata_folds &lt;- time_series_cv(\n  data        = training(split),\n  date_var    = month,\n  initial     = \"5 years\",\n  assess      = \"48 months\",\n  cumulative  = FALSE,\n  slice_limit = 20\n)\ndata_folds\n\n# Time Series Cross Validation Plan \n# A tibble: 20 × 2\n  splits          id     \n  &lt;list&gt;          &lt;chr&gt;  \n1 &lt;split [60/48]&gt; Slice01\n2 &lt;split [60/48]&gt; Slice02\n3 &lt;split [60/48]&gt; Slice03\n4 &lt;split [60/48]&gt; Slice04\n5 &lt;split [60/48]&gt; Slice05\n# ℹ 15 more rows\n\n\nThen, we evaluate the performance for a grid of different penalty values. tidymodels provides functionalities to construct a suitable grid of hyperparameters with grid_regular. The code chunk below creates a \\(10 \\times 3\\) hyperparameters grid. Then, the function tune_grid() evaluates all the models for each fold.\n\nlm_tune &lt;- lm_fit |&gt;\n  tune_grid(\n    resample = data_folds,\n    grid = grid_regular(penalty(), mixture(), levels = c(20, 3)),\n    metrics = metric_set(rmse)\n  )\n\nAfter the tuning process, we collect the evaluation metrics (the root mean-squared error in our example) to identify the optimal model. Figure 4 illustrates the average validation set’s root mean-squared error for each value of \\(\\lambda\\) and \\(\\rho\\).\n\nautoplot(lm_tune) + \n  aes(linetype = `Proportion of Lasso Penalty`) + \n  guides(linetype = \"none\") +\n  labs(\n    x = \"Penalty factor (lambda)\",\n    y = \"Root MSPE\",\n    title = \"Root MSPE for different penalty factors\"\n  ) + \n  scale_x_log10()\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\n\n\n\nFigure 4: Evaluation of manufacturing excess returns for different penalty factors (lambda) and proportions of Lasso penalty (rho). 1.0 indicates Lasso, 0.5 indicates Elastic Net, and 0.0 indicates Ridge.\n\n\n\n\n\nFigure 4 shows that the cross-validated MSPE drops for Lasso and Elastic Net and spikes afterward. For Ridge regression, the MSPE increases above a certain threshold. Recall that the larger the regularization, the more restricted the model becomes. Thus, we would choose the model with the lowest MSPE.\n\n\nParallelized workflow\nOur starting point was the question: Which factors determine industry returns? While Avramov et al. (2023) provide a Bayesian analysis related to the research question above, we choose a simplified approach: To illustrate the entire workflow, we now run the penalized regressions for all ten industries. We want to identify relevant variables by fitting Lasso models for each industry returns time series. More specifically, we perform cross-validation for each industry to identify the optimal penalty factor \\(\\lambda\\). Then, we use the set of finalize_*()-functions that take a list or tibble of tuning parameter values and update objects with those values. After determining the best model, we compute the final fit on the entire training set and analyze the estimated coefficients.\nFirst, we define the Lasso model with one tuning parameter.\n\nlasso_model &lt;- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) |&gt;\n  set_engine(\"glmnet\")\n\nlm_fit &lt;- lm_fit |&gt;\n  update_model(lasso_model)\n\nThe following task can be easily parallelized to reduce computing time substantially. We use the parallelization capabilities of furrr. Note that we can also just recycle all the steps from above and collect them in a function.\n\nselect_variables &lt;- function(input) {\n  # Split into training and testing data\n  split &lt;- initial_time_split(input, prop = 4 / 5)\n\n  # Data folds for cross-validation\n  data_folds &lt;- time_series_cv(\n    data = training(split),\n    date_var = month,\n    initial = \"5 years\",\n    assess = \"48 months\",\n    cumulative = FALSE,\n    slice_limit = 20\n  )\n\n  # Model tuning with the Lasso model\n  lm_tune &lt;- lm_fit |&gt;\n    tune_grid(\n      resample = data_folds,\n      grid = grid_regular(penalty(), levels = c(10)),\n      metrics = metric_set(rmse)\n    )\n\n  # Identify the best model and fit with the training data\n  lasso_lowest_rmse &lt;- lm_tune |&gt; select_by_one_std_err(\"rmse\")\n  lasso_final &lt;- finalize_workflow(lm_fit, lasso_lowest_rmse)\n  lasso_final_fit &lt;- last_fit(lasso_final, split, metrics = metric_set(rmse))\n\n  # Extract the estimated coefficients\n  estimated_coefficients &lt;- lasso_final_fit |&gt;\n    extract_fit_parsnip() |&gt;\n    tidy() |&gt;\n    mutate(\n      term = str_remove_all(term, \"factor_|macro_|industry_\")\n    )\n\n  return(estimated_coefficients)\n}\n\n# Parallelization\nplan(multisession, workers = availableCores())\n\n# Computation by industry\nselected_factors &lt;- data |&gt;\n  nest(data = -industry) |&gt;\n  mutate(selected_variables = future_map(\n    data, select_variables,\n    .options = furrr_options(seed = TRUE)\n  ))\n\nWhat has just happened? In principle, exactly the same as before but instead of computing the Lasso coefficients for one industry, we did it for ten in parallel. The final option seed = TRUE is required to make the cross-validation process reproducible. Now, we just have to do some housekeeping and keep only variables that Lasso does not set to zero. We illustrate the results in a heat map in Figure 5.\n\nselected_factors |&gt;\n  unnest(selected_variables) |&gt;\n  filter(\n    term != \"(Intercept)\",\n    estimate != 0\n  ) |&gt;\n  add_count(term) |&gt;\n  mutate(\n    term = str_remove_all(term, \"NA|ff_|q_\"),\n    term = str_replace_all(term, \"_x_\", \" \"),\n    term = fct_reorder(as_factor(term), n),\n    term = fct_lump_min(term, min = 2),\n    selected = 1\n  ) |&gt;\n  filter(term != \"Other\") |&gt;\n  mutate(term = fct_drop(term)) |&gt;\n  complete(industry, term, fill = list(selected = 0)) |&gt;\n  ggplot(aes(industry,\n    term,\n    fill = as_factor(selected)\n  )) +\n  geom_tile() +\n  scale_x_discrete(guide = guide_axis(angle = 70)) +\n  scale_fill_manual(values = c(\"white\", \"grey30\")) +\n  theme(legend.position = \"None\") +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Selected variables for different industries\"\n  )\n\n\n\n\n\n\n\nFigure 5: The figure shows selected variables for different industries. Dark areas indicate that the estimated Lasso regression coefficient is not set to zero. White fields show which variables get assigned a value of exactly zero.\n\n\n\n\n\nThe heat map in Figure 5 conveys two main insights. First, we see a lot of white, which means that many factors, macroeconomic variables, and interaction terms are not relevant for explaining the cross-section of returns across the industry portfolios. In fact, only the market factor and the return-on-equity factor play a role for several industries. Second, there seems to be quite some heterogeneity across different industries. While barely any variable is selected by Lasso for Utilities, many factors are selected for, e.g., High-Tech and Durable, but they do not coincide at all. In other words, there seems to be a clear picture that we do not need many factors, but Lasso does not provide a factor that consistently provides pricing abilities across industries.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#exercises",
    "href": "r/factor-selection-via-machine-learning.html#exercises",
    "title": "Factor Selection via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and lambda and then returns the Ridge estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_2\\) penalty.\nCompute the \\(L_2\\) norm (\\(\\beta'\\beta\\)) for the regression coefficients based on the predictive regression from the previous exercise for a range of \\(\\lambda\\)’s and illustrate the effect of penalization in a suitable figure.\nNow, write a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and ’lambda` and then returns the Lasso estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_1\\) penalty.\nAfter you understand what Ridge and Lasso regressions are doing, familiarize yourself with the glmnet() package’s documentation. It is a thoroughly tested and well-established package that provides efficient code to compute the penalized regression coefficients for Ridge and Lasso and for combinations, commonly called Elastic Nets.\n\n\n\n\nFigure 1: The box plots show the monthly dispersion of returns for 10 different industries.\nFigure 2: The grey area corresponds to the out of sample period.\nFigure 3: The penalty parameters are chosen iteratively to resemble the path from no penalization to a model that excludes all variables.\nFigure 4: Evaluation of manufacturing excess returns for different penalty factors (lambda) and proportions of Lasso penalty (rho). 1.0 indicates Lasso, 0.5 indicates Elastic Net, and 0.0 indicates Ridge.\nFigure 5: The figure shows selected variables for different industries. Dark areas indicate that the estimated Lasso regression coefficient is not set to zero. White fields show which variables get assigned a value of exactly zero.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Factor Selection via Machine Learning"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html",
    "href": "r/fixed-effects-and-clustered-standard-errors.html",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we provide an intuitive introduction to the two popular concepts of fixed effects regressions and clustered standard errors. When working with regressions in empirical finance, you will sooner or later be confronted with discussions around how you deal with omitted variables bias and dependence in your residuals. The concepts we introduce in this chapter are designed to address such concerns.\nWe focus on a classical panel regression common to the corporate finance literature (e.g., Fazzari et al. 1988; Erickson and Whited 2012; Gulen and Ion 2015): firm investment modeled as a function that increases in firm cash flow and firm investment opportunities.\nTypically, this investment regression uses quarterly balance sheet data provided via Compustat because it allows for richer dynamics in the regressors and more opportunities to construct variables. As we focus on the implementation of fixed effects and clustered standard errors, we use the annual Compustat data from our previous chapters and leave the estimation using quarterly data as an exercise. We demonstrate below that the regression based on annual data yields qualitatively similar results to estimations based on quarterly data from the literature, namely confirming the positive relationships between investment and the two regressors.\nThe current chapter relies on the following set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(fixest)\nCompared to previous chapters, we introduce fixest (Bergé 2018) for the fixed effects regressions, the implementation of standard error clusters, and tidy estimation output.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and annual Compustat as data sources from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. In particular, Compustat provides balance sheet and income statement data on a firm level, while CRSP provides market valuations. \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(gvkey, month, mktcap) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n  select(datadate, gvkey, year, at, be, capx, oancf, txdb) |&gt;\n  collect()\n\nThe classical investment regressions model the capital investment of a firm as a function of operating cash flows and Tobin’s q, a measure of a firm’s investment opportunities. We start by constructing investment and cash flows which are usually normalized by lagged total assets of a firm. In the following code chunk, we construct a panel of firm-year observations, so we have both cross-sectional information on firms as well as time-series information for each firm.\n\ndata_investment &lt;- compustat |&gt;\n  mutate(month = floor_date(datadate, \"month\")) |&gt;\n  left_join(compustat |&gt;\n    select(gvkey, year, at_lag = at) |&gt;\n    mutate(year = year + 1),\n  join_by(gvkey, year)\n  ) |&gt;\n  filter(at &gt; 0, at_lag &gt; 0) |&gt;\n  mutate(\n    investment = capx / at_lag,\n    cash_flows = oancf / at_lag\n  )\n\ndata_investment &lt;- data_investment |&gt;\n  left_join(data_investment |&gt;\n    select(gvkey, year, investment_lead = investment) |&gt;\n    mutate(year = year - 1),\n    join_by(gvkey, year)\n  )\n\nTobin’s q is the ratio of the market value of capital to its replacement costs. It is one of the most common regressors in corporate finance applications (e.g., Fazzari et al. 1988; Erickson and Whited 2012). We follow the implementation of Gulen and Ion (2015) and compute Tobin’s q as the market value of equity (mktcap) plus the book value of assets (at) minus book value of equity (be) plus deferred taxes (txdb), all divided by book value of assets (at). Finally, we only keep observations where all variables of interest are non-missing, and the reported book value of assets is strictly positive.\n\ndata_investment &lt;- data_investment |&gt;\n  left_join(crsp_monthly,\n            join_by(gvkey, month)) |&gt;\n  mutate(tobins_q = (mktcap + at - be + txdb) / at) |&gt;\n  select(gvkey, year, investment_lead, cash_flows, tobins_q) |&gt;\n  drop_na()\n\nAs the variable construction typically leads to extreme values that are most likely related to data issues (e.g., reporting errors), many papers include winsorization of the variables of interest. Winsorization involves replacing values of extreme outliers with quantiles on the respective end. The following function implements the winsorization for any percentage cut that should be applied on either end of the distributions. In the specific example, we winsorize the main variables (investment, cash_flows, and tobins_q) at the one percent level.\n\nwinsorize &lt;- function(x, cut) {\n  x &lt;- replace(\n    x,\n    x &gt; quantile(x, 1 - cut, na.rm = T),\n    quantile(x, 1 - cut, na.rm = T)\n  )\n  x &lt;- replace(\n    x,\n    x &lt; quantile(x, cut, na.rm = T),\n    quantile(x, cut, na.rm = T)\n  )\n  return(x)\n}\n\ndata_investment &lt;- data_investment |&gt;\n  mutate(across(\n    c(investment_lead, cash_flows, tobins_q),\n    ~ winsorize(., 0.01)\n  ))\n\nBefore proceeding to any estimations, we highly recommend tabulating summary statistics of the variables that enter the regression. These simple tables allow you to check the plausibility of your numerical variables, as well as spot any obvious errors or outliers. Additionally, for panel data, plotting the time series of the variable’s mean and the number of observations is a useful exercise to spot potential problems.\n\ndata_investment |&gt;\n  pivot_longer(\n    cols = c(investment_lead, cash_flows, tobins_q),\n    names_to = \"measure\"\n  ) |&gt;\n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 9\n  measure      mean     sd    min      q05    q50   q95    max      n\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;\n1 cash_flows 0.0111 0.274  -1.56  -4.72e-1 0.0641 0.272  0.479 127468\n2 investmen… 0.0577 0.0772  0      6.77e-4 0.0327 0.206  0.464 127468\n3 tobins_q   2.00   1.70    0.573  7.94e-1 1.39   5.37  10.9   127468",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nTo illustrate fixed effects regressions, we use the fixest package, which is both computationally powerful and flexible with respect to model specifications. We start out with the basic investment regression using the simple model \\[ \\text{Investment}_{i,t+1} = \\alpha + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t},\\] where \\(\\varepsilon_t\\) is i.i.d. normally distributed across time and firms. We use the feols()-function to estimate the simple model so that the output has the same structure as the other regressions below, but you could also use lm().\n\nmodel_ols &lt;- feols(\n  fml = investment_lead ~ cash_flows + tobins_q,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_ols\n\nOLS estimation, Dep. Var.: investment_lead\nObservations: 127,468 \nStandard-errors: IID \n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  0.04254   0.000334   127.5 &lt; 2.2e-16 ***\ncash_flows   0.04899   0.000793    61.8 &lt; 2.2e-16 ***\ntobins_q     0.00732   0.000128    57.1 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.075516   Adj. R2: 0.042996\n\n\nAs expected, the regression output shows significant coefficients for both variables. Higher cash flows and investment opportunities are associated with higher investment. However, the simple model actually may have a lot of omitted variables, so our coefficients are most likely biased. As there is a lot of unexplained variation in our simple model (indicated by the rather low adjusted R-squared), the bias in our coefficients is potentially severe, and the true values could be above or below zero. Note that there are no clear cutoffs to decide when an R-squared is high or low, but it depends on the context of your application and on the comparison of different models for the same data.\nOne way to tackle the issue of omitted variable bias is to get rid of as much unexplained variation as possible by including fixed effects; i.e., model parameters that are fixed for specific groups (e.g., Wooldridge 2010). In essence, each group has its own mean in fixed effects regressions. The simplest group that we can form in the investment regression is the firm level. The firm fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t},\\] where \\(\\alpha_i\\) is the firm fixed effect and captures the firm-specific mean investment across all years. In fact, you could also compute firms’ investments as deviations from the firms’ average investments and estimate the model without the fixed effects. The idea of the firm fixed effect is to remove the firm’s average investment, which might be affected by firm-specific variables that you do not observe. For example, firms in a specific industry might invest more on average. Or you observe a young firm with large investments but only small concurrent cash flows, which will only happen in a few years. This sort of variation is unwanted because it is related to unobserved variables that can bias your estimates in any direction.\nTo include the firm fixed effect, we use gvkey (Compustat’s firm identifier) as follows:\n\nmodel_fe_firm &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_fe_firm\n\nOLS estimation, Dep. Var.: investment_lead\nObservations: 127,468 \nFixed-effects: gvkey: 14,350\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(&gt;|t|)    \ncash_flows   0.0140   0.000923    15.2 &lt; 2.2e-16 ***\ntobins_q     0.0109   0.000133    82.1 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.049717     Adj. R2: 0.532575\n                 Within R2: 0.057328\n\n\nThe regression output shows a lot of unexplained variation at the firm level that is taken care of by including the firm fixed effect as the adjusted R-squared rises above 50 percent. In fact, it is more interesting to look at the within R-squared that shows the explanatory power of a firm’s cash flow and Tobin’s q on top of the average investment of each firm. We can also see that the coefficients changed slightly in magnitude but not in sign.\nThere is another source of variation that we can get rid of in our setting: average investment across firms might vary over time due to macroeconomic factors that affect all firms, such as economic crises. By including year fixed effects, we can take out the effect of unobservables that vary over time. The two-way fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\alpha_t + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t},\\] where \\(\\alpha_t\\) is the time fixed effect. Here you can think of higher investments during an economic expansion with simultaneously high cash flows.\n\nmodel_fe_firmyear &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_fe_firmyear\n\nOLS estimation, Dep. Var.: investment_lead\nObservations: 127,468 \nFixed-effects: gvkey: 14,350,  year: 35\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(&gt;|t|)    \ncash_flows  0.01746   0.000902    19.4 &lt; 2.2e-16 ***\ntobins_q    0.00988   0.000131    75.4 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.048428     Adj. R2: 0.556373\n                 Within R2: 0.050144\n\n\nThe inclusion of time fixed effects did only marginally affect the R-squared and the coefficients, which we can interpret as a good thing as it indicates that the coefficients are not driven by an omitted variable that varies over time.\nHow can we further improve the robustness of our regression results? Ideally, we want to get rid of unexplained variation at the firm-year level, which means we need to include more variables that vary across firm and time and are likely correlated with investment. Note that we cannot include firm-year fixed effects in our setting because then cash flows and Tobin’s q are colinear with the fixed effects, and the estimation becomes void.\nBefore we discuss the properties of our estimation errors, we want to point out that regression tables are at the heart of every empirical analysis, where you compare multiple models. Fortunately, the etable() function provides a convenient way to tabulate the regression output (with many parameters to customize and even print the output in LaTeX). We recommend printing \\(t\\)-statistics rather than standard errors in regression tables because the latter are typically very hard to interpret across coefficients that vary in size. We also do not print p-values because they are sometimes misinterpreted to signal the importance of observed effects (Wasserstein and Lazar 2016). The \\(t\\)-statistics provide a consistent way to interpret changes in estimation uncertainty across different model specifications.\n\netable(\n  model_ols, model_fe_firm, model_fe_firmyear,\n  coefstat = \"tstat\", digits = 3, digits.stats = 3\n)\n\n                       model_ols   model_fe_firm model_fe_firm..\nDependent Var.:  investment_lead investment_lead investment_lead\n                                                                \nConstant        0.043*** (127.5)                                \ncash_flows       0.049*** (61.8) 0.014*** (15.2) 0.018*** (19.4)\ntobins_q         0.007*** (57.1) 0.011*** (82.1) 0.010*** (75.4)\nFixed-Effects:  ---------------- --------------- ---------------\ngvkey                         No             Yes             Yes\nyear                          No              No             Yes\n_______________ ________________ _______________ _______________\nVCOV type                    IID             IID             IID\nObservations             127,468         127,468         127,468\nR2                         0.043           0.585           0.606\nWithin R2                     --           0.057           0.050\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Clustering Standard Errors",
    "text": "Clustering Standard Errors\nApart from biased estimators, we usually have to deal with potentially complex dependencies of our residuals with each other. Such dependencies in the residuals invalidate the i.i.d. assumption of OLS and lead to biased standard errors. With biased OLS standard errors, we cannot reliably interpret the statistical significance of our estimated coefficients.\nIn our setting, the residuals may be correlated across years for a given firm (time-series dependence), or, alternatively, the residuals may be correlated across different firms (cross-section dependence). One of the most common approaches to dealing with such dependence is the use of clustered standard errors (Petersen 2008). The idea behind clustering is that the correlation of residuals within a cluster can be of any form. As the number of clusters grows, the cluster-robust standard errors become consistent (Donald and Lang 2007; Wooldridge 2010). A natural requirement for clustering standard errors in practice is hence a sufficiently large number of clusters. Typically, around at least 30 to 50 clusters are seen as sufficient (Cameron, Gelbach, and Miller 2011).\nInstead of relying on the iid assumption, we can use the cluster option in the feols-function as above. The code chunk below applies both one-way clustering by firm as well as two-way clustering by firm and year.\n\nmodel_cluster_firm &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = \"gvkey\",\n  data = data_investment\n)\n\nmodel_cluster_firmyear &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = c(\"gvkey\", \"year\"),\n  data = data_investment\n)\n\n The table below shows the comparison of the different assumptions behind the standard errors. In the first column, we can see highly significant coefficients on both cash flows and Tobin’s q. By clustering the standard errors on the firm level, the \\(t\\)-statistics of both coefficients drop in half, indicating a high correlation of residuals within firms. If we additionally cluster by year, we see a drop, particularly for Tobin’s q, again. Even after relaxing the assumptions behind our standard errors, both coefficients are still comfortably significant as the \\(t\\)-statistics are well above the usual critical values of 1.96 or 2.576 for two-tailed significance tests.\n\netable(\n  model_fe_firmyear, model_cluster_firm, model_cluster_firmyear,\n  coefstat = \"tstat\", digits = 3, digits.stats = 3\n)\n\n                model_fe_firm.. model_cluster.. model_cluster...1\nDependent Var.: investment_lead investment_lead   investment_lead\n                                                                 \ncash_flows      0.018*** (19.4) 0.018*** (11.2)   0.018*** (9.45)\ntobins_q        0.010*** (75.4) 0.010*** (35.5)   0.010*** (15.3)\nFixed-Effects:  --------------- ---------------   ---------------\ngvkey                       Yes             Yes               Yes\nyear                        Yes             Yes               Yes\n_______________ _______________ _______________   _______________\nVCOV type                   IID       by: gvkey  by: gvkey & year\nObservations            127,468         127,468           127,468\nR2                        0.606           0.606             0.606\nWithin R2                 0.050           0.050             0.050\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInspired by Abadie et al. (2017), we want to close this chapter by highlighting that choosing the right dimensions for clustering is a design problem. Even if the data is informative about whether clustering matters for standard errors, they do not tell you whether you should adjust the standard errors for clustering. Clustering at too aggregate levels can hence lead to unnecessarily inflated standard errors.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#exercises",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#exercises",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Exercises",
    "text": "Exercises\n\nEstimate the two-way fixed effects model with two-way clustered standard errors using quarterly Compustat data from WRDS. Note that you can access quarterly data via tbl(wrds, I(\"comp.fundq\")).\nFollowing Peters and Taylor (2017), compute Tobin’s q as the market value of outstanding equity mktcap plus the book value of debt (dltt + dlc) minus the current assets atc and everything divided by the book value of property, plant and equipment ppegt. What is the correlation between the measures of Tobin’s q? What is the impact on the two-way fixed effects regressions?",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Fixed Effects and Clustered Standard Errors"
    ]
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "This website is the online version of Tidy Finance with R, a book published via Chapman & Hall/CRC. The book is the result of a joint effort of Christoph Scheuch, Stefan Voigt, and Patrick Weiss.\nWe are grateful for any kind of feedback on every aspect of the book. So please get in touch with us via contact@tidy-finance.org if you spot typos, discover any issues that deserve more attention, or if you have suggestions for additional chapters and sections. Additionally, let us know if you found the text helpful. We look forward to hearing from you!\n\n\n\n\n\n\nSupport Tidy Finance\n\n\n\nBuy our book via your preferred vendor or support us with coffee here.\n\n\n\n\nFinancial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future.\n\n\n\nWe write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from the undergraduate to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed.\n\n\n\n\nThe book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common datasets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet, the data chapters provide an important background necessary for data management in all other chapters.\n\n\n\nThis book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham, Çetinkaya-Rundel, and Grolemund (2023) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).\n\n\n\n\nWe believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019).\n\n\n\nAs you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80 percent of data analysis is spent on preparing data. By tidying data, we want to structure datasets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021), and lubridate (Grolemund and Wickham 2011).\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book we use the native pipe |&gt;, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %&gt;% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to this blog post second edition by Hadley Wickham.\n\n\n\n\n\n\nWe met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is the Head of Artificial Intelligence at the social trading platform wikifolio.com. He is responsible for researching, designing, and prototyping of cutting-edge AI-driven products using R and Python. Before his focus on AI, he was responsible for product management and business intelligence and an external lecturer at the Vienna University of Economics and Business., where he taught finance students how to manage empirical projects.\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in leading journals in financial economics.\n\n\n\n\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows:\n\nScheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237.\n\n@book{scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org},\n  doi = {https://doi.org/10.1201/b23237}\n}\n\n\n\nThis book represents a snapshot of research practices and available data at a particular time. However, time does not stop. As you read this text, there is new data, packages used here have changed, and research practices might be updated. We as authors of Tidy Finance are committed to staying up-to-date and keeping up with the newest developments. Therefore, you can expect updates to Tidy Finance on a continuous basis. The best way for you to monitor the ongoing developments, is to check our online Changelog frequently.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#why-does-this-book-exist",
    "href": "r/index.html#why-does-this-book-exist",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "Financial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#who-should-read-this-book",
    "href": "r/index.html#who-should-read-this-book",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "We write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from the undergraduate to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#what-will-you-learn",
    "href": "r/index.html#what-will-you-learn",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "The book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common datasets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet, the data chapters provide an important background necessary for data management in all other chapters.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#what-wont-you-learn",
    "href": "r/index.html#what-wont-you-learn",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "This book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham, Çetinkaya-Rundel, and Grolemund (2023) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#why-r",
    "href": "r/index.html#why-r",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "We believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019).",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#why-tidy",
    "href": "r/index.html#why-tidy",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "As you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80 percent of data analysis is spent on preparing data. By tidying data, we want to structure datasets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021), and lubridate (Grolemund and Wickham 2011).\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book we use the native pipe |&gt;, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %&gt;% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to this blog post second edition by Hadley Wickham.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#about-the-authors",
    "href": "r/index.html#about-the-authors",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "We met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is the Head of Artificial Intelligence at the social trading platform wikifolio.com. He is responsible for researching, designing, and prototyping of cutting-edge AI-driven products using R and Python. Before his focus on AI, he was responsible for product management and business intelligence and an external lecturer at the Vienna University of Economics and Business., where he taught finance students how to manage empirical projects.\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in leading journals in financial economics.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#license",
    "href": "r/index.html#license",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "This book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows:\n\nScheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237.\n\n@book{scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org},\n  doi = {https://doi.org/10.1201/b23237}\n}",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/index.html#future-updates-and-changes",
    "href": "r/index.html#future-updates-and-changes",
    "title": "Tidy Finance with R",
    "section": "",
    "text": "This book represents a snapshot of research practices and available data at a particular time. However, time does not stop. As you read this text, there is new data, packages used here have changed, and research practices might be updated. We as authors of Tidy Finance are committed to staying up-to-date and keeping up with the newest developments. Therefore, you can expect updates to Tidy Finance on a continuous basis. The best way for you to monitor the ongoing developments, is to check our online Changelog frequently.",
    "crumbs": [
      "R",
      "Tidy Finance with R",
      "Tidy Finance with R"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html",
    "href": "r/option-pricing-via-machine-learning.html",
    "title": "Option Pricing via Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nThis chapter covers machine learning methods in option pricing. First, we briefly introduce regression trees, random forests, and neural networks; these methods are advocated as highly flexible universal approximators, capable of recovering highly non-linear structures in the data. As the focus is on implementation, we leave a thorough treatment of the statistical underpinnings to other textbooks from authors with a real comparative advantage on these issues. We show how to implement random forests and deep neural networks with tidy principles using tidymodels and the torch package for more complicated network structures.\nMachine learning (ML) is seen as a part of artificial intelligence. ML algorithms build a model based on training data in order to make predictions or decisions without being explicitly programmed to do so. While ML can be specified along a vast array of different branches, this chapter focuses on so-called supervised learning for regressions. The basic idea of supervised learning algorithms is to build a mathematical model for data that contains both the inputs and the desired outputs. In this chapter, we apply well-known methods such as random forests and neural networks to a simple application in option pricing. More specifically, we create an artificial dataset of option prices for different values based on the Black-Scholes pricing equation for call options. Then, we train different models to learn how to price call options without prior knowledge of the theoretical underpinnings of the famous option pricing equation by Black and Scholes (1973).\nThroughout this chapter, we need the following R packages.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(torch)\nlibrary(brulee)\nlibrary(hardhat)\nlibrary(ranger)\nlibrary(glmnet)\nThe package torch (Falbel et al. 2023) provides functionality to define and train neural networks and is based on PyTorch (Paszke et al. 2019), while brulee (Kuhn and Falbel 2023) provides several basic modeling functions that use the torch infrastructure. The package ranger (Wright and Ziegler 2017) provides a fast implementation for random forests and hardhat (Vaughan and Kuhn 2022) is a helper function to for robust data preprocessing at fit time and prediction time.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "href": "r/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "title": "Option Pricing via Machine Learning",
    "section": "Regression Trees and Random Forests",
    "text": "Regression Trees and Random Forests\nRegression trees are a popular ML approach for incorporating multiway predictor interactions. In Finance, regression trees are gaining popularity, also in the context of asset pricing (see, e.g., Bryzgalova, Pelger, and Zhu 2022). Trees possess a logic that departs markedly from traditional regressions. Trees are designed to find groups of observations that behave similarly to each other. A tree grows in a sequence of steps. At each step, a new branch sorts the data leftover from the preceding step into bins based on one of the predictor variables. This sequential branching slices the space of predictors into partitions and approximates the unknown function \\(f(x)\\) which yields the relation between the predictors \\(x\\) and the outcome variable \\(y\\) with the average value of the outcome variable within each partition. For a more thorough treatment of regression trees, we refer to Coqueret and Guida (2020).\nFormally, we partition the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). For any predictor \\(x\\) that falls within region \\(R_j\\), we estimate \\(f(x)\\) with the average of the training observations, \\(\\hat y_i\\), for which the associated predictor \\(x_i\\) is also in \\(R_j\\). Once we select a partition \\(x\\) to split in order to create the new partitions, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, called \\(R_1(j,s)\\) and \\(R_2(j,s)\\), which split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\): \\[R_1(j,s) = \\{x \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{x \\mid x_j \\geq s\\}.\\] To pick \\(j\\) and \\(s\\), we find the pair that minimizes the residual sum of square (RSS): \\[\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\\] As in Factor Selection via Machine Learning in the context of penalized regressions, the first relevant question is: what are the hyperparameter decisions? Instead of a regularization parameter, trees are fully determined by the number of branches used to generate a partition (sometimes one specifies the minimum number of observations in each final branch instead of the maximum number of branches).\nModels with a single tree may suffer from high predictive variance. Random forests address these shortcomings of decision trees. The goal is to improve the predictive performance and reduce instability by averaging multiple decision trees. A forest basically implies creating many regression trees and averaging their predictions. To assure that the individual trees are not the same, we use a bootstrap to induce randomness. More specifically, we build \\(B\\) decision trees \\(T_1, \\ldots, T_B\\) using the training sample. For that purpose, we randomly select features to be included in the building of each tree. For each observation in the test set, we then form a prediction \\(\\hat{y} = \\frac{1}{B}\\sum\\limits_{i=1}^B\\hat{y}_{T_i}\\).",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#neural-networks",
    "href": "r/option-pricing-via-machine-learning.html#neural-networks",
    "title": "Option Pricing via Machine Learning",
    "section": "Neural Networks",
    "text": "Neural Networks\nRoughly speaking, neural networks propagate information from an input layer, through one or multiple hidden layers, to an output layer. While the number of units (neurons) in the input layer is equal to the dimension of the predictors, the output layer usually consists of one neuron (for regression) or multiple neurons for classification. The output layer predicts the future data, similar to the fitted value in a regression analysis. Neural networks have theoretical underpinnings as universal approximators for any smooth predictive association (Hornik 1991). Their complexity, however, ranks neural networks among the least transparent, least interpretable, and most highly parameterized ML tools. In finance, applications of neural networks can be found in many different contexts, e.g., Avramov, Cheng, and Metzker (2022), Chen, Pelger, and Zhu (2023), and Gu, Kelly, and Xiu (2020).\nEach neuron applies a non-linear activation function \\(f\\) to its aggregated signal before sending its output to the next layer \\[x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right)\\] Here, \\(\\theta\\) are the parameters to fit, \\(N^l\\) denotes the number of units (a hyperparameter to tune), and \\(z_j\\) are the input variables which can be either the raw data or, in the case of multiple chained layers, the outcome from a previous layer \\(z_j = x_k-1\\). While the easiest case with \\(f(x) = \\alpha + \\beta x\\) resembles linear regression, typical activation functions are sigmoid (i.e., \\(f(x) = (1+e^{-x})^{-1}\\)) or ReLu (i.e., \\(f(x) = max(x, 0)\\)).\nNeural networks gain their flexibility from chaining multiple layers together. Naturally, this imposes many degrees of freedom on the network architecture for which no clear theoretical guidance exists. The specification of a neural network requires, at a minimum, a stance on depth (number of hidden layers), the activation function, the number of neurons, the connection structure of the units (dense or sparse), and the application of regularization techniques to avoid overfitting. Finally, learning means to choose optimal parameters relying on numerical optimization, which often requires specifying an appropriate learning rate. Despite these computational challenges, implementation in R is not tedious at all because we can use the API to torch.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#option-pricing",
    "href": "r/option-pricing-via-machine-learning.html#option-pricing",
    "title": "Option Pricing via Machine Learning",
    "section": "Option Pricing",
    "text": "Option Pricing\nTo apply ML methods in a relevant field of finance, we focus on option pricing. The application in its core is taken from Hull (2020). In its most basic form, call options give the owner the right but not the obligation to buy a specific stock (the underlying) at a specific price (the strike price \\(K\\)) at a specific date (the exercise date \\(T\\)). The Black–Scholes price (Black and Scholes 1973) of a call option for a non-dividend-paying underlying stock is given by \\[\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_1 - \\sigma\\sqrt{T})Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left(\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right)\n\\end{aligned}\n\\] where \\(C(S, T)\\) is the price of the option as a function of today’s stock price of the underlying, \\(S\\), with time to maturity \\(T\\), \\(r_f\\) is the risk-free interest rate, and \\(\\sigma\\) is the volatility of the underlying stock return. \\(\\Phi\\) is the cumulative distribution function of a standard normal random variable.\nThe Black-Scholes equation provides a way to compute the arbitrage-free price of a call option once the parameters \\(S, K, r_f, T\\), and \\(\\sigma\\) are specified (arguably, in a realistic context, all parameters are easy to specify except for \\(\\sigma\\) which has to be estimated). A simple R function allows computing the price as we do below.\n\nblack_scholes_price &lt;- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  \n  d1 &lt;- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  d2 &lt;- d1 - sigma * sqrt(T)\n  price &lt;- S * pnorm(d1) - K * exp(-r * T) * pnorm(d2)\n  \n  return(price)\n}",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#learning-black-scholes",
    "href": "r/option-pricing-via-machine-learning.html#learning-black-scholes",
    "title": "Option Pricing via Machine Learning",
    "section": "Learning Black-Scholes",
    "text": "Learning Black-Scholes\nWe illustrate the concept of ML by showing how ML methods learn the Black-Scholes equation after observing some different specifications and corresponding prices without us revealing the exact pricing equation.\n\nData simulation\nTo that end, we start with simulated data. We compute option prices for call options for a grid of different combinations of times to maturity (T), risk-free rates (r), volatilities (sigma), strike prices (K), and current stock prices (S). In the code below, we add an idiosyncratic error term to each observation such that the prices considered do not exactly reflect the values implied by the Black-Scholes equation.\nIn order to keep the analysis reproducible, we use set.seed(). A random seed specifies the start point when a computer generates a random number sequence and ensures that our simulated data is the same across different machines.\n\nset.seed(420)\n\noption_prices &lt;- expand_grid(\n  S = 40:60,\n  K = 20:90,\n  r = seq(from = 0, to = 0.05, by = 0.01),\n  T = seq(from = 3 / 12, to = 2, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.8, by = 0.1)\n) |&gt;\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map_dbl(\n      black_scholes,\n      function(x) x + rnorm(1, sd = 0.15)\n    )\n  )\n\nThe code above generates more than 1.5 million random parameter constellations. For each of these values, two observed prices reflecting the Black-Scholes prices are given and a random innovation term pollutes the observed prices. The intuition of this application is simple: the simulated data provides many observations of option prices - by using the Black-Scholes equation we can evaluate the actual predictive performance of a ML method, which would be hard in a realistic context were the actual arbitrage-free price would be unknown.\nNext, we split the data into a training set (which contains 1% of all the observed option prices) and a test set that will only be used for the final evaluation. Note that the entire grid of possible combinations contains 1574496 different specifications. Thus, the sample to learn the Black-Scholes price contains only 31,489 observations and is therefore relatively small.\n\nsplit &lt;- initial_split(option_prices, prop = 1 / 100)\n\nWe process the training dataset further before we fit the different ML models. We define a recipe() that defines all processing steps for that purpose. For our specific case, we want to explain the observed price by the five variables that enter the Black-Scholes equation. The true prices (stored in column black_scholes) should obviously not be used to fit the model. The recipe also reflects that we standardize all predictors to ensure that each variable exhibits a sample average of zero and a sample standard deviation of one.\n\nrec &lt;- recipe(observed_price ~ .,\n  data = option_prices\n) |&gt;\n  step_rm(black_scholes) |&gt;\n  step_normalize(all_predictors())\n\n\n\nSingle layer networks and random forests\nNext, we show how to fit a neural network to the data. Note that this requires that torch is installed on your local machine. The function mlp() from the package parsnip provides the functionality to initialize a single layer, feed-forward neural network. The specification below defines a single layer feed-forward neural network with 10 hidden units. We set the number of training iterations to epochs = 500. The option set_mode(\"regression\") specifies a linear activation function for the output layer.\n\nnnet_model &lt;- mlp(\n  epochs = 500,\n  hidden_units = 10,\n  activation = \"sigmoid\",\n  penalty = 0.0001\n) |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"brulee\", verbose = FALSE)\n\nThe verbose = FALSE argument prevents logging the results to the console. We can follow the straightforward tidymodel workflow as in Factor Selection via Machine Learning: define a workflow, equip it with the recipe, and specify the associated model. Finally, fit the model with the training data.\n\nnn_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(nnet_model) |&gt;\n  fit(data = training(split))\n\nOnce you are familiar with the tidymodels workflow, it is a piece of cake to fit other models from the parsnip family. For instance, the model below initializes a random forest with 50 trees contained in the ensemble, where we require at least 2000 observations in a node. The random forests are trained using the package ranger, which is required to be installed in order to run the code below.\n\nrf_model &lt;- rand_forest(\n  trees = 50,\n  min_n = 2000\n) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\nFitting the model follows exactly the same convention as for the neural network before.\n\nrf_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_model) |&gt;\n  fit(data = training(split))\n\n\n\nDeep neural networks\nA deep neural network is a neural network with multiple layers between the input and output layers. By chaining multiple layers together, more complex structures can be represented with fewer parameters than simple shallow (one-layer) networks as the one implemented above. For instance, image or text recognition are typical tasks where deep neural networks are used (for applications of deep neural networks in finance, see, for instance, Jiang, Kelly, and Xiu 2023; Jensen et al. 2022).\nNote that while the tidymodels workflow is extremely convenient, these more sophisticated multi-layer (so-called deep) neural networks are not supported by tidymodels yet (as of September 2022). Instead, an implementation of a deep neural network in R requires additional computational tools. For that reason, the code snippet below illustrates how to initialize a sequential model with three hidden layers ith 10 units per layer. The brulee package provides a convenient interface to torch and is flexible enough to handle different activation functions.\n\ndeep_nnet_model &lt;- mlp(\n  epochs = 500,\n  hidden_units = c(10, 10, 10),\n  activation = \"sigmoid\",\n  penalty = 0.0001\n) |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"brulee\", verbose = FALSE)\n\nTo train the neural network, we provide the inputs (x) and the variable to predict (y) and then fit the parameters. Note the slightly tedious use of the method extract_mold(nn_fit). Instead of simply using the raw data, we fit the neural network with the same processed data that is used for the single-layer feed-forward network. What is the difference to simply calling x = training(data) |&gt; select(-observed_price, -black_scholes)? Recall that the recipe standardizes the variables such that all columns have unit standard deviation and zero mean. Further, it adds consistency if we ensure that all models are trained using the same recipe such that a change in the recipe is reflected in the performance of any model. A final note on a potentially irritating observation: fit() alters the model - this is one of the few instances, where a function in R alters the input such that after the function call the object model is not same anymore!\n\ndeep_nn_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(deep_nnet_model) |&gt;\n  fit(data = training(split))\n\n\n\nUniversal approximation\nBefore we evaluate the results, we implement one more model. In principle, any non-linear function can also be approximated by a linear model containing the input variables’ polynomial expansions. To illustrate this, we first define a new recipe, rec_linear, which processes the training data even further. We include polynomials up to the fifth degree of each predictor and then add all possible pairwise interaction terms. The final recipe step, step_lincomb(), removes potentially redundant variables (for instance, the interaction between \\(r^2\\) and \\(r^3\\) is the same as the term \\(r^5\\)). We fit a Lasso regression model with a pre-specified penalty term (consult Factor Selection via Machine Learning on how to tune the model hyperparameters).\n\nrec_linear &lt;- rec |&gt;\n  step_poly(all_predictors(),\n    degree = 5,\n    options = list(raw = TRUE)\n  ) |&gt;\n  step_interact(terms = ~ all_predictors():all_predictors()) |&gt;\n  step_lincomb(all_predictors())\n\nlm_model &lt;- linear_reg(penalty = 0.01) |&gt;\n  set_engine(\"glmnet\")\n\nlm_fit &lt;- workflow() |&gt;\n  add_recipe(rec_linear) |&gt;\n  add_model(lm_model) |&gt;\n  fit(data = training(split))",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#prediction-evaluation",
    "href": "r/option-pricing-via-machine-learning.html#prediction-evaluation",
    "title": "Option Pricing via Machine Learning",
    "section": "Prediction Evaluation",
    "text": "Prediction Evaluation\nFinally, we collect all predictions to compare the out-of-sample prediction error evaluated on 10,000 new data points. Note that for the evaluation, we use the call to extract_mold() to ensure that we use the same pre-processing steps for the testing data across each model. We also use the somewhat advanced functionality in forge(), which provides an easy, consistent, and robust pre-processor at prediction time.\n\nout_of_sample_data &lt;- testing(split) |&gt;\n  slice_sample(n = 10000)\n\npredictive_performance &lt;- deep_nn_fit |&gt;\n  predict(out_of_sample_data)|&gt;\n  rename(\"Deep NN\" = .pred) |&gt;\n  bind_cols(nn_fit |&gt;\n    predict(out_of_sample_data)) |&gt;\n  rename(\"Single layer\" = .pred) |&gt;\n  bind_cols(lm_fit |&gt; predict(out_of_sample_data)) |&gt;\n  rename(\"Lasso\" = .pred) |&gt;\n  bind_cols(rf_fit |&gt; predict(out_of_sample_data)) |&gt;\n  rename(\"Random forest\" = .pred) |&gt;\n  bind_cols(out_of_sample_data) |&gt;\n  pivot_longer(\"Deep NN\":\"Random forest\", names_to = \"Model\") |&gt;\n  mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\n\nIn the lines above, we use each of the fitted models to generate predictions for the entire test dataset of option prices. We evaluate the absolute pricing error as one possible measure of pricing accuracy, defined as the absolute value of the difference between predicted option price and the theoretical correct option price from the Black-Scholes model. We show the results graphically in Figure 1.\n\npredictive_performance |&gt;\n  ggplot(aes(\n    x = moneyness, \n    y = pricing_error, \n    color = Model,\n    linetype = Model\n    )) +\n  geom_jitter(alpha = 0.05) +\n  geom_smooth(se = FALSE, method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  facet_wrap(~Model, ncol = 2) + \n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Absolut prediction error (USD)\",\n    title = \"Prediction errors of call option prices for different models\",\n    linetype = NULL\n  )\n\n\n\n\n\n\n\nFigure 1: Absolut prediction error in USD for the different fitted methods. The prediction error is evaluated on a sample of call options that were not used for training.\n\n\n\n\n\nThe results can be summarized as follows:\n\nAll ML methods seem to be able to price call options after observing the training test set.\nThe average prediction errors increase for far in-the-money options.\nRandom forest and the Lasso seem to perform consistently worse in predicting option prices than the neural networks.\nThe complexity of the deep neural network relative to the single-layer neural network does not result in better out-of-sample predictions.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#exercises",
    "href": "r/option-pricing-via-machine-learning.html#exercises",
    "title": "Option Pricing via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that takes y and a matrix of predictors X as inputs and returns a characterization of the relevant parameters of a regression tree with 1 branch.\nCreate a function that creates predictions for a new matrix of predictors newX based on the estimated regression tree.\nUse the package rpart to grow a tree based on the training data and use the illustration tools in rpart to understand which characteristics the tree deems relevant for option pricing.\nMake use of a training and a test set to choose the optimal depth (number of sample splits) of the tree.\nUse brulee to initialize a sequential neural network that can take the predictors from the training dataset as input, contains at least one hidden layer, and generates continuous predictions. This sounds harder than it is: see a simple regression example here. How many parameters does the neural network you aim to fit have?\nCompile the object from the previous exercise. It is important that you specify a loss function. Illustrate the difference in predictive accuracy for different architecture choices.\n\n\n\n\nFigure 1: Absolut prediction error in USD for the different fitted methods. The prediction error is evaluated on a sample of call options that were not used for training.",
    "crumbs": [
      "R",
      "Modeling and Machine Learning",
      "Option Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html",
    "href": "r/parametric-portfolio-policies.html",
    "title": "Parametric Portfolio Policies",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we apply different portfolio performance measures to evaluate and compare portfolio allocation strategies. For this purpose, we introduce a direct way to estimate optimal portfolio weights for large-scale cross-sectional applications. More precisely, the approach of Brandt, Santa-Clara, and Valkanov (2009) proposes to parametrize the optimal portfolio weights as a function of stock characteristics instead of estimating the stock’s expected return, variance, and covariances with other stocks in a prior step. We choose weights as a function of the characteristics, which maximize the expected utility of the investor. This approach is feasible for large portfolio dimensions (such as the entire CRSP universe) and has been proposed by Brandt, Santa-Clara, and Valkanov (2009). See the review paper by Brandt (2010) for an excellent treatment of related portfolio choice methods.\nThe current chapter relies on the following set of R packages:\nlibrary(tidyverse)\nlibrary(RSQLite)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#data-preparation",
    "href": "r/parametric-portfolio-policies.html#data-preparation",
    "title": "Parametric Portfolio Policies",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the monthly CRSP file, which forms our investment universe. We load the data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(), \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, month, ret_excess, mktcap, mktcap_lag) |&gt;\n  collect()\n\nTo evaluate the performance of portfolios, we further use monthly market returns as a benchmark to compute CAPM alphas.\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(month, mkt_excess) |&gt;\n  collect()\n\nNext, we retrieve some stock characteristics that have been shown to have an effect on the expected returns or expected variances (or even higher moments) of the return distribution. In particular, we record the lagged one-year return momentum (momentum_lag), defined as the compounded return between months \\(t-13\\) and \\(t-2\\) for each firm. In finance, momentum is the empirically observed tendency for rising asset prices to rise further, and falling prices to keep falling (Jegadeesh and Titman 1993). The second characteristic is the firm’s market equity (size_lag), defined as the log of the price per share times the number of shares outstanding (Banz 1981). To construct the correct lagged values, we use the approach introduced in WRDS, CRSP, and Compustat.\n\ncrsp_monthly_lags &lt;- crsp_monthly |&gt;\n  transmute(permno,\n    month_13 = month %m+% months(13),\n    mktcap\n  )\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  inner_join(crsp_monthly_lags,\n    join_by(permno, month == month_13),\n    suffix = c(\"\", \"_13\")\n  )\n\ndata_portfolios &lt;- crsp_monthly |&gt;\n  mutate(\n    momentum_lag = mktcap_lag / mktcap_13,\n    size_lag = log(mktcap_lag)\n  ) |&gt;\n  drop_na(contains(\"lag\"))",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "href": "r/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "title": "Parametric Portfolio Policies",
    "section": "Parametric Portfolio Policies",
    "text": "Parametric Portfolio Policies\nThe basic idea of parametric portfolio weights is as follows. Suppose that at each date \\(t\\) we have \\(N_t\\) stocks in the investment universe, where each stock \\(i\\) has a return of \\(r_{i, t+1}\\) and is associated with a vector of firm characteristics \\(x_{i, t}\\) such as time-series momentum or the market capitalization. The investor’s problem is to choose portfolio weights \\(w_{i,t}\\) to maximize the expected utility of the portfolio return: \\[\\begin{aligned}\n\\max_{\\omega} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{i=1}^{N_t}\\omega_{i,t}r_{i,t+1}\\right)\\right]\n\\end{aligned}\\] where \\(u(\\cdot)\\) denotes the utility function.\nWhere do the stock characteristics show up? We parameterize the optimal portfolio weights as a function of the stock characteristic \\(x_{i,t}\\) with the following linear specification for the portfolio weights: \\[\\omega_{i,t} = \\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t},\\] where \\(\\bar{\\omega}_{i,t}\\) is a stock’s weight in a benchmark portfolio (we use the value-weighted or naive portfolio in the application below), \\(\\theta\\) is a vector of coefficients which we are going to estimate, and \\(\\hat{x}_{i,t}\\) are the characteristics of stock \\(i\\), cross-sectionally standardized to have zero mean and unit standard deviation.\nIntuitively, the portfolio strategy is a form of active portfolio management relative to a performance benchmark. Deviations from the benchmark portfolio are derived from the individual stock characteristics. Note that by construction the weights sum up to one as \\(\\sum_{i=1}^{N_t}\\hat{x}_{i,t} = 0\\) due to the standardization. Moreover, the coefficients are constant across assets and over time. The implicit assumption is that the characteristics fully capture all aspects of the joint distribution of returns that are relevant for forming optimal portfolios.\nWe first implement cross-sectional standardization for the entire CRSP universe. We also keep track of (lagged) relative market capitalization relative_mktcap, which will represent the value-weighted benchmark portfolio, while n denotes the number of traded assets \\(N_t\\), which we use to construct the naive portfolio benchmark.\n\ndata_portfolios &lt;- data_portfolios |&gt;\n  group_by(month) |&gt;\n  mutate(\n    n = n(),\n    relative_mktcap = mktcap_lag / sum(mktcap_lag),\n    across(contains(\"lag\"), ~ (. - mean(.)) / sd(.)),\n  ) |&gt;\n  ungroup() |&gt;\n  select(-mktcap_lag)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#computing-portfolio-weights",
    "href": "r/parametric-portfolio-policies.html#computing-portfolio-weights",
    "title": "Parametric Portfolio Policies",
    "section": "Computing Portfolio Weights",
    "text": "Computing Portfolio Weights\nNext, we move on to identify optimal choices of \\(\\theta\\). We rewrite the optimization problem together with the weight parametrization and can then estimate \\(\\theta\\) to maximize the objective function based on our sample \\[\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{i=1}^{N_t}\\left(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\right)r_{i,t+1}\\right).\n\\end{aligned}\\] The allocation strategy is straightforward because the number of parameters to estimate is small. Instead of a tedious specification of the \\(N_t\\) dimensional vector of expected returns and the \\(N_t(N_t+1)/2\\) free elements of the covariance matrix, all we need to focus on in our application is the vector \\(\\theta\\). \\(\\theta\\) contains only two elements in our application: the relative deviation from the benchmark due to size and momentum.\nTo get a feeling for the performance of such an allocation strategy, we start with an arbitrary initial vector \\(\\theta_0\\). The next step is to choose \\(\\theta\\) optimally to maximize the objective function. We automatically detect the number of parameters by counting the number of columns with lagged values.\n\nn_parameters &lt;- sum(str_detect(\n  colnames(data_portfolios), \"lag\"\n))\n\ntheta &lt;- rep(1.5, n_parameters)\n\nnames(theta) &lt;- colnames(data_portfolios)[str_detect(\n  colnames(data_portfolios), \"lag\"\n)]\n\nThe function compute_portfolio_weights() below computes the portfolio weights \\(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\) according to our parametrization for a given value \\(\\theta_0\\). Everything happens within a single pipeline. Hence, we provide a short walk-through.\nWe first compute characteristic_tilt, the tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{i, t}\\) which resemble the deviation from the benchmark portfolio. Next, we compute the benchmark portfolio weight_benchmark, which can be any reasonable set of portfolio weights. In our case, we choose either the value or equal-weighted allocation. weight_tilt completes the picture and contains the final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt which deviate from the benchmark portfolio depending on the stock characteristics.\nThe final few lines go a bit further and implement a simple version of a no-short sale constraint. While it is generally not straightforward to ensure portfolio weight constraints via parameterization, we simply normalize the portfolio weights such that they are enforced to be positive. Finally, we make sure that the normalized weights sum up to one again: \\[\\omega_{i,t}^+ = \\frac{\\max(0, \\omega_{i,t})}{\\sum_{j=1}^{N_t}\\max(0, \\omega_{i,t})}.\\]\nThe following function computes the optimal portfolio weights in the way just described.\n\ncompute_portfolio_weights &lt;- function(theta,\n                                      data,\n                                      value_weighting = TRUE,\n                                      allow_short_selling = TRUE) {\n  data |&gt;\n    group_by(month) |&gt;\n    bind_cols(\n      characteristic_tilt = data |&gt;\n        transmute(across(contains(\"lag\"), ~ . / n)) |&gt;\n        as.matrix() %*% theta |&gt; as.numeric()\n    ) |&gt;\n    mutate(\n      # Definition of benchmark weight\n      weight_benchmark = case_when(\n        value_weighting == TRUE ~ relative_mktcap,\n        value_weighting == FALSE ~ 1 / n\n      ),\n      # Parametric portfolio weights\n      weight_tilt = weight_benchmark + characteristic_tilt,\n      # Short-sell constraint\n      weight_tilt = case_when(\n        allow_short_selling == TRUE ~ weight_tilt,\n        allow_short_selling == FALSE ~ pmax(0, weight_tilt)\n      ),\n      # Weights sum up to 1\n      weight_tilt = weight_tilt / sum(weight_tilt)\n    ) |&gt;\n    ungroup()\n}\n\nIn the next step, we compute the portfolio weights for the arbitrary vector \\(\\theta_0\\). In the example below, we use the value-weighted portfolio as a benchmark and allow negative portfolio weights.\n\nweights_crsp &lt;- compute_portfolio_weights(\n  theta,\n  data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#portfolio-performance",
    "href": "r/parametric-portfolio-policies.html#portfolio-performance",
    "title": "Parametric Portfolio Policies",
    "section": "Portfolio Performance",
    "text": "Portfolio Performance\n Are the computed weights optimal in any way? Most likely not, as we picked \\(\\theta_0\\) arbitrarily. To evaluate the performance of an allocation strategy, one can think of many different approaches. In their original paper, Brandt, Santa-Clara, and Valkanov (2009) focus on a simple evaluation of the hypothetical utility of an agent equipped with a power utility function \\(u_\\gamma(r) = \\frac{(1 + r)^{(1-\\gamma)}}{1-\\gamma}\\), where \\(\\gamma\\) is the risk aversion factor.\n\npower_utility &lt;- function(r, gamma = 5) {\n  (1 + r)^(1 - gamma) / (1 - gamma)\n}\n\nWe want to note that Gehrig, Sögner, and Westerkamp (2020) warn that, in the leading case of constant relative risk aversion (CRRA), strong assumptions on the properties of the returns, the variables used to implement the parametric portfolio policy, and the parameter space are necessary to obtain a well-defined optimization problem.\nNo doubt, there are many other ways to evaluate a portfolio. The function below provides a summary of all kinds of interesting measures that can be considered relevant. Do we need all these evaluation measures? It depends: the original paper by Brandt, Santa-Clara, and Valkanov (2009) only cares about the expected utility to choose \\(\\theta\\). However, if you want to choose optimal values that achieve the highest performance while putting some constraints on your portfolio weights, it is helpful to have everything in one function.\n\nevaluate_portfolio &lt;- function(weights_crsp,\n                               capm_evaluation = TRUE,\n                               full_evaluation = TRUE,\n                               length_year = 12) {\n  \n  evaluation &lt;- weights_crsp |&gt;\n    group_by(month) |&gt;\n    summarize(\n      tilt = weighted.mean(ret_excess, weight_tilt),\n      benchmark = weighted.mean(ret_excess, weight_benchmark)\n    ) |&gt;\n    pivot_longer(-month,\n      values_to = \"portfolio_return\",\n      names_to = \"model\"\n    ) \n  \n  evaluation_stats &lt;- evaluation |&gt;\n    group_by(model) |&gt;\n    left_join(factors_ff3_monthly, \n              join_by(month)) |&gt;\n    summarize(tibble(\n      \"Expected utility\" = mean(power_utility(portfolio_return)),\n      \"Average return\" = 100 * mean(length_year * portfolio_return),\n      \"SD return\" = 100 * sqrt(length_year) * sd(portfolio_return),\n      \"Sharpe ratio\" = sqrt(length_year) * mean(portfolio_return) / sd(portfolio_return),\n\n    )) |&gt;\n    mutate(model = str_remove(model, \"return_\")) \n  \n  if (capm_evaluation) {\n    evaluation_capm &lt;- evaluation |&gt; \n      left_join(factors_ff3_monthly, \n                join_by(month)) |&gt;\n      group_by(model) |&gt;\n      summarize(\n      \"CAPM alpha\" = coefficients(lm(portfolio_return ~ mkt_excess))[1],\n      \"Market beta\" = coefficients(lm(portfolio_return ~ mkt_excess))[2]\n      )\n    \n    evaluation_stats &lt;- evaluation_stats |&gt; \n      left_join(evaluation_capm, join_by(model))\n  }\n\n  if (full_evaluation) {\n    evaluation_weights &lt;- weights_crsp |&gt;\n      select(month, contains(\"weight\")) |&gt;\n      pivot_longer(-month, values_to = \"weight\", names_to = \"model\") |&gt;\n      group_by(model, month) |&gt;\n      mutate(\n        \"Absolute weight\" = abs(weight),\n        \"Max. weight\" = max(weight),\n        \"Min. weight\" = min(weight),\n        \"Avg. sum of negative weights\" = -sum(weight[weight &lt; 0]),\n        \"Avg. fraction of negative weights\" = sum(weight &lt; 0) / n(),\n        .keep = \"none\"\n      ) |&gt;\n      group_by(model) |&gt;\n      summarize(across(-month, ~ 100 * mean(.))) |&gt;\n      mutate(model = str_remove(model, \"weight_\")) \n    \n    evaluation_stats &lt;- evaluation_stats |&gt; \n      left_join(evaluation_weights, join_by(model))\n  }\n  \n  evaluation_output &lt;- evaluation_stats |&gt; \n    pivot_longer(cols = -model, names_to = \"measure\") |&gt; \n    pivot_wider(names_from = model)\n  \n  return(evaluation_output)\n}\n\n Let us take a look at the different portfolio strategies and evaluation measures.\n\nevaluate_portfolio(weights_crsp) |&gt;\n  print(n = Inf)\n\n# A tibble: 11 × 3\n   measure                            benchmark     tilt\n   &lt;chr&gt;                                  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Expected utility                  -0.250     -0.261  \n 2 Average return                     6.65       0.194  \n 3 SD return                         15.5       21.1    \n 4 Sharpe ratio                       0.430      0.00918\n 5 CAPM alpha                         0.000139  -0.00501\n 6 Market beta                        0.994      0.952  \n 7 Absolute weight                    0.0249     0.0637 \n 8 Max. weight                        3.59       3.72   \n 9 Min. weight                        0.0000273 -0.144  \n10 Avg. sum of negative weights       0         78.0    \n11 Avg. fraction of negative weights  0         49.5    \n\n\nThe value-weighted portfolio delivers an annualized return of more than 6 percent and clearly outperforms the tilted portfolio, irrespective of whether we evaluate expected utility, the Sharpe ratio, or the CAPM alpha. We can conclude the market beta is close to one for both strategies (naturally almost identically 1 for the value-weighted benchmark portfolio). When it comes to the distribution of the portfolio weights, we see that the benchmark portfolio weight takes less extreme positions (lower average absolute weights and lower maximum weight). By definition, the value-weighted benchmark does not take any negative positions, while the tilted portfolio also takes short positions.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#optimal-parameter-choice",
    "href": "r/parametric-portfolio-policies.html#optimal-parameter-choice",
    "title": "Parametric Portfolio Policies",
    "section": "Optimal Parameter Choice",
    "text": "Optimal Parameter Choice\nNext, we move to a choice of \\(\\theta\\) that actually aims to improve some (or all) of the performance measures. We first define a helper function compute_objective_function(), which we then pass to an optimizer.\n\ncompute_objective_function &lt;- function(theta,\n                                       data,\n                                       objective_measure = \"Expected utility\",\n                                       value_weighting = TRUE,\n                                       allow_short_selling = TRUE) {\n  processed_data &lt;- compute_portfolio_weights(\n    theta,\n    data,\n    value_weighting,\n    allow_short_selling\n  )\n\n  objective_function &lt;- evaluate_portfolio(\n    processed_data,\n    capm_evaluation = FALSE,\n    full_evaluation = FALSE\n  ) |&gt;\n    filter(measure == objective_measure) |&gt;\n    pull(tilt)\n\n  return(-objective_function)\n}\n\nYou may wonder why we return the negative value of the objective function. This is simply due to the common convention for optimization procedures to search for minima as a default. By minimizing the negative value of the objective function, we get the maximum value as a result. In its most basic form, R optimization relies on the function optim(). As main inputs, the function requires an initial guess of the parameters and the objective function to minimize. Now, we are fully equipped to compute the optimal values of \\(\\hat\\theta\\), which maximize the hypothetical expected utility of the investor.\n\noptimal_theta &lt;- optim(\n  par = theta,\n  fn = compute_objective_function,\n  objective_measure = \"Expected utility\",\n  data = data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE,\n  method = \"Nelder-Mead\"\n)\n\noptimal_theta$par\n\nmomentum_lag     size_lag \n       0.347       -1.830 \n\n\nThe resulting values of \\(\\hat\\theta\\) are easy to interpret: intuitively, expected utility increases by tilting weights from the value-weighted portfolio toward smaller stocks (negative coefficient for size) and toward past winners (positive value for momentum). Both findings are in line with the well-documented size effect (Banz 1981) and the momentum anomaly (Jegadeesh and Titman 1993).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#more-model-specifications",
    "href": "r/parametric-portfolio-policies.html#more-model-specifications",
    "title": "Parametric Portfolio Policies",
    "section": "More Model Specifications",
    "text": "More Model Specifications\nHow does the portfolio perform for different model specifications? For this purpose, we compute the performance of a number of different modeling choices based on the entire CRSP sample. The next code chunk performs all the heavy lifting.\n\nevaluate_optimal_performance &lt;- function(data, \n                                         objective_measure,\n                                         value_weighting, \n                                         allow_short_selling) {\n  optimal_theta &lt;- optim(\n    par = theta,\n    fn = compute_objective_function,\n    data = data,\n    objective_measure = \"Expected utility\",\n    value_weighting = TRUE,\n    allow_short_selling = TRUE,\n    method = \"Nelder-Mead\"\n  )\n\n  processed_data = compute_portfolio_weights(\n    optimal_theta$par, \n    data,\n    value_weighting,\n    allow_short_selling\n  )\n  \n  portfolio_evaluation = evaluate_portfolio(\n    processed_data,\n    capm_evaluation = TRUE,\n    full_evaluation = TRUE\n  )\n \n  return(portfolio_evaluation) \n}\n\nspecifications &lt;- expand_grid(\n  data = list(data_portfolios),\n  objective_measure = \"Expected utility\",\n  value_weighting = c(TRUE, FALSE),\n  allow_short_selling = c(TRUE, FALSE)\n) |&gt; \n  mutate(\n    portfolio_evaluation = pmap(\n      .l = list(data, objective_measure, value_weighting, allow_short_selling),\n      .f = evaluate_optimal_performance\n    )\n)\n\nFinally, we can compare the results. The table below shows summary statistics for all possible combinations: equal- or value-weighted benchmark portfolio, with or without short-selling constraints, and tilted toward maximizing expected utility.\n\nperformance_table &lt;- specifications |&gt;\n  select(\n    value_weighting,\n    allow_short_selling,\n    portfolio_evaluation\n  ) |&gt;\n  unnest(portfolio_evaluation)\n\nperformance_table |&gt;\n  rename(\n    \" \" = benchmark,\n    Optimal = tilt\n  ) |&gt;\n  mutate(\n    value_weighting = case_when(\n      value_weighting == TRUE ~ \"VW\",\n      value_weighting == FALSE ~ \"EW\"\n    ),\n    allow_short_selling = case_when(\n      allow_short_selling == TRUE ~ \"\",\n      allow_short_selling == FALSE ~ \"(no s.)\"\n    )\n  ) |&gt;\n  pivot_wider(\n    names_from = value_weighting:allow_short_selling,\n    values_from = \" \":Optimal,\n    names_glue = \"{value_weighting} {allow_short_selling} {.value} \"\n  ) |&gt;\n  select(\n    measure,\n    `EW    `,\n    `VW    `,\n    sort(contains(\"Optimal\"))\n  ) |&gt;\n  print(n = 11)\n\n# A tibble: 11 × 7\n   measure     `EW    ` `VW    ` `VW  Optimal ` `VW (no s.) Optimal `\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Expected u… -0.251   -2.50e-1       -0.247                -0.248  \n 2 Average re… 10.0      6.65e+0       13.4                  12.3    \n 3 SD return   20.4      1.55e+1       19.9                  19.2    \n 4 Sharpe rat…  0.491    4.30e-1        0.672                 0.643  \n 5 CAPM alpha   0.00223  1.39e-4        0.00569               0.00467\n 6 Market beta  1.12     9.94e-1        1.00                  1.03   \n 7 Absolute w…  0.0249   2.49e-2        0.0360                0.0249 \n 8 Max. weight  0.0249   3.59e+0        3.42                  2.79   \n 9 Min. weight  0.0249   2.73e-5       -0.0322                0      \n10 Avg. sum o…  0        0             23.3                   0      \n11 Avg. fract…  0        0             37.6                   0      \n# ℹ 2 more variables: `EW  Optimal ` &lt;dbl&gt;,\n#   `EW (no s.) Optimal ` &lt;dbl&gt;\n\n\nThe results indicate that the average annualized Sharpe ratio of the equal-weighted portfolio exceeds the Sharpe ratio of the value-weighted benchmark portfolio. Nevertheless, starting with the weighted value portfolio as a benchmark and tilting optimally with respect to momentum and small stocks yields the highest Sharpe ratio across all specifications. Finally, imposing no short-sale constraints does not improve the performance of the portfolios in our application.",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#exercises",
    "href": "r/parametric-portfolio-policies.html#exercises",
    "title": "Parametric Portfolio Policies",
    "section": "Exercises",
    "text": "Exercises\n\nHow do the estimated parameters \\(\\hat\\theta\\) and the portfolio performance change if your objective is to maximize the Sharpe ratio instead of the hypothetical expected utility?\nThe code above is very flexible in the sense that you can easily add new firm characteristics. Construct a new characteristic of your choice and evaluate the corresponding coefficient \\(\\hat\\theta_i\\).\nTweak the function optimal_theta() such that you can impose additional performance constraints in order to determine \\(\\hat\\theta\\), which maximizes expected utility under the constraint that the market beta is below 1.\nDoes the portfolio performance resemble a realistic out-of-sample backtesting procedure? Verify the robustness of the results by first estimating \\(\\hat\\theta\\) based on past data only. Then, use more recent periods to evaluate the actual portfolio performance.\nBy formulating the portfolio problem as a statistical estimation problem, you can easily obtain standard errors for the coefficients of the weight function. Brandt, Santa-Clara, and Valkanov (2009) provide the relevant derivations in their paper in Equation (10). Implement a small function that computes standard errors for \\(\\hat\\theta\\).",
    "crumbs": [
      "R",
      "Portfolio Optimization",
      "Parametric Portfolio Policies"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html",
    "href": "r/replicating-fama-and-french-factors.html",
    "title": "Replicating Fama-French Factors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we provide a replication of the famous Fama-French factor portfolios. The Fama-French factor models are a cornerstone of empirical asset pricing Fama and French (2015). On top of the market factor represented by the traditional CAPM beta, the three-factor model includes the size and value factors to explain the cross section of returns. Its successor, the five-factor model, additionally includes profitability and investment as explanatory factors.\nWe start with the three-factor model. We already introduced the size and value factors in Value and Bivariate Sorts, and their definition remains the same: size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nAfter the replication of the three-factor model, we move to the five factors by constructing the profitability factor RMW (robust-minus-weak) as the difference between the returns of firms with high and low operating profitability and the investment factor CMA (conservative-minus-aggressive) as the difference between firms with high versus low investment rates.\nThe current chapter relies on this set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#data-preparation",
    "href": "r/replicating-fama-and-french-factors.html#data-preparation",
    "title": "Replicating Fama-French Factors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and Compustat as data sources, as we need the same variables to compute the factors in the way Fama and French do it. Hence, there is nothing new below, and we only load data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.1 \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(\n    permno, gvkey, month, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n    select(gvkey, datadate, be, op, inv) |&gt;\n    collect() \n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(month, smb, hml) |&gt;\n  collect()\n\nfactors_ff5_monthly &lt;- tbl(tidy_finance, \"factors_ff5_monthly\") |&gt;\n  select(month, smb, hml, rmw, cma) |&gt;\n  collect()\n\nYet when we start merging our dataset for computing the premiums, there are a few differences to Value and Bivariate Sorts. First, Fama and French form their portfolios in June of year \\(t\\), whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year \\(t+1\\).\nSecond, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year \\(t - 1\\) and the book equity reported in year \\(t-1\\), i.e., the datadate is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity. The other sorting variables are analogously to book equity taken from year \\(t-1\\).\nTo implement all these time lags, we again employ the temporary sorting_date-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year. We ensure this by a call of distinct() at the end of the chunk below.\n\nsize &lt;- crsp_monthly |&gt;\n  filter(month(month) == 6) |&gt;\n  mutate(sorting_date = month %m+% months(1)) |&gt;\n  select(permno, exchange, sorting_date, size = mktcap)\n\nmarket_equity &lt;- crsp_monthly |&gt;\n  filter(month(month) == 12) |&gt;\n  mutate(sorting_date = ymd(str_c(year(month) + 1, \"0701)\"))) |&gt;\n  select(permno, gvkey, sorting_date, me = mktcap)\n\nbook_to_market &lt;- compustat |&gt;\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |&gt;\n  select(gvkey, sorting_date, be) |&gt;\n  inner_join(market_equity, join_by(gvkey, sorting_date)) |&gt;\n  mutate(bm = be / me) |&gt;\n  select(permno, sorting_date, me, bm)\n\nsorting_variables &lt;- size |&gt;\n  inner_join(\n    book_to_market, join_by(permno, sorting_date)\n    ) |&gt;\n  drop_na() |&gt;\n  distinct(permno, sorting_date, .keep_all = TRUE)",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "href": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "title": "Replicating Fama-French Factors",
    "section": "Portfolio Sorts",
    "text": "Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on NYSE-specific breakpoints, they form two portfolios in the size dimension at the median and three portfolios in the dimension of each other sorting variable at the 30- and 70-percentiles, and they use dependent sorts. The sorts for book-to-market require an adjustment to the function in Value and Bivariate Sorts because the seq() we would produce does not produce the right breakpoints. Instead of n_portfolios, we now specify percentiles, which take the breakpoint-sequence as an object specified in the function’s call. Specifically, we give percentiles = c(0, 0.3, 0.7, 1) to the function. Additionally, we perform an inner_join() with our return data to ensure that we only use traded stocks when computing the breakpoints as a first step.\n\nassign_portfolio &lt;- function(data, \n                             sorting_variable, \n                             percentiles) {\n  breakpoints &lt;- data |&gt;\n    filter(exchange == \"NYSE\") |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = percentiles,\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  return(assigned_portfolios)\n}\n\nportfolios &lt;- sorting_variables |&gt;\n  group_by(sorting_date) |&gt;\n  mutate(\n    portfolio_size = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = size,\n      percentiles = c(0, 0.5, 1)\n    ),\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = bm,\n      percentiles = c(0, 0.3, 0.7, 1)\n    )\n  ) |&gt;\n  ungroup() |&gt; \n  select(permno, sorting_date, \n         portfolio_size, portfolio_bm)\n\nNext, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column sorting_date in our return data by setting the date to sort on to July of \\(t-1\\) if the month is June (of year \\(t\\)) or earlier or to July of year \\(t\\) if the month is July or later.\n\nportfolios &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = case_when(\n    month(month) &lt;= 6 ~ ymd(str_c(year(month) - 1, \"0701\")),\n    month(month) &gt;= 7 ~ ymd(str_c(year(month), \"0701\"))\n  )) |&gt;\n  inner_join(portfolios, join_by(permno, sorting_date))",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#fama-french-three-factor-model",
    "href": "r/replicating-fama-and-french-factors.html#fama-french-three-factor-model",
    "title": "Replicating Fama-French Factors",
    "section": "Fama-French Three-Factor Model",
    "text": "Fama-French Three-Factor Model\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama-French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally.\n\nfactors_replicated &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_bm, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\"\n  ) |&gt;\n  group_by(month) |&gt;\n  summarize(\n    smb_replicated = mean(ret[portfolio_size == 1]) -\n      mean(ret[portfolio_size == 2]),\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "href": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "title": "Replicating Fama-French Factors",
    "section": "Replication Evaluation",
    "text": "Replication Evaluation\nIn the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using lm(). If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to 1 (indicating a high correlation), and an adjusted R-squared close to 1 (indicating a high proportion of explained variance).\n\ntest &lt;- factors_ff3_monthly |&gt;\n  inner_join(factors_replicated, join_by(month)) |&gt;\n  mutate(\n    across(c(smb_replicated, hml_replicated), ~round(., 4))\n  )\n\nTo test the success of the SMB factor, we hence run the following regression:\n\nmodel_smb &lt;- lm(smb ~ smb_replicated, data = test)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.020103 -0.001448 -0.000012  0.001483  0.014957 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.000131   0.000131      -1     0.32    \nsmb_replicated  0.992550   0.004327     229   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00354 on 736 degrees of freedom\nMultiple R-squared:  0.986, Adjusted R-squared:  0.986 \nF-statistic: 5.26e+04 on 1 and 736 DF,  p-value: &lt;2e-16\n\n\nThe results for the SMB factor are really convincing, as all three criteria outlined above are met and the coefficient is 0.99 and the R-squared is at 99 percent.\n\nmodel_hml &lt;- lm(hml ~ hml_replicated, data = test)\nsummary(model_hml)\n\n\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02321 -0.00291 -0.00010  0.00229  0.03412 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000300   0.000218    1.37     0.17    \nhml_replicated 0.962447   0.007258  132.60   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0059 on 736 degrees of freedom\nMultiple R-squared:  0.96,  Adjusted R-squared:  0.96 \nF-statistic: 1.76e+04 on 1 and 736 DF,  p-value: &lt;2e-16\n\n\nThe replication of the HML factor is also a success, although at a slightly lower coefficient of 0.96 and an R-squared around 96 percent.\nThe evidence hence allows us to conclude that we did a relatively good job in replicating the original Fama-French size and value premiums, although we do not know their underlying code. From our perspective, a perfect match is only possible with additional information from the maintainers of the original data.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#fama-french-five-factor-model",
    "href": "r/replicating-fama-and-french-factors.html#fama-french-five-factor-model",
    "title": "Replicating Fama-French Factors",
    "section": "Fama-French Five-Factor Model",
    "text": "Fama-French Five-Factor Model\nNow, let us move to the replication of the five-factor model. We extend the other_sorting_variables table from above with the additional characteristics operating profitability op and investment inv. Note that the drop_na() statement yields different sample sizes as some firms with be values might not have op or inv values.\n\nother_sorting_variables &lt;- compustat |&gt;\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |&gt;\n  select(gvkey, sorting_date, be, op, inv) |&gt;\n  inner_join(market_equity, \n             join_by(gvkey, sorting_date)) |&gt;\n  mutate(bm = be / me) |&gt;\n  select(permno, sorting_date, me, be, bm, op, inv)\n\nsorting_variables &lt;- size |&gt;\n  inner_join(\n    other_sorting_variables, \n    join_by(permno, sorting_date)\n    ) |&gt;\n  drop_na() |&gt;\n  distinct(permno, sorting_date, .keep_all = TRUE)\n\nIn each month, we independently sort all stocks into the two size portfolios. The value, profitability, and investment portfolios, on the other hand, are the results of dependent sorts based on the size portfolios. We then merge the portfolios to the return data for the rest of the year just as above.\n\nportfolios &lt;- sorting_variables |&gt;\n  group_by(sorting_date) |&gt;\n  mutate(\n    portfolio_size = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = size,\n      percentiles = c(0, 0.5, 1)\n    )) |&gt; \n  group_by(sorting_date, portfolio_size) |&gt; \n  mutate(\n    across(c(bm, op, inv), ~assign_portfolio(\n      data = pick(everything()), \n      sorting_variable = ., \n      percentiles = c(0, 0.3, 0.7, 1)),\n      .names = \"portfolio_{.col}\"\n    )\n  ) |&gt;\n  ungroup() |&gt; \n  select(permno, sorting_date, \n         portfolio_size, portfolio_bm,\n         portfolio_op, portfolio_inv)\n\nportfolios &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = case_when(\n    month(month) &lt;= 6 ~ ymd(str_c(year(month) - 1, \"0701\")),\n    month(month) &gt;= 7 ~ ymd(str_c(year(month), \"0701\"))\n  )) |&gt;\n  inner_join(portfolios, join_by(permno, sorting_date))\n\nNow, we want to construct each of the factors, but this time the size factor actually comes last because it is the result of averaging across all other factor portfolios. This dependency is the reason why we keep the table with value-weighted portfolio returns as a separate object that we reuse later. We construct the value factor, HML, as above by going long the two portfolios with high book-to-market ratios and shorting the two portfolios with low book-to-market.\n\nportfolios_value &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_bm, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), \n    .groups = \"drop\"\n  )\n\nfactors_value &lt;- portfolios_value |&gt;\n  group_by(month) |&gt;\n  summarize(\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )\n\nFor the profitability factor, RMW, we take a long position in the two high profitability portfolios and a short position in the two low profitability portfolios.\n\nportfolios_profitability &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_op, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), \n    .groups = \"drop\"\n  ) \n\nfactors_profitability &lt;- portfolios_profitability |&gt;\n  group_by(month) |&gt;\n  summarize(\n    rmw_replicated = mean(ret[portfolio_op == 3]) -\n      mean(ret[portfolio_op == 1])\n  )\n\nFor the investment factor, CMA, we go long the two low investment portfolios and short the two high investment portfolios.\n\nportfolios_investment &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_inv, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), \n    .groups = \"drop\"\n  )\n\nfactors_investment &lt;- portfolios_investment |&gt;\n  group_by(month) |&gt;\n  summarize(\n    cma_replicated = mean(ret[portfolio_inv == 1]) -\n      mean(ret[portfolio_inv == 3])\n  )\n\nFinally, the size factor, SMB, is constructed by going long the six small portfolios and short the six large portfolios.\n\nfactors_size &lt;- bind_rows(\n  portfolios_value,\n  portfolios_profitability,\n  portfolios_investment\n) |&gt; \n  group_by(month) |&gt;\n  summarize(\n    smb_replicated = mean(ret[portfolio_size == 1]) -\n      mean(ret[portfolio_size == 2])\n  )\n\nWe then join all factors together into one dataframe and construct again a suitable table to run tests for evaluating our replication.\n\nfactors_replicated &lt;- factors_size |&gt;\n  full_join(\n    factors_value, join_by(month)\n  ) |&gt;\n  full_join(\n    factors_profitability, join_by(month)\n  ) |&gt;\n  full_join(\n    factors_investment, join_by(month)\n  )\n\ntest &lt;- factors_ff5_monthly |&gt;\n  inner_join(factors_replicated, join_by(month)) |&gt;\n  mutate(\n    across(c(smb_replicated, hml_replicated, \n             rmw_replicated, cma_replicated), ~round(., 4))\n  )\n\nLet us start the replication evaluation again with the size factor:\n\nmodel_smb &lt;- lm(smb ~ smb_replicated, data = test)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.018162 -0.001874  0.000233  0.001956  0.014321 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.000200   0.000135   -1.48     0.14    \nsmb_replicated  0.969302   0.004364  222.13   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0036 on 712 degrees of freedom\nMultiple R-squared:  0.986, Adjusted R-squared:  0.986 \nF-statistic: 4.93e+04 on 1 and 712 DF,  p-value: &lt;2e-16\n\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient is 0.97 and the R-squared is at 99 percent.\n\nmodel_hml &lt;- lm(hml ~ hml_replicated, data = test)\nsummary(model_hml)\n\n\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.04465 -0.00413 -0.00034  0.00412  0.03637 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000474   0.000297    1.59     0.11    \nhml_replicated 0.991694   0.010266   96.60   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00792 on 712 degrees of freedom\nMultiple R-squared:  0.929, Adjusted R-squared:  0.929 \nF-statistic: 9.33e+03 on 1 and 712 DF,  p-value: &lt;2e-16\n\n\nThe replication of the HML factor is also a success, although at a slightly higher coefficient of 0.99 and an R-squared around 93 percent.\n\nmodel_rmw &lt;- lm(rmw ~ rmw_replicated, data = test)\nsummary(model_rmw)\n\n\nCall:\nlm(formula = rmw ~ rmw_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.019813 -0.003075  0.000029  0.003209  0.018327 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.68e-05   2.02e-04    0.23     0.82    \nrmw_replicated 9.54e-01   8.87e-03  107.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00535 on 712 degrees of freedom\nMultiple R-squared:  0.942, Adjusted R-squared:  0.942 \nF-statistic: 1.16e+04 on 1 and 712 DF,  p-value: &lt;2e-16\n\n\nWe are also able to replicate the RMW factor quite well with a coefficient of 0.95 and an R-squared around 94 percent.\n\nmodel_cma &lt;- lm(cma ~ cma_replicated, data = test)\nsummary(model_cma)\n\n\nCall:\nlm(formula = cma ~ cma_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.015218 -0.002710 -0.000184  0.002446  0.021704 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000668   0.000172    3.89  0.00011 ***\ncma_replicated 0.964072   0.008207  117.46  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00456 on 712 degrees of freedom\nMultiple R-squared:  0.951, Adjusted R-squared:  0.951 \nF-statistic: 1.38e+04 on 1 and 712 DF,  p-value: &lt;2e-16\n\n\nFinally, the CMA factor also replicates well with a coefficient of 0.96 and an R-squared around 95 percent.\nOverall, our approach seems to replicate the Fama-French five-factor models just as well as the three factors.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#exercises",
    "href": "r/replicating-fama-and-french-factors.html#exercises",
    "title": "Replicating Fama-French Factors",
    "section": "Exercises",
    "text": "Exercises\n\nFama and French (1993) claim that their sample excludes firms until they have appeared in Compustat for two years. Implement this additional filter and compare the improvements of your replication effort.\nOn his homepage, Kenneth French provides instructions on how to construct the most common variables used for portfolio sorts. Try to replicate the univariate portfolio sort return time series for E/P (earnings/price) provided on his homepage and evaluate your replication effort using regressions.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#footnotes",
    "href": "r/replicating-fama-and-french-factors.html#footnotes",
    "title": "Replicating Fama-French Factors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that Fama and French (1992) claim to exclude financial firms. To a large extent this happens through using industry format “INDL”, as we do in WRDS, CRSP, and Compustat. Neither the original paper, nor Ken French’s website, or the WRDS replication contains any indication that financial companies are excluded using additional filters such as industry codes.↩︎",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Replicating Fama-French Factors"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html",
    "href": "r/size-sorts-and-p-hacking.html",
    "title": "Size Sorts and p-Hacking",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we continue with portfolio sorts in a univariate setting. Yet, we consider firm size as a sorting variable, which gives rise to a well-known return factor: the size premium. The size premium arises from buying small stocks and selling large stocks. Prominently, Fama and French (1993) include it as a factor in their three-factor model. Apart from that, asset managers commonly include size as a key firm characteristic when making investment decisions.\nWe also introduce new choices in the formation of portfolios. In particular, we discuss listing exchanges, industries, weighting regimes, and periods. These choices matter for the portfolio returns and result in different size premiums Walter, Weber, and Weiss (2022). Exploiting these ideas to generate favorable results is called p-hacking. There is arguably a thin line between p-hacking and conducting robustness tests. Our purpose here is to illustrate the substantial variation that can arise along the evidence-generating process.\nThe chapter relies on the following set of packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(furrr)\nlibrary(rlang)\nCompared to previous chapters, we introduce the rlang package (Henry and Wickham 2022) for more advanced parsing of functional expressions.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#data-preparation",
    "href": "r/size-sorts-and-p-hacking.html#data-preparation",
    "title": "Size Sorts and p-Hacking",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we retrieve the relevant data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. Firm size is defined as market equity in most asset pricing applications that we retrieve from CRSP. We further use the Fama-French factor returns for performance evaluation.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  collect()\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(smb) |&gt;\n  collect()",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#size-distribution",
    "href": "r/size-sorts-and-p-hacking.html#size-distribution",
    "title": "Size Sorts and p-Hacking",
    "section": "Size Distribution",
    "text": "Size Distribution\nBefore we build our size portfolios, we investigate the distribution of the variable firm size. Visualizing the data is a valuable starting point to understand the input to the analysis. Figure 8.1 shows the fraction of total market capitalization concentrated in the largest firm. To produce this graph, we create monthly indicators that track whether a stock belongs to the largest x percent of the firms. Then, we aggregate the firms within each bucket and compute the buckets’ share of total market capitalization.\nFigure 1 shows that the largest 1 percent of firms cover up to 50 percent of the total market capitalization, and holding just the 25 percent largest firms in the CRSP universe essentially replicates the market portfolio. The distribution of firm size thus implies that the largest firms of the market dominate many small firms whenever we use value-weighted benchmarks.\n\ncrsp_monthly |&gt;\n  group_by(month) |&gt;\n  mutate(\n    top01 = if_else(mktcap &gt;= quantile(mktcap, 0.99), 1, 0),\n    top05 = if_else(mktcap &gt;= quantile(mktcap, 0.95), 1, 0),\n    top10 = if_else(mktcap &gt;= quantile(mktcap, 0.90), 1, 0),\n    top25 = if_else(mktcap &gt;= quantile(mktcap, 0.75), 1, 0)\n  ) |&gt;\n  summarize(\n    total_market_cap =  sum(mktcap),\n    `Largest 1% of stocks` = sum(mktcap[top01 == 1]) / total_market_cap,\n    `Largest 5% of stocks` = sum(mktcap[top05 == 1]) / total_market_cap,\n    `Largest 10% of stocks` = sum(mktcap[top10 == 1]) / total_market_cap,\n    `Largest 25% of stocks` = sum(mktcap[top25 == 1]) / total_market_cap,\n    .groups = \"drop\"\n  ) |&gt;\n  select(-total_market_cap) |&gt; \n  pivot_longer(cols = -month) |&gt;\n  mutate(name = factor(name, levels = c(\n    \"Largest 1% of stocks\", \"Largest 5% of stocks\",\n    \"Largest 10% of stocks\", \"Largest 25% of stocks\"\n  ))) |&gt;\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = name,\n    linetype = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent, limits = c(0, 1)) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Percentage of total market capitalization in largest stocks\"\n  )\n\n\n\n\n\n\n\nFigure 1: We report the aggregate market capitalization of all stocks that belong to the 1, 5, 10, and 25 percent quantile of the largest firms in the monthly cross-section relative to the market capitalization of all stocks during the month.\n\n\n\n\n\nNext, firm sizes also differ across listing exchanges. Stocks’ primary listings were important in the past and are potentially still relevant today. Figure 2 shows that the New York Stock Exchange (NYSE) was and still is the largest listing exchange in terms of market capitalization. More recently, NASDAQ has gained relevance as a listing exchange. Do you know what the small peak in NASDAQ’s market cap around the year 2000 was?\n\ncrsp_monthly |&gt;\n  group_by(month, exchange) |&gt;\n  summarize(mktcap = sum(mktcap),\n            .groups = \"drop_last\") |&gt;\n  mutate(share = mktcap / sum(mktcap)) |&gt;\n  ggplot(aes(\n    x = month, \n    y = share, \n    fill = exchange, \n    color = exchange)) +\n  geom_area(\n    position = \"stack\",\n    stat = \"identity\",\n    alpha = 0.5\n  ) +\n  geom_line(position = \"stack\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, fill = NULL, color = NULL,\n    title = \"Share of total market capitalization per listing exchange\"\n  )\n\n\n\n\n\n\n\nFigure 2: Years are on the horizontal axis and the corresponding share of total market capitalization per listing exchange on the vertical axis.\n\n\n\n\n\nFinally, we consider the distribution of firm size across listing exchanges and create summary statistics. The function summary() does not include all statistics we are interested in, which is why we create the function create_summary() that adds the standard deviation and the number of observations. Then, we apply it to the most current month of our CRSP data on each listing exchange. We also add a row with add_row() with the overall summary statistics.\nThe resulting table shows that firms listed on NYSE in December 2021 are significantly larger on average than firms listed on the other exchanges. Moreover, NASDAQ lists the largest number of firms. This discrepancy between firm sizes across listing exchanges motivated researchers to form breakpoints exclusively on the NYSE sample and apply those breakpoints to all stocks. In the following, we use this distinction to update our portfolio sort procedure.\n\ncreate_summary &lt;- function(data, column_name) {\n  data |&gt;\n    select(value = {{ column_name }}) |&gt;\n    summarize(\n      mean = mean(value),\n      sd = sd(value),\n      min = min(value),\n      q05 = quantile(value, 0.05),\n      q50 = quantile(value, 0.50),\n      q95 = quantile(value, 0.95),\n      max = max(value),\n      n = n()\n    )\n}\n\ncrsp_monthly |&gt;\n  filter(month == max(month)) |&gt;\n  group_by(exchange) |&gt;\n  create_summary(mktcap) |&gt;\n  add_row(crsp_monthly |&gt;\n            filter(month == max(month)) |&gt;\n            create_summary(mktcap) |&gt;\n            mutate(exchange = \"Overall\"))\n\n# A tibble: 5 × 9\n  exchange   mean     sd      min      q05    q50    q95    max     n\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 AMEX       407.  2954.     3.40     5.27 4.48e1   947. 3.73e4   162\n2 NASDAQ    5684. 58811.     1.17     8.16 2.65e2 13040. 2.06e6  2777\n3 NYSE     16299. 47361.    10.8    112.   2.82e3 67924. 4.95e5  1355\n4 Other    13310.    NA  13310.   13310.   1.33e4 13310. 1.33e4     1\n5 Overall   8836. 54500.     1.17    10.6  4.87e2 34170. 2.06e6  4295",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "href": "r/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "title": "Size Sorts and p-Hacking",
    "section": "Univariate Size Portfolios with Flexible Breakpoints",
    "text": "Univariate Size Portfolios with Flexible Breakpoints\nIn Univariate Portfolio Sorts, we construct portfolios with a varying number of breakpoints and different sorting variables. Here, we extend the framework such that we compute breakpoints on a subset of the data, for instance, based on selected listing exchanges. In published asset pricing articles, many scholars compute sorting breakpoints only on NYSE-listed stocks. These NYSE-specific breakpoints are then applied to the entire universe of stocks.\nTo replicate the NYSE-centered sorting procedure, we introduce exchanges as an argument in our assign_portfolio() function. The exchange-specific argument then enters in the filter filter(exchange %in% exchanges). For example, if exchanges = 'NYSE' is specified, only stocks listed on NYSE are used to compute the breakpoints. Alternatively, you could specify exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"), which keeps all stocks listed on either of these exchanges. Overall, regular expressions are a powerful tool, and we only touch on a specific case here.\n\nassign_portfolio &lt;- function(n_portfolios,\n                             exchanges,\n                             data) {\n  # Compute breakpoints\n  breakpoints &lt;- data |&gt;\n    filter(exchange %in% exchanges) |&gt;\n    pull(mktcap_lag) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  # Assign portfolios\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(mktcap_lag,\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  # Output\n  return(assigned_portfolios)\n}",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "href": "r/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "title": "Size Sorts and p-Hacking",
    "section": "Weighting Schemes for Portfolios",
    "text": "Weighting Schemes for Portfolios\nApart from computing breakpoints on different samples, researchers often use different portfolio weighting schemes. So far, we weighted each portfolio constituent by its relative market equity of the previous period. This protocol is called value-weighting. The alternative protocol is equal-weighting, which assigns each stock’s return the same weight, i.e., a simple average of the constituents’ returns. Notice that equal-weighting is difficult in practice as the portfolio manager needs to rebalance the portfolio monthly while value-weighting is a truly passive investment.\nWe implement the two weighting schemes in the function compute_portfolio_returns() that takes a logical argument to weight the returns by firm value. The statement if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)) generates value-weighted returns if value_weighted = TRUE. Additionally, the long-short portfolio is long in the smallest firms and short in the largest firms, consistent with research showing that small firms outperform their larger counterparts. Apart from these two changes, the function is similar to the procedure in Univariate Portfolio Sorts.\n\ncompute_portfolio_returns &lt;- function(n_portfolios = 10,\n                                      exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n                                      value_weighted = TRUE,\n                                      data = crsp_monthly) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(portfolio = assign_portfolio(\n      n_portfolios = n_portfolios,\n      exchanges = exchanges,\n      data = pick(everything())\n    )) |&gt;\n    group_by(month, portfolio) |&gt;\n    summarize(\n      ret = if_else(value_weighted,\n        weighted.mean(ret_excess, mktcap_lag),\n        mean(ret_excess)\n      ),\n      .groups = \"drop_last\"\n    ) |&gt;\n    summarize(size_premium = ret[portfolio == min(portfolio)] -\n      ret[portfolio == max(portfolio)]) |&gt;\n    summarize(size_premium = mean(size_premium))\n}\n\nTo see how the function compute_portfolio_returns() works, we consider a simple median breakpoint example with value-weighted returns. We are interested in the effect of restricting listing exchanges on the estimation of the size premium. In the first function call, we compute returns based on breakpoints from all listing exchanges. Then, we computed returns based on breakpoints from NYSE-listed stocks.\n\nret_all &lt;- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\nret_nyse &lt;- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\ntibble(\n  Exchanges = c(\"NYSE, NASDAQ & AMEX\", \"NYSE\"),\n  Premium = as.numeric(c(ret_all, ret_nyse)) * 100\n)\n\n# A tibble: 2 × 2\n  Exchanges           Premium\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 NYSE, NASDAQ & AMEX  0.0796\n2 NYSE                 0.155 \n\n\nThe table shows that the size premium is more than 60 percent larger if we consider only stocks from NYSE to form the breakpoint each month. The NYSE-specific breakpoints are larger, and there are more than 50 percent of the stocks in the entire universe in the resulting small portfolio because NYSE firms are larger on average. The impact of this choice is not negligible.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "href": "r/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "title": "Size Sorts and p-Hacking",
    "section": "P-Hacking and Non-standard Errors",
    "text": "P-Hacking and Non-standard Errors\nSince the choice of the listing exchange has a significant impact, the next step is to investigate the effect of other data processing decisions researchers have to make along the way. In particular, any portfolio sort analysis has to decide at least on the number of portfolios, the listing exchanges to form breakpoints, and equal- or value-weighting. Further, one may exclude firms that are active in the finance industry or restrict the analysis to some parts of the time series. All of the variations of these choices that we discuss here are part of scholarly articles published in the top finance journals. We refer to Walter, Weber, and Weiss (2022) for an extensive set of other decision nodes at the discretion of researchers.\nThe intention of this application is to show that the different ways to form portfolios result in different estimated size premiums. Despite the effects of this multitude of choices, there is no correct way. It should also be noted that none of the procedures is wrong, the aim is simply to illustrate the changes that can arise due to the variation in the evidence-generating process (Menkveld et al. 2021). The term non-standard errors refers to the variation due to (suitable) choices made by researchers. Interestingly, in a large scale study, Menkveld et al. (2021) find that the magnitude of non-standard errors are similar than the estimation uncertainty based on a chosen model which shows how important it is to adjust for the seemingly innocent choices in the data preparation and evaluation workflow. \nFrom a malicious perspective, these modeling choices give the researcher multiple chances to find statistically significant results. Yet this is considered p-hacking, which renders the statistical inference due to multiple testing invalid (Harvey, Liu, and Zhu 2016).\nNevertheless, the multitude of options creates a problem since there is no single correct way of sorting portfolios. How should a researcher convince a reader that their results do not come from a p-hacking exercise? To circumvent this dilemma, academics are encouraged to present evidence from different sorting schemes as robustness tests and report multiple approaches to show that a result does not depend on a single choice. Thus, the robustness of premiums is a key feature.\nBelow we conduct a series of robustness tests which could also be interpreted as a p-hacking exercise. To do so, we examine the size premium in different specifications presented in the table p_hacking_setup. The function expand_grid() produces a table of all possible permutations of its arguments. Note that we use the argument data to exclude financial firms and truncate the time series.\n\np_hacking_setup &lt;- expand_grid(\n  n_portfolios = c(2, 5, 10),\n  exchanges = list(\"NYSE\", c(\"NYSE\", \"NASDAQ\", \"AMEX\")),\n  value_weighted = c(TRUE, FALSE),\n  data = parse_exprs(\n    'crsp_monthly; \n     crsp_monthly |&gt; filter(industry != \"Finance\");\n     crsp_monthly |&gt; filter(month &lt; \"1990-06-01\");\n     crsp_monthly |&gt; filter(month &gt;=\"1990-06-01\")'\n  )\n)\n\nTo speed the computation up we parallelize the (many) different sorting procedures, as in Beta Estimation. Finally, we report the resulting size premiums in descending order. There are indeed substantial size premiums possible in our data, in particular when we use equal-weighted portfolios.\n\nn_cores = availableCores() - 1\nplan(multisession, workers = n_cores)\n\np_hacking_setup &lt;- p_hacking_setup |&gt;\n  mutate(size_premium = future_pmap(\n    .l = list(\n      n_portfolios,\n      exchanges,\n      value_weighted,\n      data\n    ),\n    .f = ~ compute_portfolio_returns(\n      n_portfolios = ..1,\n      exchanges = ..2,\n      value_weighted = ..3,\n      data = eval_tidy(..4)\n    )\n  ))\n\np_hacking_results &lt;- p_hacking_setup |&gt;\n  mutate(data = map_chr(data, deparse)) |&gt;\n  unnest(size_premium) |&gt;\n  arrange(desc(size_premium))\np_hacking_results\n\n# A tibble: 48 × 5\n  n_portfolios exchanges value_weighted data             size_premium\n         &lt;dbl&gt; &lt;list&gt;    &lt;lgl&gt;          &lt;chr&gt;                   &lt;dbl&gt;\n1           10 &lt;chr [3]&gt; FALSE          \"filter(crsp_mo…       0.0174\n2           10 &lt;chr [3]&gt; FALSE          \"filter(crsp_mo…       0.0169\n3           10 &lt;chr [3]&gt; FALSE          \"crsp_monthly\"         0.0155\n4           10 &lt;chr [3]&gt; FALSE          \"filter(crsp_mo…       0.0141\n5           10 &lt;chr [3]&gt; TRUE           \"filter(crsp_mo…       0.0110\n# ℹ 43 more rows",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#size-premium-variation",
    "href": "r/size-sorts-and-p-hacking.html#size-premium-variation",
    "title": "Size Sorts and p-Hacking",
    "section": "Size-Premium Variation",
    "text": "Size-Premium Variation\nWe provide a graph in Figure 3 that shows the different premiums. The figure also shows the relation to the average Fama-French SMB (small minus big) premium used in the literature which we include as a dotted vertical line.\n\np_hacking_results |&gt;\n  ggplot(aes(x = size_premium)) +\n  geom_histogram(bins = nrow(p_hacking_results)) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of size premiums for different sorting choices\"\n  ) +\n  geom_vline(aes(xintercept = mean(factors_ff3_monthly$smb)),\n    linetype = \"dashed\"\n  ) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\nFigure 3: The dashed vertical line indicates the average Fama-French SMB premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#exercises",
    "href": "r/size-sorts-and-p-hacking.html#exercises",
    "title": "Size Sorts and p-Hacking",
    "section": "Exercises",
    "text": "Exercises\n\nWe gained several insights on the size distribution above. However, we did not analyze the average size across listing exchanges and industries. Which listing exchanges/industries have the largest firms? Plot the average firm size for the three listing exchanges over time. What do you conclude?\nWe compute breakpoints but do not take a look at them in the exposition above. This might cover potential data errors. Plot the breakpoints for ten size portfolios over time. Then, take the difference between the two extreme portfolios and plot it. Describe your results.\nThe returns that we analyze above do not account for differences in the exposure to market risk, i.e., the CAPM beta. Change the function compute_portfolio_returns() to output the CAPM alpha or beta instead of the average excess return.\nWhile you saw the spread in returns from the p-hacking exercise, we did not show which choices led to the largest effects. Find a way to investigate which choice variable has the largest impact on the estimated size premium.\nWe computed several size premiums, but they do not follow the definition of Fama and French (1993). Which of our approaches comes closest to their SMB premium?\n\n\n\n\nFigure 1: We report the aggregate market capitalization of all stocks that belong to the 1, 5, 10, and 25 percent quantile of the largest firms in the monthly cross-section relative to the market capitalization of all stocks during the month.\nFigure 2: Years are on the horizontal axis and the corresponding share of total market capitalization per listing exchange on the vertical axis.\nFigure 3: The dashed vertical line indicates the average Fama-French SMB premium.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Size Sorts and p-Hacking"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html",
    "href": "r/univariate-portfolio-sorts.html",
    "title": "Univariate Portfolio Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nIn this chapter, we dive into portfolio sorts, one of the most widely used statistical methodologies in empirical asset pricing (e.g., Bali, Engle, and Murray 2016). The key application of portfolio sorts is to examine whether one or more variables can predict future excess returns. In general, the idea is to sort individual stocks into portfolios, where the stocks within each portfolio are similar with respect to a sorting variable, such as firm size. The different portfolios then represent well-diversified investments that differ in the level of the sorting variable. You can then attribute the differences in the return distribution to the impact of the sorting variable. We start by introducing univariate portfolio sorts (which sort based on only one characteristic) and tackle bivariate sorting in Value and Bivariate Sorts.\nA univariate portfolio sort considers only one sorting variable \\(x_{t-1,i}\\). Here, \\(i\\) denotes the stock and \\(t-1\\) indicates that the characteristic is observable by investors at time \\(t\\).\nThe objective is to assess the cross-sectional relation between \\(x_{t-1,i}\\) and, typically, stock excess returns \\(r_{t,i}\\) at time \\(t\\) as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas from the previous chapter as our sorting variable.\nThe current chapter relies on the following set of R packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(lmtest)\nlibrary(broom)\nlibrary(sandwich)\nCompared to previous chapters, we introduce lmtest (Zeileis and Hothorn 2002) for inference for estimated coefficients, broom package (Robinson, Hayes, and Couch 2022) to tidy the estimation output of many estimated linear models, and sandwich (Zeileis 2006) for different covariance matrix estimators",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#data-preparation",
    "href": "r/univariate-portfolio-sorts.html#data-preparation",
    "title": "Univariate Portfolio Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start with loading the required data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat. In particular, we use the monthly CRSP sample as our asset universe. Once we form our portfolios, we use the Fama-French market factor returns to compute the risk-adjusted performance (i.e., alpha). beta is the tibble with market betas computed in the previous chapter.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, month, ret_excess, mktcap_lag) |&gt;\n  collect()\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(month, mkt_excess) |&gt;\n  collect()\n\nbeta &lt;- tbl(tidy_finance, \"beta\") |&gt;\n  select(permno, month, beta_monthly) |&gt;\n  collect()",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "href": "r/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "title": "Univariate Portfolio Sorts",
    "section": "Sorting by Market Beta",
    "text": "Sorting by Market Beta\nNext, we merge our sorting variable with the return data. We use the one-month lagged betas as a sorting variable to ensure that the sorts rely only on information available when we create the portfolios. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month \\(t\\) information is available in month \\(t+1\\). You may be tempted to simply use a call such as crsp_monthly |&gt; group_by(permno) |&gt; mutate(beta_lag = lag(beta))) instead. This procedure, however, does not work correctly if there are non-explicit missing values in the time series.\n\nbeta_lag &lt;- beta |&gt;\n  mutate(month = month %m+% months(1)) |&gt;\n  select(permno, month, beta_lag = beta_monthly) |&gt;\n  drop_na()\n\ndata_for_sorts &lt;- crsp_monthly |&gt;\n  inner_join(beta_lag, join_by(permno, month))\n\nThe first step to conduct portfolio sorts is to calculate periodic breakpoints that you can use to group the stocks into portfolios. For simplicity, we start with the median lagged market beta as the single breakpoint. We then compute the value-weighted returns for each of the two resulting portfolios, which means that the lagged market capitalization determines the weight in weighted.mean().\n\nbeta_portfolios &lt;- data_for_sorts |&gt;\n  group_by(month) |&gt;\n  mutate(\n    breakpoint = median(beta_lag),\n    portfolio = case_when(\n      beta_lag &lt;= breakpoint ~ \"low\",\n      beta_lag &gt; breakpoint ~ \"high\"\n    )\n  ) |&gt;\n  group_by(month, portfolio) |&gt;\n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), \n            .groups = \"drop\")",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#performance-evaluation",
    "href": "r/univariate-portfolio-sorts.html#performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "Performance Evaluation",
    "text": "Performance Evaluation\nWe can construct a long-short strategy based on the two portfolios: buy the high-beta portfolio and, at the same time, short the low-beta portfolio. Thereby, the overall position in the market is net-zero, i.e., you do not need to invest money to realize this strategy in the absence of frictions.\n\nbeta_longshort &lt;- beta_portfolios |&gt;\n  pivot_wider(id_cols = month, names_from = portfolio, values_from = ret) |&gt;\n  mutate(long_short = high - low)\n\nWe compute the average return and the corresponding standard error to test whether the long-short portfolio yields on average positive or negative excess returns. In the asset pricing literature, one typically adjusts for autocorrelation by using Whitney K. Newey and West (1987) \\(t\\)-statistics to test the null hypothesis that average portfolio excess returns are equal to zero. One necessary input for Newey-West standard errors is a chosen bandwidth based on the number of lags employed for the estimation. While it seems that researchers often default on choosing a pre-specified lag length of 6 months, we instead recommend a data-driven approach. This automatic selection is advocated by Whitney K. Newey and West (1994) and available in the sandwich package. To implement this test, we compute the average return via lm() and then employ the coeftest() function. If you want to implement the typical 6-lag default setting, you can enforce it by passing the arguments lag = 6, prewhite = FALSE to the coeftest() function in the code below and it passes them on to NeweyWest().\n\nmodel_fit &lt;- lm(long_short ~ 1, data = beta_longshort)\ncoeftest(model_fit, vcov = NeweyWest)\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -8.39e-05   1.30e-03   -0.06     0.95\n\n\nThe results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint hence does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM yields that the high beta stocks should yield higher expected returns. Our portfolio sort implicitly mimics an investment strategy that finances high beta stocks by shorting low beta stocks. Therefore, one should expect that the average excess returns yield a return that is above the risk-free rate.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "href": "r/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "title": "Univariate Portfolio Sorts",
    "section": "Functional Programming for Portfolio Sorts",
    "text": "Functional Programming for Portfolio Sorts\nNow we take portfolio sorts to the next level. We want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming is very handy: we employ the curly-curly-operator to give us flexibility concerning which variable to use for the sorting, denoted by sorting_variable. We use quantile() to compute breakpoints for n_portfolios. Then, we assign portfolios to stocks using the findInterval() function. The output of the following function is a new column that contains the number of the portfolio to which a stock belongs.\nIn some applications, the variable used for the sorting might be clustered (e.g., at a lower bound of 0). Then, multiple breakpoints may be identical, leading to empty portfolios. Similarly, some portfolios might have a very small number of stocks at the beginning of the sample. Cases where the number of portfolio constituents differs substantially due to the distribution of the characteristics require careful consideration and, depending on the application, might require customized sorting approaches.\n\nassign_portfolio &lt;- function(data, \n                             sorting_variable, \n                             n_portfolios) {\n  # Compute breakpoints\n  breakpoints &lt;- data |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  # Assign portfolios\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  # Output\n  return(assigned_portfolios)\n}\n\nWe can use the above function to sort stocks into ten portfolios each month using lagged betas and compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.\n\nbeta_portfolios &lt;- data_for_sorts |&gt;\n  group_by(month) |&gt;\n  mutate(\n    portfolio = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = beta_lag,\n      n_portfolios = 10\n    ),\n    portfolio = as.factor(portfolio)\n  ) |&gt;\n  group_by(portfolio, month) |&gt;\n  summarize(\n    ret_excess = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )|&gt;\n  left_join(factors_ff3_monthly, join_by(month))",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#more-performance-evaluation",
    "href": "r/univariate-portfolio-sorts.html#more-performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "More Performance Evaluation",
    "text": "More Performance Evaluation\nIn the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns.\n\nbeta_portfolios_summary &lt;- beta_portfolios |&gt;\n  nest(data = c(month, ret_excess, mkt_excess)) |&gt;\n  mutate(estimates = map(\n    data, ~ tidy(lm(ret_excess ~ 1 + mkt_excess, data = .x))\n  )) |&gt;\n  unnest(estimates) |&gt; \n  select(portfolio, term, estimate) |&gt; \n  pivot_wider(names_from = term, values_from = estimate) |&gt; \n  rename(alpha = `(Intercept)`, beta = mkt_excess) |&gt; \n  left_join(\n    beta_portfolios |&gt; \n      group_by(portfolio) |&gt; \n      summarize(ret_excess = mean(ret_excess),\n                .groups = \"drop\"), join_by(portfolio)\n  )\n\nFigure 1 illustrates the CAPM alphas of beta-sorted portfolios. It shows that low beta portfolios tend to exhibit positive alphas, while high beta portfolios exhibit negative alphas.\n\nbeta_portfolios_summary |&gt;\n  ggplot(aes(x = portfolio, y = alpha, fill = portfolio)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"CAPM alphas of beta-sorted portfolios\",\n    x = \"Portfolio\",\n    y = \"CAPM alpha\",\n    fill = \"Portfolio\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  theme(legend.position = \"None\")\n\n\n\n\n\n\n\nFigure 1: Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the entire CRSP period.\n\n\n\n\n\nThese results suggest a negative relation between beta and future stock returns, which contradicts the predictions of the CAPM. According to the CAPM, returns should increase with beta across the portfolios and risk-adjusted returns should be statistically indistinguishable from zero.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#the-security-market-line-and-beta-portfolios",
    "href": "r/univariate-portfolio-sorts.html#the-security-market-line-and-beta-portfolios",
    "title": "Univariate Portfolio Sorts",
    "section": "The Security Market Line and Beta Portfolios",
    "text": "The Security Market Line and Beta Portfolios\nThe CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time. Figure 2 illustrates the security market line: We see that (not surprisingly) the high beta portfolio returns have a high correlation with the market returns. However, it seems like the average excess returns for high beta stocks are lower than what the security market line implies would be an “appropriate” compensation for the high market risk.\n\nsml_capm &lt;- lm(ret_excess ~ 1 + beta, data = beta_portfolios_summary)$coefficients\n\nbeta_portfolios_summary |&gt;\n  ggplot(aes(\n    x = beta, \n    y = ret_excess, \n    color = portfolio\n  )) +\n  geom_point() +\n  geom_abline(\n    intercept = 0,\n    slope = mean(factors_ff3_monthly$mkt_excess),\n    linetype = \"solid\"\n  ) +\n  geom_abline(\n    intercept = sml_capm[1],\n    slope = sml_capm[2],\n    linetype = \"dashed\"\n  ) +\n  scale_y_continuous(\n    labels = percent,\n    limit = c(0, mean(factors_ff3_monthly$mkt_excess) * 2)\n  ) +\n  scale_x_continuous(limits = c(0, 2)) +\n  labs(\n    x = \"Beta\", y = \"Excess return\", color = \"Portfolio\",\n    title = \"Average portfolio excess returns and average beta estimates\"\n  )\n\n\n\n\n\n\n\nFigure 2: Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\n\nTo provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio.\n\nbeta_longshort &lt;- beta_portfolios |&gt;\n  mutate(portfolio = case_when(\n    portfolio == max(as.numeric(portfolio)) ~ \"high\",\n    portfolio == min(as.numeric(portfolio)) ~ \"low\"\n  )) |&gt;\n  filter(portfolio %in% c(\"low\", \"high\")) |&gt;\n  pivot_wider(id_cols = month, \n              names_from = portfolio, \n              values_from = ret_excess) |&gt;\n  mutate(long_short = high - low) |&gt;\n  left_join(factors_ff3_monthly, join_by(month))\n\nAgain, the resulting long-short strategy does not exhibit statistically significant returns.\n\ncoeftest(lm(long_short ~ 1, data = beta_longshort),\n  vcov = NeweyWest\n)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.00156    0.00325    0.48     0.63\n\n\nHowever, the long-short portfolio yields a statistically significant negative CAPM-adjusted alpha, although, controlling for the effect of beta, the average excess stock returns should be zero according to the CAPM. The results thus provide no evidence in support of the CAPM. The negative value has been documented as the so-called betting against beta factor (Frazzini and Pedersen 2014). Betting against beta corresponds to a strategy that shorts high beta stocks and takes a (levered) long position in low beta stocks. If borrowing constraints prevent investors from taking positions on the SML they are instead incentivized to buy high beta stocks, which leads to a relatively higher price (and therefore lower expected returns than implied by the CAPM) for such high beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital constraint investors with lower risk aversion.\n\ncoeftest(lm(long_short ~ 1 + mkt_excess, data = beta_longshort),\n  vcov = NeweyWest\n)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.00470    0.00245   -1.92    0.055 .  \nmkt_excess   1.15395    0.08893   12.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFigure 3 shows the annual returns of the extreme beta portfolios we are mainly interested in. The figure illustrates no consistent striking patterns over the last years; each portfolio exhibits periods with positive and negative annual returns.\n\nbeta_longshort |&gt;\n  group_by(year = year(month)) |&gt;\n  summarize(\n    low = prod(1 + low),\n    high = prod(1 + high),\n    long_short = prod(1 + long_short)\n  ) |&gt;\n  pivot_longer(cols = -year) |&gt;\n  ggplot(aes(x = year, y = 1 - value, fill = name)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~name, ncol = 1) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Annual returns of beta portfolios\",\n    x = NULL, y = NULL\n  )\n\n\n\n\n\n\n\nFigure 3: We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.\n\n\n\n\n\nOverall, this chapter shows how functional programming can be leveraged to form an arbitrary number of portfolios using any sorting variable and how to evaluate the performance of the resulting portfolios. In the next chapter, we dive deeper into the many degrees of freedom that arise in the context of portfolio analysis.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#exercises",
    "href": "r/univariate-portfolio-sorts.html#exercises",
    "title": "Univariate Portfolio Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nTake the two long-short beta strategies based on different numbers of portfolios and compare the returns. Is there a significant difference in returns? How do the Sharpe ratios compare between the strategies? Find one additional portfolio evaluation statistic and compute it.\nWe plotted the alphas of the ten beta portfolios above. Write a function that tests these estimates for significance. Which portfolios have significant alphas?\nThe analysis here is based on betas from monthly returns. However, we also computed betas from daily returns. Re-run the analysis and point out differences in the results.\nGiven the results in this chapter, can you define a long-short strategy that yields positive abnormal returns (i.e., alphas)? Plot the cumulative excess return of your strategy and the market excess return for comparison.\n\n\n\n\nFigure 1: Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the entire CRSP period.\nFigure 2: Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\nFigure 3: We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.",
    "crumbs": [
      "R",
      "Asset Pricing",
      "Univariate Portfolio Sorts"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html",
    "href": "r/wrds-crsp-and-compustat.html",
    "title": "WRDS, CRSP, and Compustat",
    "section": "",
    "text": "Note\n\n\n\nYou are reading Tidy Finance with R. You can find the equivalent chapter for the sibling Tidy Finance with Python here.\nThis chapter shows how to connect to Wharton Research Data Services (WRDS), a popular provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics, CRSP and Compustat. Unfortunately, this data is not freely available, but most students and researchers typically have access to WRDS through their university libraries. Assuming that you have access to WRDS, we show you how to prepare and merge the databases and store them in the SQLite-database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the WRDS database.\nIf you don’t have access to WRDS, but still want to run the code in this book, we refer to our blog post on Dummy Data for Tidy Finance Readers without Access to WRDS, where show how to create a dummy database that contains the WRDS tables and corresponding columns such that all code chunks in this book can be executed with this dummy database.\nFirst, we load the R packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(RSQLite)\nlibrary(dbplyr)\nWe use the same date range as in the previous chapter to ensure consistency.\nstart_date &lt;- ymd(\"1960-01-01\")\nend_date &lt;- ymd(\"2022-12-31\")",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#accessing-wrds",
    "href": "r/wrds-crsp-and-compustat.html#accessing-wrds",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Accessing WRDS",
    "text": "Accessing WRDS\nWRDS is the most widely used source for asset and firm-specific financial data used in academic settings. WRDS is a data platform that provides data validation, flexible delivery options, and access to many different data sources. The data at WRDS is also organized in an SQL database, although they use the PostgreSQL engine. This database engine is just as easy to handle with R as SQLite. We use the RPostgres package to establish a connection to the WRDS database (Wickham, Ooms, and Müller 2022). Note that you could also use the odbc package to connect to a PostgreSQL database, but then you need to install the appropriate drivers yourself. RPostgres already contains a suitable driver.\n\nlibrary(RPostgres)\n\nTo establish a connection to WRDS, you use the function dbConnect() with arguments that specify the WRDS server and your login credentials. We defined environment variables for the purpose of this book because we obviously do not want (and are not allowed) to share our credentials with the rest of the world. See Setting Up Your Environment for information about why and how to create an .Renviron-file. Alternatively, you can replace Sys.getenv(\"WRDS_USER\") and Sys.getenv(\"WRDS_PASSWORD\") with your own credentials (but be careful not to share them with others or the public).\nAdditionally, you have to use two-factor authentication since May 2023 when establishing a PostgreSQL or other remote connections. You have two choices to provide the additional identification. First, if you have Duo Push enabled for your WRDS account, you will receive a push notification on your mobile phone when trying to establish a connection with the code below. Upon accepting the notification, you can continue your work. Second, you can log in to a WRDS website that requires two-factor authentication with your username and the same IP address. Once you have successfully identified yourself on the website, your username-IP combination will be remembered for 30 days, and you can comfortably use the remote connection below.\n\nwrds &lt;- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"WRDS_USER\"),\n  password = Sys.getenv(\"WRDS_PASSWORD\")\n)\n\nThe remote connection to WRDS is very useful. Yet, the database itself contains many different tables. You can check the WRDS homepage to identify the table’s name you are looking for (if you go beyond our exposition). Alternatively, you can also query the data structure with the function dbSendQuery(). If you are interested, there is an exercise below that is based on WRDS’ tutorial on “Querying WRDS Data using R”. Furthermore, the penultimate section of this chapter shows how to investigate the structure of databases.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "href": "r/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Downloading and Preparing CRSP",
    "text": "Downloading and Preparing CRSP\nThe Center for Research in Security Prices (CRSP) provides the most widely used data for US stocks. We use the wrds connection object that we just created to first access monthly CRSP return data.1 Actually, we need two tables to get the desired data: (i) the CRSP monthly security file,\n\nmsf_db &lt;- tbl(wrds, I(\"crsp.msf_v2\"))\n\nand (ii) the identifying information,\n\nstksecurityinfohist_db &lt;- tbl(wrds, I(\"crsp.stksecurityinfohist\"))\n\nWe use the two remote tables to fetch the data we want to put into our local database. Just as above, the idea is that we let the WRDS database do all the work and just download the data that we actually need. We apply common filters and data selection criteria to narrow down our data of interest: (i) we use only stock prices from NYSE, Amex, and NASDAQ (primaryexch %in% c(\"N\", \"A\", \"Q\")) when or after issuance (conditionaltype %in% c(\"RW\", \"NW\")) for actively traded stocks (tradingstatusflg == \"A\")2, (ii) we keep only data in the time windows of interest, (iii) we keep only US-listed stocks as identified via no special share types (sharetype = 'NS'), security type equity (securitytype = 'EQTY'), security sub type common stock (securitysubtype = 'COM'), issuers that are a corporation (issuertype %in% c(\"ACOR\", \"CORP\")), and (iv) we keep only months within permno-specific start dates (secinfostartdt) and end dates (secinfoenddt). As of July 2022, there is no need to additionally download delisting information since it is already contained in the most recent version of msf. Additionally, the industry information in stksecurityinfohist records the historic industry and should be used instead of the one stored under same variable name in msf_v2.\n\ncrsp_monthly &lt;- msf_db |&gt;\n  filter(mthcaldt &gt;= start_date & mthcaldt &lt;= end_date) |&gt;\n  select(-c(siccd, primaryexch, conditionaltype, tradingstatusflg)) |&gt; \n  inner_join(\n    stksecurityinfohist_db |&gt;\n      filter(sharetype == \"NS\" & \n               securitytype == \"EQTY\" & \n               securitysubtype == \"COM\" & \n               usincflg == \"Y\" & \n               issuertype %in% c(\"ACOR\", \"CORP\") & \n               primaryexch %in% c(\"N\", \"A\", \"Q\") &\n               conditionaltype %in% c(\"RW\", \"NW\") &\n               tradingstatusflg == \"A\") |&gt; \n        select(permno, secinfostartdt, secinfoenddt,\n               primaryexch, siccd),\n      join_by(permno)\n  ) |&gt; \n  filter(mthcaldt &gt;= secinfostartdt & mthcaldt &lt;= secinfoenddt) |&gt;\n  mutate(month = floor_date(mthcaldt, \"month\")) |&gt;\n  select(\n    permno, # Security identifier\n    date = mthcaldt, # Date of the observation\n    month, # Month of the observation\n    ret = mthret, # Return\n    shrout, # Shares outstanding (in thousands)\n    prc = mthprc, # Last traded price in a month\n    primaryexch, # Primary exchange code\n    siccd # Industry code\n  ) |&gt;\n  collect() |&gt;\n  mutate(\n    month = ymd(month),\n    shrout = shrout * 1000\n  )\n\nNow, we have all the relevant monthly return data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters.\nThe first additional variable we create is market capitalization (mktcap), which is the product of the number of outstanding shares (shrout) and the last traded price in a month (prc). Note that in contrast to returns (ret), these two variables are not adjusted ex-post for any corporate actions like stock splits. Therefore, if you want to use a stock’s price, you need to adjust it with a cumulative adjustment factor. We also keep the market cap in millions of USD just for convenience, as we do not want to print huge numbers in our figures and tables. In addition, we set zero market capitalization to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(\n    mktcap = shrout * prc / 10^6,\n    mktcap = na_if(mktcap, 0)\n  )\n\nThe next variable we frequently use is the one-month lagged market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly CRSP data.\n\nmktcap_lag &lt;- crsp_monthly |&gt;\n  mutate(month = month %m+% months(1)) |&gt;\n  select(permno, month, mktcap_lag = mktcap)\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(mktcap_lag, join_by(permno, month))\n\nIf you wonder why we do not use the lag() function, e.g., via crsp_monthly |&gt; group_by(permno) |&gt; mutate(mktcap_lag = lag(mktcap)), take a look at the Exercises.\nNext, we transform primary listing exchange codes to explicit exchange names.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(exchange = case_when(\n    primaryexch == \"N\" ~ \"NYSE\",\n    primaryexch == \"A\" ~ \"AMEX\",\n    primaryexch == \"Q\" ~ \"NASDAQ\",\n    .default = \"Other\"\n  ))\n\nSimilarly, we transform industry codes to industry descriptions following Bali, Engle, and Murray (2016). Notice that there are also other categorizations of industries (e.g., Fama and French 1997) that are commonly used.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(industry = case_when(\n    siccd &gt;= 1 & siccd &lt;= 999 ~ \"Agriculture\",\n    siccd &gt;= 1000 & siccd &lt;= 1499 ~ \"Mining\",\n    siccd &gt;= 1500 & siccd &lt;= 1799 ~ \"Construction\",\n    siccd &gt;= 2000 & siccd &lt;= 3999 ~ \"Manufacturing\",\n    siccd &gt;= 4000 & siccd &lt;= 4899 ~ \"Transportation\",\n    siccd &gt;= 4900 & siccd &lt;= 4999 ~ \"Utilities\",\n    siccd &gt;= 5000 & siccd &lt;= 5199 ~ \"Wholesale\",\n    siccd &gt;= 5200 & siccd &lt;= 5999 ~ \"Retail\",\n    siccd &gt;= 6000 & siccd &lt;= 6799 ~ \"Finance\",\n    siccd &gt;= 7000 & siccd &lt;= 8999 ~ \"Services\",\n    siccd &gt;= 9000 & siccd &lt;= 9999 ~ \"Public\",\n    .default = \"Missing\"\n  ))\n\nNext, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop the risk-free rate from our tibble. Note that we ensure excess returns are bounded by -1 from below as a return less than -100 percent makes no sense conceptually. Before we can adjust the returns, we have to connect to our database and load the table factors_ff3_monthly.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(month, rf) |&gt;\n  collect()\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(factors_ff3_monthly,\n    join_by(month)\n  ) |&gt;\n  mutate(\n    ret_excess = ret - rf,\n    ret_excess = pmax(ret_excess, -1)\n  ) |&gt;\n  select(-rf)\n\nSince excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  drop_na(ret_excess, mktcap, mktcap_lag)\n\nFinally, we store the monthly CRSP file in our database.\n\ndbWriteTable(tidy_finance,\n  \"crsp_monthly\",\n  value = crsp_monthly,\n  overwrite = TRUE\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "href": "r/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "title": "WRDS, CRSP, and Compustat",
    "section": "First Glimpse of the CRSP Sample",
    "text": "First Glimpse of the CRSP Sample\nBefore we move on to other data sources, let us look at some descriptive statistics of the CRSP sample, which is our main source for stock returns.\nFigure 1 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks. The number of stocks listed on AMEX decreased steadily over the last couple of decades. By the end of 2022, there were 2,778 stocks with a primary listing on NASDAQ, 1,358 on NYSE, 162 on AMEX, and only one belonged to the other category. \n\ncrsp_monthly |&gt;\n  count(exchange, date) |&gt;\n  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by listing exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\nFigure 1: Number of stocks in the CRSP sample listed at each of the US exchanges.\n\n\n\n\n\nNext, we look at the aggregate market capitalization grouped by the respective listing exchanges in Figure 2. To ensure that we look at meaningful data which is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange and plotting it just as if they were in memory. All values in Figure 2 are at the end of 2022 USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.\n\ntbl(tidy_finance, \"crsp_monthly\") |&gt;\n  left_join(tbl(tidy_finance, \"cpi_monthly\"), join_by(month)) |&gt;\n  group_by(month, exchange) |&gt;\n  summarize(\n    mktcap = sum(mktcap, na.rm = TRUE) / cpi,\n    .groups = \"drop\"\n  ) |&gt;\n  collect() |&gt;\n  mutate(month = ymd(month)) |&gt;\n  ggplot(aes(\n    x = month, y = mktcap / 1000,\n    color = exchange, linetype = exchange\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly market cap by listing exchange in billions of Dec 2022 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\nFigure 2: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the horizontal axis reflect the buying power of billion USD in December 2022.\n\n\n\n\n\nOf course, performing the computation in the database is not really meaningful because we can easily pull all the required data into our memory. The code chunk above is slower than performing the same steps on tables that are already in memory. However, we just want to illustrate that you can perform many things in the database before loading the data into your memory. Before we proceed, we load the monthly CPI data.\n\ncpi_monthly &lt;- tbl(tidy_finance, \"cpi_monthly\") |&gt;\n  collect()\n\nNext, we look at the same descriptive statistics by industry. Figure 3 plots the number of stocks in the sample for each of the SIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.\n\ncrsp_monthly_industry &lt;- crsp_monthly |&gt;\n  left_join(cpi_monthly, join_by(month)) |&gt;\n  group_by(month, industry) |&gt;\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap) / mean(cpi),\n    .groups = \"drop\"\n  )\n\ncrsp_monthly_industry |&gt;\n  ggplot(aes(\n    x = month,\n    y = securities,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\nFigure 3: Number of stocks in the CRSP sample associated with different industries.\n\n\n\n\n\nWe also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in Figure 4. All values are again in terms of billions of end of 2022 USD. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.\n\ncrsp_monthly_industry |&gt;\n  ggplot(aes(\n    x = month,\n    y = mktcap / 1000,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market cap by industry in billions as of Dec 2022 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\nFigure 4: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the y-axis reflect the buying power of billion USD in December 2022.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#daily-crsp-data",
    "href": "r/wrds-crsp-and-compustat.html#daily-crsp-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Daily CRSP Data",
    "text": "Daily CRSP Data\nBefore we turn to accounting data, we provide a proposal for downloading daily CRSP data with the same filters used for the monthly data (i.e., using information from stksecurityinfohist). While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can exceed 20 GB. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire dataset to your local database), and in our experience, the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your R session.\nThere is a solution to this challenge. As with many big data problems, you can split up the big task into several smaller tasks that are easier to handle. That is, instead of downloading data about all stocks at once, download the data in small batches of stocks consecutively. Such operations can be implemented in for-loops, where we download, prepare, and store the data for a small number of stocks in each iteration. This operation might nonetheless take around 5 minutes, depending on your internet connection. To keep track of the progress, we create ad-hoc progress updates using cat(). Notice that we also use the function dbWriteTable() here with the option to append the new data to an existing table, when we process the second and all following batches. As for the monthly CRSP data, there is no need to adjust for delisting returns in the daily CRSP data since July 2022.\n\ndsf_db &lt;- tbl(wrds, I(\"crsp.dsf_v2\"))\nstksecurityinfohist_db &lt;- tbl(wrds, I(\"crsp.stksecurityinfohist\"))\n\nfactors_ff3_daily &lt;- tbl(tidy_finance, \"factors_ff3_daily\") |&gt;\n  collect()\n\npermnos &lt;- stksecurityinfohist_db |&gt;\n  distinct(permno) |&gt; \n  pull(permno)\n\nbatch_size &lt;- 500\nbatches &lt;- ceiling(length(permnos) / batch_size)\n\nfor (j in 1:batches) {\n  \n  permno_batch &lt;- permnos[\n    ((j - 1) * batch_size + 1):min(j * batch_size, length(permnos))\n  ]\n\n  crsp_daily_sub &lt;- dsf_db |&gt;\n    filter(permno %in% permno_batch) |&gt; \n    filter(dlycaldt &gt;= start_date & dlycaldt &lt;= end_date) |&gt; \n    inner_join(\n      stksecurityinfohist_db |&gt;\n        filter(sharetype == \"NS\" & \n                securitytype == \"EQTY\" & \n                securitysubtype == \"COM\" & \n                usincflg == \"Y\" & \n                issuertype %in% c(\"ACOR\", \"CORP\") & \n                primaryexch %in% c(\"N\", \"A\", \"Q\") &\n                conditionaltype %in% c(\"RW\", \"NW\") &\n                tradingstatusflg == \"A\") |&gt; \n        select(permno, secinfostartdt, secinfoenddt),\n      join_by(permno)\n    ) |&gt;\n    filter(dlycaldt &gt;= secinfostartdt & dlycaldt &lt;= secinfoenddt)  |&gt; \n    select(permno, date = dlycaldt, ret = dlyret) |&gt;\n    collect() |&gt;\n    drop_na()\n\n  if (nrow(crsp_daily_sub) &gt; 0) {\n    \n    crsp_daily_sub &lt;- crsp_daily_sub |&gt;\n      mutate(month = floor_date(date, \"month\")) |&gt;\n      left_join(factors_ff3_daily |&gt;\n        select(date, rf), join_by(date)) |&gt;\n      mutate(\n        ret_excess = ret - rf,\n        ret_excess = pmax(ret_excess, -1)\n      ) |&gt;\n      select(permno, date, month, ret, ret_excess)\n\n    dbWriteTable(tidy_finance,\n      \"crsp_daily\",\n      value = crsp_daily_sub,\n      overwrite = ifelse(j == 1, TRUE, FALSE),\n      append = ifelse(j != 1, TRUE, FALSE)\n    )\n  }\n\n  cat(\"Batch\", j, \"out of\", batches, \"done (\", percent(j / batches), \")\\n\")\n}\n\nEventually, we end up with more than 71 million rows of daily return data. Note that we only store the identifying information that we actually need, namely permno, date, and month alongside the excess returns. We thus ensure that our local database contains only the data that we actually use.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "href": "r/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Preparing Compustat Data",
    "text": "Preparing Compustat Data\nFirm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters. The commonly used source for firm financial information is Compustat provided by S&P Global Market Intelligence, which is a global data vendor that provides financial, statistical, and market information on active and inactive companies throughout the world. For US and Canadian companies, annual history is available back to 1950 and quarterly as well as monthly histories date back to 1962.\nTo access Compustat data, we can again tap WRDS, which hosts the funda table that contains annual firm-level information on North American companies.\n\nfunda_db &lt;- tbl(wrds, I(\"comp.funda\"))\n\nWe follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, which includes companies that are primarily involved in manufacturing, services, and other non-financial business activities,3 (ii) in the standard format (i.e., consolidated information in standard presentation), and (iii) only data in the desired time window.\n\ncompustat &lt;- funda_db |&gt;\n  filter(\n    indfmt == \"INDL\" &\n      datafmt == \"STD\" & \n      consol == \"C\" &\n      datadate &gt;= start_date & datadate &lt;= end_date\n  ) |&gt;\n  select(\n    gvkey, # Firm identifier\n    datadate, # Date of the accounting data\n    seq, # Stockholders' equity\n    ceq, # Total common/ordinary equity\n    at, # Total assets\n    lt, # Total liabilities\n    txditc, # Deferred taxes and investment tax credit\n    txdb, # Deferred taxes\n    itcb, # Investment tax credit\n    pstkrv, # Preferred stock redemption value\n    pstkl, # Preferred stock liquidating value\n    pstk, # Preferred stock par value\n    capx, # Capital investment\n    oancf, # Operating cash flow\n    sale,  # Revenue\n    cogs, # Costs of goods sold\n    xint, # Interest expense\n    xsga # Selling, general, and administrative expenses\n  ) |&gt;\n  collect()\n\nNext, we calculate the book value of preferred stock and equity be and the operating profitability op inspired by the variable definitions in Ken French’s data library. Note that we set negative or zero equity to missing which is a common practice when working with book-to-market ratios (see Fama and French 1992 for details).\n\ncompustat &lt;- compustat |&gt;\n  mutate(\n    be = coalesce(seq, ceq + pstk, at - lt) +\n      coalesce(txditc, txdb + itcb, 0) -\n      coalesce(pstkrv, pstkl, pstk, 0),\n    be = if_else(be &lt;= 0, NA, be),\n    op = (sale - coalesce(cogs, 0) - \n            coalesce(xsga, 0) - coalesce(xint, 0)) / be,\n  )\n\nWe keep only the last available information for each firm-year group. Note that datadate defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2022). Therefore, datadate is not the date when data was made available to the public. Check out the exercises for more insights into the peculiarities of datadate.\n\ncompustat &lt;- compustat |&gt;\n  mutate(year = year(datadate)) |&gt;\n  group_by(gvkey, year) |&gt;\n  filter(datadate == max(datadate)) |&gt;\n  ungroup()\n\nWe also compute the investment ratio inv according to Ken French’s variable definitions as the change in total assets from one fiscal year to another. Note that we again use the approach using joins as introduced with the CRSP data above to construct lagged assets.\n\ncompustat &lt;- compustat |&gt; \n  left_join(\n    compustat |&gt; \n      select(gvkey, year, at_lag = at) |&gt; \n      mutate(year = year + 1), \n    join_by(gvkey, year)\n  ) |&gt; \n  mutate(\n    inv = at / at_lag - 1,\n    inv = if_else(at_lag &lt;= 0, NA, inv)\n  )\n\nWith the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database.\n\ndbWriteTable(tidy_finance,\n  \"compustat\",\n  value = compustat,\n  overwrite = TRUE\n)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "href": "r/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Merging CRSP with Compustat",
    "text": "Merging CRSP with Compustat\nUnfortunately, CRSP and Compustat use different keys to identify stocks and firms. CRSP uses permno for stocks, while Compustat uses gvkey to identify firms. Fortunately, a curated matching table on WRDS allows us to merge CRSP and Compustat, so we create a connection to the CRSP-Compustat Merged table (provided by CRSP).\n\nccmxpf_linktable_db &lt;- tbl(wrds, I(\"crsp.ccmxpf_linktable\"))\n\nThe linking table contains links between CRSP and Compustat identifiers from various approaches. However, we need to make sure that we keep only relevant and correct links, again following the description outlined in Bali, Engle, and Murray (2016). Note also that currently active links have no end date, so we just enter the current date via today().\n\nccmxpf_linktable &lt;- ccmxpf_linktable_db |&gt;\n  filter(linktype %in% c(\"LU\", \"LC\") &\n    linkprim %in% c(\"P\", \"C\") &\n    usedflag == 1) |&gt;\n  select(permno = lpermno, gvkey, linkdt, linkenddt) |&gt;\n  collect() |&gt;\n  mutate(linkenddt = replace_na(linkenddt, today()))\n\nWe use these links to create a new table with a mapping between stock identifier, firm identifier, and month. We then add these links to the Compustat gvkey to our monthly stock data.\n\nccm_links &lt;- crsp_monthly |&gt;\n  inner_join(ccmxpf_linktable, \n             join_by(permno), relationship = \"many-to-many\") |&gt;\n  filter(!is.na(gvkey) & \n           (date &gt;= linkdt & date &lt;= linkenddt)) |&gt;\n  select(permno, gvkey, date)\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(ccm_links, join_by(permno, date))\n\nAs the last step, we update the previously prepared monthly CRSP file with the linking information in our local database.\n\ndbWriteTable(tidy_finance,\n  \"crsp_monthly\",\n  value = crsp_monthly,\n  overwrite = TRUE\n)\n\nBefore we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, Figure 5 plots the share of securities with book equity values for each exchange. It turns out that the coverage is pretty bad for AMEX- and NYSE-listed stocks in the 1960s but hovers around 80 percent for all periods thereafter. We can ignore the erratic coverage of securities that belong to the other category since there is only a handful of them anyway in our sample.\n\ncrsp_monthly |&gt;\n  group_by(permno, year = year(month)) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  left_join(compustat, join_by(gvkey, year)) |&gt;\n  group_by(exchange, year) |&gt;\n  summarize(\n    share = n_distinct(permno[!is.na(be)]) / n_distinct(permno),\n    .groups = \"drop\"\n  ) |&gt;\n  ggplot(aes(\n    x = year, \n    y = share, \n    color = exchange,\n    linetype = exchange\n    )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Share of securities with book equity values by exchange\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\nFigure 5: End-of-year share of securities with book equity values by listing exchange.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#some-tricks-for-postgresql-databases",
    "href": "r/wrds-crsp-and-compustat.html#some-tricks-for-postgresql-databases",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Some Tricks for PostgreSQL Databases",
    "text": "Some Tricks for PostgreSQL Databases\nAs we mentioned above, the WRDS database runs on PostgreSQL rather than SQLite. Finding the right tables for your data needs can be tricky in the WRDS PostgreSQL instance, as the tables are organized in schemas. If you wonder what the purpose of schemas is, check out this documetation. For instance, if you want to find all tables that live in the crsp schema, you run\n\ndbListObjects(wrds, Id(schema = \"crsp\"))\n\nThis operation returns a list of all tables that belong to the crsp family on WRDS, e.g., &lt;Id&gt; schema = crsp, table = msenames. Similarly, you can fetch a list of all tables that belong to the comp family via\n\ndbListObjects(wrds, Id(schema = \"comp\"))\n\nIf you want to get all schemas, then run\n\ndbListObjects(wrds)",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#exercises",
    "href": "r/wrds-crsp-and-compustat.html#exercises",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Exercises",
    "text": "Exercises\n\nCheck out the structure of the WRDS database by sending queries in the spirit of “Querying WRDS Data using R” and verify the output with dbListObjects(). How many tables are associated with CRSP? Can you identify what is stored within msp500?\nCompute mkt_cap_lag using lag(mktcap) rather than using joins as above. Filter out all the rows where the lag-based market capitalization measure is different from the one we computed above. Why are the two measures they different?\nPlot the average market capitalization of firms for each exchange and industry, respectively, over time. What do you find?\nIn the compustat table, datadate refers to the date to which the fiscal year of a corresponding firm refers. Count the number of observations in Compustat by month of this date variable. What do you find? What does the finding suggest about pooling observations with the same fiscal year?\nGo back to the original Compustat data in funda_db and extract rows where the same firm has multiple rows for the same fiscal year. What is the reason for these observations?\nKeep the last observation of crsp_monthly by year and join it with the compustat table. Create the following plots: (i) aggregate book equity by exchange over time and (ii) aggregate annual book equity by industry over time. Do you notice any different patterns to the corresponding plots based on market capitalization?\nRepeat the analysis of market capitalization for book equity, which we computed from the Compustat data. Then, use the matched sample to plot book equity against market capitalization. How are these two variables related?\n\n\n\n\nFigure 1: Number of stocks in the CRSP sample listed at each of the US exchanges.\nFigure 2: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the horizontal axis reflect the buying power of billion USD in December 2022.\nFigure 3: Number of stocks in the CRSP sample associated with different industries.\nFigure 4: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the y-axis reflect the buying power of billion USD in December 2022.\nFigure 5: End-of-year share of securities with book equity values by listing exchange.",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#footnotes",
    "href": "r/wrds-crsp-and-compustat.html#footnotes",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe tbl() function creates a lazy table in our R session based on the remote WRDS database. To look up specific tables, we use the I(\"schema_name.table_name\") approach.↩︎\nThese three criteria jointly replicate the filter exchcd %in% c(1, 2, 3, 31, 32, 33) used for the legacy version of CRSP. If you do not want to include stocks at issuance, you can set the conditionaltype == \"RW\", which is equivalent to the restriction of exchcd %in% c(1, 2, 3) with the old CRSP format.↩︎\nCompanies that operate in the banking, insurance, or utilities sector typically report in different industry formats that reflect their specific regulatory requirements.↩︎",
    "crumbs": [
      "R",
      "Financial Data",
      "WRDS, CRSP, and Compustat"
    ]
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Support Tidy Finance",
    "section": "",
    "text": "Tidy Finance is and will remain an open-source project. We are grateful for all the support we have received so far. Of course, we do not force anybody to support us, but every gesture is very much appreciated. We have three options if you want to give something back and support our efforts. Moreover, most options come at no additional cost to you, i.e., they just increase our share of the pie. Who does not appreciate a little more pie?"
  },
  {
    "objectID": "support.html#get-your-copy-of-the-books",
    "href": "support.html#get-your-copy-of-the-books",
    "title": "Support Tidy Finance",
    "section": "Get your copy of the books",
    "text": "Get your copy of the books\nYou can read the free online versions of Tidy Finance on this website. However, you can also get your own physical copy! The book comes with many perks, such as the joy of holding something in your hand, a fresh smell, and it certainly looks good in your library. If you decide to buy your own copy, please consider using our affiliate link from Routledge. No extra cost to you, just some pie for us.\nNote that some affiliate links track your behavior on the site and can be flagged as suspicious by your browser. We exclusively use the official paths provided by the respective vendor. Alternatively, our book is also available on Amazon and other retailers."
  },
  {
    "objectID": "support.html#spread-the-word",
    "href": "support.html#spread-the-word",
    "title": "Support Tidy Finance",
    "section": "Spread the word",
    "text": "Spread the word\nThe project grows with the attention it receives from the community. Therefore, making people aware of Tidy Finance is a great way to support it. There are certainly many possibilities how you can spread the word. For example, you could\n\nContribute to the Tidy Finance blog\nCite the book in one of your projects\nUse Tidy Finance as a teaching resource and let us know\nConnect with us and share posts about Tidy Finance via social media\nYou can also buy Tidy Finance Swag\n\nThese are just a few suggestions, yet highly effective. In any case, we rely on your support to share Tidy Finance within your own community."
  },
  {
    "objectID": "support.html#buy-us-a-coffee",
    "href": "support.html#buy-us-a-coffee",
    "title": "Support Tidy Finance",
    "section": "Buy us a coffee",
    "text": "Buy us a coffee\nEvery task requires some fuel. In particular, one key ingredient to completing the mental efforts that culminate in Tidy Finance is, of course, coffee. Hence, if you appreciate Tidy Finance, let us have a coffee. We are grateful for every small contribution to sustain our caffeine levels. Moreover, higher caffeine levels positively correlate with new content on Tidy Finance. It is a win-win situation!"
  }
]