[
  {
    "objectID": "r/replicating-fama-and-french-factors.html",
    "href": "r/replicating-fama-and-french-factors.html",
    "title": "Replicating Fama and French Factors",
    "section": "",
    "text": "In this chapter, we provide a replication of the famous Fama and French factor portfolios. The Fama and French factor models are a cornerstone of empirical asset pricing Fama and French (2015). On top of the market factor represented by the traditional CAPM beta, the three factor model includes the size and value factors to explain the cross section of returns. Its successor, the five factor model, additionally includes profitability and investment as explanatory factors.\nWe start with the three factor model. We already introduced the size and value factors in Value and Bivariate Sorts, and their definition remains the same: size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nAfter the replication of the three factor model, we move to the five factors by constructing the profitability factor RMW (robust-minus-weak) as the difference between the returns of firms with high and low operating profitability and the investment factor CMA (conservative-minus-aggressive) as the difference between firms with high versus low investment rates.\nThe current chapter relies on this set of packages.\nlibrary(tidyverse)\nlibrary(RSQLite)"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#data-preparation",
    "href": "r/replicating-fama-and-french-factors.html#data-preparation",
    "title": "Replicating Fama and French Factors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and Compustat as data sources, as we need the same variables to compute the factors in the way Fama and French do it. Hence, there is nothing new below and we only load data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(\n    permno, gvkey, month, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n    select(gvkey, datadate, be, op, inv) |&gt;\n    collect() \n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(month, smb, hml) |&gt;\n  collect()\n\nfactors_ff5_monthly &lt;- tbl(tidy_finance, \"factors_ff5_monthly\") |&gt;\n  select(month, smb, hml, rmw, cma) |&gt;\n  collect()\n\nYet when we start merging our data set for computing the premiums, there are a few differences to Value and Bivariate Sorts. First, Fama and French form their portfolios in June of year \\(t\\), whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year \\(t+1\\).\nSecond, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year \\(t - 1\\) and the book equity reported in year \\(t-1\\), i.e., the datadate is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity. The other sorting variables are analogously to book equity taken from year \\(t-1\\).\nTo implement all these time lags, we again employ the temporary sorting_date-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year. We ensure this by a call of distinct() at the end of the chunk below.\n\nsize &lt;- crsp_monthly |&gt;\n  filter(month(month) == 6) |&gt;\n  mutate(sorting_date = month %m+% months(1)) |&gt;\n  select(permno, exchange, sorting_date, size = mktcap)\n\nmarket_equity &lt;- crsp_monthly |&gt;\n  filter(month(month) == 12) |&gt;\n  mutate(sorting_date = ymd(str_c(year(month) + 1, \"0701)\"))) |&gt;\n  select(permno, gvkey, sorting_date, me = mktcap)\n\nbook_to_market &lt;- compustat |&gt;\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |&gt;\n  select(gvkey, sorting_date, be) |&gt;\n  inner_join(market_equity, by = c(\"gvkey\", \"sorting_date\")) |&gt;\n  mutate(bm = be / me) |&gt;\n  select(permno, sorting_date, me, bm)\n\nsorting_variables &lt;- size |&gt;\n  inner_join(\n    book_to_market, by = c(\"permno\", \"sorting_date\")\n    ) |&gt;\n  drop_na() |&gt;\n  distinct(permno, sorting_date, .keep_all = TRUE)"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "href": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "title": "Replicating Fama and French Factors",
    "section": "Portfolio Sorts",
    "text": "Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on NYSE-specific breakpoints, they form two portfolios in the size dimension at the median and three portfolios in the dimension of each other sorting variable at the 30%- and 70%-percentiles, and they use dependent sorts. The sorts for book-to-market require an adjustment to the function in Chapter 9 because the seq() we would produce does not produce the right breakpoints. Instead of n_portfolios, we now specify percentiles, which take the breakpoint-sequence as an object specified in the function’s call. Specifically, we give percentiles = c(0, 0.3, 0.7, 1) to the function. Additionally, we perform an inner_join() with our return data to ensure that we only use traded stocks when computing the breakpoints as a first step.\n\nassign_portfolio &lt;- function(data, \n                             sorting_variable, \n                             percentiles) {\n  breakpoints &lt;- data |&gt;\n    filter(exchange == \"NYSE\") |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = percentiles,\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  return(assigned_portfolios)\n}\n\nportfolios &lt;- sorting_variables |&gt;\n  group_by(sorting_date) |&gt;\n  mutate(\n    portfolio_size = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = size,\n      percentiles = c(0, 0.5, 1)\n    ),\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = bm,\n      percentiles = c(0, 0.3, 0.7, 1)\n    )\n  ) |&gt;\n  ungroup() |&gt; \n  select(permno, sorting_date, \n         portfolio_size, portfolio_bm)\n\nNext, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column sorting_date in our return data by setting the date to sort on to July of \\(t-1\\) if the month is June (of year \\(t\\)) or earlier or to July of year \\(t\\) if the month is July or later.\n\nportfolios &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = case_when(\n    month(month) &lt;= 6 ~ ymd(str_c(year(month) - 1, \"0701\")),\n    month(month) &gt;= 7 ~ ymd(str_c(year(month), \"0701\"))\n  )) |&gt;\n  inner_join(portfolios, by = c(\"permno\", \"sorting_date\"))"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#fama-and-french-three-factor-model",
    "href": "r/replicating-fama-and-french-factors.html#fama-and-french-three-factor-model",
    "title": "Replicating Fama and French Factors",
    "section": "Fama and French Three Factor Model",
    "text": "Fama and French Three Factor Model\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama and French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally.\n\nfactors_replicated &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_bm, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\"\n  ) |&gt;\n  group_by(month) |&gt;\n  summarize(\n    smb_replicated = mean(ret[portfolio_size == 1]) -\n      mean(ret[portfolio_size == 2]),\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "href": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "title": "Replicating Fama and French Factors",
    "section": "Replication Evaluation",
    "text": "Replication Evaluation\nIn the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using lm(). If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to 1 (indicating a high correlation), and an adjusted R-squared close to 1 (indicating a high proportion of explained variance).\n\ntest &lt;- factors_ff3_monthly |&gt;\n  inner_join(factors_replicated, by = \"month\") |&gt;\n  mutate(\n    across(c(smb_replicated, hml_replicated), ~round(., 4))\n  )\n\nTo test the success of the SMB factor, we hence run the following regression:\n\nmodel_smb &lt;- lm(smb ~ smb_replicated, data = test)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.020279 -0.001531 -0.000054  0.001505  0.015491 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.000147   0.000132   -1.11     0.27    \nsmb_replicated  0.992890   0.004374  226.99   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00356 on 724 degrees of freedom\nMultiple R-squared:  0.986, Adjusted R-squared:  0.986 \nF-statistic: 5.15e+04 on 1 and 724 DF,  p-value: &lt;2e-16\n\n\nThe results for the SMB factor are really convincing as all three criteria outlined above are met and the coefficient is 0.99 and the R-squared is at 99%.\n\nmodel_hml &lt;- lm(hml ~ hml_replicated, data = test)\nsummary(model_hml)\n\n\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02320 -0.00287 -0.00010  0.00229  0.03391 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000298   0.000221    1.35     0.18    \nhml_replicated 0.962310   0.007546  127.52   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00594 on 724 degrees of freedom\nMultiple R-squared:  0.957, Adjusted R-squared:  0.957 \nF-statistic: 1.63e+04 on 1 and 724 DF,  p-value: &lt;2e-16\n\n\nThe replication of the HML factor is also a success, although at a slightly lower coefficient of 0.96 and an R-squared around 96%.\nThe evidence hence allows us to conclude that we did a relatively good job in replicating the original Fama-French size and value premiums, although we do not know their underlying code. From our perspective, a perfect match is only possible with additional information from the maintainers of the original data."
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#fama-and-french-five-factor-model",
    "href": "r/replicating-fama-and-french-factors.html#fama-and-french-five-factor-model",
    "title": "Replicating Fama and French Factors",
    "section": "Fama and French Five Factor Model",
    "text": "Fama and French Five Factor Model\nNow, let us move to the replication of the five factor model. We extend the other_sorting_variables table from above with the additional characteristics operating profitability op and investment inv. Note that the drop_na() statement yields different sample sizes as some firms with be values might not have op or inv values.\n\nother_sorting_variables &lt;- compustat |&gt;\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |&gt;\n  select(gvkey, sorting_date, be, op, inv) |&gt;\n  inner_join(market_equity, by = c(\"gvkey\", \"sorting_date\")) |&gt;\n  mutate(bm = be / me) |&gt;\n  select(permno, sorting_date, me, be, bm, op, inv)\n\nsorting_variables &lt;- size |&gt;\n  inner_join(\n    other_sorting_variables, by = c(\"permno\", \"sorting_date\")\n    ) |&gt;\n  drop_na() |&gt;\n  distinct(permno, sorting_date, .keep_all = TRUE)\n\nIn each month, we independently sort all stocks into the two size portfolios. The value, profitability, and investment portfolios, on the other hand, are the results of dependent sorts based on the size portfolios. We then merge the portfolios to the return data for the rest of the year just as above.\n\nportfolios &lt;- sorting_variables |&gt;\n  group_by(sorting_date) |&gt;\n  mutate(\n    portfolio_size = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = size,\n      percentiles = c(0, 0.5, 1)\n    )) |&gt; \n  group_by(sorting_date, portfolio_size) |&gt; \n  mutate(\n    across(c(bm, op, inv), ~assign_portfolio(\n      data = pick(everything()), \n      sorting_variable = ., \n      percentiles = c(0, 0.3, 0.7, 1)),\n      .names = \"portfolio_{.col}\"\n    )\n  ) |&gt;\n  ungroup() |&gt; \n  select(permno, sorting_date, \n         portfolio_size, portfolio_bm,\n         portfolio_op, portfolio_inv)\n\nportfolios &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = case_when(\n    month(month) &lt;= 6 ~ ymd(str_c(year(month) - 1, \"0701\")),\n    month(month) &gt;= 7 ~ ymd(str_c(year(month), \"0701\"))\n  )) |&gt;\n  inner_join(portfolios, by = c(\"permno\", \"sorting_date\"))\n\nNow, we want to construct each of the factors, but this time the size factor actually comes last because it is the result of averaging across all other factor portfolios. This dependency is the reason why we keep the table with value-weighted portfolio returns as a separate object that we reuse later. We construct the value factor, HML, as above by going long the two portfolios with high book-to-market ratios and shorting the two portfolios with low book-to-market.\n\nportfolios_value &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_bm, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), \n    .groups = \"drop\"\n  )\n\nfactors_value &lt;- portfolios_value |&gt;\n  group_by(month) |&gt;\n  summarize(\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )\n\nFor the profitability factor, RMW, we take a long position in the two high profitability portfolios and a short position in the two low profitability portfolios.\n\nportfolios_profitability &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_op, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), \n    .groups = \"drop\"\n  ) \n\nfactors_profitability &lt;- portfolios_profitability |&gt;\n  group_by(month) |&gt;\n  summarize(\n    rmw_replicated = mean(ret[portfolio_op == 3]) -\n      mean(ret[portfolio_op == 1])\n  )\n\nFor the investment factor, CMA, we go long the two low investment portfolios and short the two high investment portfolios.\n\nportfolios_investment &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_inv, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), \n    .groups = \"drop\"\n  )\n\nfactors_investment &lt;- portfolios_investment |&gt;\n  group_by(month) |&gt;\n  summarize(\n    cma_replicated = mean(ret[portfolio_inv == 1]) -\n      mean(ret[portfolio_inv == 3])\n  )\n\nFinally, the size factor, SMB, is constructed by going long the six small portfolios and short the six large portfolios.\n\nfactors_size &lt;- bind_rows(\n  portfolios_value,\n  portfolios_profitability,\n  portfolios_investment\n) |&gt; \n  group_by(month) |&gt;\n  summarize(\n    smb_replicated = mean(ret[portfolio_size == 1]) -\n      mean(ret[portfolio_size == 2])\n  )\n\nWe then join all factors together into one data frame and construct again a suitable table to run tests for evaluating our replication.\n\nfactors_replicated &lt;- factors_size |&gt;\n  full_join(\n    factors_value, by = \"month\"\n  ) |&gt;\n  full_join(\n    factors_profitability, by = \"month\"\n  ) |&gt;\n  full_join(\n    factors_investment, by = \"month\"\n  )\n\ntest &lt;- factors_ff5_monthly |&gt;\n  inner_join(factors_replicated, by = \"month\") |&gt;\n  mutate(\n    across(c(smb_replicated, hml_replicated, \n             rmw_replicated, cma_replicated), ~round(., 4))\n  )\n\nLet us start the replication evaluation again with the size factor:\n\nmodel_smb &lt;- lm(smb ~ smb_replicated, data = test)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.018178 -0.001872  0.000205  0.001961  0.014396 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.000179   0.000135   -1.33     0.18    \nsmb_replicated  0.969389   0.004328  223.96   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00356 on 700 degrees of freedom\nMultiple R-squared:  0.986, Adjusted R-squared:  0.986 \nF-statistic: 5.02e+04 on 1 and 700 DF,  p-value: &lt;2e-16\n\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient is 0.97 and the R-squared is at 99%.\n\nmodel_hml &lt;- lm(hml ~ hml_replicated, data = test)\nsummary(model_hml)\n\n\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.04451 -0.00413 -0.00034  0.00406  0.03670 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000477   0.000300    1.59     0.11    \nhml_replicated 0.989758   0.010621   93.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00793 on 700 degrees of freedom\nMultiple R-squared:  0.925, Adjusted R-squared:  0.925 \nF-statistic: 8.68e+03 on 1 and 700 DF,  p-value: &lt;2e-16\n\n\nThe replication of the HML factor is also a success, although at a slightly higher coefficient of 0.99 and an R-squared around 93%.\n\nmodel_rmw &lt;- lm(rmw ~ rmw_replicated, data = test)\nsummary(model_rmw)\n\n\nCall:\nlm(formula = rmw ~ rmw_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.019828 -0.003065  0.000123  0.003257  0.018781 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.76e-05   2.04e-04    0.38      0.7    \nrmw_replicated 9.55e-01   9.02e-03  105.82   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00536 on 700 degrees of freedom\nMultiple R-squared:  0.941, Adjusted R-squared:  0.941 \nF-statistic: 1.12e+04 on 1 and 700 DF,  p-value: &lt;2e-16\n\n\nWe are also able to replicate the RMW factor quite well with a coefficient of 0.95 and an R-squared around 94%.\n\nmodel_cma &lt;- lm(cma ~ cma_replicated, data = test)\nsummary(model_cma)\n\n\nCall:\nlm(formula = cma ~ cma_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.015112 -0.002740 -0.000193  0.002429  0.021362 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000717   0.000173    4.15  3.7e-05 ***\ncma_replicated 0.964981   0.008591  112.32  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00455 on 700 degrees of freedom\nMultiple R-squared:  0.947, Adjusted R-squared:  0.947 \nF-statistic: 1.26e+04 on 1 and 700 DF,  p-value: &lt;2e-16\n\n\nFinally, the CMA factor also replicates well with a coefficient of 0.96 and an R-squared around 95%.\nOverall, our approach seems to replicate the Fama-French three and five factor models just as well as the three factors."
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#exercises",
    "href": "r/replicating-fama-and-french-factors.html#exercises",
    "title": "Replicating Fama and French Factors",
    "section": "Exercises",
    "text": "Exercises\n\nFama and French (1993) claim that their sample excludes firms until they have appeared in Compustat for two years. Implement this additional filter and compare the improvements of your replication effort.\nOn his homepage, Kenneth French provides instructions on how to construct the most common variables used for portfolio sorts. Try to replicate the univariate portfolio sort return time series for E/P (earnings / price) provided on his homepage and evaluate your replication effort using regressions."
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html",
    "href": "python/replicating-fama-and-french-factors.html",
    "title": "Replicating Fama and French Factors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\nIn this chapter, we provide a replication of the famous Fama and French factor portfolios. The Fama and French factor models are a cornerstone of empirical asset pricing Fama and French (2015). On top of the market factor represented by the traditional CAPM beta, the three factor model includes the size and value factors to explain the cross section of returns. Its successor, the five factor model, additionally includes profitability and investment as explanatory factors.\nWe start with the three factor model. We already introduced the size and value factors in Value and Bivariate Sorts, and their definition remains the same: size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nAfter the replication of the three factor model, we move to the five factors by constructing the profitability factor RMW (robust-minus-weak) as the difference between the returns of firms with high and low operating profitability and the investment factor CMA (conservative-minus-aggressive) as the difference between firms with high versus low investment rates.\nThe current chapter relies on this set of packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#data-preparation",
    "href": "python/replicating-fama-and-french-factors.html#data-preparation",
    "title": "Replicating Fama and French Factors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and Compustat as data sources, as we need exactly the same variables to compute the size and value factors in the way Fama and French do it. Hence, there is nothing new below and we only load data from our database introduced in Chapters 2-4.\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=\"\"\"SELECT permno, gvkey, month, ret_excess, mktcap, \n                  mktcap_lag, exchange \n            FROM crsp_monthly\"\"\",\n    con=tidy_finance,\n    parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n  .dropna()\n)\n\ncompustat = (pd.read_sql_query(\n    sql=\"\"\"SELECT gvkey, datadate, be, op, inv \n            FROM compustat\"\"\",\n    con=tidy_finance,\n    parse_dates={\"datadate\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n  .dropna()\n)\n\nfactors_ff3_monthly = (pd.read_sql_query(\n    sql=\"\"\"SELECT month, smb, hml \n            FROM factors_ff3_monthly\"\"\",\n    con=tidy_finance,\n    parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n  .dropna()\n)\n\nfactors_ff5_monthly = (pd.read_sql_query(\n    sql=\"\"\"SELECT month, smb, hml, rmw, cma \n            FROM factors_ff5_monthly\"\"\",\n    con=tidy_finance,\n    parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n  .dropna()\n)\n\nYet when we start merging our data set for computing the premiums, there are a few differences to Value and Bivariate Sorts. First, Fama and French form their portfolios in June of year \\(t\\), whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year \\(t+1\\).\nSecond, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year \\(t - 1\\) and the book equity reported in year \\(t-1\\), i.e., the datadate is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity.\nTo implement all these time lags, we again employ the temporary sorting_date-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year. We ensure this by a call of drop_duplicates() at the end of the chunk below.\n\nsize = (crsp_monthly\n  .query(\"month.dt.month == 6\")\n  .assign(\n    sorting_date = lambda x: (x[\"month\"] + \n                                pd.DateOffset(months=1))\n  )\n  .get([\"permno\", \"exchange\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"size\"})\n)\n\nmarket_equity = (crsp_monthly\n  .query(\"month.dt.month == 12\")\n  .assign(\n    sorting_date = lambda x: (x[\"month\"] + \n                                pd.DateOffset(months=7))\n  )\n  .get([\"permno\", \"gvkey\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"me\"})\n)\n\nbook_to_market = (compustat\n  .assign(\n    sorting_date = lambda x: (pd.to_datetime(\n      (x[\"datadate\"].dt.year + 1).astype(str) + \n        \"0701\", format=\"%Y%m%d\"))\n  )\n  .merge(market_equity,\n         how=\"inner\",\n         on=[\"gvkey\", \"sorting_date\"])\n  .assign(bm = lambda x: x[\"be\"] / x[\"me\"])\n  .get([\"permno\", \"sorting_date\", \"me\", \"bm\"])\n)\n\nsorting_variables = (size\n  .merge(book_to_market, \n         how=\"inner\", \n         on=[\"permno\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"permno\", \"sorting_date\"])\n )"
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#portfolio-sorts",
    "href": "python/replicating-fama-and-french-factors.html#portfolio-sorts",
    "title": "Replicating Fama and French Factors",
    "section": "Portfolio Sorts",
    "text": "Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on NYSE-specific breakpoints, they form two portfolios in the size dimension at the median and three portfolios in the dimension of book-to-market at the 30%- and 70%-percentiles, and they use independent sorts. The sorts for book-to-market require an adjustment to the function in Chapter 9 because it would not produce the right breakpoints. Instead of n_portfolios, we now specify percentiles, which take the breakpoint-sequence as an object specified in the function’s call. Specifically, we give percentiles = [0, 0.3, 0.7, 1] to the function. Additionally, we perform a join with our return data to ensure that we only use traded stocks when computing the breakpoints as a first step.\n\ndef assign_portfolio(data, sorting_variable, percentiles):\n    breakpoints = (data\n        .query(\"exchange == 'NYSE'\")\n        .get(sorting_variable)\n        .quantile(percentiles, interpolation = \"linear\")\n        )\n    breakpoints.iloc[0] = -np.Inf\n    breakpoints.iloc[breakpoints.size-1] = np.Inf\n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=pd.Series(range(1, breakpoints.size)),\n      include_lowest=True\n    )\n    return assigned_portfolios\n\nportfolios = (sorting_variables\n  .groupby(\"sorting_date\", group_keys=False)\n  .apply(lambda x: x\n         .assign(portfolio_size = assign_portfolio(\n                    x, \"size\", [0, 0.5, 1]\n                  ),\n                 portfolio_bm = assign_portfolio(\n                    x, \"bm\", [0, 0.3, 0.7, 1]))\n  )\n  .reset_index(drop=True)\n  .get([\"permno\", \"sorting_date\", \n        \"portfolio_size\", \"portfolio_bm\"])\n)\n\nNext, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column sorting_date in our return data by setting the date to sort on to July of \\(t-1\\) if the month is June (of year \\(t\\)) or earlier or to July of year \\(t\\) if the month is July or later.\n\nportfolios = (crsp_monthly\n  .assign(\n    sorting_date = lambda x: (pd.to_datetime(\n      x[\"month\"].apply(lambda x: str(x.year - 1) +\n        \"0701\" if x.month &lt;= 6 else str(x.year) + \"0701\")))\n  )\n  .merge(portfolios,\n         how=\"inner\", \n         on=[\"permno\", \"sorting_date\"])\n)"
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#fama-and-french-three-factor-model",
    "href": "python/replicating-fama-and-french-factors.html#fama-and-french-three-factor-model",
    "title": "Replicating Fama and French Factors",
    "section": "Fama and French Three Factor Model",
    "text": "Fama and French Three Factor Model\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama and French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally.\n\nfactors_replicated = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"month\"])\n  .apply(lambda x: pd.Series(\n    {\"ret\": np.average(x[\"ret_excess\"], \n                       weights=x[\"mktcap_lag\"])})\n   )\n  .reset_index()\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"smb_replicated\": x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - x[\"ret\"][x[\"portfolio_size\"] == 2].mean(),\n    \"hml_replicated\": x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() - x[\"ret\"][x[\"portfolio_bm\"] == 1].mean()\n    }))\n)"
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#replication-evaluation",
    "href": "python/replicating-fama-and-french-factors.html#replication-evaluation",
    "title": "Replicating Fama and French Factors",
    "section": "Replication Evaluation",
    "text": "Replication Evaluation\nIn the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using smf.ols(). If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to 1 (indicating a high correlation), and an adjusted R-squared close to 1 (indicating a high proportion of explained variance).\n\ntest = (factors_ff3_monthly\n  .merge(factors_replicated, \n         how=\"inner\", \n         on=\"month\")\n  .round(decimals=4)\n)\n\nTo test the success of the SMB factor, we hence run the following regression:\n\nmodel_smb = (smf.ols(\n    formula=\"smb ~ smb_replicated\", data=test\n  )\n  .fit()\n)\nfor table in model_smb.summary(slim=True).tables:\n  print(table)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    smb   R-squared:                       0.986\nModel:                            OLS   Adj. R-squared:                  0.986\nNo. Observations:                 714   F-statistic:                 5.169e+04\nCovariance Type:            nonrobust   Prob (F-statistic):               0.00\n==============================================================================\n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept         -0.0002      0.000     -1.387      0.166      -0.000    7.67e-05\nsmb_replicated     0.9936      0.004    227.345      0.000       0.985       1.002\n==================================================================================\n\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient is python np.round(model_smb.params[\"smb_replicated\"], 2) and R-squared are at python np.round(model_smb.rsquared_adj, 2) * 100%.\n\nmodel_hml = (smf.ols(\n    formula=\"hml ~ hml_replicated\", data=test\n  )\n  .fit()\n)\nfor table in model_hml.summary(slim=True).tables:\n  print(table)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    hml   R-squared:                       0.959\nModel:                            OLS   Adj. R-squared:                  0.959\nNo. Observations:                 714   F-statistic:                 1.652e+04\nCovariance Type:            nonrobust   Prob (F-statistic):               0.00\n==============================================================================\n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          0.0004      0.000      1.666      0.096   -6.56e-05       0.001\nhml_replicated     0.9622      0.007    128.544      0.000       0.948       0.977\n==================================================================================\n\n\nThe replication of the HML factor is also a success, although at a slightly lower level with coefficient of python np.round(model_hml.params[\"hml_replicated\"], 2) and R-squared around python np.round(model_hml.rsquared_adj, 2) * 100%.\nThe evidence hence allows us to conclude that we did a relatively good job in replicating the original Fama-French size and value premiums, although we do not know their underlying code. From our perspective, a perfect match is only possible with additional information from the maintainers of the original data."
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#fama-and-french-five-factor-model",
    "href": "python/replicating-fama-and-french-factors.html#fama-and-french-five-factor-model",
    "title": "Replicating Fama and French Factors",
    "section": "Fama and French Five Factor Model",
    "text": "Fama and French Five Factor Model\nNow, let us move to the replication of the five factor model. We extend the other_sorting_variables table from above with the additional characteristics operating profitability op and investment inv. Note that the dropna() statement yields different sample sizes as some firms with be values might not have op or inv values.\n\nother_sorting_variables = (compustat\n  .assign(\n    sorting_date = lambda x: (pd.to_datetime(\n      (x[\"datadate\"].dt.year + 1).astype(str) + \n        \"0701\", format=\"%Y%m%d\"))\n  )\n  .merge(market_equity,\n         how=\"inner\",\n         on=[\"gvkey\", \"sorting_date\"])\n  .assign(bm = lambda x: x[\"be\"] / x[\"me\"])\n  .get([\"permno\", \"sorting_date\", \"me\", \"bm\", \"op\", \"inv\"])\n)\n\nsorting_variables = (size\n  .merge(other_sorting_variables, \n         how=\"inner\", \n         on=[\"permno\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"permno\", \"sorting_date\"])\n )\n\nIn each month, we independently sort all stocks into the two size portfolios. The value, profitability, and investment portfolios, on the other hand, are the results of dependent sorts based on the size portfolios. We then merge the portfolios to the return data for the rest of the year just as above.\n\nportfolios = (sorting_variables\n  .groupby(\"sorting_date\", as_index=False)\n  .apply(lambda x: x\n         .assign(portfolio_size = assign_portfolio(\n                    x, \"size\", [0, 0.5, 1]))\n  )\n  .groupby([\"sorting_date\", \"portfolio_size\"], as_index=False)\n  .apply(lambda x: x\n         .assign(portfolio_bm = assign_portfolio(\n                    x, \"bm\", [0, 0.3, 0.7, 1]),\n                 portfolio_op = assign_portfolio(\n                    x, \"op\", [0, 0.3, 0.7, 1]),\n                 portfolio_inv = assign_portfolio(\n                    x, \"inv\", [0, 0.3, 0.7, 1]))\n  )\n  .get([\"permno\", \"sorting_date\",\n        \"portfolio_size\", \"portfolio_bm\",\n        \"portfolio_op\", \"portfolio_inv\"])\n)\n\nportfolios = (crsp_monthly\n  .assign(\n    sorting_date = lambda x: (pd.to_datetime(\n      x[\"month\"].apply(lambda x: str(x.year - 1) +\n        \"0701\" if x.month &lt;= 6 else str(x.year) + \"0701\")))\n  )\n  .merge(portfolios,\n         how=\"inner\", \n         on=[\"permno\", \"sorting_date\"])\n)\n\nNow, we want to construct each of the factors, but this time the size factor actually comes last because it is the result of averaging across all other factor portfolios. This dependency is the reason why we keep the table with value-weighted portfolio returns as a separate object that we reuse later. We construct the value factor, HML, as above by going long the two portfolios with high book-to-market ratios and shorting the two portfolios with low book-to-market.\n\nportfolios_value = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"month\"], \n           as_index=False)\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], \n                        weights=x[\"mktcap_lag\"])})\n  )\n)\n\nfactors_value = (portfolios_value\n  .groupby(\"month\", as_index=False)\n  .apply(lambda x: pd.Series({\n    \"hml_replicated\": (\n      x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_bm\"] == 1].mean())})\n  )\n)\n\nFor the profitability factor, RMW, we take a long position in the two high profitability portfolios and a short position in the two low profitability portfolios.\n\nportfolios_profitability = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_op\", \"month\"], \n           as_index=False)\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], \n                        weights=x[\"mktcap_lag\"])})\n  )\n)\n\nfactors_profitability = (portfolios_profitability\n  .groupby(\"month\", as_index=False)\n  .apply(lambda x: pd.Series({\n    \"rmw_replicated\": (\n      x[\"ret\"][x[\"portfolio_op\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_op\"] == 1].mean())})\n  )\n)\n\nFor the investment factor, CMA, we go long the two low investment portfolios and short the two high investment portfolios.\n\nportfolios_investment = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_inv\", \"month\"], \n           as_index=False)\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], \n                       weights=x[\"mktcap_lag\"])})\n  )\n)\n\nfactors_investment = (portfolios_investment\n  .groupby(\"month\", as_index=False)\n  .apply(lambda x: pd.Series({\n    \"cma_replicated\": (\n      x[\"ret\"][x[\"portfolio_inv\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_inv\"] == 3].mean())})\n  )\n)\n\nFinally, the size factor, SMB, is constructed by going long the six small portfolios and short the six large portfolios.\n\nfactors_size = (\n  pd.concat([portfolios_value, portfolios_profitability, \n             portfolios_investment], ignore_index=True)\n  .groupby(\"month\", as_index=False)\n  .apply(lambda x: pd.Series({\n    \"smb_replicated\": (\n      x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_size\"] == 2].mean())})\n  )\n)\n\nWe then join all factors together into one data frame and construct again a suitable table to run tests for evaluating our replication.\n\nfactors_replicated = (factors_size\n  .merge(factors_value, how=\"outer\", on=\"month\")\n  .merge(factors_profitability, how=\"outer\", on=\"month\")\n  .merge(factors_investment, how=\"outer\", on=\"month\")\n)\n\ntest = (factors_ff5_monthly\n  .merge(factors_replicated, how=\"inner\", on=\"month\")\n  .round(decimals=4)\n)\n\nLet us start the replication evaluation again with the size factor:\n\nmodel_smb = (smf.ols(\n    formula=\"smb ~ smb_replicated\", data=test\n  )\n  .fit()\n)\nfor table in model_smb.summary(slim=True).tables:\n  print(table)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    smb   R-squared:                       0.986\nModel:                            OLS   Adj. R-squared:                  0.986\nNo. Observations:                 702   F-statistic:                 5.035e+04\nCovariance Type:            nonrobust   Prob (F-statistic):               0.00\n==============================================================================\n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept         -0.0002      0.000     -1.376      0.169      -0.000     7.9e-05\nsmb_replicated     0.9699      0.004    224.381      0.000       0.961       0.978\n==================================================================================\n\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient is python np.round(model_smb.params[\"smb_replicated\"], 2) and the R-squared is at python np.round(model_smb.rsquared_adj, 2) * 100%.\n\nmodel_hml = (smf.ols(\n    formula=\"hml ~ hml_replicated\", data=test\n  )\n  .fit()\n)\nfor table in model_hml.summary(slim=True).tables:\n  print(table)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    hml   R-squared:                       0.926\nModel:                            OLS   Adj. R-squared:                  0.926\nNo. Observations:                 702   F-statistic:                     8732.\nCovariance Type:            nonrobust   Prob (F-statistic):               0.00\n==============================================================================\n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          0.0005      0.000      1.649      0.100   -9.42e-05       0.001\nhml_replicated     0.9898      0.011     93.446      0.000       0.969       1.011\n==================================================================================\n\n\nThe replication of the HML factor is also a success, although at a slightly higher coefficient of python np.round(model_hml.params[\"hml_replicated\"], 2) and an R-squared around python np.round(model_hml.rsquared_adj, 2) * 100%.\n\nmodel_rmw = (smf.ols(\n    formula=\"rmw ~ rmw_replicated\", data=test\n  )\n  .fit()\n)\nfor table in model_rmw.summary(slim=True).tables:\n  print(table)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    rmw   R-squared:                       0.941\nModel:                            OLS   Adj. R-squared:                  0.941\nNo. Observations:                 702   F-statistic:                 1.119e+04\nCovariance Type:            nonrobust   Prob (F-statistic):               0.00\n==============================================================================\n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept       8.837e-05      0.000      0.433      0.665      -0.000       0.000\nrmw_replicated     0.9547      0.009    105.800      0.000       0.937       0.972\n==================================================================================\n\n\nWe are also able to replicate the RMW factor quite well with a coefficient of python np.round(model_rmw.params[\"rmw_replicated\"], 2) and an R-squared around python np.round(model_rmw.rsquared_adj, 2) * 100%.\n\nmodel_cma = (smf.ols(\n    formula=\"cma ~ cma_replicated\", data=test\n  )\n  .fit()\n)\nfor table in model_cma.summary(slim=True).tables:\n  print(table)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    cma   R-squared:                       0.948\nModel:                            OLS   Adj. R-squared:                  0.948\nNo. Observations:                 702   F-statistic:                 1.274e+04\nCovariance Type:            nonrobust   Prob (F-statistic):               0.00\n==============================================================================\n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          0.0007      0.000      4.107      0.000       0.000       0.001\ncma_replicated     0.9651      0.009    112.872      0.000       0.948       0.982\n==================================================================================\n\n\nFinally, the CMA factor also replicates well with a coefficient of python np.round(model_cma.params[\"cma_replicated\"], 2) and an R-squared around python np.round(model_cma.rsquared_adj, 2) * 100%.\nOverall, our approach seems to replicate the Fama-French three and five factor models just as well as the three factors."
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#exercises",
    "href": "python/replicating-fama-and-french-factors.html#exercises",
    "title": "Replicating Fama and French Factors",
    "section": "Exercises",
    "text": "Exercises\n\nFama and French (1993) claim that their sample excludes firms until they have appeared in Compustat for two years. Implement this additional filter and compare the improvements of your replication effort.\nOn his homepage, Kenneth French provides instructions on how to construct the most common variables used for portfolio sorts. Pick one of them, e.g. OP (operating profitability) and try to replicate the portfolio sort return time series provided on his homepage."
  }
]