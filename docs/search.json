[
  {
    "objectID": "r/replicating-fama-and-french-factors.html",
    "href": "r/replicating-fama-and-french-factors.html",
    "title": "Replicating Fama and French Factors",
    "section": "",
    "text": "In this chapter, we provide a replication of the famous Fama and French factor portfolios. The Fama and French factor models are a cornerstone of empirical asset pricing Fama and French (2015). On top of the market factor represented by the traditional CAPM beta, the three factor model includes the size and value factors to explain the cross section of returns. Its successor, the five factor model, additionally includes profitability and investment as explanatory factors.\nWe start with the three factor model. We already introduced the size and value factors in Value and Bivariate Sorts, and their definition remains the same: size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nAfter the replication of the three factor model, we move to the five factors by constructing the profitability factor RMW (robust-minus-weak) as the difference between the returns of firms with high and low operating profitability and the investment factor CMA (conservative-minus-aggressive) as the difference between firms with high versus low investment rates.\nThe current chapter relies on this set of packages.\nlibrary(tidyverse)\nlibrary(RSQLite)"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#data-preparation",
    "href": "r/replicating-fama-and-french-factors.html#data-preparation",
    "title": "Replicating Fama and French Factors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and Compustat as data sources, as we need the same variables to compute the factors in the way Fama and French do it. Hence, there is nothing new below and we only load data from our SQLite-database introduced in Accessing and Managing Financial Data and WRDS, CRSP, and Compustat.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(\n    permno, gvkey, month, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n    select(gvkey, datadate, be, op, inv) |&gt;\n    collect() \n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt;\n  select(month, smb, hml) |&gt;\n  collect()\n\nfactors_ff5_monthly &lt;- tbl(tidy_finance, \"factors_ff5_monthly\") |&gt;\n  select(month, smb, hml, rmw, cma) |&gt;\n  collect()\n\nYet when we start merging our data set for computing the premiums, there are a few differences to Value and Bivariate Sorts. First, Fama and French form their portfolios in June of year \\(t\\), whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year \\(t+1\\).\nSecond, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year \\(t - 1\\) and the book equity reported in year \\(t-1\\), i.e., the datadate is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity. The other sorting variables are analogously to book equity taken from year \\(t-1\\).\nTo implement all these time lags, we again employ the temporary sorting_date-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year. We ensure this by a call of distinct() at the end of the chunk below.\n\nsize &lt;- crsp_monthly |&gt;\n  filter(month(month) == 6) |&gt;\n  mutate(sorting_date = month %m+% months(1)) |&gt;\n  select(permno, exchange, sorting_date, size = mktcap)\n\nmarket_equity &lt;- crsp_monthly |&gt;\n  filter(month(month) == 12) |&gt;\n  mutate(sorting_date = ymd(str_c(year(month) + 1, \"0701)\"))) |&gt;\n  select(permno, gvkey, sorting_date, me = mktcap)\n\nbook_to_market &lt;- compustat |&gt;\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |&gt;\n  select(gvkey, sorting_date, be) |&gt;\n  inner_join(market_equity, by = c(\"gvkey\", \"sorting_date\")) |&gt;\n  mutate(bm = be / me) |&gt;\n  select(permno, sorting_date, me, bm)\n\nsorting_variables &lt;- size |&gt;\n  inner_join(\n    book_to_market, by = c(\"permno\", \"sorting_date\")\n    ) |&gt;\n  drop_na() |&gt;\n  distinct(permno, sorting_date, .keep_all = TRUE)"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "href": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "title": "Replicating Fama and French Factors",
    "section": "Portfolio Sorts",
    "text": "Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on NYSE-specific breakpoints, they form two portfolios in the size dimension at the median and three portfolios in the dimension of each other sorting variable at the 30%- and 70%-percentiles, and they use dependent sorts. The sorts for book-to-market require an adjustment to the function in Chapter 9 because the seq() we would produce does not produce the right breakpoints. Instead of n_portfolios, we now specify percentiles, which take the breakpoint-sequence as an object specified in the function’s call. Specifically, we give percentiles = c(0, 0.3, 0.7, 1) to the function. Additionally, we perform an inner_join() with our return data to ensure that we only use traded stocks when computing the breakpoints as a first step.\n\nassign_portfolio &lt;- function(data, \n                             sorting_variable, \n                             percentiles) {\n  breakpoints &lt;- data |&gt;\n    filter(exchange == \"NYSE\") |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = percentiles,\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  return(assigned_portfolios)\n}\n\nportfolios &lt;- sorting_variables |&gt;\n  group_by(sorting_date) |&gt;\n  mutate(\n    portfolio_size = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = size,\n      percentiles = c(0, 0.5, 1)\n    ),\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = bm,\n      percentiles = c(0, 0.3, 0.7, 1)\n    )\n  ) |&gt;\n  ungroup() |&gt; \n  select(permno, sorting_date, \n         portfolio_size, portfolio_bm)\n\nNext, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column sorting_date in our return data by setting the date to sort on to July of \\(t-1\\) if the month is June (of year \\(t\\)) or earlier or to July of year \\(t\\) if the month is July or later.\n\nportfolios &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = case_when(\n    month(month) &lt;= 6 ~ ymd(str_c(year(month) - 1, \"0701\")),\n    month(month) &gt;= 7 ~ ymd(str_c(year(month), \"0701\"))\n  )) |&gt;\n  inner_join(portfolios, by = c(\"permno\", \"sorting_date\"))"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#fama-and-french-three-factor-model",
    "href": "r/replicating-fama-and-french-factors.html#fama-and-french-three-factor-model",
    "title": "Replicating Fama and French Factors",
    "section": "Fama and French Three Factor Model",
    "text": "Fama and French Three Factor Model\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama and French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally. \n\nfactors_replicated &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_bm, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\"\n  ) |&gt;\n  group_by(month) |&gt;\n  summarize(\n    smb_replicated = mean(ret[portfolio_size == 1]) -\n      mean(ret[portfolio_size == 2]),\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "href": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "title": "Replicating Fama and French Factors",
    "section": "Replication Evaluation",
    "text": "Replication Evaluation\nIn the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using lm(). If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to 1 (indicating a high correlation), and an adjusted R-squared close to 1 (indicating a high proportion of explained variance).\n\ntest &lt;- factors_ff3_monthly |&gt;\n  inner_join(factors_replicated, by = \"month\") |&gt;\n  mutate(\n    across(c(smb_replicated, hml_replicated), ~round(., 4))\n  )\n\nTo test the success of the SMB factor, we hence run the following regression:\n\nmodel_smb &lt;- lm(smb ~ smb_replicated, data = test)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.020279 -0.001531 -0.000054  0.001505  0.015491 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.000147   0.000132   -1.11     0.27    \nsmb_replicated  0.992890   0.004374  226.99   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00356 on 724 degrees of freedom\nMultiple R-squared:  0.986, Adjusted R-squared:  0.986 \nF-statistic: 5.15e+04 on 1 and 724 DF,  p-value: &lt;2e-16\n\n\nThe results for the SMB factor are really convincing as all three criteria outlined above are met and the coefficient is 0.99 and the R-squared is at 99%.\n\nmodel_hml &lt;- lm(hml ~ hml_replicated, data = test)\nsummary(model_hml)\n\n\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02320 -0.00287 -0.00010  0.00229  0.03391 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000298   0.000221    1.35     0.18    \nhml_replicated 0.962310   0.007546  127.52   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00594 on 724 degrees of freedom\nMultiple R-squared:  0.957, Adjusted R-squared:  0.957 \nF-statistic: 1.63e+04 on 1 and 724 DF,  p-value: &lt;2e-16\n\n\nThe replication of the HML factor is also a success, although at a slightly lower coefficient of 0.96 and an R-squared around 96%.\nThe evidence hence allows us to conclude that we did a relatively good job in replicating the original Fama-French size and value premiums, although we do not know their underlying code. From our perspective, a perfect match is only possible with additional information from the maintainers of the original data."
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#fama-and-french-five-factor-model",
    "href": "r/replicating-fama-and-french-factors.html#fama-and-french-five-factor-model",
    "title": "Replicating Fama and French Factors",
    "section": "Fama and French Five Factor Model",
    "text": "Fama and French Five Factor Model\nNow, let us move to the replication of the five factor model. We extend the other_sorting_variables table from above with the additional characteristics operating profitability op and investment inv. Note that the drop_na() statement yields to different sample sizes as some firms with be values might not have op or inv values.\n\nother_sorting_variables &lt;- compustat |&gt;\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |&gt;\n  select(gvkey, sorting_date, be, op, inv) |&gt;\n  inner_join(market_equity, by = c(\"gvkey\", \"sorting_date\")) |&gt;\n  mutate(bm = be / me) |&gt;\n  select(permno, sorting_date, me, be, bm, op, inv)\n\nsorting_variables &lt;- size |&gt;\n  inner_join(\n    other_sorting_variables, by = c(\"permno\", \"sorting_date\")\n    ) |&gt;\n  drop_na() |&gt;\n  distinct(permno, sorting_date, .keep_all = TRUE)\n\nIn each month, we independently sort all stocks into the two size portfolios. The value, profitability, and investment portfolios, on the other hand, are the results of dependent sorts based on the size portfolios. We then merge the portfolios to the return data for the rest of the year just as above.\n\nportfolios &lt;- sorting_variables |&gt;\n  group_by(sorting_date) |&gt;\n  mutate(\n    portfolio_size = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = size,\n      percentiles = c(0, 0.5, 1)\n    )) |&gt; \n  group_by(sorting_date, portfolio_size) |&gt; \n  mutate(\n    across(c(bm, op, inv), ~assign_portfolio(\n      data = pick(everything()), \n      sorting_variable = ., \n      percentiles = c(0, 0.3, 0.7, 1)),\n      .names = \"portfolio_{.col}\"\n    )\n  ) |&gt;\n  ungroup() |&gt; \n  select(permno, sorting_date, \n         portfolio_size, portfolio_bm,\n         portfolio_op, portfolio_inv)\n\nportfolios &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = case_when(\n    month(month) &lt;= 6 ~ ymd(str_c(year(month) - 1, \"0701\")),\n    month(month) &gt;= 7 ~ ymd(str_c(year(month), \"0701\"))\n  )) |&gt;\n  inner_join(portfolios, by = c(\"permno\", \"sorting_date\"))\n\nNow, we want to construct each of the factors, but this time the size factor actually comes last because it is the result of averaging across all other factor portfolios. This dependency is the reason why we keep the table with value-weighted portfolio returns as a separate object that we reuse later. We construct the value factor, HML, as above by going long the two portfolios with high book-to-market ratios and shorting the two portfolios with low book-to-market.\n\nportfolios_value &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_bm, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\"\n  )\n\nfactors_value &lt;- portfolios_value |&gt;\n  group_by(month) |&gt;\n  summarize(\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )\n\nFor the profitability factor, RMW, we take a long position in the two high profitability portfolios and a short position in the two low profitability portfolios.\n\nportfolios_profitability &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_op, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\"\n  ) \n\nfactors_profitability &lt;- portfolios_profitability |&gt;\n  group_by(month) |&gt;\n  summarize(\n    rmw_replicated = mean(ret[portfolio_op == 3]) -\n      mean(ret[portfolio_op == 1])\n  )\n\nFor the investment factor, CMA, we go long the two low investment portfolios and short the two high investment portfolios.\n\nportfolios_investment &lt;- portfolios |&gt;\n  group_by(portfolio_size, portfolio_inv, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\"\n  )\n\nfactors_investment &lt;- portfolios_investment |&gt;\n  group_by(month) |&gt;\n  summarize(\n    cma_replicated = mean(ret[portfolio_inv == 1]) -\n      mean(ret[portfolio_inv == 3])\n  )\n\nFinally, the size factor, SMB, is constructed by going long the six small portfolios and short the six large portfolios.\n\nfactors_size &lt;- bind_rows(\n  portfolios_value,\n  portfolios_profitability,\n  portfolios_investment\n) |&gt; \n  group_by(month) |&gt;\n  summarize(\n    smb_replicated = mean(ret[portfolio_size == 1]) -\n      mean(ret[portfolio_size == 2])\n  )\n\nWe then join all factors together into one data frame and construct again a suitable table to run tests for evaluating our replication.\n\nfactors_replicated &lt;- factors_size |&gt;\n  full_join(\n    factors_value, by = \"month\"\n  ) |&gt;\n  full_join(\n    factors_profitability, by = \"month\"\n  ) |&gt;\n  full_join(\n    factors_investment, by = \"month\"\n  )\n\ntest &lt;- factors_ff5_monthly |&gt;\n  inner_join(factors_replicated, by = \"month\") |&gt;\n  mutate(\n    across(c(smb_replicated, hml_replicated), ~round(., 4))\n  )\n\nLet us start the replication evaluation again with the size factor:\n\nmodel_smb &lt;- lm(smb ~ smb_replicated, data = test)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.018178 -0.001872  0.000205  0.001961  0.014396 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.000179   0.000135   -1.33     0.18    \nsmb_replicated  0.969389   0.004328  223.96   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00356 on 700 degrees of freedom\nMultiple R-squared:  0.986, Adjusted R-squared:  0.986 \nF-statistic: 5.02e+04 on 1 and 700 DF,  p-value: &lt;2e-16\n\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient is 0.97 and the R-squared is at 99%.\n\nmodel_hml &lt;- lm(hml ~ hml_replicated, data = test)\nsummary(model_hml)\n\n\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.04451 -0.00413 -0.00034  0.00406  0.03670 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000477   0.000300    1.59     0.11    \nhml_replicated 0.989758   0.010621   93.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00793 on 700 degrees of freedom\nMultiple R-squared:  0.925, Adjusted R-squared:  0.925 \nF-statistic: 8.68e+03 on 1 and 700 DF,  p-value: &lt;2e-16\n\n\nThe replication of the HML factor is also a success, although at a slightly higher coefficient of 0.99 and an R-squared around 93%.\n\nmodel_rmw &lt;- lm(rmw ~ rmw_replicated, data = test)\nsummary(model_rmw)\n\n\nCall:\nlm(formula = rmw ~ rmw_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.019790 -0.003079  0.000109  0.003292  0.018749 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.85e-05   2.04e-04    0.38      0.7    \nrmw_replicated 9.55e-01   9.03e-03  105.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00536 on 700 degrees of freedom\nMultiple R-squared:  0.941, Adjusted R-squared:  0.941 \nF-statistic: 1.12e+04 on 1 and 700 DF,  p-value: &lt;2e-16\n\n\nWe are also able to replicate the RMW factor quite well with a coefficient of 0.96 and an R-squared around 94%.\n\nmodel_cma &lt;- lm(cma ~ cma_replicated, data = test)\nsummary(model_cma)\n\n\nCall:\nlm(formula = cma ~ cma_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.015112 -0.002730 -0.000189  0.002441  0.021392 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000715   0.000173    4.14  3.9e-05 ***\ncma_replicated 0.964977   0.008590  112.33  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00455 on 700 degrees of freedom\nMultiple R-squared:  0.947, Adjusted R-squared:  0.947 \nF-statistic: 1.26e+04 on 1 and 700 DF,  p-value: &lt;2e-16\n\n\nFinally, the CMA factor also replicates well with a coefficient of 0.96 and an R-squared around 95%.\nOverall, our approach seems to replicate the Fama-French three and five factor models just as well as the three factors."
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#exercises",
    "href": "r/replicating-fama-and-french-factors.html#exercises",
    "title": "Replicating Fama and French Factors",
    "section": "Exercises",
    "text": "Exercises\n\nFama and French (1993) claim that their sample excludes firms until they have appeared in Compustat for two years. Implement this additional filter and compare the improvements of your replication effort.\nOn his homepage, Kenneth French provides instructions on how to construct the most common variables used for portfolio sorts. Try to replicate the univariate portfolio sort return time series for E/P (earnings / price) provided on his homepage and evaluate your replication effort using regressions."
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Support Tidy Finance",
    "section": "",
    "text": "Tidy Finance is and will remain an open-source project. We are grateful for all the support we have received so far. Of course, we do not force anybody to support us, but every gesture is very much appreciated. We have three options if you want to give something back and support our efforts. Moreover, most options come at no additional cost to you, i.e., they just increase our share of the pie. Who does not appreciate a little more pie?"
  },
  {
    "objectID": "support.html#get-your-copy-of-the-book",
    "href": "support.html#get-your-copy-of-the-book",
    "title": "Support Tidy Finance",
    "section": "Get your copy of the book",
    "text": "Get your copy of the book\n\nYou can read the free online version of Tidy Finance on this website. However, you can also get your own physical copy! The book comes with many perks, such as the joy of holding something in your hand, a fresh smell, and it certainly looks good in your library. If you decide to buy your own copy, please consider using our affiliate link from Routledge. No extra cost to you, just some pie for us.\nNote that some affiliate links track your behavior on the site and can be flagged as suspicious by your browser. We exclusively use the official paths provided by the respective vendor. Alternatively, our book is also available on Amazon and other retailers."
  },
  {
    "objectID": "support.html#spread-the-word",
    "href": "support.html#spread-the-word",
    "title": "Support Tidy Finance",
    "section": "Spread the word",
    "text": "Spread the word\nThe project grows with the attention it receives from the community. Therefore, making people aware of Tidy Finance is a great way to support it. There are certainly many possibilities how you can spread the word. For example, you could\n\nContribute to the Tidy Finance blog\nCite the book in one of your projects\nUse Tidy Finance as a teaching resource and let us know\nConnect with us and share posts about Tidy Finance via social media\nYou can also buy Tidy Finance Swag\n\nThese are just a few suggestions, yet highly effective. In any case, we rely on your support to share Tidy Finance within your own community."
  },
  {
    "objectID": "support.html#buy-us-a-coffee",
    "href": "support.html#buy-us-a-coffee",
    "title": "Support Tidy Finance",
    "section": "Buy us a coffee",
    "text": "Buy us a coffee\nEvery task requires some fuel. In particular, one key ingredient to completing the mental efforts that culminate in Tidy Finance is, of course, coffee. Hence, if you appreciate Tidy Finance, let us have a coffee. We are grateful for every small contribution to sustain our caffeine levels. Moreover, higher caffeine levels positively correlate with new content on Tidy Finance. It is a win-win situation!"
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "Preface",
    "section": "",
    "text": "This website is the online version of Tidy Finance with R, a book published via Chapman & Hall/CRC. The book is the result of a joint effort of Christoph Scheuch, Stefan Voigt, and Patrick Weiss.\nWe are grateful for any kind of feedback on every aspect of the book. So please get in touch with us via contact@tidy-finance.org if you spot typos, discover any issues that deserve more attention, or if you have suggestions for additional chapters and sections. Additionally, let us know if you found the text helpful. We look forward to hearing from you!\n\n\n\n\n\n\nSupport Tidy Finance\n\n\n\nBuy our book via your preferred vendor or support us with coffee here.\n\n\n\n\nFinancial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future.\n\n\n\nWe write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from undergrad to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed.\n\n\n\n\nThe book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common data sets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet the data chapters provide an important background necessary for data management in all other chapters.\n\n\n\nThis book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham, Çetinkaya-Rundel, and Grolemund (2023) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).\n\n\n\n\nWe believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019).\n\n\n\nAs you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80% of data analysis is spent on preparing data. By tidying data, we want to structure data sets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021), and lubridate (Grolemund and Wickham 2011).\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book we use the native pipe |&gt;, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %&gt;% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to this blog post second edition by Hadley Wickham.\n\n\n\n\n\n\nBefore we continue, make sure you have all the software you need for this book:\n\nInstall R and RStudio. To get a walk-through of the installation for every major operating system, follow the steps outlined in this summary. The whole process should be done in a few clicks. If you wonder about the difference: R is an open-source language and environment for statistical computing and graphics, free to download and use. While R runs the computations, RStudio is an integrated development environment that provides an interface by adding many convenient features and tools. We suggest doing all the coding in RStudio.\nOpen RStudio and install the tidyverse. Not sure how it works? You will find helpful information on how to install packages in this brief summary.\n\nIf you are new to R, we recommend starting with the following sources:\n\nA very gentle and good introduction to the workings of R can be found in the form of the weighted dice project. Once you are done setting up R on your machine, try to follow the instructions in this project.\nThe main book on the tidyverse, Wickham, Çetinkaya-Rundel, and Grolemund (2023), is available online and for free: R for Data Science explains the majority of the tools we use in our book.\nIf you are an instructor searching to effectively teach R and data science methods, we recommend taking a look at the excellent data science toolbox by Mine Cetinkaya-Rundel.\nRStudio provides a range of excellent cheat sheets with extensive information on how to use the tidyverse packages.\n\n\n\n\nWe met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is the Director of Product at the social trading platform wikifolio.com. He is responsible for product planning, execution, and monitoring and manages a team of data scientists to analyze user behavior and develop data-driven products. Christoph is also an external lecturer at the Vienna University of Economics and Business, where he teaches finance students how to manage empirical projects.\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in leading journals in financial economics.\n\n\n\n\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows:\n\nScheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237.\n\n@book{scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org},\n  doi = {https://doi.org/10.1201/b23237}\n}\n\n\n\nThis book was written in RStudio using bookdown (Xie 2016). The website was rendered using quarto (Allaire et al. 2022) and it is hosted via GitHub Pages. The complete source is available from GitHub. We generated all plots in this book using ggplot2 and its classic dark-on-light theme (theme_bw()).\nThis version of the book was built with R (R Core Team 2022) version 4.3.1 (2023-06-16, Beagle Scouts) and the following packages: \n\n\n\n\n\nPackage\nVersion\n\n\n\n\nRPostgres\n1.4.5\n\n\nRSQLite\n2.3.1\n\n\nalabama\n2022.4-1\n\n\nbroom\n1.0.4\n\n\ndbplyr\n2.3.2\n\n\ndevtools\n2.4.5\n\n\ndplyr\n1.1.2\n\n\nfixest\n0.11.1\n\n\nforcats\n1.0.0\n\n\nfrenchdata\n0.2.0\n\n\nfurrr\n0.3.1\n\n\nggplot2\n3.4.2\n\n\nglmnet\n4.1-7\n\n\ngoogledrive\n2.1.0\n\n\nhardhat\n1.3.0\n\n\nhexSticker\n0.4.9\n\n\njsonlite\n1.8.4\n\n\nkableExtra\n1.3.4\n\n\nkeras\n2.11.1\n\n\nlmtest\n0.9-40\n\n\npurrr\n1.0.1\n\n\nquadprog\n1.5-8\n\n\nranger\n0.15.1\n\n\nreadr\n2.1.4\n\n\nreadxl\n1.4.2\n\n\nrenv\n0.17.3\n\n\nrlang\n1.1.1\n\n\nrmarkdown\n2.21\n\n\nsandwich\n3.0-2\n\n\nscales\n1.2.1\n\n\nslider\n0.3.0\n\n\nstringr\n1.5.0\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.1.0\n\n\ntidyquant\n1.0.7\n\n\ntidyr\n1.3.0\n\n\ntidyverse\n2.0.0\n\n\ntimetk\n2.8.3\n\n\nwesanderson\n0.3.6"
  },
  {
    "objectID": "r/index.html#why-does-this-book-exist",
    "href": "r/index.html#why-does-this-book-exist",
    "title": "Preface",
    "section": "",
    "text": "Financial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future."
  },
  {
    "objectID": "r/index.html#who-should-read-this-book",
    "href": "r/index.html#who-should-read-this-book",
    "title": "Preface",
    "section": "",
    "text": "We write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from undergrad to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed."
  },
  {
    "objectID": "r/index.html#what-will-you-learn",
    "href": "r/index.html#what-will-you-learn",
    "title": "Preface",
    "section": "",
    "text": "The book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common data sets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet the data chapters provide an important background necessary for data management in all other chapters."
  },
  {
    "objectID": "r/index.html#what-wont-you-learn",
    "href": "r/index.html#what-wont-you-learn",
    "title": "Preface",
    "section": "",
    "text": "This book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham, Çetinkaya-Rundel, and Grolemund (2023) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages)."
  },
  {
    "objectID": "r/index.html#why-r",
    "href": "r/index.html#why-r",
    "title": "Preface",
    "section": "",
    "text": "We believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019)."
  },
  {
    "objectID": "r/index.html#why-tidy",
    "href": "r/index.html#why-tidy",
    "title": "Preface",
    "section": "",
    "text": "As you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80% of data analysis is spent on preparing data. By tidying data, we want to structure data sets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021), and lubridate (Grolemund and Wickham 2011).\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book we use the native pipe |&gt;, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %&gt;% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to this blog post second edition by Hadley Wickham."
  },
  {
    "objectID": "r/index.html#prerequisites",
    "href": "r/index.html#prerequisites",
    "title": "Preface",
    "section": "",
    "text": "Before we continue, make sure you have all the software you need for this book:\n\nInstall R and RStudio. To get a walk-through of the installation for every major operating system, follow the steps outlined in this summary. The whole process should be done in a few clicks. If you wonder about the difference: R is an open-source language and environment for statistical computing and graphics, free to download and use. While R runs the computations, RStudio is an integrated development environment that provides an interface by adding many convenient features and tools. We suggest doing all the coding in RStudio.\nOpen RStudio and install the tidyverse. Not sure how it works? You will find helpful information on how to install packages in this brief summary.\n\nIf you are new to R, we recommend starting with the following sources:\n\nA very gentle and good introduction to the workings of R can be found in the form of the weighted dice project. Once you are done setting up R on your machine, try to follow the instructions in this project.\nThe main book on the tidyverse, Wickham, Çetinkaya-Rundel, and Grolemund (2023), is available online and for free: R for Data Science explains the majority of the tools we use in our book.\nIf you are an instructor searching to effectively teach R and data science methods, we recommend taking a look at the excellent data science toolbox by Mine Cetinkaya-Rundel.\nRStudio provides a range of excellent cheat sheets with extensive information on how to use the tidyverse packages."
  },
  {
    "objectID": "r/index.html#about-the-authors",
    "href": "r/index.html#about-the-authors",
    "title": "Preface",
    "section": "",
    "text": "We met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is the Director of Product at the social trading platform wikifolio.com. He is responsible for product planning, execution, and monitoring and manages a team of data scientists to analyze user behavior and develop data-driven products. Christoph is also an external lecturer at the Vienna University of Economics and Business, where he teaches finance students how to manage empirical projects.\nStefan Voigt is an Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute Teaching Award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is an Assistant Professor of Finance at Reykjavik University and an external lecturer at the Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in leading journals in financial economics."
  },
  {
    "objectID": "r/index.html#license",
    "href": "r/index.html#license",
    "title": "Preface",
    "section": "",
    "text": "This book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows:\n\nScheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237.\n\n@book{scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org},\n  doi = {https://doi.org/10.1201/b23237}\n}"
  },
  {
    "objectID": "r/index.html#colophon",
    "href": "r/index.html#colophon",
    "title": "Preface",
    "section": "",
    "text": "This book was written in RStudio using bookdown (Xie 2016). The website was rendered using quarto (Allaire et al. 2022) and it is hosted via GitHub Pages. The complete source is available from GitHub. We generated all plots in this book using ggplot2 and its classic dark-on-light theme (theme_bw()).\nThis version of the book was built with R (R Core Team 2022) version 4.3.1 (2023-06-16, Beagle Scouts) and the following packages: \n\n\n\n\n\nPackage\nVersion\n\n\n\n\nRPostgres\n1.4.5\n\n\nRSQLite\n2.3.1\n\n\nalabama\n2022.4-1\n\n\nbroom\n1.0.4\n\n\ndbplyr\n2.3.2\n\n\ndevtools\n2.4.5\n\n\ndplyr\n1.1.2\n\n\nfixest\n0.11.1\n\n\nforcats\n1.0.0\n\n\nfrenchdata\n0.2.0\n\n\nfurrr\n0.3.1\n\n\nggplot2\n3.4.2\n\n\nglmnet\n4.1-7\n\n\ngoogledrive\n2.1.0\n\n\nhardhat\n1.3.0\n\n\nhexSticker\n0.4.9\n\n\njsonlite\n1.8.4\n\n\nkableExtra\n1.3.4\n\n\nkeras\n2.11.1\n\n\nlmtest\n0.9-40\n\n\npurrr\n1.0.1\n\n\nquadprog\n1.5-8\n\n\nranger\n0.15.1\n\n\nreadr\n2.1.4\n\n\nreadxl\n1.4.2\n\n\nrenv\n0.17.3\n\n\nrlang\n1.1.1\n\n\nrmarkdown\n2.21\n\n\nsandwich\n3.0-2\n\n\nscales\n1.2.1\n\n\nslider\n0.3.0\n\n\nstringr\n1.5.0\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.1.0\n\n\ntidyquant\n1.0.7\n\n\ntidyr\n1.3.0\n\n\ntidyverse\n2.0.0\n\n\ntimetk\n2.8.3\n\n\nwesanderson\n0.3.6"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tidy Finance Blog",
    "section": "",
    "text": "Experimental and external contributions based on Tidy Finance with R. Contribute your ideas!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Fama-French Three vs Five Factors\n\n\n3 min\n\n\n\nData\n\n\nReplication\n\n\n\nAn explanation for the difference in the value factors of FF3 and FF5 data\n\n\n\nChristoph Scheuch\n\n\nSep 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvert Raw TRACE Data to a Local SQLite Database\n\n\n37 min\n\n\n\nData\n\n\n\nAn R code that converts TRACE files from FINRA into a SQLite for facilitated analysis and filtering\n\n\n\nKevin Riehl, Lukas Müller\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Collaborative Filtering: Building A Stock Recommender\n\n\n13 min\n\n\n\nRecommender System\n\n\n\nA simple implementation for prototyping multiple collaborative filtering algorithms\n\n\n\nChristoph Scheuch\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Standard Errors in Portfolio Sorts\n\n\n39 min\n\n\n\nReplications\n\n\n\nAn all-in-one implementation of non-standard errors in portfolio sorts\n\n\n\nPatrick Weiss\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstruction of a Historical S&P 500 Total Return Index\n\n\n8 min\n\n\n\nData\n\n\n\nAn approximation of total returns using Robert Shiller’s stock market data\n\n\n\nChristoph Scheuch\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Tidy Finance?\n\n\n5 min\n\n\n\nOp-Ed\n\n\n\nAn op-ed about the motives behind Tidy Finance with R\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at Workshops for Ukraine\n\n\n2 min\n\n\n\nWorkshops\n\n\n\nYou can learn Tidy Finance and support Ukraine at the same time\n\n\n\nPatrick Weiss\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at the useR!2022 Conference\n\n\n1 min\n\n\n\nConferences\n\n\n\nTidy Finance presentation at the gathering supported by the R Foundation\n\n\n\nPatrick Weiss\n\n\nJun 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/fama-french-three-vs-five-factors/index.html",
    "href": "blog/fama-french-three-vs-five-factors/index.html",
    "title": "Comparing Fama-French Three vs Five Factors",
    "section": "",
    "text": "In our book chapter Replicating Fama and French Factors, we show how to construct factor portfolios that are fairly close to the popular data from Prof. Kenneth French finance data library. In this blog post, I want to elaborate a bit more on the subtle differences between the size and value data in the Fama-French three (FF3) and five (FF5) factor data.\n\nlibrary(tidyverse)\nlibrary(RSQLite)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(), \n  \"../../data/tidy_finance.sqlite\", \n  extended_types = TRUE\n)\n\nfactors_ff3_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") |&gt; \n  collect()\n\nfactors_ff5_monthly &lt;- tbl(tidy_finance, \"factors_ff5_monthly\") |&gt; \n  collect()\n\nfactors_ff &lt;- factors_ff3_monthly |&gt; \n  rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff3\")) |&gt; \n  inner_join(\n    factors_ff5_monthly |&gt; \n      select(month, mkt_excess, rf, smb, hml) |&gt; \n      rename_with(.cols = c(mkt_excess, rf, smb, hml),\n              ~str_c(.x, \"_ff5\")), by = \"month\")\n\nLet us first inspect the summary statistics of each factor.\n\nfactors_ff |&gt; \n  pivot_longer(cols = - month) |&gt; \n  select(name, value) |&gt;\n  drop_na() |&gt;\n  group_by(name) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  ) |&gt; \n  print(n = Inf)\n\n# A tibble: 8 × 9\n  name        mean      sd    min     q05     q50    q95    max     n\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 hml_ff3  0.00274 0.0290  -0.139 -0.0410 0.00225 0.0528 0.125    702\n2 hml_ff5  0.00274 0.0290  -0.139 -0.0410 0.00225 0.0528 0.125    702\n3 mkt_exc… 0.00589 0.0445  -0.232 -0.0718 0.0097  0.0704 0.161    702\n4 mkt_exc… 0.00589 0.0445  -0.232 -0.0718 0.0097  0.0704 0.161    702\n5 rf_ff3   0.00366 0.00268  0      0      0.0038  0.0081 0.0135   702\n6 rf_ff5   0.00366 0.00268  0      0      0.0038  0.0081 0.0135   702\n7 smb_ff3  0.00198 0.0305  -0.172 -0.0419 0.0012  0.0505 0.214    702\n8 smb_ff5  0.00228 0.0303  -0.153 -0.0431 0.001   0.0494 0.183    702\n\n\nThe above table shows that rf, mkt_excess, and hml show de facto identical value across all statistics for FF3 and FF5. However, the smb vales seem to be different between the both data sets. Another way to show the difference is running regressions, as we do in Replicating Fama and French Factors:\n\nmodel_smb &lt;- lm(smb_ff3 ~ smb_ff5, data = factors_ff)\nsummary(model_smb)\n\n\nCall:\nlm(formula = smb_ff3 ~ smb_ff5, data = factors_ff)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.03546 -0.00192  0.00028  0.00201  0.03369 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.000269   0.000218   -1.23     0.22    \nsmb_ff5      0.986977   0.007180  137.46   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00576 on 700 degrees of freedom\nMultiple R-squared:  0.964, Adjusted R-squared:  0.964 \nF-statistic: 1.89e+04 on 1 and 700 DF,  p-value: &lt;2e-16\n\n\nRegressing the FF3 SMB factor on its FF5 counterpart yields a coefficient of 0.99 and an R-squared around 96%, so definitely no perfect co-movement. Where does this difference come from?\nThere are actually two explanations for the differences:\n\nThe value portfolios portfolio_bm are constructed as independent sorts in FF3, while they are the result of dependent sorts in FF5 (depending on portfolio_size).\nThe FF3 and FF5 data are based on slightly different samples: while the FF3 data considers all stocks with valid bm values, the FF5 data requires valid bm, op, and inv values. So if firms don’t report any of the values that enter into these three sorting variables, then they are dropped from the portfolio sorts.\n\nWhat are the implications for empirical applications? You should be careful when you want to test your portfolios against FF3 and FF5 factors. It is strictly speaking not correct to just use a subsample of factors from FF5 if you want to test against the FF3 factors. I rather recommend downloading both FF3 and FF5 and run tests with each data set separately."
  }
]