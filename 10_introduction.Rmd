# Introduction to Tidy Finance

The main aim of this chapter is to familiarize yourself with the `tidyverse`. We start out by downloading and visualizing stock data before we move to a simple portfolio choice problem. These examples introduce you to our approach of *tidy finance*.

## Download and work with stock market data {#stock_market_data}

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = TRUE,
  cache = FALSE,
  fig.width = 8,
  fig.height = 4,
  fig.align = "center"
)
```

To download price data, you can use the convenient `tidyquant`package. 
If you have trouble using `tidyquant`, check out the [documentation](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ01-core-functions-in-tidyquant.html#yahoo-finance). Start the session by loading the `tidyverse` and the `tidyquant` package as shown below. 

```{r}
# install.packages("tidyverse")
# install.packages("tidyquant")
library(tidyverse)
library(tidyquant)
```

We first download daily prices for one stock market ticker, e.g. *AAPL*, directly from the data provider Yahoo!Finance. To download the data, you can use the command `tq_get`. If you do not know how to use it, make sure you read the help file by calling `?tq_get`. We especially recommend taking a look at the documentation's examples section. 

```{r}
prices <- tq_get("AAPL", get = "stock.prices")
prices %>% head() # Take a glimpse at the data
```

`tq_get` downloads stock market data from Yahoo!Finance if you do not specify another data source. The function returns a tibble with eight quite self-explanatory columns: *symbol*, *date*, the market prices at the *open, high, low* and *close*, the daily *volume* (in number of shares), and the *adjusted* price in USD, which factors in anything that might affect the stock price after the market closes, e.g. stock splits, repurchases and dividends.  

Next, we use `ggplot` to visualize the time series of adjusted prices. 
```{r}
prices %>% # Simple visualization of the downloaded price time series
  ggplot(aes(x = date, y = adjusted)) +
  geom_line() +
  labs(
    x = NULL,
    y = NULL,
    title = "AAPL stock prices",
    subtitle = "Prices in USD, adjusted for dividend payments and stock splits"
  ) +
  theme_bw()
```
Next, we compute daily returns defined as $(p_t - p_{t-1}) / p_{t-1}$ where $p_t$ is the adjusted day $t$ price. 
```{r}
returns <- prices %>%
  arrange(date) %>%
  mutate(ret = (adjusted / lag(adjusted) - 1)) %>%
  select(symbol, date, ret)
returns %>% head()
```

The resulting tibble contains three columns where the last contains the daily returns. Note that the first entry naturally contains `NA` because there is no previous price. Note also that the computations require that the time series is ordered by date - otherwise, `lag` would be meaningless.
For the upcoming examples we remove missing values as these would require careful treatment when computing, e.g., sample averages. In general, however, make sure you understand why `NA` values occur and if you can simply get rid of these observations. 

```{r}
returns <- returns %>%
  drop_na(ret)
```
Next, we visualize the distribution of daily returns in a histogram. Just for fun, we also add a dashed red line that indicates the 5\% quantile of the daily returns to the histogram - this value is a (crude) proxy for the worst return of the stock with a probability of at least 5\%. 
```{r}
quantile_05 <- quantile(returns %>% pull(ret), 0.05) # Compute the 5 % quantile of the returns

returns %>% # create a histogram for daily returns
  ggplot(aes(x = ret)) +
  geom_histogram(bins = 100) +
  geom_vline(aes(xintercept = quantile_05),
    color = "red",
    linetype = "dashed"
  ) +
  labs(
    x = NULL, y = NULL,
    title = "Distribution of daily AAPL returns",
    subtitle = "The dotted vertical line indicates the historical 5% quantile"
  ) +
  theme_bw()
```

Here, `bins = 100` determines the number of bins and hence implicitly the width of the bins. Before proceeding, make sure you understand how to use the geom `geom_vline()` to add a dotted red line that indicates the 5\% quantile of the daily returns. 
A typical task before proceeding with *any* data is to compute summary statistics for the main variables of interest. 

```{r}
returns %>%
  summarise(across(
    ret,
    list(
      daily_mean = mean,
      daily_sd = sd,
      daily_min = min,
      daily_max = max
    )
  )) %>%
  kableExtra::kable(digits = 3)
```

```{r}
# Alternatively: compute summary statistics for each year
returns %>%
  group_by(year = year(date)) %>%
  summarise(across(
    ret,
    list(
      daily_mean = mean,
      daily_sd = sd,
      daily_min = min,
      daily_max = max
    ),
    .names = "{.fn}"
  )) %>%
  kableExtra::kable(digits = 3)
```
In case you wonder: the additional argument `.names = "{.fn}"` in `across()` determines how to name the output columns. The specification is rather flexible and allows to next to arbitrary column names which can be useful for reporting.

## Scale the analysis up: tidyverse-magic
As a next step, we take the code from before and generalize it such that all the computations are performed for an arbitrary vector of tickers or even for all stocks that represent an index. Following tidy principles, it turns out to be quite easy to automate the download, generate the plot of the price time series and the table of summary statistics for an arbitrary number of assets.

This is where the `tidyverse` magic starts: tidy data makes it extremely easy to generalize the computations from before to as many assets you like. The following code takes any vector of tickers, e.g., `ticker <- c("AAPL", "MMM", "BA")`, and automates the download as well as the plot of the price time series. At the end, we create the table of summary statistics for an arbitrary number of assets. 

Figure @ref(fig:prices) illustrates the time series of downloaded *adjusted* prices for each of the 30 constituents of the Dow Jones index. Make sure you understand every single line of code! (What is the purpose of `%>%`? What are the arguments of `aes()`? Which alternative *geoms* could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes). 

```{r prices, fig.cap = "DOW index stock prices."}
ticker <- tq_index("DOW") # tidyquant delivers all constituents of the Dow Jones index
index_prices <- tq_get(ticker,
  get = "stock.prices",
  from = "2000-01-01"
) %>% # Exactly the same code as in the first part
  filter(symbol != "DOW") # Exclude the index itself

index_prices <- index_prices %>% # Remove assets that did not trade since January 1st 2000
  group_by(symbol) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  filter(n == max(n)) %>%
  select(-n)

index_prices %>%
  ggplot(aes(
    x = date,
    y = adjusted,
    color = symbol
  )) +
  geom_line() +
  labs(
    x = NULL,
    y = NULL,
    color = NULL,
    title = "DOW index stock prices",
    subtitle = "Prices in USD, adjusted for dividend payments and stock splits"
  ) +
  theme_bw() +
  theme(legend.position = "none")
```

Do you notice the small differences relative to the code we used before? `tq_get(ticker)` returns a tibble for several symbols as well. All we need to do to illustrate all tickers instead of only one is to include `color = symbol` in the `ggplot` aesthetics. In this way, we can generate a separate line for each ticker.

The same holds for returns as well. Before computing returns as before, we use `group_by(symbol)` such that the `mutate` command is performed for each symbol individually. Exactly the same logic applies for the computation of summary statistics: `group_by(symbol)` is the key to aggregate the time series into ticker-specific variables of interest. 

```{r}
all_returns <- index_prices %>%
  group_by(symbol) %>% # we perform the computations per symbol
  mutate(ret = adjusted / lag(adjusted) - 1) %>%
  select(symbol, date, ret) %>%
  drop_na(ret)

all_returns %>%
  group_by(symbol) %>%
  summarise(across(
    ret,
    list(
      daily_mean = mean,
      daily_sd = sd,
      daily_min = min,
      daily_max = max
    )
  )) %>%
  kableExtra::kable(digits = 3)
```

Note that you are now also equipped with all tools to download price data for *each* ticker listed in the S&P 500 index with the same number of lines of code. Just use `ticker <- tq_index("SP500")` which provides you with a tibble that contains each symbol that is (currently) part of the S&P 500. However, don't try this if you are not prepared to wait for a couple of minutes.


## Other forms of data aggregation 
Sometimes, aggregation across other variables than `symbol` makes sense as well. For instance, suppose you are interested in the question: are days with high aggregate trading volume followed by high aggregate trading volume days? To provide some initial analysis on this question we take the downloaded tibble with prices and compute aggregate daily trading volume for all Dow Jones constituents in USD. Recall that the column *volume* is denoted in the number of traded shares. We multiply the trading volume with the daily closing price to get a measure of the aggregate trading volume in USD. Scaling by `1e9` denotes daily trading volume in billion USD.  

```{r}
volume <- index_prices %>%
  mutate(volume_usd = volume * close / 1e9) %>%
  group_by(date) %>%
  summarise(volume = sum(volume_usd))

volume %>% # Plot the time series of aggregate trading volume
  ggplot(aes(x = date, y = volume)) +
  geom_line() +
  labs(
    x = NULL, y = NULL,
    title = "Aggregate trading volume (billion USD)"
  ) +
  theme_bw()
```

One way to illustrate the persistence of trading volume would be to plot volume on day $t$ against volume on day $t-1$ as in the example below:

```{r aggregate-volume}
volume %>%
  ggplot(aes(x = lag(volume), y = volume)) +
  geom_point() +
  geom_abline(aes(intercept = 0, slope = 1), linetype = "dotted") +
  labs(
    x = "Previous day aggregate trading volume (billion USD)",
    y = "Aggregate trading volume (billion USD)",
    title = "Persistence of trading volume"
  ) +
  theme_bw() +
  theme(legend.position = "None")
```

Do you understand where the warning `## Warning: Removed 1 rows containing missing values (geom_point).
` comes from and what it means? Pure eye-balling reveals that days with high trading volume are often followed by similarly high trading volume days.  

## Portfolio choice problems

A very typically question in Finance is how to optimally allocate wealth across different assets. The standard framework for optimal portfolio selection is based on investors that dislike portfolio return volatility and like higher expected returns: the mean-variance investor. An essential tool to evaluate portfolios is the efficient frontier, the set of portfolios which satisfy the condition that no other portfolio exists with a higher expected return but with the same standard deviation of return (i.e., the risk). Let us compute and visualize the efficient frontier for a number of stocks. First, we use our dataset to compute the *monthly* returns for each asset. 

```{r}
returns <- index_prices %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(symbol, month) %>%
  summarise(price = last(adjusted), .groups = "drop_last") %>%
  mutate(ret = price / lag(price) - 1) %>%
  drop_na(ret) %>%
  select(-price)
```

Next, we transform the returns from a tidy tibble into a $(T \times N)$ matrix with one column for each ticker to compute the covariance matrix $\Sigma$ and also the expected return vector $\mu$.
We compute the vector of sample average returns and the sample variance covariance matrix. 

```{r}
returns_matrix <- returns %>%
  pivot_wider(
    names_from = symbol,
    values_from = ret
  ) %>%
  select(-month)

sigma <- cov(returns_matrix)
mu <- colMeans(returns_matrix)
```

Then, we compute the minimum variance portfolio weight $\omega_\text{mvp}$ as well as the expected return $\omega_\text{mvp}'\mu$ and volatility $\sqrt{\omega_\text{mvp}'\Sigma\omega_\text{mvp}}$ of this portfolio. Recall that the minimum variance portfolio is the vector of portfolio weights that are the solution to 
$$\arg\min w'\Sigma w \text{ s.t. } \sum\limits_{i=1}^Nw_i = 1.$$
It is easy to show analytically, that $\omega_\text{mvp} = \frac{\Sigma^{-1}\iota}{\iota'\Sigma^{-1}\iota}$ where $\iota$ is a vector of ones.

```{r}
N <- ncol(returns_matrix)
iota <- rep(1, N)
wmvp <- solve(sigma) %*% iota
wmvp <- wmvp / sum(wmvp)

c(t(wmvp) %*% mu, sqrt(t(wmvp) %*% sigma %*% wmvp)) # Expected return and volatility
```

Note that the *monthly* volatility of the minimum variance portfolio is of the same order of magnitude than the *daily* standard deviation of the individual components. Thus, the diversification benefits are tremendous!
Next we compute the efficient portfolio weights which achieves 3 times the expected return of the minimum variance portfolio. If you wonder where the solution $\omega_\text{eff}$ comes from: The efficient portfolio is chosen by an investor who aims to achieve minimum variance *given a minimum acceptable expected return* $\bar{\mu}$ such that her objective function is to choose $\omega_\text{eff}$ as the solution to
$$\arg\min w'\Sigma w \text{ s.t. } w'\iota = 1 \text{ and } \omega'\mu \geq \bar{\mu}.$$
The code below implements the analytic solution to this optimization problem, we encourage you to verify that it is correct. 

```{r}
# Compute efficient portfolio weights for given level of expected return
mu_bar <- 3 * t(wmvp) %*% mu # some benchmark return: 3 times the minimum variance portfolio expected return

C <- as.numeric(t(iota) %*% solve(sigma) %*% iota)
D <- as.numeric(t(iota) %*% solve(sigma) %*% mu)
E <- as.numeric(t(mu) %*% solve(sigma) %*% mu)

lambda_tilde <- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))
weff <- wmvp + lambda_tilde / 2 * (solve(sigma) %*% mu - D / C * solve(sigma) %*% iota)
```

## The efficient frontier 

The two mutual fund theorem states that as soon as we have two efficient portfolios (such as the minimum variance portfolio and the efficient portfolio for another required level of expected returns like above), we can characterize the entire efficient frontier by combining these two portfolios. This is done in the code below. Make sure to familiarize yourself with the inner workings of the `for` loop!.

```{r}
# Use the two mutual fund theorem
c <- seq(from = -0.4, to = 1.9, by = 0.01) # Some values for a linear combination of two efficient portfolio weights
res <- tibble(
  c = c,
  mu = NA,
  sd = NA
)

for (i in seq_along(c)) { # A for loop
  w <- (1 - c[i]) * wmvp + (c[i]) * weff # A portfolio of minimum variance and efficient portfolio
  res$mu[i] <- 12 * 100 * t(w) %*% mu # Portfolio expected return (annualized, in percent)
  res$sd[i] <- 12 * 10 * sqrt(t(w) %*% sigma %*% w) # Portfolio volatility (annualized, in percent)
}
```

Finally, it is simple to visualize everything within one, powerful figure using `ggplot2`. 
```{r}
# Visualize the efficient frontier
res %>%
  ggplot(aes(x = sd, y = mu)) +
  geom_point() + # Plot all sd/mu portfolio combinations
  geom_point(
    data = res %>% filter(c %in% c(0, 1)),
    color = "red",
    size = 4
  ) + # locate the minimum variance and efficient portfolio
  geom_point(
    data = tibble(mu = 12 * 100 * mu, sd = 12 * 10 * sqrt(diag(sigma))),
    aes(y = mu, x = sd), color = "blue", size = 1
  ) + # locate the individual assets
  theme_bw() + # make the plot a bit nicer
  labs(
    x = "Annualized standard deviation (in percent)",
    y = "Annualized expected return (in percent)",
    title = "Dow Jones asset returns and efficient frontier",
    subtitle = "Red dots indicate the location of the minimum variance and efficient tangency portfolio"
  )
```
The black line indicates the efficient frontier: the set of portfolios a mean-variance efficient investor would choose from. Compare the performance relative to the individual assets (the blue dots) - it should become clear that diversifying yields massive performance gains (at least as long as we take the parameters $\Sigma$ and $\mu$ as given).

## Exercises

1. Download daily prices for another stock market ticker of your choice from Yahoo!Finance with `tq_get` from the `tidyquant` package. Plot the time series of adjusted closing prices.
1. Compute daily net returns for the asset and visualize the distribution of daily returns in a histogram. Also, use `geom_vline()` to add a dashed red line that indicates the 5% quantile of the daily returns within the histogram. Compute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns
1. Take your code from before and generalize it such all the computations are performed for an arbitrary vector of tickers (e.g., `ticker <- c("AAPL", "MMM", "BA")`). Automate the download, the plot of the price time series and create a table of return summary statistics for this arbitrary number of assets.
1. Consider the research question Are days with high aggregate trading volume often also days with large absolute price changes? Find an appropriate visualization to analyze the question.
1. Compute monthly returns from the downloaded stock market prices. Compute the vector of historical average returns and the sample variance-covariance matrix. After computing the minimum variance portfolio weights and the portfolio volatility and average returns, visualize the mean-variance efficient frontier. Choose one of your assets and identify the portfolio which yields exactly the same historical volatility but achieves the highest possible average return.
