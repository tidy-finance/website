---
title: Fama-MacBeth Regressions
metadata:
  pagetitle: Fama-MacBeth Regressions with Python
  description-meta: Estimate risk premiums via Fama-MacBeth regressions using the programming language Python.
---

```{python}
#| echo: false
exec(open("./render-settings.py").read())
```

::: {.callout-note}
You are reading **Tidy Finance with Python**. You can find the equivalent chapter for the sibling **Tidy Finance with R** [here](../r/fama-macbeth-regressions.qmd).
:::

In this chapter, we present a simple implementation of @Fama1973, a regression approach commonly called Fama-MacBeth regressions. Fama-MacBeth regressions are widely used in empirical asset pricing studies. We use individual stocks as test assets to estimate the risk premium associated with the three factors included in @Fama1993.

Researchers use the two-stage regression approach to estimate risk premiums in various markets, but predominately in the stock market. Essentially, the two-step Fama-MacBeth regressions exploit a linear relationship between expected returns and exposure to (priced) risk factors. The basic idea of the regression approach is to project asset returns on factor exposures or characteristics that resemble exposure to a risk factor in the cross-section in each time period. Then, in the second step, the estimates are aggregated across time to test if a risk factor is priced. In principle, Fama-MacBeth regressions can be used in the same way as portfolio sorts introduced in previous chapters.

\index{Regression!Fama-MacBeth}\index{Fama-MacBeth} The Fama-MacBeth procedure is a simple two-step approach: 
The first step uses the exposures (characteristics) as explanatory variables in $T$ cross-sectional regressions. For example, if $r_{i,t+1}$ denote the excess returns of asset $i$ in month $t+1$, then the famous Fama-French three-factor model implies the following return generating process [see also @Campbell1998]:
$$\begin{aligned}r_{i,t+1} = \alpha_i + \lambda^{M}_t \beta^M_{i,t} + \lambda^{SMB}_t \beta^{SMB}_{i,t} + \lambda^{HML}_t \beta^{HML}_{i,t} + \epsilon_{i,t}.\end{aligned}$${#eq-fama-3-factor} 
Here, we are interested in the compensation $\lambda^{f}_t$ for the exposure to each risk factor $\beta^{f}_{i,t}$ at each time point, i.e., the risk premium. Note the terminology: $\beta^{f}_{i,t}$ is an asset-specific characteristic, e.g., a factor exposure or an accounting variable. *If* there is a linear relationship between expected returns and the characteristic in a given month, we expect the regression coefficient to reflect the relationship, i.e., $\lambda_t^{f}\neq0$. 

In the second step, the time-series average $\frac{1}{T}\sum_{t=1}^T \hat\lambda^{f}_t$ of the estimates $\hat\lambda^{f}_t$ can then be interpreted as the risk premium for the specific risk factor $f$. We follow @Zaffaroni2022 and consider the standard cross-sectional regression to predict future returns. If the characteristics are replaced with time $t+1$ variables, then the regression approach captures risk attributes rather than risk premiums. 

Before we move to the implementation, we want to highlight that the characteristics, e.g., $\hat\beta^{f}_{i}$, are often estimated in a separate step before applying the actual Fama-MacBeth methodology. You can think of this as a *step 0*. You might thus worry that the errors of $\hat\beta^{f}_{i}$ impact the risk premiums' standard errors. Measurement error in $\hat\beta^{f}_{i}$ indeed affects the risk premium estimates, i.e., they lead to biased estimates. The literature provides adjustments for this bias [see, e.g., @Shanken1992; @Kim1995; @Chen2015, among others] but also shows that the bias goes to zero as $T \to \infty$. We refer to @Gagliardini2016 for an in-depth discussion also covering the case of time-varying betas. Moreover, if you plan to use Fama-MacBeth regressions with individual stocks, @Hou2020 advocates using weighted-least squares to estimate the coefficients such that they are not biased toward small firms. Without this adjustment, the high number of small firms would drive the coefficient estimates.

The current chapter relies on this set of Python packages. 

```{python}
import pandas as pd
import numpy as np
import sqlite3
import statsmodels.formula.api as smf
```

## Data Preparation

We illustrate @Fama1973 with the monthly CRSP sample and use three characteristics to explain the cross-section of returns: Market capitalization, the book-to-market ratio, and the CAPM beta (i.e., the covariance of the excess stock returns with the market excess returns). We collect the data from our SQLite database introduced in [Accessing and Managing Financial Data](accessing-and-managing-financial-data.qmd) and [WRDS, CRSP, and Compustat](wrds-crsp-and-compustat.qmd).\index{Data!CRSP}\index{Data!Compustat}\index{Beta}

```{python}
tidy_finance = sqlite3.connect(database="data/tidy_finance_python.sqlite")

crsp_monthly = pd.read_sql_query(
  sql="SELECT permno, gvkey, date, ret_excess, mktcap FROM crsp_monthly",
  con=tidy_finance,
  parse_dates={"date"}
)

compustat = pd.read_sql_query(
  sql="SELECT datadate, gvkey, be FROM compustat",
  con=tidy_finance,
  parse_dates={"datadate"}
)

beta = pd.read_sql_query(
  sql="SELECT date, permno, beta_monthly FROM beta",
  con=tidy_finance,
  parse_dates={"date"}
)
```

We use the Compustat and CRSP data to compute the book-to-market ratio and the (log) market capitalization.\index{Book-to-market ratio}\index{Market capitalization} Furthermore, we also use the CAPM betas based on monthly returns we computed in the previous chapters.\index{Beta}\index{CAPM}

```{python}
characteristics = (compustat
  .assign(date=lambda x: x["datadate"].dt.to_period("M").dt.to_timestamp())
  .merge(crsp_monthly, how="left", on=["gvkey", "date"], )
  .merge(beta, how="left", on=["permno", "date"])
  .assign(
    bm=lambda x: x["be"]/x["mktcap"],
    log_mktcap=lambda x: np.log(x["mktcap"]),
    sorting_date=lambda x: x["date"]+pd.DateOffset(months=6)
  )
  .get(["gvkey", "bm", "log_mktcap", "beta_monthly", "sorting_date"])
  .rename(columns={"beta_monthly": "beta"})
)

data_fama_macbeth = (crsp_monthly
  .merge(characteristics, 
         how="left",
         left_on=["gvkey", "date"], right_on=["gvkey", "sorting_date"])
  .sort_values(["date", "permno"])
  .groupby("permno")
  .apply(lambda x: x.assign(
      beta=x["beta"].fillna(method="ffill"),
      bm=x["bm"].fillna(method="ffill"),
      log_mktcap=x["log_mktcap"].fillna(method="ffill")
    )
  )
  .reset_index(drop=True)  
)

data_fama_macbeth_lagged = (data_fama_macbeth
  .assign(date=lambda x: x["date"]-pd.DateOffset(months=1))
  .get(["permno", "date", "ret_excess"])
  .rename(columns={"ret_excess": "ret_excess_lead"})
)

data_fama_macbeth = (data_fama_macbeth
  .merge(data_fama_macbeth_lagged, how="left", on=["permno", "date"])
  .get(["permno", "date", "ret_excess_lead", "beta", "log_mktcap", "bm"])
  .dropna()
)
```

## Cross-Sectional Regression

Next, we run the cross-sectional regressions with the characteristics as explanatory variables for each month. We regress the returns of the test assets at a particular time point on the characteristics of each asset. By doing so, we get an estimate of the risk premiums $\hat\lambda^{f}_t$ for each point in time. \index{Regression!Cross-section}

```{python}
risk_premiums = (data_fama_macbeth
  .groupby("date")
  .apply(lambda x: smf.ols(
      formula="ret_excess_lead ~ beta + log_mktcap + bm", 
      data=x
    ).fit()
    .params
  )
  .reset_index()
)
```

## Time-Series Aggregation

Now that we have the risk premiums' estimates for each period, we can average across the time-series dimension to get the expected risk premium for each characteristic. Similarly, we manually create the $t$-test statistics for each regressor, which we can then compare to usual critical values of 1.96 or 2.576 for two-tailed significance tests at a five percent and a one percent significance level.

```{python}
price_of_risk = (risk_premiums
  .melt(id_vars="date", var_name="factor", value_name="estimate")
  .groupby("factor")["estimate"]
  .apply(lambda x: pd.Series({
      "risk_premium": x.mean(),
      "t_statistic": x.mean()/x.std()*np.sqrt(len(x))
    })
  )
  .reset_index()
  .pivot(index="factor", columns="level_1", values="estimate")
  .reset_index()
)
```

It is common to adjust for autocorrelation when reporting standard errors of risk premiums. As in [Univariate Portfolio Sorts](univariate-portfolio-sorts.qmd), the typical procedure for this is computing @Newey1987 standard errors.\index{Standard errors!Newey-West}

```{python}
price_of_risk_newey_west = (risk_premiums
  .melt(id_vars="date", var_name="factor", value_name="estimate")
  .groupby("factor")
  .apply(lambda x: (
      x["estimate"].mean()/ 
        smf.ols("estimate ~ 1", x)
        .fit(cov_type="HAC", cov_kwds={"maxlags": 6}).bse
    )
  )
  .reset_index()
  .rename(columns={"Intercept": "t_statistic_newey_west"})
)

(price_of_risk
  .merge(price_of_risk_newey_west, on="factor")
  .round(3)
)
```

Finally, let us interpret the results. Stocks with higher book-to-market ratios earn higher expected future returns, which is in line with the value premium. The negative value for log market capitalization reflects the size premium for smaller stocks. Consistent with results from earlier chapters, we detect no relation between beta and future stock returns.

```{python}
#| output: false
import tidyfinance as tf

tf.estimate_fama_macbeth(
  data=data_fama_macbeth,
  model="ret_excess_lead ~ beta + bm + log_mktcap",
  vcov="newey-west"
)
```

## Exercises

1. Download a sample of test assets from Kenneth French's homepage and reevaluate the risk premiums for industry portfolios instead of individual stocks.
1. Use individual stocks with weighted-least squares based on a firm's size as suggested by @Hou2020. Then, repeat the Fama-MacBeth regressions without the weighting-scheme adjustment but drop the smallest 20 percent of firms each month. Compare the results of the three approaches. 