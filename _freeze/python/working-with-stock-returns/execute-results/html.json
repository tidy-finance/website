{
  "hash": "261663101cdedcdb8c45d51e66e5d685",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Working with Stock Returns\naliases:\n  - ../python/introduction-to-tidy-finance.html\nmetadata:\n  pagetitle: Working with Stock Returns in Python\n  description-meta: Learn how to use the programming language Python for downloading and analyzing stock market data.\n---\n\n\n\n::: {.callout-note}\nYou are reading **Tidy Finance with Python**. You can find the equivalent chapter for the sibling **Tidy Finance with R** [here](../r/working-with-stock-returns.qmd).\n:::\n\nThe main aim of this chapter is to familiarize yourself with the core packages for working with stock market data. We focus on downloading and visualizing stock data from data provider Yahoo Finance.\n\nAt the start of each session, we load the required Python packages. Throughout the entire book, we always use the `pandas` [@mckinney-proc-scipy-2010] and `numpy` [@harris2020array] packages. In this chapter, we also load the `tidyfinance` package to download stock price data. This package provides a convenient wrapper for various quantitative functions compatible with the core packages and our book.\n\nYou typically have to install a package once before you can load it into your active R session. In case you have not done this yet, call, for instance, `pip install tidyfinance` in your terminal.\\index{tidyfinance}\n\n::: {#88a952ce .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\n```\n:::\n\n\n## Downloading Data\n\nNote that `import pandas as pd` implies that we can call all pandas functions later with a simple `pd.function()`. Instead, utilizing `from pandas import *` is generally discouraged, as it leads to namespace pollution. This statement imports all functions and classes from `pandas` into your current namespace, potentially causing conflicts with functions you define or those from other imported libraries. Using the `pd` abbreviation is a very convenient way to prevent this.\\index{pandas}\n\nWe first download daily prices for one stock symbol, e.g., the Apple stock (*AAPL*), directly from the data provider Yahoo Finance. To download the data, you can use the function `tf.download_data()`.\n\nIn the following code, we request daily data from the beginning of 2000 to the end of the last year, which is a period of more than 20 years.\\index{Stock prices}\n\n::: {#63cf5867 .cell execution_count=3}\n``` {.python .cell-code}\nprices = tf.download_data(\n  domain=\"stock_prices\", \n  symbols=\"AAPL\",\n  start_date=\"2000-01-01\", \n  end_date=\"2023-12-31\"\n)\nprices.head().round(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>symbol</th>\n      <th>volume</th>\n      <th>open</th>\n      <th>low</th>\n      <th>high</th>\n      <th>close</th>\n      <th>adjusted_close</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2000-01-03</td>\n      <td>AAPL</td>\n      <td>535796800</td>\n      <td>0.936</td>\n      <td>0.908</td>\n      <td>1.004</td>\n      <td>0.999</td>\n      <td>0.842</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2000-01-04</td>\n      <td>AAPL</td>\n      <td>512377600</td>\n      <td>0.967</td>\n      <td>0.903</td>\n      <td>0.988</td>\n      <td>0.915</td>\n      <td>0.771</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2000-01-05</td>\n      <td>AAPL</td>\n      <td>778321600</td>\n      <td>0.926</td>\n      <td>0.920</td>\n      <td>0.987</td>\n      <td>0.929</td>\n      <td>0.782</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2000-01-06</td>\n      <td>AAPL</td>\n      <td>767972800</td>\n      <td>0.948</td>\n      <td>0.848</td>\n      <td>0.955</td>\n      <td>0.848</td>\n      <td>0.715</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2000-01-07</td>\n      <td>AAPL</td>\n      <td>460734400</td>\n      <td>0.862</td>\n      <td>0.853</td>\n      <td>0.902</td>\n      <td>0.888</td>\n      <td>0.749</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\\index{Data!Yahoo Finance} `tf.download_data(domain=\"stock_prices\")` downloads stock market data from Yahoo Finance. The above code chunk returns a data frame with eight self-explanatory columns: `date`, `symbol`, the daily `volume` (in the number of traded shares), the market prices at the `open`, `low`, `high`, `close`, and the `adjusted_close` price in USD. The adjusted prices are corrected for anything that might affect the stock price after the market closes, e.g., stock splits and dividends. These actions affect the quoted prices, but they have no direct impact on the investors who hold the stock. Therefore, we often rely on adjusted prices when it comes to analyzing the returns an investor would have earned by holding the stock continuously.\\index{Stock price adjustments}\n\nNext, we use the `plotnine` package [@plotnine] to visualize the time series of adjusted prices in @fig-100. This package takes care of visualization tasks based on the principles of the grammar of graphics [@Wilkinson2012].\\index{Graph!Time series} Note that generally, we do not recommend using the `*` import style. However, we use it here only for the plotting functions, which are distinct to `plotnine` and have very plotting-related names. So, the risk of misuse through a polluted namespace is marginal.\n\n::: {#129ef351 .cell execution_count=4}\n``` {.python .cell-code}\nfrom plotnine import *\n```\n:::\n\n\n\n\nCreating figures becomes very intuitive with the Grammar of Graphics, as the following code chunk demonstrates.\n\n::: {#cell-fig-100 .cell execution_count=6}\n``` {.python .cell-code}\napple_prices_figure = (\n  ggplot(prices, aes(y=\"adjusted_close\", x=\"date\"))\n  + geom_line()\n  + labs(x=\"\", y=\"\", title=\"Apple stock prices from 2000 to 2023\")\n)\napple_prices_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Prices are in USD, adjusted for dividend payments and stock splits.](working-with-stock-returns_files/figure-html/fig-100-output-1.png){#fig-100 width=2100 height=1500 fig-alt='Title: Apple stock prices between beginning of 2000 and end of 2023. The figure shows that the stock price of Apple increased from about 1 USD to around 125 USD.'}\n:::\n:::\n\n\n## Computing Returns\n\nInstead of analyzing prices, we compute daily returns defined as $r_t = p_t / p_{t-1} - 1$, where $p_t$ is the adjusted price at the end of day $t$.\\index{Returns} In that context, the function `lag()` is helpful by returning the previous value.\n\n::: {#efe71ddb .cell execution_count=7}\n``` {.python .cell-code}\nreturns = (prices\n  .sort_values(\"date\")\n  .assign(ret=lambda x: x[\"adjusted_close\"].pct_change())\n  .get([\"symbol\", \"date\", \"ret\"])\n)\nreturns\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>date</th>\n      <th>ret</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AAPL</td>\n      <td>2000-01-03</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AAPL</td>\n      <td>2000-01-04</td>\n      <td>-0.084310</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AAPL</td>\n      <td>2000-01-05</td>\n      <td>0.014633</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AAPL</td>\n      <td>2000-01-06</td>\n      <td>-0.086539</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AAPL</td>\n      <td>2000-01-07</td>\n      <td>0.047369</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6032</th>\n      <td>AAPL</td>\n      <td>2023-12-22</td>\n      <td>-0.005548</td>\n    </tr>\n    <tr>\n      <th>6033</th>\n      <td>AAPL</td>\n      <td>2023-12-26</td>\n      <td>-0.002841</td>\n    </tr>\n    <tr>\n      <th>6034</th>\n      <td>AAPL</td>\n      <td>2023-12-27</td>\n      <td>0.000518</td>\n    </tr>\n    <tr>\n      <th>6035</th>\n      <td>AAPL</td>\n      <td>2023-12-28</td>\n      <td>0.002226</td>\n    </tr>\n    <tr>\n      <th>6036</th>\n      <td>AAPL</td>\n      <td>2023-12-29</td>\n      <td>-0.005424</td>\n    </tr>\n  </tbody>\n</table>\n<p>6037 rows Ã— 3 columns</p>\n</div>\n```\n:::\n:::\n\n\nThe resulting data frame has three columns, the last of which contains the daily returns (`ret`). Note that the first entry naturally contains a missing value (`NaN`) because there is no previous price.\\index{Missing value} Obviously, the use of `pct_change()` would be meaningless if the time series is not ordered by ascending dates. The function `sort_values()` provides a convenient way to order observations in the correct way for our application. In case you want to order observations by descending dates, you can use the parameter `ascending=False`.\n\nFor the upcoming examples, we remove missing values as these would require separate treatment for many applications. For example, missing values can affect sums and averages by reducing the number of valid data points if not properly accounted for. In general, always ensure you understand why `NaN` values occur and carefully examine if you can simply get rid of these observations.\n\n::: {#48f6767d .cell execution_count=8}\n``` {.python .cell-code}\nreturns = returns.dropna() \n```\n:::\n\n\nNext, we visualize the distribution of daily returns in a histogram in @fig-101. \\index{Graph!Histogram} Additionally, we draw a dashed line that indicates the historical five percent quantile of the daily returns to the histogram, which is a crude proxy for the worst possible return of the stock with a probability of at most five percent. This quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators.\\index{Value-at-risk} We refer to @Tsay2010 for a more thorough introduction to the stylized facts of financial returns.\n\n::: {#cell-fig-101 .cell execution_count=9}\n``` {.python .cell-code}\nfrom mizani.formatters import percent_format\n\nquantile_05 = returns[\"ret\"].quantile(0.05)\n\napple_returns_figure = (\n  ggplot(returns, aes(x=\"ret\"))\n  + geom_histogram(bins=100)\n  + geom_vline(aes(xintercept=quantile_05), linetype=\"dashed\")\n  + labs(x=\"\", y=\"\", title=\"Distribution of daily Apple stock returns\")\n  + scale_x_continuous(labels=percent_format())\n)\napple_returns_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The dotted vertical line indicates the historical five percent quantile.](working-with-stock-returns_files/figure-html/fig-101-output-1.png){#fig-101 width=2100 height=1500 fig-alt='Title: Distribution of daily Apple stock returns in percent. The figure shows a histogram of daily returns. The range indicates a few large negative values, while the remaining returns are distributed around zero. The vertical line indicates that the historical five percent quantile of daily returns was around negative three percent.'}\n:::\n:::\n\n\nHere, `bins=100` determines the number of bins used in the illustration and, hence, implicitly sets the width of the bins. Before proceeding, make sure you understand how to use the geom `geom_vline()` to add a dashed line that indicates the historical five percent quantile of the daily returns. Before proceeding with *any* data, a typical task is to compute and analyze the summary statistics for the main variables of interest.\n\n::: {#20e1eb56 .cell execution_count=10}\n``` {.python .cell-code}\npd.DataFrame(returns[\"ret\"].describe()).round(3).T\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ret</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.025</td>\n      <td>-0.519</td>\n      <td>-0.01</td>\n      <td>0.001</td>\n      <td>0.013</td>\n      <td>0.139</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe see that the maximum *daily* return was 13.9 percent. Perhaps not surprisingly, the average daily return is close to but slightly above 0. In line with the illustration above, the large losses on the day with the minimum returns indicate a strong asymmetry in the distribution of returns.\n\nYou can also compute these summary statistics for each year individually by imposing `groupby(returns[\"date\"].dt.year)`, where the call `dt.year` returns the year. More specifically, the few lines of code below compute the summary statistics from above for individual groups of data defined by the values of the column year. The summary statistics, therefore, allow an eyeball analysis of the time-series dynamics of the daily return distribution.\\index{Summary statistics}\n\n::: {#d0c3eab8 .cell execution_count=11}\n``` {.python .cell-code}\n(returns[\"ret\"]\n  .groupby(returns[\"date\"].dt.year)\n  .describe()\n  .round(3)\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2000</th>\n      <td>251.0</td>\n      <td>-0.003</td>\n      <td>0.055</td>\n      <td>-0.519</td>\n      <td>-0.034</td>\n      <td>-0.002</td>\n      <td>0.027</td>\n      <td>0.137</td>\n    </tr>\n    <tr>\n      <th>2001</th>\n      <td>248.0</td>\n      <td>0.002</td>\n      <td>0.039</td>\n      <td>-0.172</td>\n      <td>-0.023</td>\n      <td>-0.001</td>\n      <td>0.027</td>\n      <td>0.129</td>\n    </tr>\n    <tr>\n      <th>2002</th>\n      <td>252.0</td>\n      <td>-0.001</td>\n      <td>0.031</td>\n      <td>-0.150</td>\n      <td>-0.019</td>\n      <td>-0.003</td>\n      <td>0.018</td>\n      <td>0.085</td>\n    </tr>\n    <tr>\n      <th>2003</th>\n      <td>252.0</td>\n      <td>0.002</td>\n      <td>0.023</td>\n      <td>-0.081</td>\n      <td>-0.012</td>\n      <td>0.002</td>\n      <td>0.015</td>\n      <td>0.113</td>\n    </tr>\n    <tr>\n      <th>2004</th>\n      <td>252.0</td>\n      <td>0.005</td>\n      <td>0.025</td>\n      <td>-0.056</td>\n      <td>-0.009</td>\n      <td>0.003</td>\n      <td>0.016</td>\n      <td>0.132</td>\n    </tr>\n    <tr>\n      <th>2005</th>\n      <td>252.0</td>\n      <td>0.003</td>\n      <td>0.024</td>\n      <td>-0.092</td>\n      <td>-0.010</td>\n      <td>0.003</td>\n      <td>0.017</td>\n      <td>0.091</td>\n    </tr>\n    <tr>\n      <th>2006</th>\n      <td>251.0</td>\n      <td>0.001</td>\n      <td>0.024</td>\n      <td>-0.063</td>\n      <td>-0.014</td>\n      <td>-0.002</td>\n      <td>0.014</td>\n      <td>0.118</td>\n    </tr>\n    <tr>\n      <th>2007</th>\n      <td>251.0</td>\n      <td>0.004</td>\n      <td>0.024</td>\n      <td>-0.070</td>\n      <td>-0.009</td>\n      <td>0.003</td>\n      <td>0.018</td>\n      <td>0.105</td>\n    </tr>\n    <tr>\n      <th>2008</th>\n      <td>253.0</td>\n      <td>-0.003</td>\n      <td>0.037</td>\n      <td>-0.179</td>\n      <td>-0.024</td>\n      <td>-0.001</td>\n      <td>0.019</td>\n      <td>0.139</td>\n    </tr>\n    <tr>\n      <th>2009</th>\n      <td>252.0</td>\n      <td>0.004</td>\n      <td>0.021</td>\n      <td>-0.050</td>\n      <td>-0.009</td>\n      <td>0.002</td>\n      <td>0.015</td>\n      <td>0.068</td>\n    </tr>\n    <tr>\n      <th>2010</th>\n      <td>252.0</td>\n      <td>0.002</td>\n      <td>0.017</td>\n      <td>-0.050</td>\n      <td>-0.006</td>\n      <td>0.002</td>\n      <td>0.011</td>\n      <td>0.077</td>\n    </tr>\n    <tr>\n      <th>2011</th>\n      <td>252.0</td>\n      <td>0.001</td>\n      <td>0.017</td>\n      <td>-0.056</td>\n      <td>-0.009</td>\n      <td>0.001</td>\n      <td>0.011</td>\n      <td>0.059</td>\n    </tr>\n    <tr>\n      <th>2012</th>\n      <td>250.0</td>\n      <td>0.001</td>\n      <td>0.019</td>\n      <td>-0.064</td>\n      <td>-0.008</td>\n      <td>0.000</td>\n      <td>0.012</td>\n      <td>0.089</td>\n    </tr>\n    <tr>\n      <th>2013</th>\n      <td>252.0</td>\n      <td>0.000</td>\n      <td>0.018</td>\n      <td>-0.124</td>\n      <td>-0.009</td>\n      <td>-0.000</td>\n      <td>0.011</td>\n      <td>0.051</td>\n    </tr>\n    <tr>\n      <th>2014</th>\n      <td>252.0</td>\n      <td>0.001</td>\n      <td>0.014</td>\n      <td>-0.080</td>\n      <td>-0.006</td>\n      <td>0.001</td>\n      <td>0.010</td>\n      <td>0.082</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>252.0</td>\n      <td>0.000</td>\n      <td>0.017</td>\n      <td>-0.061</td>\n      <td>-0.009</td>\n      <td>-0.001</td>\n      <td>0.009</td>\n      <td>0.057</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>252.0</td>\n      <td>0.001</td>\n      <td>0.015</td>\n      <td>-0.066</td>\n      <td>-0.006</td>\n      <td>0.001</td>\n      <td>0.008</td>\n      <td>0.065</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>251.0</td>\n      <td>0.002</td>\n      <td>0.011</td>\n      <td>-0.039</td>\n      <td>-0.004</td>\n      <td>0.001</td>\n      <td>0.007</td>\n      <td>0.061</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>251.0</td>\n      <td>-0.000</td>\n      <td>0.018</td>\n      <td>-0.066</td>\n      <td>-0.009</td>\n      <td>0.001</td>\n      <td>0.009</td>\n      <td>0.070</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>252.0</td>\n      <td>0.003</td>\n      <td>0.016</td>\n      <td>-0.100</td>\n      <td>-0.005</td>\n      <td>0.003</td>\n      <td>0.012</td>\n      <td>0.068</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>253.0</td>\n      <td>0.003</td>\n      <td>0.029</td>\n      <td>-0.129</td>\n      <td>-0.010</td>\n      <td>0.002</td>\n      <td>0.017</td>\n      <td>0.120</td>\n    </tr>\n    <tr>\n      <th>2021</th>\n      <td>252.0</td>\n      <td>0.001</td>\n      <td>0.016</td>\n      <td>-0.042</td>\n      <td>-0.008</td>\n      <td>0.001</td>\n      <td>0.012</td>\n      <td>0.054</td>\n    </tr>\n    <tr>\n      <th>2022</th>\n      <td>251.0</td>\n      <td>-0.001</td>\n      <td>0.022</td>\n      <td>-0.059</td>\n      <td>-0.016</td>\n      <td>-0.001</td>\n      <td>0.014</td>\n      <td>0.089</td>\n    </tr>\n    <tr>\n      <th>2023</th>\n      <td>250.0</td>\n      <td>0.002</td>\n      <td>0.013</td>\n      <td>-0.048</td>\n      <td>-0.006</td>\n      <td>0.002</td>\n      <td>0.009</td>\n      <td>0.047</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Scaling Up the Analysis\n\nAs a next step, we generalize the previous code so that all computations can handle an arbitrary number of symbols (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\n\nThis is where the `tidyverse` magic starts: Tidy data makes it extremely easy to generalize the computations from before to as many assets or groups as you like. The following code takes any number of symbols, e.g., `symbol = [\"AAPL\", \"MMM\", \"BA\"]`, and automates the download as well as the plot of the price time series. In the end, we create the table of summary statistics for all assets at once. For this example, we analyze data from all current constituents of the [Dow Jones Industrial Average index.](https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average)\\index{Data!Dow Jones Index}\n\nWe first download a table with DOW Jones constituents again using `tf.download_data()`, but this time with `domain=\"constituents\"`.\n\n::: {#e7d50aae .cell execution_count=12}\n``` {.python .cell-code}\nsymbols = tf.download_data(\n  domain=\"constituents\", \n  index=\"Dow Jones Industrial Average\"\n)\n```\n:::\n\n\nConveniently, `tf.download_data()` provides the functionality to get all stock prices from an index for a specific point in time with a single call.\\index{Exchange!NASDAQ}\n\n::: {#58bed1eb .cell execution_count=13}\n``` {.python .cell-code}\nprices_daily = tf.download_data(\n  domain=\"stock_prices\", \n  symbols=symbols[\"symbol\"].tolist(),\n  start_date=\"2000-01-01\", \n  end_date=\"2023-12-31\"\n)\n```\n:::\n\n\nThe resulting data frame contains 177\\,925 daily observations for 30 different stocks. @fig-102 illustrates the time series of the downloaded *adjusted* prices for each of the constituents of the Dow index. Make sure you understand every single line of code! What are the arguments of `aes()`? Which alternative `geoms` could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes.\n\n::: {#cell-fig-102 .cell execution_count=14}\n``` {.python .cell-code}\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\n\nprices_figure = (\n  ggplot(prices_daily, aes(y=\"adjusted_close\", x=\"date\", color=\"symbol\"))\n  + geom_line()\n  + scale_x_datetime(date_breaks=\"5 years\", date_labels=\"%Y\")\n  + labs(x=\"\", y=\"\", color=\"\", title=\"Stock prices of DOW index constituents\")\n  + theme(legend_position=\"none\")\n)\nprices_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Prices in USD, adjusted for dividend payments and stock splits.](working-with-stock-returns_files/figure-html/fig-102-output-1.png){#fig-102 width=2100 height=1500 fig-alt='Title: Stock prices of Dow Jones index constituents. The figure shows many time series with daily prices. The general trend seems positive for most stocks in the Dow Jones index.'}\n:::\n:::\n\n\nDo you notice the small differences relative to the code we used before? All we needed to do to illustrate all stock symbols simultaneously is to include `color = symbol` in the `ggplot` aesthetics. In this way, we generate a separate line for each symbol. Of course, there are simply too many lines on this graph to identify the individual stocks properly, but it illustrates our point of how to generalize a specific analysis to an arbitrage number of subjects quite well.\n\nThe same holds for stock returns. Before computing the returns, we use `groupby(\"symbol\")` such that the `assign()` command is performed for each symbol individually. The same logic also applies to the computation of summary statistics: `groupby(\"symbol\")` is the key to aggregating the time series into symbol-specific variables of interest.\\index{Summary statistics}\n\n::: {#9203e1ce .cell execution_count=15}\n``` {.python .cell-code}\nreturns_daily = (prices_daily\n  .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n  .get([\"symbol\", \"date\", \"ret\"])\n  .dropna(subset=\"ret\")\n)\n\n(returns_daily\n  .groupby(\"symbol\")[\"ret\"]\n  .describe()\n  .round(3)\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>symbol</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AAPL</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.025</td>\n      <td>-0.519</td>\n      <td>-0.010</td>\n      <td>0.001</td>\n      <td>0.013</td>\n      <td>0.139</td>\n    </tr>\n    <tr>\n      <th>AMGN</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.019</td>\n      <td>-0.134</td>\n      <td>-0.009</td>\n      <td>0.000</td>\n      <td>0.009</td>\n      <td>0.151</td>\n    </tr>\n    <tr>\n      <th>AMZN</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.032</td>\n      <td>-0.248</td>\n      <td>-0.012</td>\n      <td>0.000</td>\n      <td>0.014</td>\n      <td>0.345</td>\n    </tr>\n    <tr>\n      <th>AXP</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.023</td>\n      <td>-0.176</td>\n      <td>-0.009</td>\n      <td>0.000</td>\n      <td>0.010</td>\n      <td>0.219</td>\n    </tr>\n    <tr>\n      <th>BA</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.022</td>\n      <td>-0.238</td>\n      <td>-0.010</td>\n      <td>0.001</td>\n      <td>0.011</td>\n      <td>0.243</td>\n    </tr>\n    <tr>\n      <th>CAT</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.020</td>\n      <td>-0.145</td>\n      <td>-0.010</td>\n      <td>0.001</td>\n      <td>0.011</td>\n      <td>0.147</td>\n    </tr>\n    <tr>\n      <th>CRM</th>\n      <td>4914.0</td>\n      <td>0.001</td>\n      <td>0.027</td>\n      <td>-0.271</td>\n      <td>-0.012</td>\n      <td>0.001</td>\n      <td>0.014</td>\n      <td>0.260</td>\n    </tr>\n    <tr>\n      <th>CSCO</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.023</td>\n      <td>-0.162</td>\n      <td>-0.009</td>\n      <td>0.000</td>\n      <td>0.010</td>\n      <td>0.244</td>\n    </tr>\n    <tr>\n      <th>CVX</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.018</td>\n      <td>-0.221</td>\n      <td>-0.008</td>\n      <td>0.001</td>\n      <td>0.009</td>\n      <td>0.227</td>\n    </tr>\n    <tr>\n      <th>DIS</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.019</td>\n      <td>-0.184</td>\n      <td>-0.009</td>\n      <td>0.000</td>\n      <td>0.009</td>\n      <td>0.160</td>\n    </tr>\n    <tr>\n      <th>GS</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.023</td>\n      <td>-0.190</td>\n      <td>-0.010</td>\n      <td>0.000</td>\n      <td>0.011</td>\n      <td>0.265</td>\n    </tr>\n    <tr>\n      <th>HD</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.019</td>\n      <td>-0.287</td>\n      <td>-0.008</td>\n      <td>0.001</td>\n      <td>0.009</td>\n      <td>0.141</td>\n    </tr>\n    <tr>\n      <th>HON</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.019</td>\n      <td>-0.174</td>\n      <td>-0.008</td>\n      <td>0.001</td>\n      <td>0.009</td>\n      <td>0.282</td>\n    </tr>\n    <tr>\n      <th>IBM</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.016</td>\n      <td>-0.155</td>\n      <td>-0.007</td>\n      <td>0.000</td>\n      <td>0.008</td>\n      <td>0.120</td>\n    </tr>\n    <tr>\n      <th>JNJ</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.012</td>\n      <td>-0.158</td>\n      <td>-0.005</td>\n      <td>0.000</td>\n      <td>0.006</td>\n      <td>0.122</td>\n    </tr>\n    <tr>\n      <th>JPM</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.024</td>\n      <td>-0.207</td>\n      <td>-0.009</td>\n      <td>0.000</td>\n      <td>0.010</td>\n      <td>0.251</td>\n    </tr>\n    <tr>\n      <th>KO</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.013</td>\n      <td>-0.101</td>\n      <td>-0.005</td>\n      <td>0.000</td>\n      <td>0.006</td>\n      <td>0.139</td>\n    </tr>\n    <tr>\n      <th>MCD</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.015</td>\n      <td>-0.159</td>\n      <td>-0.006</td>\n      <td>0.001</td>\n      <td>0.007</td>\n      <td>0.181</td>\n    </tr>\n    <tr>\n      <th>MMM</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.015</td>\n      <td>-0.129</td>\n      <td>-0.007</td>\n      <td>0.000</td>\n      <td>0.008</td>\n      <td>0.126</td>\n    </tr>\n    <tr>\n      <th>MRK</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.017</td>\n      <td>-0.268</td>\n      <td>-0.007</td>\n      <td>0.000</td>\n      <td>0.008</td>\n      <td>0.130</td>\n    </tr>\n    <tr>\n      <th>MSFT</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.019</td>\n      <td>-0.156</td>\n      <td>-0.008</td>\n      <td>0.000</td>\n      <td>0.009</td>\n      <td>0.196</td>\n    </tr>\n    <tr>\n      <th>NKE</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.019</td>\n      <td>-0.198</td>\n      <td>-0.008</td>\n      <td>0.001</td>\n      <td>0.009</td>\n      <td>0.155</td>\n    </tr>\n    <tr>\n      <th>NVDA</th>\n      <td>6036.0</td>\n      <td>0.002</td>\n      <td>0.038</td>\n      <td>-0.352</td>\n      <td>-0.016</td>\n      <td>0.001</td>\n      <td>0.018</td>\n      <td>0.424</td>\n    </tr>\n    <tr>\n      <th>PG</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.013</td>\n      <td>-0.302</td>\n      <td>-0.005</td>\n      <td>0.000</td>\n      <td>0.006</td>\n      <td>0.120</td>\n    </tr>\n    <tr>\n      <th>SHW</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.018</td>\n      <td>-0.208</td>\n      <td>-0.008</td>\n      <td>0.001</td>\n      <td>0.009</td>\n      <td>0.153</td>\n    </tr>\n    <tr>\n      <th>TRV</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.018</td>\n      <td>-0.208</td>\n      <td>-0.007</td>\n      <td>0.001</td>\n      <td>0.008</td>\n      <td>0.256</td>\n    </tr>\n    <tr>\n      <th>UNH</th>\n      <td>6036.0</td>\n      <td>0.001</td>\n      <td>0.020</td>\n      <td>-0.186</td>\n      <td>-0.008</td>\n      <td>0.001</td>\n      <td>0.010</td>\n      <td>0.348</td>\n    </tr>\n    <tr>\n      <th>V</th>\n      <td>3973.0</td>\n      <td>0.001</td>\n      <td>0.019</td>\n      <td>-0.136</td>\n      <td>-0.008</td>\n      <td>0.001</td>\n      <td>0.009</td>\n      <td>0.150</td>\n    </tr>\n    <tr>\n      <th>VZ</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.015</td>\n      <td>-0.118</td>\n      <td>-0.007</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.146</td>\n    </tr>\n    <tr>\n      <th>WMT</th>\n      <td>6036.0</td>\n      <td>0.000</td>\n      <td>0.015</td>\n      <td>-0.114</td>\n      <td>-0.007</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.117</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Different Frequencies\n\nFinancial data often exists at different frequencies due to varying reporting schedules, trading calendars, and economic data releases. For example, stock prices are typically recorded daily, while macroeconomic indicators such as GDP or inflation are reported monthly or quarterly. Additionally, some datasets are recorded only when transactions occur, resulting in irregular timestamps. To compare data meaningfully, we have to align different frequencies appropriately. For example, to compare returns across different frequencies, we use annualization techniques.\n\nSo far, we have worked with daily returns, but we can easily convert our data to other frequencies. Let's create monthly returns from our daily data:\n\n::: {#c2849403 .cell execution_count=16}\n``` {.python .cell-code}\nreturns_monthly = (returns_daily\n  .assign(date=returns_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp())\n  .groupby([\"symbol\", \"date\"], as_index=False)\n  .agg(ret=(\"ret\", lambda x: np.prod(1 + x) - 1))\n)\n```\n:::\n\n\nIn this code, we first group the data by symbol and month and then compute monthly returns by compounding the daily returns: $(1+r_1)(1+r_2)\\ldots(1+r_n)-1$. To visualize how return characteristics change across different frequencies, we can compare histograms as in @fig-103:\n\n::: {#cell-fig-103 .cell execution_count=17}\n``` {.python .cell-code}\napple_daily = (returns_daily\n  .query(\"symbol == 'AAPL'\")\n  .assign(frequency=\"Daily\")\n)\n\napple_monthly = (returns_monthly\n  .query(\"symbol == 'AAPL'\")\n  .assign(frequency=\"Monthly\")\n)\n\napple_returns = pd.concat([apple_daily, apple_monthly], ignore_index=True)\n\napple_returns_figure = (\n  ggplot(apple_returns, aes(x=\"ret\", fill=\"frequency\"))\n  + geom_histogram(position=\"identity\", bins=50)\n  + labs(\n      x=\"\", y=\"\", fill=\"Frequency\",\n      title=\"Distribution of Apple returns across different frequencies\"\n  )\n  + scale_x_continuous(labels=percent_format())\n  + facet_wrap(\"frequency\", scales=\"free\")\n  + theme(legend_position=\"none\")\n)\napple_returns_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Returns are based on prices adjusted for dividend payments and stock splits.](working-with-stock-returns_files/figure-html/fig-103-output-1.png){#fig-103 width=2100 height=1500 fig-alt='Title: Distribution of Apple returns across different frequencies. The figure shows the distribution of daily and monthly returns in two separate facets.'}\n:::\n:::\n\n\n## Other Forms of Data Aggregation\n\nOf course, aggregation across variables other than `symbol` or `date` can also make sense. For instance, suppose you are interested in answering questions like: Are days with high aggregate trading volume likely followed by days with high aggregate trading volume? To provide some initial analysis on this question, we take the downloaded data and compute aggregate daily trading volume for all Dow index constituents in USD. Recall that the column `volume` is denoted in the number of traded shares.\\index{Trading volume} Thus, we multiply the trading volume with the daily adjusted closing price to get a proxy for the aggregate trading volume in USD. Scaling by `1e-9` (Python can handle scientific notation) denotes daily trading volume in billion USD.\n\n::: {#cell-fig-104 .cell execution_count=18}\n``` {.python .cell-code}\ntrading_volume = (prices_daily\n  .assign(trading_volume=lambda x: (x[\"volume\"]*x[\"adjusted_close\"])/1e9)\n  .groupby(\"date\")[\"trading_volume\"]\n  .sum()\n  .reset_index()\n  .assign(trading_volume_lag=lambda x: x[\"trading_volume\"].shift(periods=1))\n)\n\ntrading_volume_figure = (\n  ggplot(trading_volume, aes(x=\"date\", y=\"trading_volume\"))\n  + geom_line()\n  + scale_x_datetime(date_breaks=\"5 years\", date_labels=\"%Y\")\n  + labs(\n      x=\"\", y=\"\",\n      title=\"Aggregate daily trading volume of DOW index constituents in billion USD\"\n    )\n)\ntrading_volume_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Total daily trading volume in billion USD.](working-with-stock-returns_files/figure-html/fig-104-output-1.png){#fig-104 width=2100 height=1500 fig-alt='Title: Aggregate daily trading volume of Dow Jones index constitutens. The figure shows a volatile time series of daily trading volume, ranging from 15 in 2000 to 20.5 in 2023, with a maximum of more than 100.'}\n:::\n:::\n\n\n@fig-104 indicates a clear upward trend in aggregated daily trading volume. In particular, since the outbreak of the COVID-19 pandemic, markets have processed substantial trading volumes, as analyzed, for instance, by @Goldstein2021.\\index{Covid 19} One way to illustrate the persistence of trading volume would be to plot volume on day $t$ against volume on day $t-1$ as in the example below. In @fig-105, we add a dotted 45Â°-line to indicate a hypothetical one-to-one relation by `geom_abline()`, addressing potential differences in the axes' scales.\n\n::: {#cell-fig-105 .cell execution_count=19}\n``` {.python .cell-code}\npersistence_figure = (\n  ggplot(trading_volume, aes(x=\"trading_volume_lag\", y=\"trading_volume\"))\n  + geom_point()\n  + geom_abline(aes(intercept=0, slope=1), linetype=\"dashed\")\n  + labs(\n      x=\"Previous day aggregate trading volume\",\n      y=\"Aggregate trading volume\",\n      title=\"Persistence in daily trading volume of DOW constituents in billion USD\"\n    )\n)\npersistence_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Total daily trading volume in billion USD.](working-with-stock-returns_files/figure-html/fig-105-output-1.png){#fig-105 width=2100 height=1500 fig-alt='Title: Persistence in daily trading volume of Dow Jones index constituents. The figure shows a scatterplot where aggregate trading volume and previous-day aggregate trading volume neatly line up along a 45-degree line.'}\n:::\n:::\n\n\nPurely eye-balling reveals that days with high trading volume are often followed by similarly high trading volume days.\n\n## Key Takeaways\n\nIn this chapter, you learned how to effectively use R to download, analyze, and visualize stock market data using tidy principles:\n\n-   Tidy data principles enable efficient analysis of financial data.\n-   Adjusted prices account for corporate actions like stock splits and dividends.\n-   Summary statistics help identify key patterns in financial data.\n-   Visualization techniques reveal trends and distributions in returns.\n-   Data manipulations with the `tidyverse` scale easily to multiple assets.\n-   Consistent workflows form the foundation for advanced financial analysis.\n\n## Exercises\n\n1.  Download daily prices for another stock market symbol of your choice from Yahoo Finance using `tf.download_data()`. Plot two time series of the symbolâ€™s un-adjusted and adjusted closing prices. Explain any visible differences.\n2.  Compute daily net returns for an asset of your choice and visualize the distribution of daily returns in a histogram using 100 bins. Also, use `geom_vline()` to add a dashed red vertical line that indicates the 5 percent quantile of the daily returns. Compute summary statistics (mean, standard deviation, minimum, and maximum) for the daily returns.\n3.  Take your code from the previous exercises and generalize it such that you can perform all the computations for an arbitrary number of symbols (e.g., `symbol = [\"AAPL\", \"MMM\", \"BA\"]`). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\n4.  To facilitate the computation of the annualization factor, write a function that takes a vector of return dates as input and determines the frequency before returning the appropriate annualization factor.\n5.  Are days with high aggregate trading volume often also days with large absolute returns? Find an appropriate visualization to analyze the question using the symbol `AAPL`.\n\n",
    "supporting": [
      "working-with-stock-returns_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}