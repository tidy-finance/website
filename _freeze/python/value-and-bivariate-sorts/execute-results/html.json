{
  "hash": "22d2d9b7267ccba14c24efd57c52b6c0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Value and Bivariate Sorts\nmetadata:\n  pagetitle: Value and Bivariate Sorts with Python\n  description-meta: Perform bivariate portfolio sorts to test for return predictability in asset pricing applications using the programming language Python. \n---\n\n\n\n::: callout-note\nYou are reading **Tidy Finance with Python**. You can find the equivalent chapter for the sibling **Tidy Finance with R** [here](../r/value-and-bivariate-sorts.qmd).\n:::\n\nIn this chapter, we extend the univariate portfolio analysis of [Univariate Portfolio Sorts](univariate-portfolio-sorts.qmd) to bivariate sorts, which means we assign stocks to portfolios based on two characteristics. Bivariate sorts are regularly used in the academic asset pricing literature and are the basis for factors in the Fama-French three-factor model. However, some scholars also use sorts with three grouping variables. Conceptually, portfolio sorts are easily applicable in higher dimensions.\n\nWe form portfolios on firm size and the book-to-market ratio. To calculate book-to-market ratios, accounting data is required, which necessitates additional steps during portfolio formation. In the end, we demonstrate how to form portfolios on two sorting variables using so-called independent and dependent portfolio sorts.\n\nThe current chapter relies on this set of Python packages. \n\n::: {#51031de8 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport sqlite3\n```\n:::\n\n\nCompared to previous chapters, we introduce the `datetime` module that is part of the Python standard library for manipulating dates. \n\n## Data Preparation\n\nFirst, we load the necessary data from our SQLite database introduced in [Accessing and Managing Financial Data](accessing-and-managing-financial-data.qmd) and [WRDS, CRSP, and Compustat](wrds-crsp-and-compustat.qmd). We conduct portfolio sorts based on the CRSP sample but keep only the necessary columns in our memory. We use the same data sources for firm size as in [Size Sorts and P-Hacking](size-sorts-and-p-hacking.qmd).\\index{Data!CRSP}\\index{Data!Fama-French factors}\n\n::: {#860f4064 .cell execution_count=3}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=(\"SELECT permno, gvkey, date, ret_excess, mktcap, \" \n         \"mktcap_lag, exchange FROM crsp_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .dropna()\n)\n```\n:::\n\n\nFurther, we utilize accounting data. The most common source of accounting data is Compustat. We only need book equity data in this application, which we select from our database. Additionally, we convert the variable `datadate` to its monthly value, as we only consider monthly returns here and do not need to account for the exact date.\\index{Data!Compustat}\n\n::: {#ebc5e80e .cell execution_count=4}\n``` {.python .cell-code}\nbook_equity = (pd.read_sql_query(\n    sql=\"SELECT gvkey, datadate, be FROM compustat\",\n    con=tidy_finance, \n    parse_dates={\"datadate\"})\n  .dropna()\n  .assign(\n    date=lambda x: (\n      pd.to_datetime(x[\"datadate\"]).dt.to_period(\"M\").dt.to_timestamp()\n    )\n  )\n)\n```\n:::\n\n\n## Book-to-Market Ratio\n\nA fundamental problem in handling accounting data is the *look-ahead bias*; we must not include data in forming a portfolio that was not available knowledge at the time. Of course, researchers have more information when looking into the past than agents actually had at that moment. However, abnormal excess returns from a trading strategy should not rely on an information advantage because the differential cannot be the result of informed agents' trades. Hence, we have to lag accounting information.\n\nWe continue to lag market capitalization and firm size by one month.\\index{Market capitalization}\\index{Firm size} Then, we compute the book-to-market ratio, which relates a firm's book equity to its market equity.\\index{Book equity}\\index{Book-to-market ratio} Firms with high (low) book-to-market ratio are called value (growth) firms. After matching the accounting and market equity information from the same month, we lag book-to-market by six months. This is a sufficiently conservative approach because accounting information is usually released well before six months pass. However, in the asset pricing literature, even longer lags are used as well.^[The definition of a time lag is another choice a researcher has to make, similar to breakpoint choices as we describe in [Size Sorts and P-Hacking](size-sorts-and-p-hacking.qmd).]\n\nHaving both variables, i.e., firm size lagged by one month and book-to-market lagged by six months, we merge these sorting variables to our returns using the `sorting_date`-column created for this purpose. The final step in our data preparation deals with differences in the frequency of our variables. Returns and firm size are recorded monthly. Yet, the accounting information is only released on an annual basis. Hence, we only match book-to-market to one month per year and have eleven empty observations. To solve this frequency issue, we carry the latest book-to-market ratio of each firm to the subsequent months, i.e., we fill the missing observations with the most current report. This is done via the `fillna(method=\"ffill\")`-function after sorting by date and firm (which we identify by `permno` and `gvkey`) and on a firm basis (which we do by `.groupby()` as usual). We filter out all observations with accounting data that is older than a year. As the last step, we remove all rows with missing entries because the returns cannot be matched to any annual report.\n\n::: {#11f151ff .cell execution_count=5}\n``` {.python .cell-code}\nme = (crsp_monthly\n  .assign(sorting_date=lambda x: x[\"date\"]+pd.DateOffset(months=1))\n  .rename(columns={\"mktcap\": \"me\"})\n  .get([\"permno\", \"sorting_date\", \"me\"])\n)\n\nbm = (book_equity\n  .merge(crsp_monthly, how=\"inner\", on=[\"gvkey\", \"date\"])\n  .assign(bm=lambda x: x[\"be\"]/x[\"mktcap\"],\n          sorting_date=lambda x: x[\"date\"]+pd.DateOffset(months=6))\n  .assign(comp_date=lambda x: x[\"sorting_date\"])\n  .get([\"permno\", \"gvkey\", \"sorting_date\", \"comp_date\", \"bm\"])\n)\n\ndata_for_sorts = (crsp_monthly\n  .merge(bm, \n         how=\"left\", \n         left_on=[\"permno\", \"gvkey\", \"date\"], \n         right_on=[\"permno\", \"gvkey\", \"sorting_date\"])\n  .merge(me, \n         how=\"left\", \n         left_on=[\"permno\", \"date\"], \n         right_on=[\"permno\", \"sorting_date\"])\n  .get([\"permno\", \"gvkey\", \"date\", \"ret_excess\", \n        \"mktcap_lag\", \"me\", \"bm\", \"exchange\", \"comp_date\"])\n)\n\ndata_for_sorts = (data_for_sorts\n  .sort_values(by=[\"permno\", \"gvkey\", \"date\"])\n  .groupby([\"permno\", \"gvkey\"])\n  .apply(lambda x: x.assign(\n      bm=x[\"bm\"].fillna(method=\"ffill\"), \n      comp_date=x[\"comp_date\"].fillna(method=\"ffill\")\n    )\n  )\n  .reset_index(drop=True)\n  .assign(threshold_date = lambda x: (x[\"date\"]-pd.DateOffset(months=12)))\n  .query(\"comp_date > threshold_date\")\n  .drop(columns=[\"comp_date\", \"threshold_date\"])\n  .dropna()\n)\n```\n:::\n\n\nThe last step of preparation for the portfolio sorts is the computation of breakpoints.\\index{Breakpoints} We continue to use the same function, allowing for the specification of exchanges to be used for the breakpoints. Additionally, we reintroduce the argument `sorting_variable` into the function for defining different sorting variables.\n\n::: {#ca4fce9f .cell execution_count=6}\n``` {.python .cell-code}\ndef assign_portfolio(data, exchanges, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolio for a given sorting variable.\"\"\"\n    \n    breakpoints = (data\n      .query(f\"exchange in {exchanges}\")\n      .get(sorting_variable)\n      .quantile(np.linspace(0, 1, num=n_portfolios+1), \n                interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    breakpoints.iloc[0] = -np.Inf\n    breakpoints.iloc[breakpoints.size-1] = np.Inf\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n```\n:::\n\n\nNote that the `tidyfinance` package also provides an `assing_portfolio()` function, albeit with more flexibility. For ease of exposition, we continue to use the function that we just defined.\n\nAfter these data preparation steps, we present bivariate portfolio sorts on an independent and dependent basis.\n\n## Independent Sorts\n\nBivariate sorts create portfolios within a two-dimensional space spanned by two sorting variables. It is then possible to assess the return impact of either sorting variable by the return differential from a trading strategy that invests in the portfolios at either end of the respective variables spectrum. We create a five-by-five matrix using book-to-market and firm size as sorting variables in our example below. We end up with 25 portfolios. Since we are interested in the *value premium* (i.e., the return differential between high and low book-to-market firms), we go long the five portfolios of the highest book-to-market firms and short the five portfolios of the lowest book-to-market firms.\\index{Value premium} The five portfolios at each end are due to the size splits we employed alongside the book-to-market splits.\\index{Long-short}\n\nTo implement the independent bivariate portfolio sort, we assign monthly portfolios for each of our sorting variables separately to create the variables `portfolio_bm` and `portfolio_me`, respectively.\\index{Portfolio sorts!Independent bivariate} Then, these separate portfolios are combined to the final sort stored in `portfolio_combined`. After assigning the portfolios, we compute the average return within each portfolio for each month. Additionally, we keep the book-to-market portfolio as it makes the computation of the value premium easier. The alternative would be to disaggregate the combined portfolio in a separate step. Notice that we weigh the stocks within each portfolio by their market capitalization, i.e., we decide to value-weight our returns.\n\n::: {#27f94ec7 .cell execution_count=7}\n``` {.python .cell-code}\nvalue_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio_bm=assign_portfolio(\n        data=x, sorting_variable=\"bm\", n_portfolios=5, exchanges=[\"NYSE\"]\n      ),\n      portfolio_me=assign_portfolio(\n        data=x, sorting_variable=\"me\", n_portfolios=5, exchanges=[\"NYSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"date\", \"portfolio_bm\", \"portfolio_me\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n```\n:::\n\n\nEquipped with our monthly portfolio returns, we are ready to compute the value premium. However, we still have to decide how to invest in the five high and the five low book-to-market portfolios. The most common approach is to weigh these portfolios equally, but this is yet another researcher's choice. Then, we compute the return differential between the high and low book-to-market portfolios and show the average value premium.\n\n::: {#60c2c551 .cell execution_count=8}\n``` {.python .cell-code}\nvalue_premium = (value_portfolios\n  .groupby([\"date\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"value_premium\": (\n        x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() - \n          x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean()\n      )\n    })\n  )\n  .aggregate({\"value_premium\": \"mean\"})\n)\n```\n:::\n\n\nThe resulting monthly value premium is 0.42 percent with an annualized return of 5.1 percent.\n\n## Dependent Sorts\n\nIn the previous exercise, we assigned the portfolios without considering the second variable in the assignment. This protocol is called independent portfolio sorts. The alternative, i.e., dependent sorts, creates portfolios for the second sorting variable within each bucket of the first sorting variable.\\index{Portfolio sorts!Dependent bivariate} In our example below, we sort firms into five size buckets, and within each of those buckets, we assign firms to five book-to-market portfolios. Hence, we have monthly breakpoints that are specific to each size group. The decision between independent and dependent portfolio sorts is another choice for the researcher. Notice that dependent sorts ensure an equal amount of stocks within each portfolio.\n\nTo implement the dependent sorts, we first create the size portfolios by calling `assign_portfolio()` with `sorting_variable=\"me\"`. Then, we group our data again by month and by the size portfolio before assigning the book-to-market portfolio. The rest of the implementation is the same as before. Finally, we compute the value premium.\n\n::: {#836ded33 .cell execution_count=9}\n``` {.python .cell-code}\nvalue_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio_me=assign_portfolio(\n        data=x, sorting_variable=\"me\", n_portfolios=5, exchanges=[\"NYSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"date\", \"portfolio_me\"])\n  .apply(lambda x: x.assign(\n      portfolio_bm=assign_portfolio(\n        data=x, sorting_variable=\"bm\", n_portfolios=5, exchanges=[\"NYSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"date\", \"portfolio_bm\", \"portfolio_me\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nvalue_premium = (value_portfolios\n  .groupby([\"date\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"value_premium\": (\n        x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() -\n          x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean()\n      )\n    })\n  )\n  .aggregate({\"value_premium\": \"mean\"})\n)\n```\n:::\n\n\nThe monthly value premium from dependent sorts is 0.36 percent, which translates to an annualized premium of 4.4 percent per year.\n\nOverall, we show how to conduct bivariate portfolio sorts in this chapter. In one case, we sort the portfolios independently of each other. Yet we also discuss how to create dependent portfolio sorts. Along the lines of [Size Sorts and P-Hacking](size-sorts-and-p-hacking.qmd), we see how many choices a researcher has to make to implement portfolio sorts, and bivariate sorts increase the number of choices.\n\n## Key Takeaways\n\n- Bivariate portfolio sorts assign stocks based on two characteristics, such as firm size and book-to-market ratio, to better capture return patterns in asset pricing.\n- Independent sorts treat each variable separately, while dependent sorts condition the second sort on the first.\n- Proper handling of accounting data, especially lagging the book-to-market ratio, is essential to avoid look-ahead bias and ensure valid backtesting.\n- Value premiums are derived by comparing returns of high versus low book-to-market portfolios, with results sensitive to sorting choices and weighting schemes.\n\n## Exercises\n\n1. In [Size Sorts and P-Hacking](size-sorts-and-p-hacking.qmd), we examine the distribution of market equity. Repeat this analysis for book equity and the book-to-market ratio (alongside a plot of the breakpoints, i.e., deciles).\n1. When we investigate the portfolios, we focus on the returns exclusively. However, it is also of interest to understand the characteristics of the portfolios. Write a function to compute the average characteristics for size and book-to-market across the 25 independently and dependently sorted portfolios.\n1. As for the size premium, also the value premium constructed here does not follow @Fama1993. Implement a p-hacking setup as in [Size Sorts and P-Hacking](size-sorts-and-p-hacking.qmd) to find a premium that comes closest to their HML premium.\n\n",
    "supporting": [
      "value-and-bivariate-sorts_files"
    ],
    "filters": [],
    "includes": {}
  }
}