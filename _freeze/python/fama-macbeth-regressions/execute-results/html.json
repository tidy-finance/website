{
  "hash": "98ed3058d2b59807db865ac209833ac5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Fama-MacBeth Regressions\nmetadata:\n  pagetitle: Fama-MacBeth Regressions with Python\n  description-meta: Estimate risk premiums via Fama-MacBeth regressions using the programming language Python.\n---\n\n\n\n::: {.callout-note}\nYou are reading **Tidy Finance with Python**. You can find the equivalent chapter for the sibling **Tidy Finance with R** [here](../r/fama-macbeth-regressions.qmd).\n:::\n\nIn this chapter, we present a simple implementation of @Fama1973, a regression approach commonly called Fama-MacBeth regressions. Fama-MacBeth regressions are widely used in empirical asset pricing studies. We use individual stocks as test assets to estimate the risk premium associated with the three factors included in @Fama1993.\n\nResearchers use the two-stage regression approach to estimate risk premiums in various markets, but predominately in the stock market. Essentially, the two-step Fama-MacBeth regressions exploit a linear relationship between expected returns and exposure to (priced) risk factors. The basic idea of the regression approach is to project asset returns on factor exposures or characteristics that resemble exposure to a risk factor in the cross-section in each time period. Then, in the second step, the estimates are aggregated across time to test if a risk factor is priced. In principle, Fama-MacBeth regressions can be used in the same way as portfolio sorts introduced in previous chapters.\n\n\\index{Regression!Fama-MacBeth}\\index{Fama-MacBeth} The Fama-MacBeth procedure is a simple two-step approach: \nThe first step uses the exposures (characteristics) as explanatory variables in $T$ cross-sectional regressions. For example, if $r_{i,t+1}$ denote the excess returns of asset $i$ in month $t+1$, then the famous Fama-French three-factor model implies the following return generating process [see also @Campbell1998]:\n$$\\begin{aligned}r_{i,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{i,t} + \\lambda^{SMB}_t \\beta^{SMB}_{i,t} + \\lambda^{HML}_t \\beta^{HML}_{i,t} + \\epsilon_{i,t}.\\end{aligned}$${#eq-fama-3-factor} \nHere, we are interested in the compensation $\\lambda^{f}_t$ for the exposure to each risk factor $\\beta^{f}_{i,t}$ at each time point, i.e., the risk premium. Note the terminology: $\\beta^{f}_{i,t}$ is an asset-specific characteristic, e.g., a factor exposure or an accounting variable. *If* there is a linear relationship between expected returns and the characteristic in a given month, we expect the regression coefficient to reflect the relationship, i.e., $\\lambda_t^{f}\\neq0$. \n\nIn the second step, the time-series average $\\frac{1}{T}\\sum_{t=1}^T \\hat\\lambda^{f}_t$ of the estimates $\\hat\\lambda^{f}_t$ can then be interpreted as the risk premium for the specific risk factor $f$. We follow @Zaffaroni2022 and consider the standard cross-sectional regression to predict future returns. If the characteristics are replaced with time $t+1$ variables, then the regression approach captures risk attributes rather than risk premiums. \n\nBefore we move to the implementation, we want to highlight that the characteristics, e.g., $\\hat\\beta^{f}_{i}$, are often estimated in a separate step before applying the actual Fama-MacBeth methodology. You can think of this as a *step 0*. You might thus worry that the errors of $\\hat\\beta^{f}_{i}$ impact the risk premiums' standard errors. Measurement error in $\\hat\\beta^{f}_{i}$ indeed affects the risk premium estimates, i.e., they lead to biased estimates. The literature provides adjustments for this bias [see, e.g., @Shanken1992; @Kim1995; @Chen2015, among others] but also shows that the bias goes to zero as $T \\to \\infty$. We refer to @Gagliardini2016 for an in-depth discussion also covering the case of time-varying betas. Moreover, if you plan to use Fama-MacBeth regressions with individual stocks, @Hou2020 advocates using weighted-least squares to estimate the coefficients such that they are not biased toward small firms. Without this adjustment, the high number of small firms would drive the coefficient estimates.\n\nThe current chapter relies on this set of Python packages. \n\n::: {#afac3669 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n```\n:::\n\n\n## Data Preparation\n\nWe illustrate @Fama1973 with the monthly CRSP sample and use three characteristics to explain the cross-section of returns: Market capitalization, the book-to-market ratio, and the CAPM beta (i.e., the covariance of the excess stock returns with the market excess returns). We collect the data from our SQLite database introduced in [Accessing and Managing Financial Data](accessing-and-managing-financial-data.qmd) and [WRDS, CRSP, and Compustat](wrds-crsp-and-compustat.qmd).\\index{Data!CRSP}\\index{Data!Compustat}\\index{Beta}\n\n::: {#1e64bac6 .cell execution_count=3}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT permno, gvkey, date, ret_excess, mktcap FROM crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\ncompustat = pd.read_sql_query(\n  sql=\"SELECT datadate, gvkey, be FROM compustat\",\n  con=tidy_finance,\n  parse_dates={\"datadate\"}\n)\n\nbeta = pd.read_sql_query(\n  sql=\"SELECT date, permno, beta_monthly FROM beta\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n```\n:::\n\n\nWe use the Compustat and CRSP data to compute the book-to-market ratio and the (log) market capitalization.\\index{Book-to-market ratio}\\index{Market capitalization} Furthermore, we also use the CAPM betas based on monthly returns we computed in the previous chapters.\\index{Beta}\\index{CAPM}\n\n::: {#10cd0562 .cell execution_count=4}\n``` {.python .cell-code}\ncharacteristics = (compustat\n  .assign(date=lambda x: x[\"datadate\"].dt.to_period(\"M\").dt.to_timestamp())\n  .merge(crsp_monthly, how=\"left\", on=[\"gvkey\", \"date\"], )\n  .merge(beta, how=\"left\", on=[\"permno\", \"date\"])\n  .assign(\n    bm=lambda x: x[\"be\"]/x[\"mktcap\"],\n    log_mktcap=lambda x: np.log(x[\"mktcap\"]),\n    sorting_date=lambda x: x[\"date\"]+pd.DateOffset(months=6)\n  )\n  .get([\"gvkey\", \"bm\", \"log_mktcap\", \"beta_monthly\", \"sorting_date\"])\n  .rename(columns={\"beta_monthly\": \"beta\"})\n)\n\ndata_fama_macbeth = (crsp_monthly\n  .merge(characteristics, \n         how=\"left\",\n         left_on=[\"gvkey\", \"date\"], right_on=[\"gvkey\", \"sorting_date\"])\n  .sort_values([\"date\", \"permno\"])\n  .groupby(\"permno\")\n  .apply(lambda x: x.assign(\n      beta=x[\"beta\"].fillna(method=\"ffill\"),\n      bm=x[\"bm\"].fillna(method=\"ffill\"),\n      log_mktcap=x[\"log_mktcap\"].fillna(method=\"ffill\")\n    )\n  )\n  .reset_index(drop=True)  \n)\n\ndata_fama_macbeth_lagged = (data_fama_macbeth\n  .assign(date=lambda x: x[\"date\"]-pd.DateOffset(months=1))\n  .get([\"permno\", \"date\", \"ret_excess\"])\n  .rename(columns={\"ret_excess\": \"ret_excess_lead\"})\n)\n\ndata_fama_macbeth = (data_fama_macbeth\n  .merge(data_fama_macbeth_lagged, how=\"left\", on=[\"permno\", \"date\"])\n  .get([\"permno\", \"date\", \"ret_excess_lead\", \"beta\", \"log_mktcap\", \"bm\"])\n  .dropna()\n)\n```\n:::\n\n\n## Cross-Sectional Regression\n\nNext, we run the cross-sectional regressions with the characteristics as explanatory variables for each month. We regress the returns of the test assets at a particular time point on the characteristics of each asset. By doing so, we get an estimate of the risk premiums $\\hat\\lambda^{f}_t$ for each point in time. \\index{Regression!Cross-section}\n\n::: {#d5b8f099 .cell execution_count=5}\n``` {.python .cell-code}\nrisk_premiums = (data_fama_macbeth\n  .groupby(\"date\")\n  .apply(lambda x: smf.ols(\n      formula=\"ret_excess_lead ~ beta + log_mktcap + bm\", \n      data=x\n    ).fit()\n    .params\n  )\n  .reset_index()\n)\n```\n:::\n\n\n## Time-Series Aggregation\n\nNow that we have the risk premiums' estimates for each period, we can average across the time-series dimension to get the expected risk premium for each characteristic. Similarly, we manually create the $t$-test statistics for each regressor, which we can then compare to usual critical values of 1.96 or 2.576 for two-tailed significance tests at a five percent and a one percent significance level.\n\n::: {#f4498b67 .cell execution_count=6}\n``` {.python .cell-code}\nprice_of_risk = (risk_premiums\n  .melt(id_vars=\"date\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")[\"estimate\"]\n  .apply(lambda x: pd.Series({\n      \"risk_premium\": x.mean(),\n      \"t_statistic\": x.mean()/x.std()*np.sqrt(len(x))\n    })\n  )\n  .reset_index()\n  .pivot(index=\"factor\", columns=\"level_1\", values=\"estimate\")\n  .reset_index()\n)\n```\n:::\n\n\nIt is common to adjust for autocorrelation when reporting standard errors of risk premiums. As in [Univariate Portfolio Sorts](univariate-portfolio-sorts.qmd), the typical procedure for this is computing @Newey1987 standard errors.\\index{Standard errors!Newey-West}\n\n::: {#0a9ab4c2 .cell execution_count=7}\n``` {.python .cell-code}\nprice_of_risk_newey_west = (risk_premiums\n  .melt(id_vars=\"date\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")\n  .apply(lambda x: (\n      x[\"estimate\"].mean()/ \n        smf.ols(\"estimate ~ 1\", x)\n        .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6}).bse\n    )\n  )\n  .reset_index()\n  .rename(columns={\"Intercept\": \"t_statistic_newey_west\"})\n)\n\n(price_of_risk\n  .merge(price_of_risk_newey_west, on=\"factor\")\n  .round(3)\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>factor</th>\n      <th>risk_premium</th>\n      <th>t_statistic</th>\n      <th>t_statistic_newey_west</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Intercept</td>\n      <td>0.012</td>\n      <td>4.712</td>\n      <td>4.102</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>beta</td>\n      <td>0.000</td>\n      <td>0.056</td>\n      <td>0.052</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bm</td>\n      <td>0.002</td>\n      <td>2.978</td>\n      <td>2.803</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>log_mktcap</td>\n      <td>-0.001</td>\n      <td>-2.907</td>\n      <td>-2.731</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFinally, let us interpret the results. Stocks with higher book-to-market ratios earn higher expected future returns, which is in line with the value premium. The negative value for log market capitalization reflects the size premium for smaller stocks. Consistent with results from earlier chapters, we detect no relation between beta and future stock returns.\n\n::: {#e6249481 .cell execution_count=8}\n``` {.python .cell-code}\nimport tidyfinance as tf\n\ntf.estimate_fama_macbeth(\n  data=data_fama_macbeth,\n  model=\"ret_excess_lead ~ beta + bm + log_mktcap\",\n  vcov=\"newey-west\"\n)\n```\n:::\n\n\n## Exercises\n\n1. Download a sample of test assets from Kenneth French's homepage and reevaluate the risk premiums for industry portfolios instead of individual stocks.\n1. Use individual stocks with weighted-least squares based on a firm's size as suggested by @Hou2020. Then, repeat the Fama-MacBeth regressions without the weighting-scheme adjustment but drop the smallest 20 percent of firms each month. Compare the results of the three approaches. \n\n",
    "supporting": [
      "fama-macbeth-regressions_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}