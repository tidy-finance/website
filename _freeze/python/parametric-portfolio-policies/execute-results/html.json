{
  "hash": "ada76059f164afcb1e9ea4fa31ba575c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Parametric Portfolio Policies\nmetadata:\n  pagetitle: Parametric Portfolio Policies with Python\n  description-meta: Evaluate portfolio allocation strategies based on Brandt, Santa-Clara, and Valkanov (2009) using the programming language Python.\n---\n\n\n\n::: callout-note\nYou are reading **Tidy Finance with Python**. You can find the equivalent chapter for the sibling **Tidy Finance with R** [here](../r/parametric-portfolio-policies.qmd).\n:::\n\nIn this chapter, we apply different portfolio performance measures to evaluate and compare portfolio allocation strategies. For this purpose, we introduce a direct way to estimate optimal portfolio weights for large-scale cross-sectional applications. More precisely, the approach of @Brandt2009 proposes to parametrize the optimal portfolio weights as a function of stock characteristics instead of estimating the stock's expected return, variance, and covariances with other stocks in a prior step. We choose weights as a function of characteristics that maximize the expected utility of the investor. This approach is feasible for large portfolio dimensions (such as the entire CRSP universe) and has been proposed by @Brandt2009. See the review paper by @Brandt2010 for an excellent treatment of related portfolio choice methods.\\index{Brandt, Santa Clara, and Valkanov (2009)}\n\nThe current chapter relies on the following set of Python packages:\n\n::: {#0131d9b5 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n\nfrom itertools import product, starmap\nfrom scipy.optimize import minimize\n```\n:::\n\n\nCompared to previous chapters, we introduce the `scipy.optimize` module from the `scipy` [@scipy] for solving optimization problems.\n\n## Data Preparation\n\nTo get started, we load the monthly CRSP file, which forms our investment universe. We load the data from our SQLite database introduced in [Accessing and Managing Financial Data](accessing-and-managing-financial-data.qmd) and [WRDS, CRSP, and Compustat](wrds-crsp-and-compustat.qmd).\\index{Data!CRSP}\n\n::: {#ac5c5e33 .cell execution_count=3}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=(\"SELECT permno, date, ret_excess, mktcap, mktcap_lag \"\n         \"FROM crsp_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .dropna()\n)\n```\n:::\n\n\nTo evaluate the performance of portfolios, we further use monthly market returns as a benchmark to compute CAPM alphas.\\index{Data!Fama-French factors} \n\n::: {#ba9c91ed .cell execution_count=4}\n``` {.python .cell-code}\nfactors_ff_monthly = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n```\n:::\n\n\nNext, we retrieve some stock characteristics that have been shown to have an effect on the expected returns or expected variances (or even higher moments) of the return distribution. \\index{Momentum} In particular, we record the lagged one-year return momentum (`momentum_lag`), defined as the compounded return between months $t-13$ and $t-2$ for each firm, which we calculate using market capitalization for simplicity. In finance, momentum is the empirically observed tendency for rising asset prices to rise further and falling prices to keep falling [@Jegadeesh1993]. We refer to the exercise for a more elaborate measure of momentum. \\index{Size!Size effect} The second characteristic is the firm's market equity (`size_lag`), defined as the log of the price per share times the number of shares outstanding [@Banz1981]. To construct the correct lagged values, we use the approach introduced in [WRDS, CRSP, and Compustat](wrds-crsp-and-compustat.qmd).\\index{Data!CRSP}\n\n::: {#63b942f0 .cell execution_count=5}\n``` {.python .cell-code}\ncrsp_monthly_lags = (crsp_monthly\n  .assign(date=lambda x: x[\"date\"]+pd.DateOffset(months=13))\n  .get([\"permno\", \"date\", \"mktcap\"])\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(crsp_monthly_lags, \n         how=\"inner\", on=[\"permno\", \"date\"], suffixes=[\"\", \"_13\"])\n)\n\ndata_portfolios = (crsp_monthly\n  .assign(\n    momentum_lag=lambda x: x[\"mktcap_lag\"]/x[\"mktcap_13\"],\n    size_lag=lambda x: np.log(x[\"mktcap_lag\"])\n  )\n  .dropna(subset=[\"momentum_lag\", \"size_lag\"])\n)\n```\n:::\n\n\n## Parametric Portfolio Policies\n\nThe basic idea of parametric portfolio weights is as follows. Suppose that at each date $t$, we have $N_t$ stocks in the investment universe, where each stock $i$ has a return of $r_{i, t+1}$ and is associated with a vector of firm characteristics $x_{i, t}$ such as time-series momentum or the market capitalization. The investor's problem is to choose portfolio weights $w_{i,t}$ to maximize the expected utility of the portfolio return:\n$$\\begin{aligned}\n\\max_{\\omega} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{i=1}^{N_t}\\omega_{i,t}\\cdot r_{i,t+1}\\right)\\right]\n\\end{aligned}$${#eq-expected-utility}\nwhere $u(\\cdot)$ denotes the utility function.\n\nWhere do the stock characteristics show up? We parameterize the optimal portfolio weights as a function of the stock characteristic $x_{i,t}$ with the following linear specification for the portfolio weights: \n$$\\omega_{i,t} = \\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t},$${#eq-portfolio-policy}\nwhere $\\bar{\\omega}_{i,t}$ is a stock's weight in a benchmark portfolio (we use the value-weighted or naive portfolio in the application below), $\\theta$ is a vector of coefficients which we are going to estimate, and $\\hat{x}_{i,t}$ are the characteristics of stock $i$, cross-sectionally standardized to have zero mean and unit standard deviation. \n\nIntuitively, the portfolio strategy is a form of active portfolio management relative to a performance benchmark. Deviations from the benchmark portfolio are derived from the individual stock characteristics. Note that by construction, the weights sum up to one as $\\sum_{i=1}^{N_t}\\hat{x}_{i,t} = 0$ due to the standardization. Moreover, the coefficients are constant across assets and over time. The implicit assumption is that the characteristics fully capture all aspects of the joint distribution of returns that are relevant for forming optimal portfolios.\n\nWe first implement cross-sectional standardization for the entire CRSP universe. We also keep track of (lagged) relative market capitalization `relative_mktcap`, which will represent the value-weighted benchmark portfolio, while `n` denotes the number of traded assets $N_t$, which we use to construct the naive portfolio benchmark.\n\n::: {#e8b8d3ee .cell execution_count=6}\n``` {.python .cell-code}\ndata_portfolios = (data_portfolios\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      relative_mktcap=x[\"mktcap_lag\"]/x[\"mktcap_lag\"].sum()\n    )\n  )\n  .reset_index(drop=True)\n  .set_index(\"date\")\n  .groupby(level=\"date\")\n  .transform(\n    lambda x: (x-x.mean())/x.std() if x.name.endswith(\"lag\") else x\n  )\n  .reset_index()\n  .drop([\"mktcap_lag\"], axis=1)\n)\n```\n:::\n\n\n## Computing Portfolio Weights\n\nNext, we move on to identify optimal choices of $\\theta$.\\index{Optimization} We rewrite the optimization problem together with the weight parametrization and can then estimate $\\theta$ to maximize the objective function based on our sample \n$$\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{i=1}^{N_t}\\left(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\right)r_{i,t+1}\\right).\n\\end{aligned}$${#eq-empirical-expected-utility}\nThe allocation strategy is straightforward because the number of parameters to estimate is small. Instead of a tedious specification of the $N_t$ dimensional vector of expected returns and the $N_t(N_t+1)/2$ free elements of the covariance matrix, all we need to focus on in our application is the vector $\\theta$. $\\theta$ contains only two elements in our application: the relative deviation from the benchmark due to *size* and *momentum*. \n\nTo get a feeling for the performance of such an allocation strategy, we start with an arbitrary initial vector $\\theta_0$. The next step is to choose $\\theta$ optimally to maximize the objective function. We automatically detect the number of parameters by counting the number of columns with lagged values. Note that the value for $\\theta$ of 1.5 is an arbitrary choice.\n\n::: {#c14b1f7d .cell execution_count=7}\n``` {.python .cell-code}\nlag_columns = [i for i in data_portfolios.columns if \"lag\" in i]\nn_parameters = len(lag_columns)\ntheta = pd.DataFrame({\"theta\": [1.5]*n_parameters}, index=lag_columns)\n```\n:::\n\n\nThe function `compute_portfolio_weights()` below computes the portfolio weights $\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}$ according to our parametrization for a given value $\\theta_0$. Everything happens within a single pipeline. Hence, we provide a short walk-through.\n\nWe first compute `characteristic_tilt`, the tilting values $\\frac{1}{N_t}\\theta'\\hat{x}_{i, t}$ which resemble the deviation from the benchmark portfolio. Next, we compute the benchmark portfolio `weight_benchmark`, which can be any reasonable set of portfolio weights. In our case, we choose either the value or equal-weighted allocation. `weight_tilt` completes the picture and contains the final portfolio weights `weight_tilt = weight_benchmark + characteristic_tilt`, which deviate from the benchmark portfolio depending on the stock characteristics.\n\nThe final few lines go a bit further and implement a simple version of a no-short sale constraint. While it is generally not straightforward to ensure portfolio weight constraints via parameterization, we simply normalize the portfolio weights such that they are enforced to be positive. Finally, we make sure that the normalized weights sum up to one again:\n$$\\omega_{i,t}^+ = \\frac{\\max(0, \\omega_{i,t})}{\\sum_{j=1}^{N_t}\\max(0, \\omega_{i,t})}.$${#eq-short-sale-constraint}\n\nThe following function computes the optimal portfolio weights in the way just described.\\index{Functional programming}\n\n::: {#a5b34ba2 .cell execution_count=8}\n``` {.python .cell-code}\ndef compute_portfolio_weights(theta, \n                              data,\n                              value_weighting=True,\n                              allow_short_selling=True):\n    \"\"\"Compute portfolio weights for different strategies.\"\"\"\n    \n    lag_columns = [i for i in data.columns if \"lag\" in i]\n    theta = pd.DataFrame(theta, index=lag_columns)\n\n    data = (data\n      .groupby(\"date\")\n      .apply(lambda x: x.assign(\n          characteristic_tilt=x[theta.index] @ theta / x.shape[0]\n        )\n      )\n      .reset_index(drop=True)\n      .assign(\n        weight_benchmark=lambda x: \n          x[\"relative_mktcap\"] if value_weighting else 1/x.shape[0],\n        weight_tilt=lambda x: \n          x[\"weight_benchmark\"] + x[\"characteristic_tilt\"]\n      )\n      .drop(columns=[\"characteristic_tilt\"])\n    )\n\n    if not allow_short_selling:\n        data = (data\n          .assign(weight_tilt=lambda x: np.maximum(0, x[\"weight_tilt\"]))\n        )\n\n    data = (data\n      .groupby(\"date\")\n      .apply(lambda x: x.assign(\n        weight_tilt=lambda x: x[\"weight_tilt\"]/x[\"weight_tilt\"].sum()))\n      .reset_index(drop=True)\n    )\n\n    return data\n```\n:::\n\n\nIn the next step, we compute the portfolio weights for the arbitrary vector $\\theta_0$. In the example below, we use the value-weighted portfolio as a benchmark and allow negative portfolio weights.\n\n::: {#3c364a30 .cell execution_count=9}\n``` {.python .cell-code}\nweights_crsp = compute_portfolio_weights(\n  theta,\n  data_portfolios,\n  value_weighting=True,\n  allow_short_selling=True\n)\n```\n:::\n\n\n## Portfolio Performance\n\n\\index{Performance evaluation}\nAre the computed weights optimal in any way? Most likely not, as we picked $\\theta_0$ arbitrarily. To evaluate the performance of an allocation strategy, one can think of many different approaches. In their original paper, @Brandt2009 focus on a simple evaluation of the hypothetical utility of an agent equipped with a power utility function $$u_\\gamma(r) = \\frac{(1 + r)^{(1-\\gamma)}}{1-\\gamma},$${#eq-power-utility} where $\\gamma$ is the risk aversion factor.\\index{Power utility}\n\n::: {#b0c4f8d2 .cell execution_count=10}\n``` {.python .cell-code}\ndef power_utility(r, gamma=5):\n    \"\"\"Calculate power utility for given risk aversion.\"\"\"\n    \n    utility = ((1+r)**(1-gamma))/(1-gamma)\n    \n    return utility\n```\n:::\n\n\nWe want to note that @Gehrig2020 warn that, in the leading case of constant relative risk aversion (CRRA), strong assumptions on the properties of the returns, the variables used to implement the parametric portfolio policy, and the parameter space are necessary to obtain a well-defined optimization problem.\n\nNo doubt, there are many other ways to evaluate a portfolio. The function below provides a summary of all kinds of interesting measures that can be considered relevant. Do we need all these evaluation measures? It depends: The original paper by @Brandt2009 only cares about the expected utility to choose $\\theta$. However, if you want to choose optimal values that achieve the highest performance while putting some constraints on your portfolio weights, it is helpful to have everything in one function.\n\n::: {#3d2c7113 .cell execution_count=11}\n``` {.python .cell-code}\ndef evaluate_portfolio(weights_data,\n                       full_evaluation=True,\n                       capm_evaluation=True,\n                       length_year=12):\n    \"\"\"Calculate portfolio evaluation measures.\"\"\"\n    evaluation = (weights_data\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series(\n          np.average(x[[\"ret_excess\", \"ret_excess\"]],\n                     weights=x[[\"weight_tilt\", \"weight_benchmark\"]],\n                     axis=0),\n          [\"return_tilt\", \"return_benchmark\"])\n        )\n        .reset_index()\n        .melt(id_vars=\"date\", var_name=\"model\",\n              value_vars=[\"return_tilt\", \"return_benchmark\"],\n              value_name=\"portfolio_return\")\n        .assign(model=lambda x: x[\"model\"].str.replace(\"return_\", \"\"))\n    )\n\n    evaluation_stats = (evaluation\n        .groupby(\"model\")[\"portfolio_return\"]\n        .aggregate([\n          (\"Expected utility\", lambda x: np.mean(power_utility(x))),\n          (\"Average return\", lambda x: np.mean(length_year*x)*100),\n          (\"SD return\", lambda x: np.std(x)*np.sqrt(length_year)*100),\n          (\"Sharpe ratio\", lambda x: (np.mean(x)/np.std(x)* \n                                        np.sqrt(length_year)))\n        ])\n    )\n\n    if capm_evaluation:\n        evaluation_capm = (evaluation\n            .merge(factors_ff_monthly, how=\"left\", on=\"date\")\n            .groupby(\"model\")\n            .apply(lambda x: \n              smf.ols(formula=\"portfolio_return ~ 1 + mkt_excess\", data=x)\n              .fit().params\n            )\n            .rename(columns={\"const\": \"CAPM alpha\",\n                             \"mkt_excess\": \"Market beta\"})\n            )\n        evaluation_stats = (evaluation_stats\n          .merge(evaluation_capm, how=\"left\", on=\"model\")\n        )\n\n    if full_evaluation:\n        evaluation_weights = (weights_data\n          .melt(id_vars=\"date\", var_name=\"model\",\n                value_vars=[\"weight_benchmark\", \"weight_tilt\"],\n                value_name=\"weight\")\n          .groupby([\"model\", \"date\"])[\"weight\"]\n          .aggregate([\n            (\"Mean abs. weight\", lambda x: np.mean(abs(x))),\n            (\"Max. weight\", lambda x: max(x)),\n            (\"Min. weight\", lambda x: min(x)),\n            (\"Avg. sum of neg. weights\", lambda x: -np.sum(x[x < 0])),\n            (\"Avg. share of neg. weights\", lambda x: np.mean(x < 0))\n          ])\n          .reset_index()\n          .drop(columns=[\"date\"])\n          .groupby([\"model\"])\n          .aggregate(lambda x: np.average(x)*100)\n          .reset_index()\n          .assign(model=lambda x: x[\"model\"].str.replace(\"weight_\", \"\"))\n        )\n        \n        evaluation_stats = (evaluation_stats\n          .merge(evaluation_weights, how=\"left\", on=\"model\")\n          .set_index(\"model\")\n        )\n        \n    evaluation_stats = (evaluation_stats\n      .transpose()\n      .rename_axis(columns=None)\n    )\n\n    return evaluation_stats\n```\n:::\n\n\n\\index{Sharpe ratio}\nLet us take a look at the different portfolio strategies and evaluation measures.\n\n::: {#6ead699c .cell execution_count=12}\n``` {.python .cell-code}\nevaluate_portfolio(weights_crsp).round(2)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>benchmark</th>\n      <th>tilt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Expected utility</th>\n      <td>-0.25</td>\n      <td>-0.26</td>\n    </tr>\n    <tr>\n      <th>Average return</th>\n      <td>6.87</td>\n      <td>0.53</td>\n    </tr>\n    <tr>\n      <th>SD return</th>\n      <td>15.46</td>\n      <td>21.18</td>\n    </tr>\n    <tr>\n      <th>Sharpe ratio</th>\n      <td>0.44</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>Intercept</th>\n      <td>0.00</td>\n      <td>-0.00</td>\n    </tr>\n    <tr>\n      <th>Market beta</th>\n      <td>0.99</td>\n      <td>0.94</td>\n    </tr>\n    <tr>\n      <th>Mean abs. weight</th>\n      <td>0.03</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>Max. weight</th>\n      <td>4.09</td>\n      <td>4.25</td>\n    </tr>\n    <tr>\n      <th>Min. weight</th>\n      <td>0.00</td>\n      <td>-0.17</td>\n    </tr>\n    <tr>\n      <th>Avg. sum of neg. weights</th>\n      <td>0.00</td>\n      <td>78.13</td>\n    </tr>\n    <tr>\n      <th>Avg. share of neg. weights</th>\n      <td>0.00</td>\n      <td>49.06</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe value-weighted portfolio delivers an annualized return of more than six percent and clearly outperforms the tilted portfolio, irrespective of whether we evaluate expected utility, the Sharpe ratio, or the CAPM alpha. We can conclude the market beta is close to one for both strategies (naturally almost identically one for the value-weighted benchmark portfolio). When it comes to the distribution of the portfolio weights, we see that the benchmark portfolio weight takes less extreme positions (lower average absolute weights and lower maximum weight). By definition, the value-weighted benchmark does not take any negative positions, while the tilted portfolio also takes short positions.\n\n## Optimal Parameter Choice\n\nNext, we move to a choice of $\\theta$ that actually aims to improve some (or all) of the performance measures. We first define the helper function `compute_objective_function()`, which we then pass to an optimizer.\n\n::: {#aeda4903 .cell execution_count=13}\n``` {.python .cell-code}\ndef objective_function(theta,\n                       data,\n                       objective_measure=\"Expected utility\",\n                       value_weighting=True,\n                       allow_short_selling=True):\n    \"\"\"Define portfolio objective function.\"\"\"\n    \n    processed_data = compute_portfolio_weights(\n      theta, data, value_weighting, allow_short_selling\n    )\n\n    objective_function = evaluate_portfolio(\n      processed_data, \n      capm_evaluation=False, \n      full_evaluation=False\n    )\n\n    objective_function = -objective_function.loc[objective_measure, \"tilt\"]\n\n    return objective_function\n```\n:::\n\n\nYou may wonder why we return the negative value of the objective function. This is simply due to the common convention for optimization procedures to search for minima as a default. By minimizing the negative value of the objective function, we get the maximum value as a result. In its most basic form, Python optimization uses the function `minimize()`.\\index{Optimization} As main inputs, the function requires an initial guess of the parameters and the objective function to minimize. Now, we are fully equipped to compute the optimal values of $\\hat\\theta$, which maximize the hypothetical expected utility of the investor.\n\n::: {#a6b71060 .cell execution_count=14}\n``` {.python .cell-code}\noptimal_theta = minimize(\n  fun=objective_function,\n  x0=[1.5]*n_parameters,\n  args=(data_portfolios, \"Expected utility\", True, True),\n  method=\"Nelder-Mead\",\n  tol=1e-2\n)\n\n(pd.DataFrame(\n  optimal_theta.x,\n  columns=[\"Optimal theta\"],\n  index=[\"momentum_lag\", \"size_lag\"]).T.round(3)\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>momentum_lag</th>\n      <th>size_lag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Optimal theta</th>\n      <td>0.304</td>\n      <td>-1.704</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe resulting values of $\\hat\\theta$ are easy to interpret: intuitively, expected utility increases by tilting weights from the value-weighted portfolio toward smaller stocks (negative coefficient for size) and toward past winners (positive value for momentum). Both findings are in line with the well-documented size effect [@Banz1981] and the momentum anomaly [@Jegadeesh1993].\n\n## More Model Specifications\n\nHow does the portfolio perform for different model specifications? For this purpose, we compute the performance of a number of different modeling choices based on the entire CRSP sample. The next code chunk performs all the heavy lifting.\n\n::: {#153177b9 .cell execution_count=15}\n``` {.python .cell-code}\ndef evaluate_optimal_performance(data,\n                                 objective_measure=\"Expected utility\",\n                                 value_weighting=True,\n                                 allow_short_selling=True):\n    \"\"\"Calculate optimal portfolio performance.\"\"\"\n    \n    optimal_theta = minimize(\n      fun=objective_function,\n      x0=[1.5]*n_parameters,\n      args=(data, objective_measure, value_weighting, allow_short_selling),\n      method=\"Nelder-Mead\",\n      tol=10e-2\n    ).x\n\n    processed_data = compute_portfolio_weights(\n      optimal_theta, data, \n      value_weighting, allow_short_selling\n    )\n\n    portfolio_evaluation = evaluate_portfolio(processed_data)\n\n    weight_text = \"VW\" if value_weighting else \"EW\"\n    short_text = \"\" if allow_short_selling else \" (no s.)\"\n\n    strategy_name_dict = {\n      \"benchmark\": weight_text,\n      \"tilt\": f\"{weight_text} Optimal{short_text}\"\n    }\n\n    portfolio_evaluation.columns = [\n      strategy_name_dict[i] for i in portfolio_evaluation.columns\n    ]\n    \n    return(portfolio_evaluation)\n```\n:::\n\n\nFinally, we can compare the results. The table below shows summary statistics for all possible combinations: equal- or value-weighted benchmark portfolio, with or without short-selling constraints, and tilted toward maximizing expected utility. \n\n::: {#6102d8ef .cell execution_count=16}\n``` {.python .cell-code}\ndata = [data_portfolios]\nvalue_weighting = [True, False]\nallow_short_selling = [True, False]\nobjective_measure = [\"Expected utility\"]\n\npermutations = product(\n  data, objective_measure,\n  value_weighting, allow_short_selling\n)\nresults = list(starmap(\n  evaluate_optimal_performance, \n  permutations\n))\nperformance_table = (pd.concat(results, axis=1)\n  .T.drop_duplicates().T.round(3)\n)\nperformance_table.get([\"EW\", \"VW\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EW</th>\n      <th>VW</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Expected utility</th>\n      <td>-0.251</td>\n      <td>-0.250</td>\n    </tr>\n    <tr>\n      <th>Average return</th>\n      <td>10.012</td>\n      <td>6.867</td>\n    </tr>\n    <tr>\n      <th>SD return</th>\n      <td>20.467</td>\n      <td>15.461</td>\n    </tr>\n    <tr>\n      <th>Sharpe ratio</th>\n      <td>0.489</td>\n      <td>0.444</td>\n    </tr>\n    <tr>\n      <th>Intercept</th>\n      <td>0.002</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>Market beta</th>\n      <td>1.130</td>\n      <td>0.994</td>\n    </tr>\n    <tr>\n      <th>Mean abs. weight</th>\n      <td>0.000</td>\n      <td>0.030</td>\n    </tr>\n    <tr>\n      <th>Max. weight</th>\n      <td>0.000</td>\n      <td>4.091</td>\n    </tr>\n    <tr>\n      <th>Min. weight</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>Avg. sum of neg. weights</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>Avg. share of neg. weights</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#89095de5 .cell execution_count=17}\n``` {.python .cell-code}\nperformance_table.get([\"EW Optimal\", \"VW Optimal\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EW Optimal</th>\n      <th>VW Optimal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Expected utility</th>\n      <td>-4.630</td>\n      <td>-0.261</td>\n    </tr>\n    <tr>\n      <th>Average return</th>\n      <td>-4891.585</td>\n      <td>0.532</td>\n    </tr>\n    <tr>\n      <th>SD return</th>\n      <td>14402.492</td>\n      <td>21.176</td>\n    </tr>\n    <tr>\n      <th>Sharpe ratio</th>\n      <td>-0.340</td>\n      <td>0.025</td>\n    </tr>\n    <tr>\n      <th>Intercept</th>\n      <td>-3.614</td>\n      <td>-0.005</td>\n    </tr>\n    <tr>\n      <th>Market beta</th>\n      <td>-81.790</td>\n      <td>0.944</td>\n    </tr>\n    <tr>\n      <th>Mean abs. weight</th>\n      <td>60.120</td>\n      <td>0.077</td>\n    </tr>\n    <tr>\n      <th>Max. weight</th>\n      <td>1009.558</td>\n      <td>4.254</td>\n    </tr>\n    <tr>\n      <th>Min. weight</th>\n      <td>-213.264</td>\n      <td>-0.173</td>\n    </tr>\n    <tr>\n      <th>Avg. sum of neg. weights</th>\n      <td>75807.814</td>\n      <td>78.128</td>\n    </tr>\n    <tr>\n      <th>Avg. share of neg. weights</th>\n      <td>51.744</td>\n      <td>49.064</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#cff13199 .cell execution_count=18}\n``` {.python .cell-code}\nperformance_table.get([\"EW Optimal (no s.)\", \"VW Optimal (no s.)\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EW Optimal (no s.)</th>\n      <th>VW Optimal (no s.)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Expected utility</th>\n      <td>-0.252</td>\n      <td>-0.250</td>\n    </tr>\n    <tr>\n      <th>Average return</th>\n      <td>7.970</td>\n      <td>7.414</td>\n    </tr>\n    <tr>\n      <th>SD return</th>\n      <td>19.148</td>\n      <td>16.705</td>\n    </tr>\n    <tr>\n      <th>Sharpe ratio</th>\n      <td>0.416</td>\n      <td>0.444</td>\n    </tr>\n    <tr>\n      <th>Intercept</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>Market beta</th>\n      <td>1.137</td>\n      <td>1.056</td>\n    </tr>\n    <tr>\n      <th>Mean abs. weight</th>\n      <td>0.030</td>\n      <td>0.030</td>\n    </tr>\n    <tr>\n      <th>Max. weight</th>\n      <td>1.307</td>\n      <td>2.351</td>\n    </tr>\n    <tr>\n      <th>Min. weight</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>Avg. sum of neg. weights</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>Avg. share of neg. weights</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe results indicate that the average annualized Sharpe ratio of the equal-weighted portfolio exceeds the Sharpe ratio of the value-weighted benchmark portfolio. Nevertheless, starting with the weighted value portfolio as a benchmark and tilting optimally with respect to momentum and small stocks yields the highest Sharpe ratio across all specifications. Finally, imposing no short-sale constraints does not improve the performance of the portfolios in our application.\n\n## Key Takeaways\n\n- Parametric portfolio policies estimate portfolio weights directly as functions of stock characteristics like momentum and size, avoiding the need to forecast expected returns or covariances.\n- This method, based on @Brandt2009, is computationally efficient and scalable for large cross-sectional datasets such as CRSP.\n- Optimization focuses on maximizing expected utility, and evaluation includes measures such as Sharpe ratio, CAPM alpha, and utility-based performance.\n- Results highlight that tilting value-weighted portfolios toward small-cap and high-momentum stocks improves performance, aligning with known anomalies in finance.\n\n## Exercises\n\n1. Define momentum as the rolling 12-month cumulative returns skipping the most recent month. Calculate the correlation with the measure based on market capitalization from above and compare summary statistics. How do the two measures differ?\n1. How do the estimated parameters $\\hat\\theta$ and the portfolio performance change if your objective is to maximize the Sharpe ratio instead of the hypothetical expected utility?\n1. The code above is very flexible in the sense that you can easily add new firm characteristics. Construct a new characteristic of your choice and evaluate the corresponding coefficient $\\hat\\theta_i$. \n1. Tweak the function `optimal_theta()` such that you can impose additional performance constraints in order to determine $\\hat\\theta$, which maximizes expected utility under the constraint that the market beta is below 1.\n1. Does the portfolio performance resemble a realistic out-of-sample backtesting procedure? Verify the robustness of the results by first estimating $\\hat\\theta$ based on *past data* only. Then, use more recent periods to evaluate the actual portfolio performance. \n1. By formulating the portfolio problem as a statistical estimation problem, you can easily obtain standard errors for the coefficients of the weight function. @Brandt2009 provide the relevant derivations in their paper in Equation (10). Implement a small function that computes standard errors for $\\hat\\theta$.\n\n",
    "supporting": [
      "parametric-portfolio-policies_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}