{
  "hash": "ce54d8239a7ea56c5ada1defa736a68a",
  "result": {
    "markdown": "---\ntitle: \"Non-standard errors in portfolio sorts\"\nauthor: \"Patrick Weiss\"\ndate: \"2023-05-10\"\ndescription: An all-in-one implementation of non-standard errors in portfolio sorts.\nimage: thumbnail.jpg\ncategories: \n  - Replications\n---\n\n\n# What does this post cover?\n\nPersonally, I find non-standard errors[^1] very exciting and an important angle on academic research activity on a general level. Generally, non-standard errors are induced by methodological choices, which adds another layer of uncertainty to reported estimates. Luckily, Stefan Voigt asked me to join forces contributing to Menkveld et al. (2023). Furthermore, if you look back at the history of Tidy Finance, we included a chapter on [Size sorts and p-hacking](https://www.tidy-finance.org/size-sorts-and-p-hacking.html) early, starting an assessment of different choices in portfolio sorts. Recently, my co-authors (shout out to [Dominik Walter](https://sites.google.com/view/dominikwalter/startseite) and [Rüdiger Weber](https://sites.google.com/site/ruedigercweber/)) and I uploaded an update to our working paper on the topic. [Non-Standard Errors in Portfolio Sorts](http://dx.doi.org/10.2139/ssrn.4164117)[^2] (WWW below) thoroughly studies how methodological choices influence return differentials estimated from portfolio sorts. One of the conclusions is that we should embrace non-standard errors and report distributions of return differentials.\n\nThis blog article will teach you how to do portfolio sorts with non-standard errors in mind, i.e., vary over possible decisions to gain a deeper insight into return differentials. We will estimate one premium's distribution instead of a single return differential. Tidy Finance with R is the inspiration for the code alongside the replication code for WWW in this [Github repository](https://github.com/patrick-weiss/PortfolioSorts_NSE). Ultimately, you will know how to sort portfolios on the variable *asset growth* in nearly 70,000 ways.\n\nThis blog post is long, albeit not overly complicated. If you are new to `R` or portfolio sorts, I suggest reading the Chapter [Size sorts and p-hacking](https://www.tidy-finance.org/size-sorts-and-p-hacking.html) before jumping into this post. Unfortunately, I have to skip many interesting points on the implementation and the economic background (read WWW for this part). Otherwise, this blog would be far too long. If there is a need to expand on individual aspects, let me know. \n\n# Data\n\nFirst, we need some data. On the one hand, we need the monthly return time series for the CRSP universe. On the other hand, we need some accounting data from Compustat for constructing the sorting variable itself. To save space and because there is a chapter in Tidy Finance on it, I refer you to our chapter on [WRDS, CRSP, and Compustat](https://www.tidy-finance.org/wrds-crsp-compustat.html) for the details downloading the data. Here, we only read the data from my preprepared SQLite database.\n\nWe first need a few packages. The `tidyverse` (of course) and the `RSQLite`-package for the database. Additionally, we connect to my database, which contains all the necessary data. The prefix `../` in the path argument moves one directory up.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(RSQLite)\n\n# Database\ndata_tidy_nse <- dbConnect(SQLite(), \n                           \"../../data/data_nse.sqlite\", \n                           extended_types = TRUE)\n```\n:::\n\n\nNext, I load the necessary stock market (CRSP) and accounting (Compustat) data from my SQLite database.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_monthly <- dbReadTable(data_tidy_nse, \"crsp_monthly\")\ncompustat <- dbReadTable(data_tidy_nse, \"compustat\")\n```\n:::\n\n\nThen, we need to construct the sorting variable. As an example for this post, we will use *asset growth*, suggested as a predictor of the cross-section of stock prices by Cooper, Gulen, and Schill (2008)[^3]. Asset growth is measured as the relative change in *total assets* of a firm. Additionally, we have to compute three *filters* relating to the firm's stock price, book equity, and earnings. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Lag variable\ncompustat_lag <- compustat |> \n  select(gvkey, year, at) |> \n  mutate(year = year + 1) |> \n  rename_with(.cols = at, ~ paste0(.x, \"_lag\"))\n\n# Compute asset growth\ncompustat <- compustat |> \n  left_join(compustat_lag, by = c(\"gvkey\", \"year\")) |> \n  mutate(sv_ag = (at - at_lag) / at_lag)\n\n# Compute filters\ncompustat <- compustat |> \n  mutate(filter_be = coalesce(seq, ceq + pstk, at - lt) + coalesce(txditc, txdb + itcb, 0) - coalesce(pstkrv, pstkl, pstk, 0),\n         filter_price = prcc_f,\n         filter_earnings = ib)\n\n# Select required variables\ncompustat <- compustat |> \n  select(gvkey, month, datadate, starts_with(\"filter_\"), starts_with(\"sv_\")) |> \n  drop_na()\n```\n:::\n\n\nFor the CRSP data, we also construct a variable that we will need below. We compute the stock age filter as the time in years between the stock's first appearance in CRSP and the current month. To do this reliably, we again leverage `group_by()`'s power. We also lag the filter by one month to avoid inducing a look-ahead bias before merging it back to our main stock market data. Following our main data construction, market capitalization is already in the CRSP database.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_monthly_filter <- crsp_monthly |>\n  group_by(permno) |>\n  arrange(month) |>\n  mutate(filter_stock_age = as.numeric(difftime(month, min(month), units = \"days\"))/365,\n         month = month %m+% months(1)) |> \n  ungroup() |> \n  select(permno, month, filter_stock_age)\n\ncrsp_monthly <- crsp_monthly |> \n  select(permno, gvkey, month, industry, exchange, mktcap, mktcap_lag, ret_excess) |> \n  left_join(crsp_monthly_filter, by = c(\"permno\", \"month\")) |> \n  drop_na()\n```\n:::\n\n\nAt the moment, we only have panels of stock returns and characteristics. These panels still need to be matched together yet, because this also constitutes a decision. Hence, let us move to discuss these decisions.\n\n# The decision nodes\n\nIn WWW, we identify 14 methodological choices that must be made to estimate a premium from portfolio sorts. We split these into decisions on the sample construction and the portfolio construction. The table below illustrates the choices, and for further reference, you can refer to WWW for details on these nodes.\n\nNode | Choices\n:------|:------\nSize restriction | none, NYSE 5%, NYSE 20%\nFinancials | include, exclude\nUtilities | include, exclude\nPos. book equity | include, exclude\nPos. earnings | include, exclude\nStock-age restriction | none, >2 years\nPrice restriction | none, >\\$1, >\\$5 \nSorting variable lag | 3 months, 6 months, Fama-French\nRebalancing | monthly, annually\nBreakpoint quantiles main | 5, 10\nDouble sort | single, double dependent, double independent\nBreakpoint quantiles secondary | 2, 5\nBreakpoint exchanges | NYSE, all\nWeighting scheme | equal-weighting, value-weighting\n\nIn principle, there are more decisions to be made. However, this set of 14 choices appears in published, peer-reviewed articles and covers different aspects. If you think another choice is essential, I want to hear about it.\n\nLet us now create all possible combinations of choices that are feasible. We use `expand_grid()` on the tibble of individual nodes and their branches. Note that single sorts do not use the node regarding the number of secondary portfolios, i.e., we remove these paths after combining all choices. This leaves us with 69,120 choices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sorting grid\nsetup_grid <- expand_grid(sorting_variable = \"sv_ag\",\n                          drop_smallNYSE_at = c(0, 0.05, 0.2),\n                          include_financials = c(TRUE, FALSE),\n                          include_utilities = c(TRUE, FALSE),\n                          drop_bookequity = c(TRUE, FALSE),\n                          drop_earnings = c(TRUE, FALSE),\n                          drop_stock_age_at = c(0, 2),\n                          drop_price_at = c(0, 1, 5),\n                          sv_lag = c(\"3m\", \"6m\", \"FF\"),\n                          formation_time = c(\"monthly\", \"FF\"),\n                          n_portfolios_main = c(5, 10),\n                          sorting_method = c(\"single\", \"dbl_ind\", \"dbl_dep\"),\n                          n_portfolios_secondary = c(2, 5),\n                          exchanges = c(\"NYSE\", \"NYSE|NASDAQ|AMEX\"),\n                          value_weighted = c(TRUE, FALSE))\n\n# Remove information on double sorting for univariate sorts\nsetup_grid <- setup_grid |> \n  filter(!(sorting_method == \"single\" & n_portfolios_secondary > 2)) |> \n  mutate(n_portfolios_secondary = case_when(sorting_method == \"single\" ~ NA_real_, \n                                            TRUE ~ n_portfolios_secondary))\n```\n:::\n\n\n## Merge data\n\nOne key decision node is the sorting variable lag. However, merging data is an expensive operation, and doing it repeatedly is unnecessary. Hence, we merge the data in the three possible lag configurations and store them as separate tibbles. Thereby, we can later reference the correct table instead of merging the desired output.\n\nFirst, let us consider the Fama-French (FF) lag. Here, we consider accounting information published in year $t-1$ starting from July of year $t$. That is, we use the accounting information published 6 to 18 months ago. We first match the accounting data to the stock market data before we fill in the missing observations. A few pitfalls exist when using the `fill()`-function. First, one might easily forget to order and group the data. Second, the function does not care how outdated the information becomes. In principle, you can end up with data that is decades old. Therefore, we ensure that these filled data points are not older than 12 months. Finally, notice that this code provides much flexibility. All variables with prefixes *sv_* and *filter_* get filled. So you can easily adapt my code to your needs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_FF <- crsp_monthly |> \n  mutate(sorting_date = month) |> \n  left_join(compustat |> \n              mutate(sorting_date = floor_date(datadate, \"year\") %m+% months(18),\n                     sv_date_COMP_y = sorting_date) |> \n              select(-month, -datadate), \n            by = c(\"gvkey\", \"sorting_date\"))\n  \n# Fill variables and ensure timeliness of data\ndata_FF <- data_FF |> \n  arrange(permno, month) |> \n  group_by(permno, gvkey) |> \n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |> \n  ungroup() |> \n  filter(sv_date_COMP_y > month %m-% months(12)) |>\n  select(-sv_date_COMP_y, -sorting_date, -gvkey)\n```\n:::\n\n\nNext, we create the basis with lags of three and six months. The process is exactly the same as above for the FF lag, but without the `floor_date()` as we apply a constant lag to all observations. Again, we make sure that after the call to `fill()` our information does not become too old.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3 months\n## Merge data\ndata_3m <- crsp_monthly |> \n  mutate(sorting_date = month) |> \n  left_join(compustat |> \n              mutate(sorting_date = floor_date(datadate, \"month\") %m+% months(3),\n                     sv_date_COMP_y = sorting_date) |> \n              select(-month, -datadate), \n            by = c(\"gvkey\", \"sorting_date\"))\n\n## Fill variables and ensure timeliness of data\ndata_3m <- data_3m |> \n  arrange(permno, month) |> \n  group_by(permno, gvkey) |>\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |> \n  ungroup() |> \n  filter(sv_date_COMP_y > month %m-% months(12)) |>\n  select(-sv_date_COMP_y, -sorting_date, -gvkey) \n\n# 6 months\n## Merge data\ndata_6m <- crsp_monthly |> \n  mutate(sorting_date = month) |> \n  left_join(compustat |> \n              mutate(sorting_date = floor_date(datadate, \"month\") %m+% months(6),\n                     sv_date_COMP_y = sorting_date) |> \n              select(-month, -datadate), \n            by = c(\"gvkey\", \"sorting_date\"))\n\n## Fill variables and ensure timeliness of data\ndata_6m <- data_6m |> \n  arrange(permno, month) |> \n  group_by(permno, gvkey) |>\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |> \n  ungroup() |> \n  filter(sv_date_COMP_y > month %m-% months(12)) |>\n  select(-sv_date_COMP_y, -sorting_date, -gvkey) \n```\n:::\n\n\n#  Portfolio sorts\n\nWe are equipped with the necessary data and the set of decisions we consider. Next, we implement our decisions into actual portfolio sorts. Well. First, we have to define a few functions to make the implementation feasible. Thinking in functions is an important aspect that enables you to accomplish the task set for this blog post. Then, we will apply these functions.\n\n## Functions\n\nWe write functions that complete specific tasks and then combine them to generate the desired output. Breaking it up into smaller steps makes the whole process more tractable and easier to test.\n\n### Select the sample\n\nThe first function gets the name `handle_data()` because it is intended to select the sample according to the sample construction choices. The function first selects the data based on the desired sorting variable lag (specified in `sv_lag`). Then, we apply the various filters we discussed above. As you see, this is relatively simple, but it already covers our sample construction nodes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhandle_data <- function(include_financials, include_utilities, \n                        drop_smallNYSE_at, drop_price_at, drop_stock_age_at, \n                        drop_earnings, drop_bookequity, \n                        sv_lag) {\n\n  # Select dataset\n  if(sv_lag == \"FF\") data_all <- data_FF\n  if(sv_lag == \"3m\") data_all <- data_3m\n  if(sv_lag == \"6m\") data_all <- data_6m\n  \n  # Size filter based on NYSE percentile\n  if(drop_smallNYSE_at > 0) {\n    data_all <- data_all |> \n      group_by(month) |> \n      mutate(NYSE_breakpoint = quantile(mktcap_lag[exchange == \"NYSE\"], drop_smallNYSE_at)) |> \n      ungroup() |> \n      filter(mktcap_lag >= NYSE_breakpoint) |> \n      select(-NYSE_breakpoint)\n  }\n  \n  # Exclude industries\n  data_all <- data_all |> \n    filter(if(include_financials) TRUE else !grepl(\"Finance\", industry)) |> \n    filter(if(include_utilities) TRUE else !grepl(\"Utilities\", industry))\n  \n  # Book equity filter\n  if(drop_bookequity) {\n    data_all <- data_all |> \n      filter(filter_be > 0)\n  }\n  \n  # Earnings filter\n  if(drop_earnings) {\n    data_all <- data_all |> \n      filter(filter_earnings > 0)\n  }\n  \n  # Stock age filter\n  if(drop_stock_age_at > 0) {\n    data_all <- data_all |> \n      filter(filter_stock_age >= drop_stock_age_at)\n  }\n\n  # Price filter\n  if(drop_price_at > 0) {\n    data_all <- data_all |> \n      filter(filter_price >= drop_price_at)\n  }\n  \n  # Define ME\n  data_all <- data_all |> \n    mutate(me = mktcap_lag) |> \n    drop_na(me) |> \n    select(-starts_with(\"filter_\"), -industry)\n  \n  # Return\n  return(data_all)\n}\n```\n:::\n\n\n### Assign portfolios\n\nNext, we define a function that assigns portfolios based on the specified sorting variable, the number of portfolios, and the exchanges. The function only works on a single cross-section of data, i.e., it has to be applied to individual months of data. The central part of the function is to compute the $n$ breakpoints based on the exchange filter. Then, `findInterval()` assigns the respective portfolio number.\n\nThe function also features two sanity checks. First, it does not assign portfolios if there are too few stocks in the cross-section. Second, sometimes the sorting variable creates scenarios where some portfolios are overpopulated. For example, if the variable in question is bounded from below by 0. In such a case, an unexpectedly large number of firms might end up in the lowest bucket, covering multiple quantiles.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nassign_portfolio <- function(data, sorting_variable, n_portfolios, exchanges) {\n  # Escape small sets (i.e., less than 10 firms per portfolio)\n  if(nrow(data) < n_portfolios * 10) return(NA)\n  \n  # Compute breakpoints\n  breakpoints <- data |> \n    filter(grepl(exchanges, exchange)) |> \n    pull(all_of(sorting_variable)) |> \n    quantile(probs = seq(0, 1, length.out = n_portfolios + 1),\n             na.rm = TRUE,\n             names = FALSE)\n  \n  # Assign portfolios\n  portfolios <- data |> \n    mutate(portfolio = findInterval(pick(everything()) |> \n                                      pull(all_of(sorting_variable)), \n                                    breakpoints, \n                                    all.inside = TRUE)) |> \n    pull(portfolio)\n  \n  # Check if breakpoints are well defined\n  if(length(unique(breakpoints)) == n_portfolios + 1) {\n    return(portfolios)\n  } else {\n    print(breakpoints)\n    cat(paste0(\"\\n Breakpoint issue! Month \", as.Date(as.numeric(cur_group())), \"\\n\"))\n    stop()\n  }\n}\n```\n:::\n\n\n### Single and double sorts\n\nOur goal is to construct portfolios for single sorts, independent double sorts, and dependent double sorts. Hence, our next three functions do exactly that. The double sorts considered always take a first sort on market equity (the variable `me`) before sorting on the actual sorting variable.\n\nLet us start with single sorts. As you see, we group by month as the function `assign_portfolio()` we wrote above handles one cross-section at a time. The rest of the function just passes the arguments to the portfolio assignment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort_single <- function(data, sorting_variable, exchanges, n_portfolios_main) {\n  data |> \n    group_by(month) |> \n    mutate(portfolio = assign_portfolio(data = pick(all_of(sorting_variable), exchange),\n                                        sorting_variable = sorting_variable,\n                                        n_portfolios = n_portfolios_main,\n                                        exchanges = exchanges)) |> \n    drop_na(portfolio) |> \n    ungroup()\n}\n```\n:::\n\n\nFor double sorts, things are more interesting. First, we have the issue of independent and dependent double sorts. An independent sort considers the two sorting variables (*size* and *asset growth*) independently. In contrast, dependent sorts are, in our case, first sorting on *size* and within these buckets on *asset growth*. We group by the secondary portfolio to achieve the dependent sort before generating the main portfolios. Second, we need to generate an overall portfolio of the two sorts - we will see this later. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort_double_ind <- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |> \n    group_by(month) |>\n    mutate(portfolio_secondary = assign_portfolio(data = pick(me, exchange),\n                                                    sorting_variable = \"me\",\n                                                    n_portfolios = n_portfolios_secondary,\n                                                    exchanges = exchanges),\n           portfolio_main = assign_portfolio(data = pick(all_of(sorting_variable), exchange),\n                                               sorting_variable = sorting_variable,\n                                               n_portfolios = n_portfolios_main,\n                                               exchanges = exchanges),\n           portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)) |>\n    drop_na(portfolio_main, portfolio_secondary) |> \n    ungroup()\n}\n\nsort_double_dep <- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |>\n    group_by(month) |>\n    mutate(portfolio_secondary = assign_portfolio(data = pick(me, exchange),\n                                                  sorting_variable = \"me\",\n                                                  n_portfolios = n_portfolios_secondary,\n                                                  exchanges = exchanges)) |>\n    drop_na(portfolio_secondary) |>\n    group_by(month, portfolio_secondary) |>\n    mutate(portfolio_main = assign_portfolio(data = pick(all_of(sorting_variable), exchange),\n                                             sorting_variable = sorting_variable,\n                                             n_portfolios = n_portfolios_main,\n                                             exchanges = exchanges),\n           portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)) |>\n    drop_na(portfolio_main) |> \n    ungroup()\n}\n```\n:::\n\n\n### Annual vs monthly rebalancing\n\nNow, we still have one decision node to cover: Rebalancing. We can either rebalance annually in July or monthly. To achieve this, we write two more functions - the last functions before finishing up. Let us start with monthly rebalancing because it is much easier. All we need to do is to use the assigned portfolio numbers to generate portfolio returns. Inside the function, we use three `if()` calls to decide the sorting method. Notice that the double sorts use the simple average for aggregating the extreme portfolios of the size buckets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrebalance_mon <- function(data, sorting_variable, sorting_method, \n                          n_portfolios_main, n_portfolios_secondary, \n                          exchanges, value_weighted) {\n  # Single sort\n  if(sorting_method == \"single\") {\n    data_rets <- data |> \n      sort_single(sorting_variable = sorting_variable, \n                  exchanges = exchanges, \n                  n_portfolios_main = n_portfolios_main) |> \n      group_by(month, portfolio) |> \n      summarize(ret = if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)),\n                .groups = \"drop\")\n  }\n  \n  # Double independent sort\n  if(sorting_method == \"dbl_ind\") {\n    data_rets <- data |> \n      sort_double_ind(sorting_variable = sorting_variable, \n                      exchanges = exchanges, \n                      n_portfolios_main = n_portfolios_main,\n                      n_portfolios_secondary = n_portfolios_secondary) |> \n      group_by(month, portfolio) |>\n      summarize(ret = if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)),\n                portfolio_main = unique(portfolio_main),\n                .groups = \"drop\") |>\n      group_by(month, portfolio_main) |>\n      summarize(ret = mean(ret),\n                .groups = \"drop\") |> \n      rename(portfolio = portfolio_main)\n  }\n    \n  # Double dependent sort\n  if(sorting_method == \"dbl_dep\") {\n    data_rets <- data |> \n      sort_double_dep(sorting_variable = sorting_variable, \n                      exchanges = exchanges, \n                      n_portfolios_main = n_portfolios_main,\n                      n_portfolios_secondary = n_portfolios_secondary) |> \n      group_by(month, portfolio) |>\n      summarize(ret = if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)),\n                portfolio_main = unique(portfolio_main),\n                .groups = \"drop\") |>\n      group_by(month, portfolio_main) |>\n      summarize(ret = mean(ret),\n                .groups = \"drop\") |> \n      rename(portfolio = portfolio_main)\n  }\n  \n  return(data_rets)\n}\n```\n:::\n\n\nNow, let us move to the annual rebalancing. Here, we first assign a portfolio on the data in July based on single or independent/dependent double sorts. Then, we fill the remaining months forward before computing returns. Hence, we need one extra step for each sort.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrebalance_ann <- function(data, sorting_variable, sorting_method, \n                          n_portfolios_main, n_portfolios_secondary, \n                          exchanges, value_weighted) {\n  data_sorting <- data |> \n    filter(month(month) == 7) |>\n    group_by(month)\n  \n  # Single sort\n  if(sorting_method == \"single\") {\n    # Assign portfolios \n    data_sorting <- data_sorting |> \n      sort_single(sorting_variable = sorting_variable, \n                  exchanges = exchanges, \n                  n_portfolios_main = n_portfolios_main) |> \n      select(permno, month, portfolio) |> \n      mutate(sorting_month = month)\n  }\n  \n  # Double independent sort\n  if(sorting_method == \"dbl_ind\") {\n    # Assign portfolios\n    data_sorting <- data_sorting |> \n      sort_double_ind(sorting_variable = sorting_variable, \n                      exchanges = exchanges, \n                      n_portfolios_main = n_portfolios_main,\n                      n_portfolios_secondary = n_portfolios_secondary) |> \n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |> \n      mutate(sorting_month = month)\n  }\n    \n  # Double dependent sort\n  if(sorting_method == \"dbl_dep\") {\n    # Assign portfolios\n    data_sorting <- data_sorting |> \n      sort_double_dep(sorting_variable = sorting_variable, \n                      exchanges = exchanges, \n                      n_portfolios_main = n_portfolios_main,\n                      n_portfolios_secondary = n_portfolios_secondary)  |> \n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |> \n      mutate(sorting_month = month)\n  }\n  \n  # Compute portfolio return\n  if(sorting_method == \"single\") {\n    data |>\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |>\n      group_by(permno) |>\n      arrange(month) |>\n      fill(portfolio, sorting_month) |>\n      filter(sorting_month >= month %m-% months(12)) |>\n      drop_na(portfolio) |>\n      group_by(month, portfolio) |>\n      summarize(ret = if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)),\n                .groups = \"drop\")\n  } else {\n    data |>\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |>\n      group_by(permno) |>\n      arrange(month) |>\n      fill(portfolio_main, portfolio_secondary, portfolio, sorting_month) |>\n      filter(sorting_month >= month %m-% months(12)) |>\n      drop_na(portfolio_main, portfolio_secondary) |>\n      group_by(month, portfolio) |>\n      summarize(ret = if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)),\n                portfolio_main = unique(portfolio_main),\n                .groups = \"drop\") |>\n      group_by(month, portfolio_main) |>\n      summarize(ret = mean(ret),\n                .groups = \"drop\") |> \n      rename(portfolio = portfolio_main)\n  }\n}\n```\n:::\n\n\n### Combining the functions\n\nNow, we have everything to compute our 69,120 portfolio sorts for *asset growth* to understand the variation our decisions induce. To achieve this, our function considers all choices as arguments and passes them to the sample selection and portfolio construction functions.\n\nFinally, the function computes the return differential for each month. Since we are only interested in the mean here, we simply take the mean of these time series and call it our premium estimate. Notice that we also multiply the estimate by `-100` because the sorting variable *asset growth* predicts slow-growing firms to outperform, i.e., our portfolio sort would go in the wrong direction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexecute_sorts <- function(sorting_variable, drop_smallNYSE_at, include_financials, include_utilities, drop_bookequity,\n                          drop_earnings, drop_stock_age_at, drop_price_at, sv_lag, formation_time, n_portfolios_main,\n                          sorting_method, n_portfolios_secondary, exchanges, value_weighted) {\n  # Select data\n  data_sorts <- handle_data(include_financials = include_financials, \n                            include_utilities = include_utilities, \n                            drop_smallNYSE_at = drop_smallNYSE_at,  \n                            drop_price_at = drop_price_at,\n                            drop_stock_age_at = drop_stock_age_at,\n                            drop_earnings = drop_earnings, \n                            drop_bookequity = drop_bookequity,\n                            sv_lag = sv_lag)\n  \n  # Rebalancing\n  ## Monthly\n  if(formation_time == \"monthly\") {\n    data_return <- rebalance_mon(data = data_sorts, \n                                 sorting_variable = sorting_variable, \n                                 sorting_method = sorting_method, \n                                 n_portfolios_main = n_portfolios_main, \n                                 n_portfolios_secondary = n_portfolios_secondary, \n                                 exchanges = exchanges, \n                                 value_weighted = value_weighted)\n  }\n  \n  ## Monthly\n  if(formation_time == \"FF\") {\n    data_return <- rebalance_ann(data = data_sorts, \n                                 sorting_variable = sorting_variable, \n                                 sorting_method = sorting_method, \n                                 n_portfolios_main = n_portfolios_main, \n                                 n_portfolios_secondary = n_portfolios_secondary, \n                                 exchanges = exchanges, \n                                 value_weighted = value_weighted)\n  }\n  \n  # Compute return differential\n  data_return |> \n    group_by(month) |> \n    summarize(premium = ret[portfolio == max(portfolio)] - ret[portfolio == min(portfolio)],\n              .groups = \"drop\") |> \n    pull(premium) |> \n    mean() * -100\n}\n```\n:::\n\n\n\n## Applying the functions\n\nFinally, we have data, decisions, and functions. Indeed, we are now ready to implement the portfolio sort, right? Yes! Just let me briefly discuss how the implementation works.\n\nWe have a grid with 69,120 specifications. For each of these specifications, we want to estimate a return differential. This is most easily achieved with a `pmap()` call. However, we want to parallelize the operation to leverage the multiple cores of our device. Hence, we have to use the package `furrr` and a `future_pmap()` instead. As a side note, I also have to increase my maximum memory for each worker, which I do with the call of `options()`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(furrr)\n\noptions(future.globals.maxSize= 891289600)\n\nplan(multisession, workers = availableCores())\n```\n:::\n\n\nWith the parallel environment set and ready to go, we map the arguments our function needs into the final function `execute_sorts()` from above. Then, we go and have some tea. And some lunch, breakfast, second breakfast, and so on. In short, it takes a while - depending on your device even more than a day. Each result requires roughly 13 seconds, but you must remember that you are computing 69,120 results. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_premia <- setup_grid |>\n  mutate(premium_estimate = future_pmap(\n    .l = list(\n      sorting_variable, drop_smallNYSE_at, include_financials, include_utilities, drop_bookequity,\n      drop_earnings, drop_stock_age_at, drop_price_at, sv_lag, formation_time, n_portfolios_main,\n      sorting_method, n_portfolios_secondary, exchanges, value_weighted\n    ),\n    .f = ~ execute_sorts(\n      sorting_variable = ..1,\n      drop_smallNYSE_at = ..2,\n      include_financials = ..3,\n      include_utilities = ..4,\n      drop_bookequity = ..5,\n      drop_earnings = ..6,\n      drop_stock_age_at = ..7,\n      drop_price_at = ..8,\n      sv_lag = ..9,\n      formation_time = ..10,\n      n_portfolios_main = ..11,\n      sorting_method = ..12,\n      n_portfolios_secondary = ..13,\n      exchanges = ..14,\n      value_weighted = ..15\n    )\n  ))\n```\n:::\n\n\nAt this point, we can also consider storing the data in our SQLite database for future reference. I skip this step today.\n\n\n\n\n\n# The premium distribution\n\nGiven all the estimates for the premium, we can now take a look at their distribution with a call to `geom_density()` - of course, in the main Tidy Finance color scheme.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_premia |>\n  unnest(premium_estimate) |> \n  ggplot() +\n  geom_density(aes(x  = premium_estimate),\n               alpha = 0.25, \n               linewidth = 1.2,\n               color = \"#3B9AB2\",\n               fill = \"#3B9AB2\") + \n  labs(x = \"Premium (in %, p.m.)\",\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![Asset growth return differentials with non-standard errors.](index_files/figure-html/fig-nse-1-1.png){#fig-nse-1 fig-alt='Title: Asset growth return differentials with non-standard errors. The figure shows a density of estimated return differentials in percent per month. There is significant variation in the estimates.' width=2100}\n:::\n:::\n\n\n\n\nYou can immediately see one of the results in WWW: There is a lot of variation depending on your choices. However, despite the variation, the premium is always positive. I would argue that this is a pretty strong sign.\n\n# Conclusion\n\nIn essence, any paper proposing a new predictor reports one of the specifications we considered here. Then, they run a few robustness checks, varying some choices. However, space constraints severely limit the number of these checks. But how about our robustness test here? It is actually nearly 70,000 robustness tests if you think about it. However, it is just one graph (maybe two if showing the t-statistics as well), which condenses much information in a digestible way. Personally, I find this very convincing and attractive.\n\nOf course, we have yet to test the return differentials' time series against some factor model to show that existing factors do not explain the premia. I did not show you this step because the implementation is just an addition to our existing code. You can try it yourself.\n\nIf you want to learn more about non-standard errors in portfolio sorts, I refer you to WWW. We not only cover many variables there but also dive much deeper into understanding the variation itself. \n\n\n[^1]: Menkveld, A. J. et al. (2023). “Non-standard Errors”, Journal of Finance (forthcoming). http://dx.doi.org/10.2139/ssrn.3961574\n\n[^2]:  Walter, D., Weber, R., and Weiss, P. (2023). \"Non-Standard Errors in Portfolio Sorts\". [http://dx.doi.org/10.2139/ssrn.4164117](http://dx.doi.org/10.2139/ssrn.4164117)\n\n[^3]: Cooper, M. J., Gulen, H., and Schill, M. J. (2008). \"Asset growth and the cross‐section of stock returns\", The Journal of Finance, 63(4), 1609-1651.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}